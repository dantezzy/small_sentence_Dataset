this paper describes the process of inducting theory using case studies from specifying the research questions to reaching closure some features of the process such as problem definition and construct validation are similar to hypothesis testing research others such as within case analysis and replication logic are unique to the inductive case oriented process overall the process described here is highly iterative and tightly linked to data this research approach is especially appropriate in new topic areas the resultant theory is often novel testable and empirically valid finally framebreaking insights the tests of good theory e g parsimony logical coherence and convincing grounding in the evidence are the key criteria for evaluating this type of research abstract from author copyright of academy of management review is the property of academy of management and its content may not be copied or emailed to multiple sites or posted to a listserv without copyright
we argue that because of important epistemological differences between the fields of information technology and organization studies much can be gained from greater interaction between them in particular we argue that information technology research can benefit from incorporating institutional analysis from organization studies while organization studies can benefit even more by following the lead of information technology research in taking the material properties of technologies into account we further suggest that the transformations currently occurring in the nature of work and organizing cannot be understood without considering both the technological changes and the institutional contexts that are reshaping economic and organizational activity thus greater interaction between the fields of information technology and organization studies should be viewed as more than a matter of enrichment in the intellectual engagement of these two fields lies the potential for an important fusion of perspectives a fusion more carefully attuned to explaining the nature and consequences of the techno social phenomena that increasingly pervade lives
background identification of differentially expressed genes degs under different experimental conditions is an important task in many microarray studies however choosing which method to use for a particular application is problematic because its performance depends on the evaluation metric the dataset and so on in addition when using the affymetrix genechip r system researchers must select a preprocessing algorithm from a number of competing algorithms such as mas rma and dfw for obtaining expression level measurements to achieve optimal performance for detecting degs a suitable combination of gene selection method and preprocessing algorithm needs to be selected for a given probe level dataset results we introduce a new fold change fc based method the weighted average difference method wad for ranking degs it uses the average difference and relative average signal intensity so that highly expressed genes are highly ranked on the average for the different conditions the idea is based on our observation that known or potential marker genes or proteins tend to have high expression levels we compared wad with seven other methods average difference ad fc rank products rp moderated t statistic modt significance analysis of microarrays samt shrinkage t statistic shrinkt and intensity based moderated t statistic ibmt the evaluation was performed using a total of different binary two class probe level datasets two artificial spike in datasets and real experimental datasets the results indicate that wad outperforms the other methods when sensitivity and specificity are considered simultaneously the area under the receiver operating characteristic curve for wad was the highest on average for the datasets the gene ranking for wad was also the most consistent when subsets of top ranked genes produced from three different preprocessed data mas rma and dfw were compared overall wad performed the best for mas preprocessed data and the fc based methods ad wad fc or rp performed well for rma and dfw preprocessed data conclusion wad is a promising alternative to existing methods for ranking degs with two classes its high performance should increase researchers confidence in analyses
new generation sequencing technologies offer unique opportunities and challenges for re sequencing studies in this article we focus on re sequencing experiments using the solexa technology based on bacterial artificial chromosome bac clones and address an experimental design problem in these specific experiments approximate coordinates of the bacs on a reference genome are known and fine scale differences between the bac sequences and the reference are of interest the high throughput characteristics of the sequencing technology makes it possible to multiplex bac sequencing experiments by pooling bacs for a cost effective operation however the way bacs are pooled in such re sequencing experiments has an effect on the downstream analysis of the generated data mostly due to subsequences common to multiple bacs the experimental design strategy we develop in this article offers combinatorial solutions based on approximation algorithms for the well known max n cut problem and the related max n section problem on hypergraphs our algorithms when applied to a number of sample cases give more than a fold performance improvement over random partitioning contact cenk cs sfu bioinformatics
as a result of the redundancy of the genetic code adjacent pairs of amino acids can be encoded by as many as different pairs of synonymous codons a species specific codon pair bias provides that some synonymous codon pairs are used more or less frequently than statistically predicted we synthesized de novo large dna molecules using hundreds of over or underrepresented synonymous codon pairs to encode the poliovirus capsid protein underrepresented codon pairs caused decreased rates of protein translation and polioviruses containing such amino acid independent changes were attenuated in mice polioviruses thus customized were used to immunize mice and provided protective immunity after challenge this death by a thousand cuts strategy could be generally applicable to attenuating many kinds viruses
motivation the identification of transcription factor tf binding sites and the regulatory circuitry that they define is currently an area of intense research data from whole genome chromatin immunoprecipitation chip chip whole genome expression microarrays and sequencing of multiple closely related genomes have all proven useful by and large existing methods treat the interpretation of functional data as a classification problem between bound and unbound dna and the analysis of comparative data as a problem of local alignment to recover phylogenetic footprints of presumably functional elements both of these approaches suffer from the inability to model and detect low affinity binding sites which have recently been shown to be abundant and functional results we have developed a method that discovers functional regulatory targets of tfs by predicting the total affinity of each promoter for those factors and then comparing that affinity across orthologous promoters in closely related species at each promoter we consider the minimum affinity among orthologs to be the fraction of the affinity that is functional because we calculate the affinity of the entire promoter our method is independent of local alignment by comparing with functional annotation information and gene expression data in saccharomyces cerevisiae we have validated that this biophysically motivated use of evolutionary conservation gives rise to dramatic improvement in prediction of regulatory connectivity and factor factor interactions compared to the use of a single genome we propose novel biological functions for several yeast tfs including the factors and for which no function has been reported our affinity based approach towards comparative genomics may allow a more quantitative analysis of the principles governing the evolution of non coding dna availability the matrixreduce software package is available from http www bussemakerlab org software matrixreduce contact harmen bussemaker columbia edu supplementary information supplementary data are available at bioinformatics bioinformatics
motivation with the exponential growth of expression and protein protein interaction ppi data the frontier of research in systems biology shifts more and more to the integrated analysis of these large datasets of particular interest is the identification of functional modules in ppi networks sharing common cellular function beyond the scope of classical pathways by means of detecting differentially expressed regions in ppi networks this requires on the one hand an adequate scoring of the nodes in the network to be identified and on the other hand the availability of an effective algorithm to find the maximally scoring network regions various heuristic approaches have been proposed in the literature results here we present the first exact solution for this problem which is based on integer linear programming and its connection to the well known prize collecting steiner tree problem from operations research despite the np hardness of the underlying combinatorial problem our method typically computes provably optimal subnetworks in large ppi networks in a few minutes an essential ingredient of our approach is a scoring function defined on network nodes we propose a new additive score with two desirable properties i it is scalable by a statistically interpretable parameter and ii it allows a smooth integration of data from various sources we apply our method to a well established lymphoma microarray dataset in combination with associated survival data and the large interaction network of hprd to identify functional modules by computing optimal scoring subnetworks in particular we find a functional interaction module associated with proliferation over expressed in the aggressive abc subtype as well as modules derived from non malignant by stander cells availability our software is available freely for non commercial purposes at http www planet lisa net contact tobias mueller biozentrum uni wuerzburg bioinformatics
two complementary approaches both using next generation sequencing have successfully tackled the scale and the complexity of mammalian transcriptomes at once revealing unprecedented detail and allowing better quantification for over a decade dna microarrays have provided a powerful approach to achieve parallel interrogation of biological systems at a genomic scale but two new reports in this issue of nature demonstrate that massively parallel dna sequencing may be on its way to supplanting microarrays as the technology of choice for quantifying and transcriptomes
the gene expression pattern specified by an animal regulatory sequence is generally viewed as arising from the particular arrangement of transcription factor binding sites it contains however we demonstrate here that regulatory sequences whose binding sites have been almost completely rearranged can still produce identical outputs we sequenced the even skipped locus from six species of scavenger flies sepsidae that are highly diverged from the model species drosophila melanogaster but share its basic patterns of developmental gene expression although there is little sequence similarity between the sepsid eve enhancers and their well characterized d melanogaster counterparts the sepsid and drosophila enhancers drive nearly identical expression patterns in transgenic d melanogaster embryos we conclude that the molecular machinery that connects regulatory sequences to the transcription apparatus is more flexible than previously appreciated in exploring this diverse collection of sequences to identify the shared features that account for their similar functions we found a small number of short bp sequences nearly perfectly conserved among the species these highly conserved sequences are strongly enriched for pairs of overlapping or adjacent binding sites together these observations suggest that the local arrangement of binding sites relative to each other is more important than their overall arrangement into larger units of cis function
protein protein interaction ppi maps provide insight into cellular biology and have received considerable attention in the post genomic era while large scale experimental approaches have generated large collections of experimentally determined ppis technical limitations preclude certain ppis from detection recently we demonstrated that yeast ppis can be computationally predicted using re occurring short polypeptide sequences between known interacting protein pairs however the computational requirements and low specificity made this method unsuitable for large scale investigations here we report an improved approach which exhibits a specificity of approximately and executes times faster importantly we report the first all to all sequence based computational screen of ppis in yeast saccharomyces cerevisiae in which we identify high confidence interactions of approximately x possible pairs of these ppis have not been previously reported and may represent novel interactions in particular these results reveal a richer set of membrane protein interactions not readily amenable to experimental investigations from the novel ppis a novel putative protein complex comprised largely of membrane proteins was revealed in addition two novel gene functions were predicted and experimentally confirmed to affect the efficiency of non homologous end joining providing further support for the usefulness of the identified ppis in investigations
pmid short lived protein interactions determine signal transduction specificity among genetically amplified structurally identical two component signaling systems interacting protein pairs evolve recognition precision by varying residues at specific positions in the interaction surface consistent with constraints of charge size and chemical properties such positions can be detected by covariance analyses of two component protein databases here covariance is shown to identify a cluster of co evolving dynamic residues in two component proteins nmr dynamics and structural studies of both wild type and mutant proteins in this cluster suggest that motions serve to precisely arrange the site of phosphoryl transfer within complex
background a survey of microarray databases reveals that most of the repository contents and data models are heterogeneous i e data obtained from different chip manufacturers and that the repositories provide only basic biological keywords linking to pubmed as a result it is difficult to find datasets using research context or analysis parameters information beyond a few keywords for example to reduce the curse of dimension problem in microarray analysis the number of samples is often increased by merging array data from different datasets knowing chip data parameters such as pre processing steps e g normalization artefact removal etc and knowing any previous biological validation of the dataset is essential due to the heterogeneity of the data however most of the microarray repositories do not have meta data information in the first place and do not have a a mechanism to add or insert this information thus there is a critical need to create intelligent microarray repositories that enable update of meta data with the raw array data and provide standardized archiving protocols to minimize bias from the raw data sources results to address the problems discussed we have developed a community maintained system called arraywiki that unites disparate meta data of microarray meta experiments from multiple primary sources with four key features first arraywiki provides a user friendly knowledge management interface in addition to a programmable interface using standards developed by wikipedia second arraywiki includes automated quality control processes cacorrect and novel visualization methods biopng gel plots which provide extra information about data quality unavailable in other microarray repositories third it provides a user curation capability through the familiar wiki interface fourth arraywiki provides users with simple text based searches across all experiment meta data and exposes data to search engine crawlers semantic agents such as google to further enhance data discovery conclusions microarray data and meta information in arraywiki are distributed and visualized using a novel and compact data storage format biopng also they are open to the research community for curation modification and contribution by making a small investment of time to learn the syntax and structure common to all sites running mediawiki software domain scientists and practioners can all contribute to make better use of microarray technologies in research and medical practices arraywiki is available at http www bio miblab arraywiki
micrornas mirnas are noncoding rnas that base pair imperfectly to homologous regions in target mrnas and negatively influence the synthesis of the corresponding proteins repression is mediated by a number of mechanisms one of which is the direct inhibition of protein synthesis surprisingly previous studies have suggested that two mutually exclusive mechanisms exist one acting at the initiation phase of protein synthesis and the other at a postinitiation event here we resolve this apparent dichotomy by demonstrating that the promoter used to transcribe the mrna influences the type of mirna mediated translational repression transcripts derived from the promoter that contain let target sites in their utrs are repressed at the initiation stage of translation whereas essentially identical mrnas derived from the tk promoter are repressed at a postinitiation step we also show that there is a mir target site within the utr of c myc mrna and that promoter dependency is also true for this endogenous utr overall these data establish a link between the nuclear history of an mrna and the mechanism of mirna mediated translational regulation in the pnas
internet applications increasingly rely on scalable data structures that must support high throughput and store huge amounts of data these data structures can be hard to implement efficiently recent proposals have overcome this problem by giving up on generality and implementing specialized interfaces and functionality e g dynamo we present the design of a more general and flexible solution a fault tolerant and scalable distributed b tree in addition to the usual b tree operations our b tree provides some important practical features transactions for atomically executing several operations in one or more b trees online migration of b tree nodes between servers for load balancing and dynamic addition and removal of servers for supporting incremental growth of the system our design is conceptually simple rather than using complex concurrency and locking protocols we use distributed transactions to make changes to b tree nodes we show how to extend the b tree and keep additional information so that these transactions execute quickly and efficiently our design relies on an underlying distributed data sharing service sinfonia which provides fault tolerance and a light weight distributed atomic primitive we use this primitive to commit our transactions we implemented our b tree and show that it performs comparably to an existing open source b tree and that it scales to hundreds of machines we believe that our approach is general and can be used to implement other distributed data easily
when species is a breathtaking meditation on the intersection between humankind and dog philosophy and science and macro and micro cultures cameron woo publisher of magazine in about million u s households had pets giving homes to around million dogs million cats and million birds and spending over billion dollars on companion animals as never before in history our pets are truly members of the family but the notion of companion speciesknotted from human beings animals and other organisms landscapes and technologiesincludes much more than companion animals in when species donna j haraway digs into this larger phenomenon to contemplate the interactions of humans with many kinds of critters especially with those called domestic at the heart of the book are her experiences in agility training with her dogs cayenne and roland but haraways vision here also encompasses wolves chickens cats baboons sheep microorganisms and whales wearing video cameras from designer pets to lab animals to trained therapy dogs she deftly explores philosophical cultural and biological aspects of animal human encounters in this deeply personal yet intellectually groundbreaking work haraway develops the idea of companion species those who meet and break bread together but not without some indigestion a great deal is at stake in such meetings she writes and outcomes are not guaranteed there is no assured happy or unhappy endingsocially ecologically or scientifically there is only the chance for getting on together with some grace ultimately she finds that respect curiosity and knowledge spring from animal human associations and work powerfully against ideas about human exceptionalism one of the founders of the posthumanities donna j haraway is professor in the history of consciousness department at the university of california santa cruz author of many books and widely read essays including the companion species manifesto dogs people and significant and the now classic essay the cyborg manifesto received the j d bernal prize in a lifetime achievement award from the society for social studies science
recent scientific discoveries that resulted from the application of next generation dna sequencing technologies highlight the striking impact of these massively parallel platforms on genetics these new methods have expanded previously focused readouts from a variety of dna preparation protocols to a genome wide scale and have fine tuned their resolution to single base precision the sequencing of rna also has transitioned and now includes full length cdna analyses serial analysis of gene expression sage based methods and noncoding rna discovery next generation sequencing has also enabled novel applications such as the sequencing of ancient dna samples and has substantially widened the scope of metagenomic analysis of environmentally derived samples taken together an astounding potential exists for these technologies to bring enormous change in genetic and biological research and to enhance our fundamental knowledge
as the popularity of the web increases particularly the use of social networking sites and style sharing platforms users are becoming increasingly connected sharing more and more information resources and opinions this vast array of information presents unique opportunities to harvest knowledge about user activities and interests through the exploitation of large scale complex systems communal tagging sites and their respective folksonomies are one example of such a complex system providing huge amounts of information about users spanning multiple domains of interest however the current web infrastructure provides no mechanism for users to consolidate and exploit this information since it is spread over many desperate and unconnected resources in this paper we compare user tag clouds from multiple folksonomies to a show how they tend to overlap regardless of the focus of the folksonomy b demonstrate how this comparison helps finding and aligning the user s separate identities and c show that cross linking distributed user tag clouds enriches users profiles during this process we find that significant user interests are often reflected in multiple profiles even though they may operate over different domains however due to the free form nature of tagging some correlations are lost a problem we address through the implementation and evaluation of a user tag architecture
recent genomic sequencing of additional drosophila genomes provides a rich resource for comparative genomics analyses aimed at understanding the similarities and differences between species and between drosophila and mammals using a phylogenetic approach we identified genomic elements that have been highly conserved over most of the drosophila tree but that have experienced a recent burst of evolution along the drosophila melanogaster lineage compared to similarly defined elements in humans these regions of rapid lineage specific evolution in drosophila differ dramatically in location mechanism of evolution and functional properties of associated genes notably the majority reside in protein coding regions and primarily result from rapid adaptive synonymous site evolution in fact adaptive evolution appears to be driving substitutions to unpreferred codons our analysis also highlights interesting noncoding genomic regions such as regulatory regions in the gene gooseberry neuro and a putative mirna
abstract although research emphasizes the importance of integrating technology into the curriculum the use of technology can only be effective if teachers themselves possess the expertise to use technology in a meaningful way in the classroom the aim of this study was to assist egyptian teachers in developing teaching and learning through the application of a particular digital technology students were encouraged to work through the process of producing their own digital stories using ms photo story while being introduced to desktop production and editing tools they also presented published and shared their own stories with other students in the class quantitative and qualitative instruments including digital story evaluation rubric integration of technology observation instruments and interviews for evaluating the effectiveness of digital storytelling into learning were implemented to examine the extent to which students were engaged in authentic learning tasks using digital storytelling the findings from the analysis of students produced stories revealed that overall students did well in their projects and their stories met many of the pedagogical and technical attributes of digital stories the findings from classroom observations and interviews revealed that despite problems observed and reported by teachers they believed that the digital storytelling projects could increase students understanding of curricular content and they were willing to transform their pedagogy and curriculum to include storytelling
abstract background research in the field of systems biology requires software for a variety of purposes software must be used to store retrieve analyze and sometimes even to collect the data obtained from system level often high throughput experiments software must also be used to implement mathematical models and algorithms required for simulation and theoretical predictions on the system level results we introduce a free easy to use open source integrated software platform called the systems biology research tool sbrt to facilitate the computational aspects of systems biology the sbrt currently performs methods for analyzing stoichiometric networks and methods from fields such as graph theory geometry algebra and combinatorics new computational techniques can be added to the sbrt via process plug ins providing a high degree of evolvability and a unifying framework for software development in systems biology conclusions the systems biology research tool represents a technological advance for systems biology this software can be used to make sophisticated computational techniques accessible to everyone including those with no programming ability to facilitate cooperation among researchers and to expedite progress in the field of biology
it is difficult to deny that comparison between recommender systems requires a common way for evaluating them nevertheless at present they have been evaluated in many often incompatible ways we affirm this problem is mainly due to the lack of a common framework for recommender systems a framework general enough so that we may include the whole range of recommender systems to date but specific enough so that we can obtain solid results in this paper we propose such a framework attempting to extract the essential features of recommender systems in this framework the most essential feature is the objective of the recommender system what is more in this paper recommender systems are viewed as applications with the following essential objective recommender systems must i choose which of the items should be shown to the user ii decide when and how the recommendations must be shown next we will show that a new metric emerges naturally from this framework finally we will conclude by comparing the properties of this new metric with the traditional ones among other things we will show that we may evaluate the whole range of recommender systems with this metric
background benefits from high throughput sequencing using pyrosequencing technology may be most apparent for species with high societal or economic value but few genomic resources rapid means of gene sequence and snp discovery using this novel sequencing technology provide a set of baseline tools for genome level research however it is questionable how effective the sequencing of large numbers of short reads for species with essentially no prior gene sequence information will support contig assemblies and sequence annotation results with the purpose of generating the first broad survey of gene sequences in eucalyptus grandis the most widely planted hardwood tree species we used technology to sequence and assemble mbp of expressed sequences est est sequences were generated from a normalized cdna pool comprised of multiple tissues and genotypes promoting discovery of homologues to almost half of arabidopsis genes and a comprehensive survey of allelic variation in the transcriptome by aligning the sequencing reads from multiple genotypes we detected snps of which were validated in a sample genome wide nucleotide diversity was estimated for contigs using a modified theta theta parameter adapted for measuring genetic diversity from polymorphisms detected by randomly sequencing a multi genotype cdna pool diversity estimates in non synonymous nucleotides were on average smaller than in synonymous suggesting purifying selection non synonymous to synonymous substitutions ka ks among contigs averaged and was skewed to the right further supporting that most genes are under purifying selection comparison of these estimates among contigs identified major functional classes of genes under purifying and diversifying selection in agreement with previous researches conclusion in providing an abundance of foundational transcript sequences where limited prior genomic information existed this work created part of the foundation for the annotation of the e grandis genome that is being sequenced by the us department of energy in addition we demonstrated that snps sampled in large scale with pyrosequencing can be used to detect evolutionary signatures among genes providing one of the first genome wide assessments of nucleotide diversity and ka ks for a non model species
we employ scanning probe microscopy to reveal atomic structures and nanoscale morphology of graphene based electronic devices i e a graphene sheet supported by an insulating silicon dioxide substrate for the first time atomic resolution scanning tunneling microscopy images reveal the presence of a strong spatially dependent perturbation which breaks the hexagonal lattice symmetry of the graphitic lattice structural corrugations of the graphene sheet partially conform to the underlying silicon oxide substrate these effects are obscured or modified on graphene devices processed with normal lithographic methods as they are covered with a layer of photoresist residue we enable our experiments by a novel cleaning process to produce atomically clean sheets
we present a general information theoretic approach for identifying functional subgraphs in complex networks we show that the uncertainty in a variable can be written as a sum of information quantities where each term is generated by successively conditioning mutual informations on new measured variables in a way analogous to a discrete differential calculus the analogy to a taylor series suggests efficient optimization algorithms for determining the state of a target variable in terms of functional groups of other nodes we apply this methodology to electrophysiological recordings of cortical neuronal networks grown invitro each cells firing is generally explained by the activity of a few neurons we identify these neuronal subgraphs in terms of their redundant or synergetic character and reconstruct neuronal circuits that account for the state of cells
coarse grained cg models provide a computationally efficient method for rapidly investigating the long time and length scale processes that play a critical role in many important biological and soft matter processes recently izvekov and voth introduced a new multiscale coarse graining ms cg method j phys chem b j chem phys for determining the effective interactions between cg sites using information from simulations of atomically detailed models the present work develops a formal statistical mechanical framework for the ms cg method and demonstrates that the variational principle underlying the method may in principle be employed to determine the many body potential of mean force pmf that governs the equilibrium distribution of positions of the cg sites for the ms cg models a cg model that employs such a pmf as a potential energy function will generate an equilibrium probability distribution of cg sites that is consistent with the atomically detailed model from which the pmf is derived consequently the ms cg method provides a formal multiscale bridge rigorously connecting the equilibrium ensembles generated with atomistic and cg models the variational principle also suggests a class of practical algorithms for calculating approximations to this many body pmf that are optimal these algorithms use computer simulation data from the atomically detailed model finally important generalizations of the ms cg method are introduced for treating systems with rigid intramolecular constraints and for developing cg models whose equilibrium momentum distribution is consistent with that of an atomically detailed model american institute physics
the multiscale coarse graining ms cg method s izvekov and g a voth j phys chem b j chem phys employs a variational principle to determine an interaction potential for a cg model from simulations of an atomically detailed model of the same system the companion paper proved that if no restrictions regarding the form of the cg interaction potential are introduced and if the equilibrium distribution of the atomistic model has been adequately sampled then the ms cg variational principle determines the exact many body potential of mean force pmf governing the equilibrium distribution of cg sites generated by the atomistic model in practice though cg force fields are not completely flexible but only include particular types of interactions between cg sites e g nonbonded forces between pairs of sites if the cg force field depends linearly on the force field parameters then the vector valued functions that relate the cg forces to these parameters determine a set of basis vectors that span a vector subspace of cg force fields the companion paper introduced a distance metric for the vector space of cg force fields and proved that the ms cg variational principle determines the cg force force field that is within that vector subspace and that is closest to the force field determined by the many body pmf the present paper applies the ms cg variational principle for parametrizing molecular cg force fields and derives a linear least squares problem for the parameter set determining the optimal approximation to this many body pmf linear systems of equations for these cg force field parameters are derived and analyzed in terms of equilibrium structural correlation functions numerical calculations for a one site cg model of methanol and a molecular cg model of the emim no sub sub sup sup ionic liquid are provided to illustrate the method american institute physics
this publication explores the inner life of a dedicated teacher it examines how the teacher s inner life shapes teaching and learning both positively and negatively and guides teachers in a process of rigorous self appraisal with the goal of recovering personal vocation and passion for teaching while enhancing the ability to make an impact in the classroom after the introduction teaching from within the book is in seven chapters the heart of a teacher identity and integrity in teaching a culture of fear education and the disconnected life the hidden wholeness paradox in teaching and learning knowing in community joined by the grace of great things teaching in community a subject centered education learning in community the conversation of colleagues and divided no more teaching from a heart of hope references are included in chapter nd
this is the first of a two part article that will discuss the history of the field of instructional design and technology in the united states a definition of the field is provided and the major features of the definition are identified a rational for using instructional design and technology as the label for the field is also presented events in the history of instructional media from the early to the present day are described the birth of school museums the visual and audiovisual instruction movements the use of media during world war ii and the interest in instructional television computers and the internet are among the topics discussed the article concludes with a summarization of the effects media have had on instructional practices and a prediction regarding the effect computers the internet and other digital media will have on such practices over the decade
motivation computational assignment of protein function may be the single most vital application of bioinformatics in the post genome era these assignments are made based on various protein features where one is the presence of identifiable domains the relationship between protein domain content and function is important to investigate to understand how domain combinations encode complex functions results two different models are presented for how protein domain combinations yield specific functions one rule based and one probabilistic we demonstrate how these are useful for gene ontology annotation transfer the first is an intuitive generalization of the mapping and detects cases of strict functional implications of sets of domains the second uses a probabilistic model to represent the relationship between domain content and annotation terms and was found to be better suited for incomplete training sets we implemented these models as predictors of gene ontology functional annotation terms both predictors were more accurate than conventional best blast hit annotation transfer and more sensitive than a single domain model on a large scale dataset we present a number of cases where combinations of pfam a protein domains predict functional terms that do not follow from the individual domains availability scripts and documentation are available for download at http sonnhammer sbc su se tar contact kristoffer forslund sbc su se supplementary information bioinformatics
amazon com has introduced the simple storage service a commodity priced storage utility aims to provide storage as a low cost highly available service with a simple pay as you go charging model this article makes three contributions first we evaluate s ability to provide storage support to large scale science projects from a cost availability and performance perspective second we identify a set of additional functionalities that storage services targeting data intensive science applications should support third we propose unbundling the success metrics for storage utility performance as a solution to reduce costs
the increasing volume and diversity of information in biomedical research is demanding new approaches for data integration in this domain semantic web technologies and applications can leverage the potential of biomedical information integration and discovery facing the problem of semantic heterogeneity of biomedical information sources in such an environment agent technology can assist users in discovering and invoking the services available on the internet in this paper we present semmas an ontology based domain independent framework for seamlessly integrating intelligent agents and semantic web services our approach is backed with a proof of concept implementation where the breakthrough and efficiency of integrating disparate biomedical information sources have tested
motivation biomedical literature is the principal repository of biomedical knowledge with pubmed being the most complete database collecting organizing and analyzing such textual knowledge there are numerous efforts that attempt to exploit this information by using text mining and machine learning techniques we developed a novel approach called pured mcl pubmed related documents mcl which is based on the graph clustering algorithm mcl and relevant resources from pubmed methods pured mcl avoids using natural language processing nlp techniques directly instead it takes advantage of existing resources available from pubmed pured mcl then clusters documents efficiently using the mcl graph clustering algorithm which is based on graph flow simulation this process allows users to analyse the results by highlighting important clues and finally to visualize the clusters and all relevant information using an interactive graph layout algorithm for instance biolayout express results the methodology was applied to two different datasets previously used for the validation of the document clustering tool textquest the first dataset involves the organisms escherichia coli and yeast whereas the second is related to drosophila development pured mcl successfully reproduces the annotated results obtained from textquest while at the same time provides additional insights into the clusters and the corresponding documents availability source code in perl and r are available from http tartara csd auth gr theodos contact theodos csd auth bioinformatics
in recent literature several models were proposed for reproducing and understanding the tagging behavior of users they all assume that the tagging behavior is influenced by the previous tag assignments of other users but they are only partially successful in reproducing characteristic properties found in tag streams we argue that this inadequacy of existing models results from their inability to include user s background knowledge into their model of tagging behavior this paper presents a generative tagging model that integrates both components the background knowledge and the influence of previous tag assignments our model successfully reproduces characteristic properties of tag streams it even explains effects of the user interface on the stream
structurally segregated and functionally specialized regions of the human cerebral cortex are interconnected by a dense network of cortico cortical axonal pathways by using diffusion spectrum imaging we noninvasively mapped these pathways within and across cortical hemispheres in individual human participants an analysis of the resulting large scale structural brain networks reveals a structural core within posterior medial and parietal cerebral cortex as well as several distinct temporal and frontal modules brain regions within the structural core share high degree strength and betweenness centrality and they constitute connector hubs that link all major structural modules the structural core contains brain regions that form the posterior components of the human default network looking both within and outside of core regions we observed a substantial correspondence between structural connectivity and resting state functional connectivity measured in the same participants the spatial and topological centrality of the core within cortex suggests an important role in integration
molecular structures and sequences are generally more revealing of evolutionary relationships than are classical phenotypes particularly so among microorganisms consequently the basis for the definition of taxa has progressively shifted from the organismal to the cellular to the molecular level molecular comparisons show that life on this planet divides into three primary groupings commonly known as the eubacteria the archaebacteria and the eukaryotes the three are very dissimilar the differences that separate them being of a more profound nature than the differences that separate typical kingdoms such as animals and plants unfortunately neither of the conventionally accepted views of the natural relationships among living systems i e the five kingdom taxonomy or the eukaryote prokaryote dichotomy reflects this primary tripartite division of the living world to remedy this situation we propose that a formal system of organisms be established in which above the level of kingdom there exists a new taxon called a domain life on this planet would then be seen as comprising three domains the bacteria the archaea and the eucarya each containing two or more kingdoms the eucarya for example contain animalia plantae fungi and a number of others yet to be defined although taxonomic structure within the bacteria and eucarya is not treated herein archaea is formally subdivided into the two kingdoms euryarchaeota encompassing the methanogens and their phenotypically diverse relatives and crenarchaeota comprising the relatively tight clustering of extremely thermophilic archaebacteria whose general phenotype appears to resemble most the ancestral phenotype of archaea
motivation understanding the role of genetics in diseases is one of the most important aims of the biological sciences the completion of the human genome project has led to a rapid increase in the number of publications in this area however the coverage of curated databases that provide information manually extracted from the literature is limited another challenge is that determining disease related genes requires laborious experiments therefore predicting good candidate genes before experimental analysis will save time and effort we introduce an automatic approach based on text mining and network analysis to predict gene disease associations we collected an initial set of known disease related genes and built an interaction network by automatic literature mining based on dependency parsing and support vector machines our hypothesis is that the central genes in this disease specific network are likely to be related to the disease we used the degree eigenvector betweenness and closeness centrality metrics to rank the genes in the network results the proposed approach can be used to extract known and to infer unknown gene disease associations we evaluated the approach for prostate cancer eigenvector and degree centrality achieved high accuracy a total of of the top genes ranked by these methods are confirmed to be related to prostate cancer on the other hand betweenness and closeness centrality predicted more genes whose relation to the disease is currently unknown and are candidates for experimental study availability a web based system for browsing the disease specific gene interaction networks is available at http gin org
extinction risk in natural populations depends on stochastic factors that affect individuals and is estimated by incorporating such factors into stochastic stochasticity can be divided into four categories which include the probabilistic nature of birth and death at the level of individuals demographic variation in population level birth and death rates among times or locations environmental the sex of and variation in vital rates among individuals within a population demographic mechanistic stochastic models that include all of these factors have not previously been developed to examine their combined effects on extinction risk here we derive a family of stochastic ricker models using different combinations of all these stochastic factors and show that extinction risk depends strongly on the combination of factors that contribute to stochasticity furthermore we show that only with the full stochastic model can the relative importance of environmental and demographic variability and therefore extinction risk be correctly determined using the full model we find that demographic sources of stochasticity are the prominent cause of variability in a laboratory population of tribolium castaneum red flour beetle whereas using only the standard simpler models would lead to the erroneous conclusion that environmental variability dominates our results demonstrate that current estimates of extinction risk for natural populations could be greatly underestimated because variability has been mistakenly attributed to the environment rather than the demographic factors described here that entail much higher extinction risk for the same level
the realization of conservation goals requires strategies for managing whole landscapes including areas allocated to both production and protection reserves alone are not adequate for nature conservation but they are the cornerstone on which regional strategies are built reserves have two main roles they should sample or represent the biodiversity of each region and they should separate this biodiversity from processes that threaten its persistence existing reserve systems throughout the world contain a biased sample of biodiversity usually that of remote places and other areas that are unsuitable for commercial activities a more systematic approach to locating and designing reserves has been evolving and this approach will need to be implemented if a large proportion of today s biodiversity is to exist in a future of increasing numbers of people and their demands on resources
this book presents the powerful basics of the original teachings of within these pages youll learn how all things wanted and unwanted are brought to you by this most powerful law of the universe law of attraction that which is like unto itself is drawn youve most likely heard the saying like attracts like birds of a feather flock together or it is done unto you as you believe a belief is only a thought you keep thinking and although the of attraction been alluded to by some of the greatest teachers in history it has never before been explained in as clear and practical terms as in this latest book by york times selling authors esther and jerry hicks learn here about the omnipresent laws that govern this universe and how to make them work to your advantage the understanding that youll achieve by reading this book will take all the guesswork out of daily living youll finally understand everything thats happening in your own life as well as in the lives of those youre interacting with this book will help you to joyously be do or have anything that desire
among the many authentic flavors of japan tsukemono or pickled vegetables has been a must for everyday meals and with tea for most of the japanese nothing can replace enjoying plain hot rice with tsukemono and dinner is not complete without it as the final course today most dishes are available at japanese grocery stores or specialty supermarkets but they often lack the seasonal quality and freshness of true tsukemono the term tsukemono covers a wide range of dishes from a marinated salad to preserved foods traditional tsukemono such as takuan or umeboshi might seem difficult to prepare but quick easy tsukemono makes these and many more easy with its simple step by step full color photo instructions there are myriads of methods to make them some as simple as just rubbing fruits and vegetables with salt just before serving while other require several days to fully marinate packed with over mouthwatering recipes for easily preserving fruits and vegetables quick easy tsukemono is the perfect book for beginning cooks and seasoned alike
quickstart quick and easy way to learn now revised and updated this book uses simple step by step instructions loads of screen shots and an array of time saving tips and tricks serving both as the quickest route to illustrator mastery for new users and a handy reference for more experienced designers this edition of the covers illustrator s newest features including new tools in live color enhancements to essential drawing tools and controls improvements to the powerful control panel the new eraser tool and crop area tool and much more a full color section of inspiring illustrator artwork and highly readable text create a winning combination for illustrator users of every level easy visual approach uses pictures to guide you through illustrator and show you what to do concise steps and explanations let you get up and running in no time page for page the best content and around
an astonishing new science called neuroplasticity is overthrowing the centuries old notion that the human brain is immutable in this revolutionary look at the brain psychiatrist and psychoanalyst norman doidge m d provides an introduction to both the brilliant scientists championing neuroplasticity and the people whose lives theyve transformed from stroke patients learning to speak again to the remarkable case of a woman born with half a brain that rewired itself to work as a whole brain that changes will permanently alter the way we look at our brains human nature and potential
oaths sworn loyalties tested forces collide following the colossal battle against the empires warriors on the burning plains eragon and his dragon saphira have narrowly escaped with their lives still there is more at hand for the rider and his dragon as eragon finds himself bound by a tangle of promises he may not be able to keep first is eragons oath to his cousin roran to help rescue rorans beloved katrina from king galbatorixs clutches but eragon owes his loyalty to others too the varden are in desperate need of his talents and strengthas are the elves and dwarves when unrest claims the rebels and danger strikes from every corner eragon must make choices choices that take him across the empire and beyond choices that may lead to unimagined sacrifice eragon is the greatest hope to rid the land of tyranny can this once simple farm boy unite the rebel forces and defeat the king the edition
nowadays it is difficult to conceive of the impact that gustave courbet s paintings made on french art of the mid nineteenth century at once casting himself as revolutionary bohemian and peasant courbet overturned a deeply entrenched tradition of academic painting in france and eschewing the romanticism of delacroix and the neoclassicism of ingres coined instead an idiom he named realism realism was not pretty classically proportioned or literary rather it confronted the conditions of rural working life then an unimaginable subject for art the first masterpiece of this new style was burial at ornans a colossal anti epic that depicted an ordinary funeral in courbet s home town the contrast between the work s scale and its subject matter was pronounced and its murky earth tones struck critics as willfully ugly a defining reaction that would recur throughout modernism particularly in the reception of early works by manet and picasso courbet s palette emphasized mass and body politically that is in a manner that affirmed the world itself rather than the transcendence of it his equally famous the origin of the world of which presented the female genitalia close up made this stance explicit the conceptual beginnings of the painting of modern life are as much in courbet s realism as in charles baudelaire s famous essay of the same name in this new assessment published on the occasion of the major traveling exhibition renowned experts shed light on the development of courbet s realistic critical style and trace his influence on his contemporaries and subsequent generations as well as his relationship to early photography at pages this monumental volume provides a long overdue reckoning of this great artist work
the world wide web has revolutionized how researchers from various disciplines collaborate over long distances this is nowhere more important than in the life sciences where interdisciplinary approaches are becoming increasingly powerful as a driver of both integration and discovery data access data quality identity and provenance are all critical ingredients to facilitate and accelerate these collaborative enterprises and it is here where semantic web technologies promise to have a profound impact this paper reviews the need for and explores advantages of as well as challenges with these novel internet information tools as illustrated with examples from the community
there has been a significant amount of excitement and recent work on column oriented database systems column stores these database systems have been shown to perform more than an order of magnitude better than traditional row oriented database systems row stores on analytical workloads such as those found in data warehouses decision support and business intelligence applications the elevator pitch behind this performance difference is straightforward column stores are more i o efficient for read only queries since they only have to read from disk or from memory those attributes accessed by query
the mutation process ultimately defines the genetic features of all populations and hence has a bearing on a wide range of issues involving evolutionary genetics inheritance and genetic disorders including the predisposition to cancer nevertheless formidable technical barriers have constrained our understanding of the rate at which mutations arise and the molecular spectrum of their effects here we report on the use of complete genome sequencing in the characterization of spontaneously arising mutations in the yeast saccharomyces cerevisiae our results confirm some findings previously obtained by indirect methods but also yield numerous unexpected findings in particular a very high rate of point mutation and skewed distribution of base substitution types in the mitochondrion a very high rate of segmental duplication and deletion in the nuclear genome and substantial deviations in the mutational profile among various organisms
horizontal gene transfer hgt also known as lateral gene transfer has had an important role in eukaryotic genome evolution but its importance is often overshadowed by the greater prevalence and our more advanced understanding of gene transfer in prokaryotes recurrent endosymbioses and the generally poor sampling of most nuclear genes from diverse lineages have also complicated the search for transferred genes nevertheless the number of well supported cases of transfer from both prokaryotes and eukaryotes many with significant functional implications is now expanding rapidly major recent trends include the important role of hgt in adaptation to certain specialized niches and the highly variable impact of hgt in lineages
organisms are constantly exposed to a wide range of environmental changes including both short term changes during their lifetime and longer term changes across generations stress related gene expression programmes characterized by distinct transcriptional mechanisms and high levels of noise in their expression patterns need to be balanced with growth related gene expression programmes a range of recent studies give fascinating insight into cellular strategies for keeping gene expression in tune with physiological needs dictated by the environment promoting adaptation to both short and long term environmental changes not only do organisms show great resilience to external challenges but emerging data suggest that they also exploit these challenges to fuel phenotypic variation and innovation
basic local alignment search tool blast is a sequence similarity search program the public interface of blast http www ncbi nlm nih gov blast at the ncbi website has recently been reengineered to improve usability and performance key new features include simplified search forms improved navigation a list of recent blast results saved search strategies and a documentation directory here we describe the blast web application s new features explain design decisions and outline plans for improvement
bibliometrics has changed out of all recognition since becoming established as a field being taught widely in library and information science schools and being at the core of a number of science evaluation research groups around the world this was all made possible by the work of eugene garfield and his science citation index this article reviews the distance that bibliometrics has travelled since by comparing early bibliometrics with current practice and by giving an overview of a range of recent developments such as patent analysis national research evaluation exercises visualization techniques new applications online citation indexes and the creation of digital libraries webometrics a modern fast growing offshoot of bibliometrics is reviewed in detail finally future prospects are discussed with regard to both bibliometrics webometrics
as microbial ecology investigations have progressed from descriptive characterizations of a community to hypothesis driven ecological research a number of different statistical techniques have been developed to describe and compare the structure of microbial communities thus far these methods have only been evaluated using rrna gene sequence data obtained from incomplete characterizations of microbial communities in this investigation simulations were designed to test the statistical power of different methods to differentiate between communities with known memberships and structures these simulations revealed three important results that affect how the results of the tests are interpreted first libshuff treeclimber unifrac analysis of molecular variance amova and homogeneity of molecular variance homova compare the structure of communities and not just their memberships second libshuff is unable to detect cases when one community structure is a subset of another third amova determines whether the genetic diversity within two or more communities is greater than their pooled genetic diversity and homova determines whether the amount of genetic diversity in each community is significantly different libshuff treeclimber and unifrac lump these and other factors together when performing their analysis making it difficult to discern the nature of the differences that are detected between communities these findings demonstrate that when correctly employed the current statistical toolbox has the ability to address specific ecological questions concerning the differences between communities
abstract background sequencing and annotation of several mammalian genomes have revealed that segmental duplications are a common architectural feature of primate genomes in fact about of the human genome is composed of large blocks of interspersed segmental duplications these segmental duplications have been implicated in genomic copy number variation gene novelty and various genomic disorders however the molecular processes involved in the evolution and regulation of duplicated sequences remain largely unexplored results in this study the profile of about histone modifications within human segmental duplications was characterized using high resolution genome wide data derived from a chip seq study the analysis demonstrates that derivative loci of segmental duplications often differ significantly from the original with respect to many histone methylations further investigation showed that genes are present three times more frequently in the original than in the derivative whereas pseudogenes exhibit the opposite trend these asymmetries tend to increase with the age of segmental duplications the uneven distribution of genes and pseudogenes does not however fully account for the asymmetry in the profile of histone modifications conclusion the first systematic analysis of histone modifications between segmental duplications demonstrates that two seemingly identical genomic copies are distinct in their epigenomic properties results here suggest that local chromatin environments may be implicated in the discrimination of derived copies of segmental duplications from their originals leading to a biased pseudogenization of the new duplicates the data also indicate that further exploration of the interactions between histone modification and sequence degeneration is necessary in order to understand the divergence of sequences
molecular sequence data have been sampled from of all species known to science although it is not yet feasible to assemble these data into a single phylogenetic tree of life it is possible to quantify how much phylogenetic signal is present analysis of phylogenies built from million sequences in genbank suggests that signal is strong in vertebrates and specific groups of nonvertebrate model organisms across eukaryotes however although phylogenetic evidence is very broadly distributed for the average species in the database it is equivalent to less than one well supported gene tree this analysis shows that a stronger sampling effort aimed at genomic depth in addition to taxonomic breadth will be required to build high resolution phylogenetic trees at this science
a simple negative feedback loop of interacting genes or proteins has the potential to generate sustained oscillations however many biological oscillators also have a positive feedback loop raising the question of what advantages the extra loop imparts through computational studies we show that it is generally difficult to adjust a negative feedback oscillator s frequency without compromising its amplitude whereas with positive plus negative feedback one can achieve a widely tunable frequency and near constant amplitude this tunability makes the latter design suitable for biological rhythms like heartbeats and cell cycles that need to provide a constant output over a range of frequencies positive plus negative oscillators also appear to be more robust and easier to evolve rationalizing why they are found in contexts where an adjustable frequency is science
the functional complexity of the human transcriptome is not yet fully elucidated we report a high throughput sequence of the human transcriptome from a human embryonic kidney and a b cell line we used shotgun sequencing of transcripts to generate randomly distributed reads of these mapped to unique genomic locations of which corresponded to known exons we found that of the polyadenylated transcriptome mapped to known genes and to nonannotated genomic regions on the basis of known transcripts rna seq can detect more genes than can microarrays a global survey of messenger rna splicing events identified splice junctions of which were previously unidentified and showed that exon skipping is the most prevalent form of splicing
abstract biodiversity a central component of earth s life support systems is directly relevant to human societies we examine the dimensions and nature of the earth s terrestrial biodiversity and review the scientific facts concerning the rate of loss of biodiversity and the drivers of this loss the estimate for the total number of species of eukaryotic organisms possible lies in the million range with a best guess of million species diversity is unevenly distributed the highest concentrations are in tropical ecosystems endemisms are concentrated in a few hotspots which are in turn seriously threatened by habitat destruction the most prominent driver of biodiversity loss for the past years recorded extinctions for a few groups of organisms reveal rates of extinction at least several hundred times the rate expected on the basis of the geological record the loss of biodiversity is the only truly irreversible global environmental change the earth today
summary the ontologizer is a java application that can be used to perform statistical analysis for overrepresentation of gene ontology go terms in sets of genes or proteins derived from an experiment the ontologizer implements the standard approach to statistical analysis based on the one sided fisher s exact test the novel parent child method as well as topology based algorithms a number of multiple testing correction procedures are provided the ontologizer allows users to visualize data as a graph including all significantly overrepresented go terms and to explore the data by linking go terms to all genes proteins annotated to the term and by linking individual terms to child terms availability the ontologizer application is available under the terms of the gnu gpl it can be started as a webstart application from the project homepage where source code is also provided http compbio charite de ontologizer requirements ontologizer requires a java se compliant java runtime engine and graphviz for the optional graph visualization tool contact sebastian bauer charite de peter robinson charite bioinformatics
motivation software applications for structural similarity searching and clustering of small molecules play an important role in drug discovery and chemical genomics here we present the first open source compound mining framework for the popularstatistical programming environment r the integration with a powerful statistical environment maximizes the flexibility expandability and programmability of the provided analysis functions results we discuss the algorithms and compound mining utilities provided by the r package chemminer it contains functions for structural similarity searching clustering of compound libraries with a wide spectrum of classification algorithms and various utilities for managing complex compound data it also offers a wide range of visualization functions for compound clusters and chemical structures the package is well integrated with the online chemmine environment and allows bidirectional communications between the two services availability chemminer is freely available as an r package from the chemmine project site http bioweb ucr edu chemminer contact thomas girke ucr bioinformatics
motivation upgma average linking is probably the most popular algorithm for hierarchical data clustering especially in computational biology however upgma requires the entire dissimilarity matrix in memory due to this prohibitive requirement upgma is not scalable to very large datasets application we present a novel class of memory constrained upgma mc upgma algorithms given any practical memory size constraint this framework guarantees the correct clustering solution without explicitly requiring all dissimilarities in memory the algorithms are general and are applicable to any dataset we present a data dependent characterization of hardness and clustering efficiency the presented concepts are applicable to any agglomerative clustering formulation results we apply our algorithm to the entire collection of protein sequences to automatically build a comprehensive evolutionary driven hierarchy of proteins from sequence alone the newly created tree captures protein families better than state of the art large scale methods such as clustr or single linkage clustering we demonstrate that leveraging the entire mass embodied in all sequence similarities allows to significantly improve on current protein family clusterings which are unable to directly tackle the sheer mass of this data furthermore we argue that non metric constraints are an inherent complexity of the sequence space and should not be overlooked the robustness of upgma allows significant improvement especially for multidomain proteins and for large or divergent families availability a comprehensive tree built from all uniprot sequence similarities together with navigation and classification tools will be made available as part of the protonet service a c implementation of the algorithm is available on request contact lonshy cs huji ac bioinformatics
motivation we present an algorithm to identify allelic variation given a whole genome shotgun wgs assembly of haploid sequences and to produce a set of haploid consensus sequences rather than a single consensus sequence existing wgs assemblers take a column by column approach to consensus generation and produce a single consensus sequence which can be inconsistent with the underlying haploid alleles and inconsistent with any of the aligned sequence reads our new algorithm uses a dynamic windowing approach it detects alleles by simultaneously processing the portions of aligned reads spanning a region of sequence variation assigns reads to their respective alleles phases adjacent variant alleles and generates a consensus sequence corresponding to each confirmed allele this algorithm was used to produce the first diploid genome sequence of an individual human it can also be applied to assemblies of multiple diploid individuals and hybrid assemblies of multiple haploid organisms results being applied to the individual human genome assembly the new algorithm detects exactly two confirmed alleles and reports two consensus sequences in of the total number detected regions of sequence variation in out of detected regions of size bp it fixes the constructed errors of a mosaic haploid representation of a diploid locus as produced by the original celera assembler consensus algorithm using an optimized procedure calibrated against known snps it detects new heterozygous snps with false positive rate availability the open source code is available at http wgs assembler cvs sourceforge net wgs assembler contact gdenisov jcvi bioinformatics
expression quantitative trait loci eqtl mapping studies have become a widely used tool for identifying genetic variants that affect gene regulation in these studies expression levels are viewed as quantitative traits and gene expression phenotypes are mapped to particular genomic loci by combining studies of variation in gene expression patterns with genome wide genotyping results from recent eqtl mapping studies have revealed substantial heritable variation in gene expression within and between populations in many cases genetic factors that influence gene expression levels can be mapped to proximal putatively cis eqtls and less often to distal putatively trans eqtls beyond providing great insight into the biology of gene regulation a combination of eqtl studies with results from traditional linkage or association studies of human disease may help predict a specific regulatory role for polymorphic sites previously associated disease
methods are described for the isolation complementation and mapping of mutants of caenorhabditis elegans a small free living nematode worm about ems induced mutants affecting behavior and morphology have been characterized and about one hundred genes have been defined mutations in of these alter the movement of the animal estimates of the induced mutation frequency of both the visible mutants and x chromosome lethals suggests that just as in drosophila the genetic units in c elegans large
tasks like image recognition are trivial for humans but continue to challenge even the most sophisticated computer programs this talk discusses a paradigm for utilizing human processing power to solve problems that computers cannot yet solve traditional approaches to solving such problems focus on improving software i advocate a novel approach constructively channel human brainpower using computer games for example the esp game described in this talk is an enjoyable online game many people play over hours a week and when people play they help label images on the web with descriptive keywords these keywords can be used to significantly improve the accuracy of image search people play the game not because they want to help but because they it
we assess the variability of protein function in protein sequence and structure space various regions in this space exhibit considerable difference in the local conservation of molecular function we analyze and capture local function conservation by means of logistic curves based on this analysis we propose a method for predicting molecular function of a query protein with known structure but unknown function the prediction method is rigorously assessed and compared with a previously published function predictor furthermore we apply the method to functionally unannotated pdb structures and discuss selected examples the proposed approach provides a simple yet consistent statistical model for the complex relations between protein sequence structure and function the godot method is available online http godot bioinf mpi inf de
we propose a simple method to extract the community structure of large networks our method is a heuristic method that is based on modularity optimization it is shown to outperform all other known community detection method in terms of computation time moreover the quality of the communities detected is very good as measured by the so called modularity this is shown first by identifying language communities in a belgian mobile phone network of million customers and by analyzing a web graph of million nodes and more than one billion links the accuracy of our algorithm is also verified on ad hoc networks
comparative analysis of small subunit ribosomal rna ss rrna gene sequences forms the basis for much of what we know about the phylogenetic diversity of both cultured and uncultured microorganisms as sequencing costs continue to decline and throughput increases sequences of ss rrna genes are being obtained at an ever increasing rate this increasing flow of data has opened many new windows into microbial diversity and evolution and at the same time has created significant methodological challenges those processes which commonly require time consuming human intervention such as the preparation of multiple sequence alignments simply cannot keep up with the flood of incoming data fully automated methods of analysis are needed notably existing automated methods avoid one or more steps that though computationally costly or difficult we consider to be important in particular we regard both the building of multiple sequence alignments and the performance of high quality phylogenetic analysis to be necessary we describe here our fully automated ss rrna taxonomy and alignment pipeline stap it generates both high quality multiple sequence alignments and phylogenetic trees and thus can be used for multiple purposes including phylogenetically based taxonomic assignments and analysis of species diversity in environmental samples the pipeline combines publicly available packages phyml blastn and clustalw with our automatic alignment masking and tree parsing programs most importantly this automated process yields results comparable to those achievable by manual analysis yet offers speed and capacity that are unattainable by efforts
background dna microarrays and other genomics inspired technologies provide large datasets that often include hidden patterns of correlation between genes reflecting the complex processes that underlie cellular metabolism and physiology the challenge in analyzing large scale expression data has been to extract biologically meaningful inferences regarding these processes often represented as networks in an environment where the datasets are often imperfect and biological noise can obscure the actual signal although many techniques have been developed in an attempt to address these issues to date their ability to extract meaningful and predictive network relationships has been limited here we describe a method that draws on prior information about gene gene interactions to infer biologically relevant pathways from microarray data our approach consists of using preliminary networks derived from the literature and or protein protein interaction data as seeds for a bayesian network analysis of microarray results results through a bootstrap analysis of gene expression data derived from a number of leukemia studies we demonstrate that seeded bayesian networks have the ability to identify high confidence gene gene interactions which can then be validated by comparison to other sources of pathway data conclusion the use of network seeds greatly improves the ability of bayesian network analysis to learn gene interaction networks from gene expression data we demonstrate that the use of seeds derived from the biomedical literature or high throughput protein protein interaction data or the combination provides improvement over a standard bayesian network analysis allowing networks involving dynamic processes to be deduced from the static snapshots of biological systems that represent the most common source of microarray data software implementing these methods has been included in the widely used microarray package
in the cerebral cortex diverse types of neurons form intricate circuits and cooperate in time for the processing and storage of information recent advances reveal a spatiotemporal division of labor in cortical circuits as exemplified in the hippocampal area in particular distinct gabaergic gamma aminobutyric acid releasing cell types subdivide the surface of pyramidal cells and act in discrete time windows either on the same or on different subcellular compartments they also interact with glutamatergic pyramidal cell inputs in a domain specific manner and support synaptic temporal dynamics network oscillations selection of cell assemblies and the implementation of brain states the spatiotemporal specializations in cortical circuits reveal that cellular diversity and temporal dynamics coemerged during evolution providing a basis for behavior
dna methylation is essential for normal and has been implicated in many pathologies including our knowledge about the genome wide distribution of dna methylation how it changes during cellular differentiation and how it relates to histone methylation and other chromatin modifications in mammals remains limited here we report the generation and analysis of genome scale dna methylation profiles at nucleotide resolution in mammalian cells using high throughput reduced representation bisulphite and single molecule based sequencing we generated dna methylation maps covering most cpg islands and a representative sampling of conserved non coding elements transposons and other genomic features for mouse embryonic stem cells embryonic stem cell derived and primary neural cells and eight other primary tissues several key findings emerge from the data first dna methylation patterns are better correlated with histone methylation patterns than with the underlying genome sequence context second methylation of cpgs are dynamic epigenetic marks that undergo extensive changes during cellular differentiation particularly in regulatory regions outside of core promoters third analysis of embryonic stem cell derived and primary cells reveals that weak cpg islands associated with a specific set of developmentally regulated genes undergo aberrant hypermethylation during extended proliferation in vitro in a pattern reminiscent of that reported in some primary tumours more generally the results establish reduced representation bisulphite sequencing as a powerful technology for epigenetic profiling of cell populations relevant to developmental biology cancer and medicine
the rapid growth of the number of videos in youtube provides enormous potential for users to find content of interest to them unfortunately given the difficulty of searching videos the size of the video repository also makes the discovery of new content a daunting task in this paper we present a novel method based upon the analysis of the entire user video graph to provide personalized video suggestions for users the resulting algorithm termed adsorption provides a simple method to efficiently propagate preference information through a variety of graphs we extensively test the results of the recommendations on a three month snapshot of live data youtube
the resurgent science of consciousness has been accompanied by a recent emphasis on the problem of measurement having dependable measures of consciousness is essential both for mapping experimental evidence to theory and for designing perspicuous experiments here we review a series of behavioural and brain based measures assessing their ability to track graded consciousness and clarifying how they relate to each other by showing what theories are presupposed by each we identify possible and actual conflicts among measures that can stimulate new experiments and we conclude that measures must prove themselves by iteratively building knowledge in the context of theoretical frameworks advances in measuring consciousness have implications for basic cognitive neuroscience for comparative studies of consciousness and for applications
the ability to quickly and reliably engineer many component systems from libraries of standard interchangeable parts is one hallmark of modern technologies whether the apparent complexity of living systems will permit biological engineers to develop similar capabilities is a pressing research question we propose to adapt existing frameworks for describing engineered devices to biological objects in order to i direct the refinement and use of biological parts and devices ii support research on enabling reliable composition of standard biological parts and iii facilitate the development of abstraction hierarchies that simplify biological engineering we use the resulting framework to describe one engineered biological device a genetically encoded cell cell communication receiver named the description of the receiver is summarized via a datasheet similar to those widely used in engineering the process of refinement and characterization leading to the datasheet may serve as a starting template for producing many standardized genetically objects
standards for characterization manufacture and sharing of information about modular biological devices may lead to a more efficient predictable and design driven genetic engineering science although genetic engineeringthe technical ability to edit dnahas led to impressive biotechnology applications these generally require many years of work and trial and error experiments to a concerted effort among synthetic biologists and allied fields might increase efficiency by developing rigorous characterization and manufacturing protocols linked to formal sharing of information and material through registries of biological parts and standard box
dna methylation is an indispensible epigenetic modification required for regulating the expression of mammalian genomes immunoprecipitation based methods for dna methylome analysis are rapidly shifting the bottleneck in this field from data generation to data analysis necessitating the development of better analytical tools in particular an inability to estimate absolute methylation levels remains a major analytical difficulty associated with immunoprecipitation based dna methylation profiling to address this issue we developed a cross platform algorithmbayesian tool for methylation analysis batman for analyzing methylated dna immunoprecipitation medip profiles generated using oligonucleotide arrays medip chip or next generation sequencing medip seq we developed the latter approach to provide a high resolution whole genome dna methylation profile dna methylome of a mammalian genome strong correlation of our data obtained using mature human spermatozoa with those obtained using bisulfite sequencing suggest that combining medip seq or medip chip with batman provides a robust quantitative and cost effective functional genomic strategy for elucidating the function of methylation
search engine researchers typically depict search as the solitary activity of an individual searcher in contrast results from our critical incident survey of users on amazons mechanical turk service suggest that social interactions play an important role throughout the search process our main contribution is that we have integrated models from previous work in sensemaking and information seeking behavior to present a canonical social model of user activities before during and after search suggesting where in the search process both explicitly and implicitly shared information may be valuable to searchers
we describe the comprehensive characterization of homeodomain dna binding specificities from a metazoan genome the analysis of all independent homeodomains from d melanogaster reveals the breadth of dna sequences that can be specified by this recognition motif the majority of these factors can be organized into different specificity groups where the preferred recognition sequence between these groups can differ at up to four of the six core recognition positions analysis of the recognition motifs within these groups led to a catalog of common specificity determinants that may cooperate or compete to define the binding site preference with these recognition principles a homeodomain can be reengineered to create factors where its specificity is altered at the majority of recognition positions this resource also allows prediction of homeodomain specificities from other organisms which is demonstrated by the prediction and analysis of human specificities
most homeodomains are unique within a genome yetmany are highly conserved across vast evolutionary distances implying strong selection on their precise dna binding specificities we determined the binding preferences of the majority of mouse homeodomains to all possible base sequences revealing rich and complex patterns of sequence specificity and showing that there are at least distinct homeodomain dna binding activities we developed a computational system that successfully predicts binding sites for homeodomain proteins as distant from mouse as drosophila and c elegans and we infer full mer binding profiles for the majority of known animal homeodomains our results provide an unprecedented level of resolution in the analysis of this simple domain structure and suggest that variation in sequence recognition may be a factor in its functional diversity and success
abstract background false discovery rate fdr methods play an important role in analyzing high dimensional data there are two types of fdr tail area based fdr and local fdr as well as numerous statistical algorithms for estimating or controlling fdr these differ in terms of underlying test statistics and procedures employed for statistical learning results a unifying algorithm for simultaneous estimation of both local fdr and tail area based fdr is presented that can be applied to a diverse range of test statistics including p values correlations z and t scores this approach is semipararametric and is based on a modified grenander density estimator for test statistics other than p values it allows for empirical null modeling so that dependencies among tests can be taken into account the inference of the underlying model employs truncated maximum likelihood estimation with the cut off point chosen according to the false non discovery rate conclusions the proposed procedure generalizes a number of more specialized algorithms and thus offers a common framework for fdr estimation consistent across test statistics and types of fdr in a comparative study the unified approach performs on par with the best competing yet more specialized alternatives the algorithm is implemented in r in the fdrtool package available under the gnu gpl from http strimmerlab org software fdrtool and from the r package cran
pnas current global fisheries production of million tons is rising as a result of increases in aquaculture production a number of climate related threats to both capture fisheries and aquaculture are identified but we have low confidence in predictions of future fisheries production because of uncertainty over future global aquatic net primary production and the transfer of this production through the food chain to human consumption recent changes in the distribution and productivity of a number of fish species can be ascribed with high confidence to regional climate variability such as the el niosouthern oscillation future production may increase in some high latitude regions because of warming and decreased ice cover but the dynamics in low latitude regions are governed by different processes and production may decline as a result of reduced vertical mixing of the water column and hence reduced recycling of nutrients there are strong interactions between the effects of fishing and the effects of climate because fishing reduces the age size and geographic diversity of populations and the biodiversity of marine ecosystems making both more sensitive to additional stresses such as climate change inland fisheries are additionally threatened by changes in precipitation and water management the frequency and intensity of extreme climate events is likely to have a major impact on future fisheries production in both inland and marine systems reducing fishing mortality in the majority of fisheries which are currently fully exploited or overexploited is the principal feasible means of reducing the impacts of climate er
abstract nbsp nbsp vulnerability is an emerging concept for climate science and policy over the past decade efforts to assess vulnerability to climate change triggered a process of theory development and assessment practice which is reflected in the reports of the intergovernmental panel on climate change ipcc this paper reviews the historical development of the conceptual ideas underpinning assessments of vulnerability to climate change we distinguish climate impact assessment first and second generation vulnerability assessment and adaptation policy assessment the different generations of assessments are described by means of a conceptual framework that defines key concepts of the assessment and their analytical relationships the purpose of this conceptual framework is two fold first to present a consistent visual glossary of the main concepts underlying the ipcc approach to vulnerability and its assessment second to show the evolution of vulnerability assessments this evolution is characterized by the progressive inclusion of non climatic determinants of vulnerability to climate change including adaptive capacity and the shift from estimating expected damages to attempting to reduce them we hope that this paper improves the understanding of the main approaches to climate change vulnerability assessment and their evolution not only within the climate change community but also among researchers from other scientific communities who are sometimes puzzled by the unfamiliar use of technical terms in the context of change
riparian zones possess an unusually diverse array of species and environmental processes the ecological diversity is related to variable flood regimes geographically unique channel processes altitudinal climate shifts and upland influences on the fluvial corridor the resulting dynamic environment supports a variety of life history strategies biogeochemical cycles and rates and organisms adapted to disturbance regimes over broad spatial and temporal scales innovations in riparian zone management have been effective in ameliorating many ecological issues related to land use and environmental quality riparian zones play essential roles in water and landscape planning in restoration of aquatic systems and in catalyzing institutional and societal cooperation for efforts
in recent years the deluge of complicated molecular and cellular microscopic images creates compelling challenges for the image computing community there has been an increasing focus on developing novel image processing data mining database and visualization techniques to extract compare search and manage the biological knowledge in these data intensive problems this emerging new area of bioinformatics can be called bioimage informatics this article reviews the advances of this field from several aspects including applications key techniques available tools and resources application examples such as high throughput high content phenotyping and atlas building for model organisms demonstrate the importance of bioimage informatics the essential techniques to the success of these applications such as bioimage feature identification segmentation and tracking registration annotation mining image data management and visualization are further summarized along with a brief overview of the available bioimage databases analysis tools and resources
humans often cooperate in public goods and situations ranging from family issues to global however evolutionary game theory that the temptation to forgo the public good mostly wins over collective cooperative action and this is often also seen in economic here we show how social diversity provides an escape from this apparent paradox up to now individuals have been treated as equivalent in all in sharp contrast with real life situations where diversity is ubiquitous we introduce social diversity by means of heterogeneous graphs and show that cooperation is promoted by the diversity associated with the number and size of the public goods game in which each individual participates and with the individual contribution to each such game when social ties follow a scale free cooperation is enhanced whenever all individuals are expected to contribute a fixed amount irrespective of the plethora of public goods games in which they engage our results may help to explain the emergence of cooperation in the absence of mechanisms based on individual reputation and combining social diversity with reputation and punishment will provide instrumental clues on the self organization of social communities and their implications
a major change has occurred in the way web technology is being used in society the change is grounded in user empowerment using web tools and processes students are already sophisticated users of these tools and processes but outside of the mainstream instructional practices in higher education in this reflection the educational potential of web tools and processes is discussed followed by three sets of perspectives relating to the potential quality of such practices in higher education course settings for each perspective an analysis of key factors affecting the perceived value of web tools and processes is given followed by suggestions for overcoming predictable barriers to uptake in mainstream instructional practice b les outils et les processus dinternet dans lenseignement suprieur une perspective de qualit b un changement trs important sest produit dans la faon dutiliser les technologies du web dans la socit ce changement repose sur la capacit quont les utilisateurs demployer les outils et les processus de web les tudiants sont dj des utilisateurs avertis de ces outils et processus mais cela en dehors des pratiques ducatives habituelles dans lenseignement suprieur la rflexion mene ici examine le potentiel ducatif des outils et processus dinternet le tout accompagn de trois ensembles de perspectives lies la qualit potentielle de ces pratiques dans le cadre de cours universitaires dans chacune de ces perspectives on offre une analyse des facteurs principaux qui affectent la perception de la valeur des outils et processus du web le tout suivi de suggestions pour surmonter les obstacles prvisibles ladoption dans la pratique ducative courante b web hilfsprogramme und prozesse in der hochschulbildung qualitts aspekte b ein bedeutender wandel ist in der art erfolgt wie die webtechnologie in der gesellschaft benutzt wird dieser wandel beruht darauf dass die nutzer hhere handlungskompetenz fr web hilfsprogramme und prozesse besitzen studenten sind schon fortgeschrittene anwender dieser werkzeuge und prozesse allerdings auerhalb der gngigen lehrpraktiken im hochschulbereich in dieser betrachtung werden die pdagogischen mglichkeiten von web programmen und prozessen diskutiert gefolgt von drei anlagen mit perspektiven der potentiellen qualitten dieser techniken in hochschulkursen fr jede perspektive wird eine analyse von schlsselfaktoren bezglich der erwarteten werte von web werkzeugen und prozessen angeboten gekoppelt mit vorschlgen zur berwindung von denkhindernissen bei der umsetzung in die durchschnittliche lehrpraxis b las herramientas y los procesos de internet en la enseanza superior una perspectiva de calidad b un cambio muy importante ha ocurrido en la manera de aprovechar la tecnologa de la web en la sociedad este cambio est basado en la toma de control por parte de los usuarios para el uso de las herramientas y procesos del web los estudiantes ya son usuarios expertos de esas herramientas y procesos pero esto ocurre fuera de las prcticas educativas habituales en la enseanza superior la presente reflexin examina el potencial educativo de las herramientas y procesos del web seguido por tres conjuntos de perspectivas relacionadas con la calidad potencial de esas prcticas dentro del marco de cursos universitarios en cada una de esas perspectivas se ofrece un anlisis de los factores claves que afectan la percepcin del valor de las herramientas y procesos del web esto siendo seguido por sugerencias para superar los obstculos a una adopcin dentro de la prctica habitual
creating a mashup a web application that integrates data from multiple web sources to provide a unique service involves solving multiple problems such as extracting data from multiple web sources cleaning it and combining it together existing work relies on a widget paradigm where users address those problems during a mashup building process by selecting customizing and connecting widgets together while these systems claim that their users do not have to write a single line of code merely abstracting programming methods into widgets has several disadvantages first as the number of widgets increases to support more operations locating the right widget for the task can be confusing and time consuming second customizing and connecting these widgets usually requires users to understand programming concepts in this paper we present a mashup building approach that a combines most problem areas in mashup building into a unified interactive framework that requires no widgets and b allows users with no programming background to easily create mashups example
switching between exploratory and defensive behaviour is fundamental to survival of many animals but how this transition is achieved by specific neuronal circuits is not known here using the converse behavioural states of fear extinction and its context dependent renewal as a model in mice we show that bi directional transitions between states of high and low fear are triggered by a rapid switch in the balance of activity between two distinct populations of basal amygdala neurons these two populations are integrated into discrete neuronal circuits differentially connected with the hippocampus and the medial prefrontal cortex targeted and reversible neuronal inactivation of the basal amygdala prevents behavioural changes without affecting memory or expression of behaviour our findings indicate that switching between distinct behavioural states can be triggered by selective activation of specific neuronal circuits integrating sensory and contextual information these observations provide a new framework for understanding context dependent changes of behaviour
the brain is noisy neurons receive tens of thousands of highly fluctuating inputs and generate spike trains that appear highly irregular much of this activity is spontaneous uncoupled to overt stimuli or motor outputs leading to questions about the functional impact of this noise although noise is most often thought of as disrupting patterned activity and interfering with the encoding of stimuli recent theoretical and experimental work has shown that noise can play a constructive role leading to increased reliability or regularity of neuronal firing in single neurons and across populations these results raise fundamental questions about how noise can influence neural function computation
meiotic recombination has a central role in the evolution of sexually reproducing organisms the two recombination outcomes crossover and non crossover increase genetic diversity but have the potential to homogenize alleles by gene conversion whereas crossover rates vary considerably across the genome non crossovers and gene conversions have only been identified in a handful of loci to examine recombination genome wide and at high spatial resolution we generated maps of crossovers crossover associated gene conversion and non crossover gene conversion using dense genetic marker data collected from all four products of fifty six yeast saccharomyces cerevisiae meioses our maps reveal differences in the distributions of crossovers and non crossovers showing more regions where either crossovers or non crossovers are favoured than expected by chance furthermore we detect evidence for interference between crossovers and non crossovers a phenomenon previously only known to occur between crossovers up to of the genome of each meiotic product is subject to gene conversion in a single meiosis with detectable bias towards gc nucleotides to our knowledge the maps represent the first high resolution genome wide characterization of the multiple outcomes of recombination in any organism in addition because non crossover hotspots create holes of reduced linkage within haplotype blocks our results stress the need to incorporate non crossovers into genetic analysis
the characterization of the topological architecture of complex networks underlying the structural and functional organization of the brain is a basic challenge in neuroscience however direct evidence for anatomical connectivity networks in the human brain remains scarce here we utilized diffusion tensor imaging deterministic tractography to construct a macroscale anatomical network capturing the underlying common connectivity pattern of human cerebral cortex in a large sample of subjects young adults and further quantitatively analyzed its topological properties with graph theoretical approaches the cerebral cortex was divided into cortical regions each representing a network node and cortical regions were considered connected if the probability of fiber connections exceeded a statistical criterion the topological parameters of the established cortical network binarized resemble that of a small world architecture characterized by an exponentially truncated power law distribution these characteristics imply high resilience to localized damage furthermore this cortical network was characterized by major hub regions in association cortices that were connected by bridge connections following long range white matter pathways our results are compatible with previous structural and functional brain networks studies and provide insight into the organizational principles of human brain anatomical networks that underlie functional cercor
targets for drugs have so far been predicted on the basis of molecular or cellular features for example by exploiting similarity in chemical structure or in activity across cell lines we used phenotypic side effect similarities to infer whether two drugs share a target applied to marketed drugs a network of side effect driven drug drug relations became apparent of which are formed by chemically dissimilar drugs from different therapeutic indications we experimentally tested of these unexpected drug drug relations and validated implied drug target relations by in vitro binding assays of which reveal inhibition constants equal to less than micromolar nine of these were tested and confirmed in cell assays documenting the feasibility of using phenotypic information to infer molecular interactions and hinting at new uses of drugs
background there is a growing recognition of the value of synthesising qualitative research in the evidence base in order to facilitate effective and appropriate health care in response to this methods for undertaking these syntheses are currently being developed thematic analysis is a method that is often used to analyse data in primary qualitative research this paper reports on the use of this type of analysis in systematic reviews to bring together and integrate the findings of multiple qualitative studies methods we describe thematic synthesis outline several steps for its conduct and illustrate the process and outcome of this approach using a completed review of health promotion research thematic synthesis has three stages the coding of text line by line the development of descriptive themes and the generation of analytical themes while the development of descriptive themes remains close to the primary studies the analytical themes represent a stage of interpretation whereby the reviewers go beyond the primary studies and generate new interpretive constructs explanations or hypotheses the use of computer software can facilitate this method of synthesis detailed guidance is given on how this can be achieved results we used thematic synthesis to combine the studies of children s views and identified key themes to explore in the intervention studies most interventions were based in school and often combined learning about health benefits with hands on experience the studies of children s views suggested that fruit and vegetables should be treated in different ways and that messages should not focus on health warnings interventions that were in line with these suggestions tended to be more effective thematic synthesis enabled us to stay close to the results of the primary studies synthesising them in a transparent way and facilitating the explicit production of new concepts and hypotheses conclusion we compare thematic synthesis to other methods for the synthesis of qualitative research discussing issues of context and rigour thematic synthesis is presented as a tried and tested method that preserves an explicit and transparent link between conclusions and the text of primary studies as such it preserves principles that have traditionally been important to reviewing
biologists have long sought to understand which genes and what kinds of changes in their sequences are responsible for the evolution of morphological diversity here i outline eight principles derived from molecular and evolutionary developmental biology and review recent studies of species divergence that have led to a genetic theory of morphological evolution which states that form evolves largely by altering the expression of functionally conserved proteins and such changes largely occur through mutations in the cis regulatory sequences of pleiotropic developmental regulatory loci and of the target genes within the vast networks control
to find inherited causes of autism spectrum disorders we studied families in which parents share ancestors enhancing the role of inherited factors we mapped several loci some containing large inherited homozygous deletions that are likely mutations the largest deletions implicated genes including protocadherin and deleted in or whose level of expression changes in response to neuronal activity a marker of genes involved in synaptic changes that underlie learning a subset of genes including na h exchanger showed additional potential mutations in patients with unrelated parents our findings highlight the utility of homozygosity mapping in heterogeneous disorders like autism but also suggest that defective regulation of gene expression after neural activity may be a mechanism common to seemingly diverse autism science
four subjects ecology applied mathematics sociology and economics were selected to assess whether there is a citation advantage between journal articles that have an open access oa version on the internet compared to those articles that are exclusively toll access ta citations were counted using the web of science and the oa status of articles was determined by searching oaister opendoar google and google scholar of a sample of articles examined were oa and had a mean citation count of whereas the mean for ta articles was there appears to be a clear citation advantage for those articles that are oa as opposed to those that are ta this advantage however varies between disciplines with sociology having the highest citation advantage but the lowest number of oa articles from the sample taken and ecology having the highest individual citation count for oa articles but the smallest citation advantage tests of correlation or association between oa status and a number of variables were generally found to weak or inconsistent the cause of this citation advantage has not determined
we review data demonstrating that single neuron sensory responses change with the states of the neural networks indexed in terms of spectral properties of lfps in which those neurons are embedded we start with broad network changes different levels of anesthesia and sleep and then move to studies demonstrating that the sensory response plasticity associated with attention and experience can also be conceptualized as functions of network state changes this leads naturally to the recent data that can be interpreted to suggest that even brief experience can change sensory responses via changes in network states and that trial to trial variability in sensory responses is a non random function of network fluctuations as well we suggest that the central nervous system may have evolved specifically to deal with stimulus variability and that the coupling network states may be central to processing
networks play a crucial role in computational biology yet their analysis and representation is still an open problem power graph analysis is a lossless transformation of biological networks into a compact less redundant representation exploiting the abundance of cliques and bicliques as elementary topological motifs we demonstrate with five examples the advantages of power graph analysis investigating protein protein interaction networks we show how the catalytic subunits of the casein kinase ii complex are distinguishable from the regulatory subunits how interaction profiles and sequence phylogeny of domains correlate and how false positive interactions among high throughput interactions are spotted additionally we demonstrate the generality of power graph analysis by applying it to two other types of networks we show how power graphs induce a clustering of both transcription factors and target genes in bipartite transcription networks and how the erosion of a phosphatase domain in type non receptor tyrosine phosphatases is detected we apply power graph analysis to high throughput protein interaction networks and show that up to on average of the information is redundant experimental networks are more compressible than rewired ones of same degree distribution indicating that experimental networks are rich in cliques and bicliques power graphs are a novel representation of networks which reduces network complexity by explicitly representing re occurring network motifs power graphs compress up to of the edges in protein interaction networks and are applicable to all types of networks such as protein interactions regulatory networks or networks
gene regulatory networks are perhaps the most important organizational level in the cell where signals from the cell state and the outside environment are integrated in terms of activation and inhibition of genes for the last decade the study of such networks has been fueled by large scale experiments and renewed attention from the theoretical field different models have been proposed to for instance investigate expression dynamics explain the network topology we observe in bacteria and yeast and for the analysis of evolvability and robustness of such networks yet how these gene regulatory networks evolve and become evolvable remains an open question an individual oriented evolutionary model is used to shed light on this matter each individual has a genome from which its gene regulatory network is derived mutations such as gene duplications and deletions alter the genome while the resulting network determines the gene expression pattern and hence fitness with this protocol we let a population of individuals evolve under darwinian selection in an environment that changes through time our work demonstrates that long term evolution of complex gene regulatory networks in a changing environment can lead to a striking increase in the efficiency of generating beneficial mutations we show that the population evolves towards genotype phenotype mappings that allow for an orchestrated network wide change in the gene expression pattern requiring only a few specific gene indels the genes involved are hubs of the networks or directly influencing the hubs moreover throughout the evolutionary trajectory the networks maintain their mutational robustness in other words evolution in an alternating environment leads to a network that is sensitive to a small class of beneficial mutations while the majority of mutations remain neutral an example of evolution evolvability
background random forests are becoming increasingly popular in many scientific fields because they can cope with small n large p problems complex interactions and even highly correlated predictor variables their variable importance measures have recently been suggested as screening tools for e g gene expression studies however these variable importance measures show a bias towards correlated predictor variables results we identify two mechanisms responsible for this finding i a preference for the selection of correlated predictors in the tree building process and ii an additional advantage for correlated predictor variables induced by the unconditional permutation scheme that is employed in the computation of the variable importance measure based on these considerations we develop a new conditional permutation scheme for the computation of the variable importance measure conclusion the resulting conditional variable importance reflects the true impact of each predictor variable more reliably than the original approach
jamia recently there has been a remarkable upsurge in activity surrounding the adoption of personal health record phr systems for patients and consumers the biomedical literature does not yet adequately describe the potential capabilities and utility of phr systems in addition the lack of a proven business case for widespread deployment hinders phr adoption in a working symposium the american medical informatics association s college of medical informatics discussed the issues surrounding personal health record systems and developed recommendations for phr promoting activities personal health record systems are more than just static repositories for patient data they combine data knowledge and software tools which help patients to become active participants in their own care when phrs are integrated with electronic health record systems they provide greater benefits than would stand alone systems for consumers this paper summarizes the college symposium discussions on phr systems and provides definitions system characteristics technical architectures benefits barriers to adoption and strategies for adoption
networks in nature possess a remarkable amount of structure via a series of data driven discoveries the cutting edge of network science has recently progressed from positing that the random graphs of mathematical graph theory might accurately describe real networks to the current viewpoint that networks in nature are highly complex and structured entities the identification of high order structures in networks unveils insights into their functional organization recently clauset moore and newman introduced a new algorithm that identifies such heterogeneities in complex networks by utilizing the hierarchy that necessarily organizes the many levels of structure here we anchor their algorithm in a general community detection framework and discuss the future of community detection bioessays wiley inc
we present the results of theoretical and experimental studies of dispersively coupled or membrane in the middle optomechanical systems we calculate the linear optical properties of a high finesse cavity containing a thin dielectric membrane we focus on the cavity s transmission reflection and finesse as a function of the membrane s position along the cavity axis and as a function of its optical loss we compare these calculations with measurements and find excellent agreement in cavities with empty cavity finesses in the range to the imaginary part of the membrane s index of refraction is found to be approximately we calculate the laser cooling performance of this system with a particular focus on the less intuitive regime in which photons tunnel through the membrane on a time scale comparable to the membrane s period of oscillation lastly we present calculations of quantum non demolition measurements of the membrane s phonon number in the low signal to noise regime where the phonon lifetime is comparable to the qnd time
programmatic access to data and tools through the web using so called web services has an important role to play in bioinformatics in this article we discuss the most popular approaches based on soap ws i and rest and describe our a cross section of the community experiences with providing and using web services in the context of biological sequence analysis we briefly review main technological approaches as well as best practice hints that are useful for both users and developers finally syntactic and semantic data integration issues with multiple web services are bib
internal brain states form key determinants for sensory perception sensorimotor coordination and learning a prominent reflection of different brain states in the mammalian central nervous system is the presence of distinct patterns of cortical synchrony as revealed by extracellular recordings of the electroencephalogram local field potential and action potentials such temporal correlations of cortical activity are thought to be fundamental mechanisms of neuronal computation however it is unknown how cortical synchrony is reflected in the intracellular membrane potential v m dynamics of behaving animals here we show using dual whole cell recordings from layer primary somatosensory barrel cortex in behaving mice that the v m of nearby neurons is highly correlated during quiet wakefulness however when the mouse is whisking an internally generated state change reduces the v m correlation resulting in a desynchronized local field potential and electroencephalogram action potential activity was sparse during both quiet wakefulness and active whisking single action potentials were driven by a large brief and specific excitatory input that was not present in the v m of neighbouring cells action potential initiation occurs with a higher signal to noise ratio during active whisking than during quiet periods therefore we show that an internal brain state dynamically regulates cortical membrane potential synchrony during behaviour and defines different modes of processing
attention exerts a strong influence over neuronal processing in cortical it selectively increases firing and affects tuning including changing receptive field locations and although these effects are well studied their cellular mechanisms are poorly understood to study the cellular mechanisms we combined iontophoretic pharmacological analysis of cholinergic receptors with single cell recordings in while rhesus macaque monkeys macaca mulatta performed a task that demanded top down spatial attention attending to the receptive field of the neuron under study caused an increase in firing rates here we show that this attentional modulation was enhanced by low doses of acetylcholine furthermore applying the muscarinic antagonist scopolamine reduced attentional modulation whereas the nicotinic antagonist mecamylamine had no systematic effect these results demonstrate that muscarinic cholinergic mechanisms play a central part in mediating the effects of in
spin systems and harmonic oscillators comprise two archetypes in quantum mechanics the spin system with two quantum energy levels is essentially the most nonlinear system found in nature whereas the harmonic oscillator represents the most linear with an infinite number of evenly spaced quantum levels a significant difference between these systems is that a two level spin can be prepared in an arbitrary quantum state using classical excitations whereas classical excitations applied to an oscillator generate a coherent state nearly indistinguishable from a classical state quantum behaviour in an oscillator is most obvious in fock states which are states with specific numbers of energy quanta but such states are hard to create here we demonstrate the controlled generation of multi photon fock states in a solid state system we use a superconducting phase qubit which is a close approximation to a two level spin system coupled to a microwave resonator which acts as a harmonic oscillator to prepare and analyse pure fock states with up to six photons we contrast the fock states with coherent states generated using classical pulses applied directly to resonator
the input to an algorithm that learns a binary classifier normally consists of two sets of examples where one set consists of positive examples of the concept to be learned and the other set consists of negative examples however it is often the case that the available training data are an incomplete set of positive examples and a set of unlabeled examples some of which are positive and some of which are negative the problem solved in this paper is how to learn a standard binary classifier given a nontraditional training set of nature
cavity opto mechanics studies the coupling between a mechanical oscillator and a cavity field with the aim to shed light on the border between classical and quantum physics here we report on a cavity opto mechanical system in which a collective density excitation of a bose einstein condensate is shown to serve as the mechanical oscillator coupled to the cavity field we observe that a few photons inside the ultrahigh finesse cavity trigger a strongly driven back action dynamics in quantitative agreement with a cavity opto mechanical model with this experiment we approach the strong coupling regime of cavity opto mechanics where a single excitation of the mechanical oscillator significantly influences the cavity field the work opens up new directions to investigate mechanical oscillators in the quantum regime and quantum gases with non coupling
imputation based association methods provide a powerful framework for testing untyped variants for association with phenotypes and for combining results from multiple studies that use different genotyping platforms here we consider several issues that arise when applying these methods in practice including i factors affecting imputation accuracy including choice of reference panel ii the effects of imputation accuracy on power to detect associations iii the relative merits of bayesian and frequentist approaches to testing imputed genotypes for association with phenotype and iv how to quickly and accurately compute bayes factors for testing imputed snps we find that imputation based methods can be robust to imputation accuracy and can improve power to detect associations even when average imputation accuracy is poor we explain how ranking snps for association by a standard likelihood ratio test gives the same results as a bayesian procedure that uses an unnatural prior assumptionspecifically that difficult to impute snps tend to have larger effectsand assess the power gained from using a bayesian approach that does not make this assumption within the bayesian framework we find that good approximations to a full analysis can be achieved by simply replacing unknown genotypes with a point estimatetheir posterior mean this approximation considerably reduces computational expense compared with published sampling based approaches and the methods we present are practical on a genome wide scale with very modest computational resources e g a single desktop computer the approximation also facilitates combining information across studies using only summary data for each snp methods discussed here are implemented in the software package bimbam which is available from http stephenslab uchicago edu html
the circumplex model of affect proposes that all affective states arise from cognitive interpretations of core neural sensations that are the product of two independent neurophysiological systems this model stands in contrast to theories of basic emotions which posit that a discrete and independent neural system subserves every emotion we propose that basic emotion theories no longer explain adequately the vast number of empirical observations from studies in affective neuroscience and we suggest that a conceptual shift is needed in the empirical approaches taken to the study of emotion and affective psychopathologies the circumplex model of affect is more consistent with many recent findings from behavioral cognitive neuroscience neuroimaging and developmental studies of affect moreover the model offers new theoretical and empirical approaches to studying the development of affective disorders as well as the genetic and cognitive underpinnings of affective processing within the central system
summarystrikingly consistent correlations between rates of coding sequence evolution and gene expression levels are apparent across taxa but the biological causes behind the selective pressures on coding sequence evolution remain controversial here we demonstrate conserved patterns of simple covariation between sequence evolution codon usage and mrna level in e coli yeast worm fly mouse and human that suggest that all observed trends stem largely from a unified underlying selective pressure in metazoans these trends are strongest in tissues composed of neurons whose structure and lifetime confer extreme sensitivity to protein misfolding we propose and demonstrate using a molecular level evolutionary simulation that selection against toxicity of misfolded proteins generated by ribosome errors suffices to create all of the observed covariation the mechanistic model of molecular evolution that emerges yields testable biochemical predictions calls into question the use of nonsynonymous to synonymous substitution ratios ka ks to detect functional selection and suggests how mistranslation may contribute to neurodegenerative disease introductionevolutionary cell biologists seek to understand how natural selection shapes cellular features and processes recent work has revealed the molecular basis of organism level adaptations by drawing upon phylogenetic information to infer recreate and functionally interrogate the sequences of evolving proteins dean and thornton such as receptors bound by distinct ligands bridgham etal or enzymes with distinct coenzyme preferences zhu etal but if gene sequences reveal the lineage specific fits and starts of adaptive evolution they must also bear signs of cell biological constraints common to all organisms such as the biophysical challenges of producing folded polypeptides in a crowded intracellular milieu selection for proper protein folding and function causes coding sequences to accumulate nonsynonymous amino acid altering substitutions at a slower rate than the synonymous amino acid preserving substitution rate differing evolutionary rates between coding sequences in the same lineage hint at further constraints for example histones evolve slower than hormones wilson etal because residues directly involved in protein function tend not to tolerate substitutions anfinsen guo etal wilson etal and zuckerkandl it has long been hypothesized that slow evolving proteins have more sites committed to function zuckerkandl or are more functionally important wilson etal plausible alternatives exist genes may evolve faster or slower for reasons unrelated to their functions such as regional variation in mutation rates indeed genome scale data have revealed that measures of functional importance such as essentiality or the number of protein protein interactions are surprisingly weak correlates of evolutionary rate pl etal concurrently a striking and apparently universal link between gene regulation and coding sequence evolution has emerged genes with high mrna expression levels encode slow evolving proteins from bacteria drummond etal and rocha and danchin yeast drummond etal and pl etal and algae popescu etal to nematodes krylov etal plants ingvarsson and wright etal fruit flies lemos etal mice and humans subramanian and kumar among duplicated yeast genes the higher expressed gene generally evolves slower drummond etal while often showing minimal functional differences from its paralog the faster evolving lower expression pyruvate kinase can completely substitute for the enzymatic activity of the slower evolving higher expressed pyruvate kinase boles etal evolutionary effects linked strongly to mrna level but not to function have been argued to indicate selection on translation including adaptations to combat ribosomal infidelity drummond etal missense errors in translation occur at rates of one per codons kramer and farabaugh ogle and ramakrishnan and parker at an error rate of of proteins expressed from an average length codon gene contain at least one missense substitution roughly of random substitutions disrupt protein function guo etal and markiewicz etal and most loss of function mutations appear to be loss of folding mutations pakula and sauer also shown by large scale folding and functional assays bloom etal finally misfolded proteins possess generic cytotoxicity bucciantini etal destabilized proteins expose natively buried hydrophobic residues that seek nonpolar surface area and find it in other destabilized proteins causing protein protein aggregation at cell membranes protein membrane aggregation disrupts membrane integrity kourie and henry and stefani and dobson and with it crucial ionic balances e g required for viability stefani the expectation that ribosomal infidelity generates cytotoxic species suggests a general selective pressure for genetic adaptations that reduce these costs longstanding efforts to understand connections between genetic change and organismal biology have uncovered a dense set of interrelationships between the nonsynonymous evolutionary rate dn the synonymous rate ds and other variables see box available online dn and ds are sometimes called ka and ks the ratio dn ds measures the strength of selection assuming synonymous changes have no effect on fitness however synonymous changes are weakly selected against in most organisms including humans chamary etal and yang and nielsen in some organisms selection favors efficiently translated so called optimal codons in highly expressed genes ikemura a bias measured most simply as the fraction of optimal codons fop per gene codon preference slows the rate of synonymous change ds ikemura notably a positive dn ds correlation was detected early on in mammals li etal and bacteria sharp and li but remains unexplained as have a negative correlation between fop and dn marais etal and sharp and li and recently a positive relationship between dn ds and ds in mammals wyckoff etal whereas an analysis in yeast suggested that variation in dn and ds reflects a common determinant drummond etal analyses of a wider array of organisms have concluded that the rates of evolution vary for different reasons in microbes and multicellular eukaryotes choi etal koonin and wolf and liao etal whether these divergent conclusions reflect primarily biological methodological or interpretive differences has remained unclear here we pursue a cell biological understanding of these long studied evolutionary patterns we first carry out a comprehensive methodologically unified evolutionary analysis across a set of six distantly related model organisms the results confirm multiple conserved genome wide signatures of selection including several unexplained and novel observations and firmly establish selection for translational accuracy in mammals we introduce a model in which selection against cytotoxic protein misfolding produces all of these conserved features to test the model s ability to generate the observed evolutionary patterns we create and analyze a large scale simulation that incorporates known biological constraints on translation and protein folding into an evolutionary framework we repeat all analyses on these in silico evolved genomes and show that all conserved patterns emerge finally we examine the simulation results at the molecular level to determine what molecular adaptations arise under selection against misfolding in this simplified system these analyses yield predictions for future experimental studies results organisms show a consistent correlation structureto systematically examine patterns of coding sequence evolution we assembled measures of commonly studied variables dn ds microarray quantified mrna expression level and codon bias measured by the fraction of optimal codons fop controlled for guanine cytosine gc content in mammals see the supplemental experimental procedures and the less well studied transition transversion ts tv ratio for each of six model organisms a gram negative bacterium e coli baker s yeast s cerevisiae a nematode worm c elegans fruit fly d melanogaster mouse m musculus and human h sapiens each with orthologous genes in a distinct species see the experimental procedures as shown in the data display remarkably consistent patterns of covariation despite considerable variability between organisms spearman rank correlation matrices show distinct similarities revealed clearly by the signs of the relationships and table all matrices show a block like structure with all signs absolutely conserved from e coli to mouse deviations in human are consistent with known hypermutation at methyl cytosine in the primate lineage supplemental results and largely disappear after intronic gc content a reporter for such hypermutation is controlled for weakening of correlations in higher organisms is limited to those involving mrna level and fop suggesting reduced precision or accuracy in these measurements as an alternative to weaker selection the block like correlation structure evident in suggests that the correlations reflect a unified underlying selective force principal component analysis pca of each organism s correlation matrix confirmed that a single underlying component explains of the variance in all five analyzed variables and is conserved across species red box correlations involving the transition transversion ratio an important measure of sequence change wakeley whose variation between genes is little studied are both consistent and surprising for example because most substitutions occur in the third codon position and most transition mutations ct and ag in the third position are synonymous the ts tv ratio and ds should positively correlate instead the relationship is negative in all organisms we consider explanations for these unexpected results later selection for translational accuracy affects all organismsselection on translational efficiency in mammals is viewed as weak and contentious chamary etal or absent dos reis etal using akashi s test akashi an elegant and sensitive test for selection on translational accuracy that was previously only applied to fly akashi and e coli stoletzki and eyre walker we demonstrate that selection on translational accuracy affects mammals as well the test quantifies the association between optimal codons and conserved residues under the assumption that selection favors accurate codons at sites where substitutions are most harmful the test s comparison of only those codons within a gene that encode instances of the same amino acid renders it immune to biases arising from between gene differences in expression level evolutionary rate amino acid composition mrna stability and local nucleotide content in every organism including human and mouse we found that optimal codons significantly associated with conserved sites table implying that selection has positioned optimal codons to reduce the consequences of translation errors we then computed the akashi association score for all possible alternative sets of synonymous optimal codons as table shows the previously identified optimal codon set scored significantly higher than alternative sets in all organisms distributions are shown in these results suggest that both the set of optimal codons i e the abundant trnas rocha and codon positions within genes interact to enhance translational accuracy in yeast worm and fly likelihood of finding optimal codons at conserved sites grew markedly stronger in the top highest expressed genes table mouse and human showed little change as did e coli the latter finding may be rationalized by strong selection for translational speed in high expression genes which optimizes all codons and mutes the signature of accuracy selection hypothesis protein misfolding costs impose a major evolutionary constraintthe extraordinary lineage spanning consistency of covariation in evolutionary rates codon choice and gene expression within and between genes table and demands a unified explanation that the covariation structure suggests a dominant underlying factor or cost motivates the search for the identity of that cost we propose that adaptations to reduce the cellular burden imposed by protein misfolding create all these covariation patterns furthermore we suggest that selection against mistranslation induced misfolding is necessary and sufficient to create them misfolding costs can be reduced by four main adaptations increasing the proportion of properly translated proteins increased translational accuracy decreasing the proportion of proteins that misfold or unfold because of mistranslation increased translational robustness and decreasing the proportion of properly translated proteins that either fail to attain or prematurely lose their native structure decreased stochastic misfolding and unfolding for transcriptionally regulated genes translation frequency increases with expression level and absent selection so will the number of costly misfolded proteins generated by mistranslation or by stochastic misfolding thus high expression genes impose steeper costs and should disproportionately display adaptations indicated in assuming that highly adapted alleles are rare fewer mutations will lead to viable alternative alleles so costly well adapted genes will reject a higher proportion of mutations and evolve slowly neuronal tissues appear to be particularly sensitive to protein misfolding neurodegenerative diseases disproportionately involve protein misfolding and aggregation soto the elaborately ramified structures and extraordinary cell length of many neurons confer a particularly high surface area to volume ratio increasing the likelihood of disruptive protein membrane interactions kourie and henry limited neuronal turnover makes cell loss more probable and more likely to be permanent under sustained chronic stress induced by misfolding malfunctioning of broadly expressed proteins involved in translation and protein folding manifests specifically neurotoxic effects in mouse lee etal and zhao etal under our hypothesis the differing sensitivity of certain cell types to misfolding will cause all of the trends we identify in to be amplified in tissues comprised of sensitive cell types and muted in less sensitive tissues tissue expression modulates coding sequence evolutionto assess the prediction that evolutionary patterns should vary systematically across tissues we computed correlations between dn ds or ts tv ratio and tissue specific mrna levels in fly mouse and human uncovering several strikingly consistent patterns in all three organisms expression in neural tissues shows the strongest correlation with dn ds and ts tv ratio in mouse and fly all neural tissues show a stronger correlation of tissue expression with dn than do nonneural tissues moreover the degrees to which each evolutionary variable correlates with tissue expression changes in a correlated way minimum r p in all cases as expected if all correlations arise from an underlying cost that varies by tissue both trends persist after the analysis is restricted to those genes with below median tissue specificity of expression codon bias measured by fop correlates with tissue expression similarly to dn ds and ts tv ratio in fly but not in mouse or human not shown likely because of tissue specific codon usage dittmar etal a large scale evolutionary simulation reproduces conserved patternsthe above results suggest that selection against misfolding underlies broad patterns of sequence evolution to test our narrower hypothesis that selection against mistranslation induced protein misfolding suffices to create all of these patterns we turned to a large scale computer simulation we created a population of simulated organisms with kb genomes consisting of coding nucleotide sequences genes expressed at different levels and translated with occasional codon dependent translation errors into computationally foldable model proteins translationally optimal codons were designated arbitrarily and were translated several fold more accurately than their synonyms table the population evolved subject to mutation drift and selection with fitness depending only on stable folding of the wild type protein and the number of misfolded proteins generated by mistranslation proteins that failed to fold into the structure encoded by the native amino acid sequence with a free energy of unfolding above kcal mol were designated misfolded we ran simulations from identical initial conditions with and without a cost imposed by mistranslation induced misfolding bottom shows the covariation of expression level dn ds fop and ts tv ratio in the simulation for comparison with real organisms as shows the simulated genes display all the consensus correlations found from e coli to mouse including the surprising negative correlation of ts tv ratio with ds r when misfolding costs were eliminated all but two correlations vanished and as expected in the absence of synonymous site selection the relationship between ts tv ratio and ds turned positive r we hypothesized that the novel and counterintuitive negative association of transition transversion ratio with ds was mediated by synonymous codon choice this hypothesis predicts that the negative correlation between ts tv ratio and ds will be strongest at third codon position sites where transitions mostly lead to synonymous substitutions weaker at first codon position sites where only a small fraction of transitions are synonymous and weakest at second codon position sites where all transitions are nonsynonymous confirms these predictions all organisms and in the simulation but only when misfolding costs are applied these results recall the recent report of a positive correlation between ds and dn ds in mouse and human claimed to be inconsistent with present models of molecular evolution wyckoff etal we find that this correlation exists in all organisms but worm is reproduced in our simulation only in the presence of misfolding costs and is expected when dn and ds are not independent supplemental results as when both are constrained by adaptations to combat misfolding tests for translational accuracy table and the results of principal component analysis on the simulation s correlation matrix matched the organismal results but only when misfolding imposed a fitness cost to examine whether inaccuracy or imprecision of mrna measurements and codon bias in indicating translational frequency and accuracy could produce the weaker results in higher organisms we added noise to expression level and fop performed pca on this noisy simulation and found close qualitative agreement with mouse and human in the previous simulations all genes were essential suppressing any potential evolutionary variation due to differing importance of genes we then asked whether the covariation patterns could be explained by functional importance as measured by the fitness effect of deletion or dispensability wall etal we performed simulations under a fitness function in which each molecule expressed from a gene contributed equally to fitness but the total contribution of that gene to fitnessthe cost of losing all molecules i e the dispensabilitywas given by the fitness defect upon gene deletion to ensure the proper relationship between mrna level and dispensability we sampled values from the joint distribution observed in yeast all correlations with mrna level weakened substantially the strong link between mrna level and dn and the correlation between mrna level and ts tv ratio both found in all real organisms became insignificant to ensure that the alternate fitness function not the altered distribution of expression levels was responsible for the failure to match biological observations we evolved the new sampled genome under the original fitness function where each misfolded molecule imposed equal fitness costs and obtained results similar to the original ones by tracking the fates of proteins translated from each evolved gene we dissected the precise adaptations that occurred as a function of expression level the only nonrandom independent variable in the simulation figures show that translational accuracy translational robustness the propensity of proteins to fold properly despite mistranslation production of full length polypeptides and overall propensity to fold properly all increase with expression level relative to the baseline observed in the no cost simulation after each gene was recoded with randomly chosen synonymous codons the proportion of accurately translated proteins regressed to the mean of the no cost simulation tying differences in translational accuracy to synonymous codon usage by contrast the fraction of tolerated errors was dominated by adaptation at the protein level as predicted drummond etal translationally robust proteins achieved error tolerance through increased stability as measured by the free energy of unfolding and fewer translation errors resulted in destabilization past the threshold for stable folding note that relative to the average protein high expression proteins tolerate more substitutions at the ribosome but tolerate fewer substitutions over evolutionary time bottom genes evolve slowly because most mutations are deleterious most mutations to a highly expressed gene encoding a translationally robust protein yield sequences encoding other folded proteins but not other robust proteins because loss of translational robustness elevates mistranslation induced misfolding and is therefore deleterious such mutations will fail to rise to fixation and the gene will accumulate few changes over evolutionary time wilke and drummond discussionour analyses reveal conserved patterns of covariation linking common measures of evolutionary change and gene expression across the genomes of e coli yeast worm fly mouse and human principal component analysis suggests that all these pairwise correlations are manifestations of a unified underlying selective pressure we propose and demonstrate usinga molecular level simulation that selection against protein misfolding particularly misfolding induced by missense errors at the ribosome suffices to create all the observed patterns because we focus on patterns of covariation our results do not imply that all variation in molecular evolution can be explained by protein misfolding or that protein function plays a negligible role what is the nature of the fitness cost imposed by misfolding misfolded proteins manifest fitness costs unrelated to their functionsuch costs may involve direct toxicity such as disruption of membrane integrity or inappropriate interactions with other cellular components stefani and dobson indirect toxicity such as heightened sensitivity to stresses such as heat shock or other indirect efficiency burdens such as the energy expended in synthesizing detecting and degrading misfolded proteins or the squandering of ribosomal capacity on useless products stoebel etal although our results cannot rule out any of these costs previous analyses have shown little to no role for amino acid cost or polypeptide length the major correlates of synthesis and degradation cost in shaping evolutionary rate variation drummond etal and rocha and danchin under theoriesoften informed by results of microbial competition assays stoebel etal that link fitness costs to ribosomal throughput or translational efficiency the strengthening of evolutionary effects with expression in neurons has no clear explanation since many neurons do not divide at all and it seems unlikely that neuronal translational capacity limits animal reproduction in a way analogous to microbes by emphasizing neuronal cell loss with its clear consequences for the animal lee etal the hypothesis that cells are sensitive to cytotoxic misfolded proteins accounts for these trends in a way which also applies to microbial growth misfolding also steals a potentially functional molecule from the cell if the loss of functional molecules imposes the dominant fitness cost essential genes determined by systematic deletion studies should evolve much more slowly than dispensable genes because deletions result in the loss of all functional molecules instead in yeast essential proteins evolve slower than all proteins compared to an fold difference linked to gene expression mrna level codon usage and protein abundance drummond etal when our simulation was modified to model functional importance as measured by yeast deletion studies wall etal and warringer etal the strong correlation between mrna level and dn vanished in this alternate model highly expressed proteins evolve slowly only as a side effect of the relationship between dispensability and mrna level a weak correlation in yeast r in general functional arguments to explain why low expression proteins evolve rapidly imply that low expression proteins have little functional importance cell biological considerations undermine this notion many essential gene products such as those involved in dna replication have intrinsically low copy number targets such as replication forks and are therefore weakly expressed the misfolding hypothesis resolves the conflict by decoupling evolutionary rate from importance however although our results suggest that loss of functional molecules is unlikely to be the dominant evolutionary cost uncovered here function doubtless applies some constraint the nature of the cellular cost imposed by misfolding remains unresolved in studies of misfolding diseases after substantial study and obtaining unambiguous evidence to resolve this question in an evolutionary context will likelyrequire similar sustained efforts particularly experimental measurements of the magnitude and mechanistic basis of growth rate costs arising from chronic low frequency protein misfolding influences of the distribution of expression across tissuessubstantial attention has been devoted to the dependence of evolutionary rate particularly dn on the breadth tissue specificity and aggregate level of expression in multicellular organisms duret and mouchiroud liao etal pl etal parmley etal and wright etal our results suggest that such summaries of expression patterns although predictive of evolutionary rates table obscure key biological differences between tissues recent studies show that mutations in broadly expressed genes involved in translation and protein folding produce brain specific phenotypes lee etal and zhao etal suggesting that neural tissues suffer disproportionately given systemic misfolding our findings hint that such sensitivity shapes sequence change over evolutionary time mrna levels inanimal neural tissues consistently predict evolutionary rates better than levels in other tissues even when tissue specific genes are discarded previous studies have identified brain expressed genes as slow evolving wang etal and zhang and li that slow evolution accompanies expression in neural tissues outside the brain e g spinal cord or ventral nerve cord in fly implicates a constraint operating on neurons rather than on the brain per se that ds and ts tv ratio relationships parallel dn across tissues demands an explanation beyond functional selection or complex regulation neuronal sensitivity to mistranslation induced misfolding lee etal predicts all of these observations experimental predictionsour hypothesis makes a number of experimentally testable predictions given a pair of genes of similar structure where the first is expressed more highly and evolves more slowly than its partner the first encoded protein is predicted to misfold less often than the second in yeast duplicated genes provide ample such pairs drummond etal including many constitutive and glucose repressed enzyme isoforms e g pyruvate kinases and similarly the higher expressed slower evolving duplicate gene is predicted to encode a protein with higher thermodynamic stability than its paralog we predict that misfolded proteins will disproportionately contain translation errors when compared to folded proteins if instead stochastic misfolding of error free polypeptides dominates protein misfolding this prediction would be falsified we further predict that putatively translationally robust higher expressed slower evolving genes should more often generate folded proteins after ribosomal errorsand dna point mutations that simulate those errorsthan their more fragile counterparts in formulating our hypotheses we have drawn heavily on studies of disease related protein misfolding on virtually any dimension population of the unfolded state rate of premature unfolding failure to export aggregation propensity and so on proteins containing translational missense errors are generally expected to show pathological behavior more often than error free proteinsindeed this is why genetic missense mutations are thought to cause disease decreased translational fidelity arising from mutations in translation related genes such as alanyl trna synthetase lee etal has been argued to contribute to heritable disease bacher and schimmel but is likely to yield widespread misfolding inconsistent with some misfolding diseases in which specific proteins unrelated to translation or protein folding carry mutations our hypothesis emphasizes that proteins differ in their tolerance to translation errors that occur given normal fidelity and that lower tolerance to translation errors of certain proteins and associated mutants provides a new mechanism to explain their propensity to misfold pathologically such a mechanism is most likely to operate when the disease arises despite proper folding of the majority of disease associated protein molecules disease symptoms are associated with toxic protein misfolding and cell stress disease severity increases with the degree of mutation induced destabilization of the protein many missense mutations across the gene cause disease and the protein is highly expressed in the affected tissues linked familial and sporadic forms of amyotrophic lateral sclerosis als gruzman etal and central nervous system amyloidosis ttr cnsa linked to kinetically stable thermodynamically unstable variants of transthyretin sekijima etal meet all of these criteria mistranslation induced misfolding offers a largely unexplored mechanism for generating pathological misfolding of a subset of protein molecules in the absence of genetic mutations a potentially valuable avenue of inquiry given the large fraction of sporadic als cases our hypothesis which holds that mistranslation is a contributor but not the sole cause makes the following testable predictions misfolded aggregated and ttr molecules particularly those in early forming toxic oligomers will contain significantly more missense errors than the soluble species and aggregation and premature degradation predictors of disease onset and severity will be reduced under conditions of elevated translational fidelity broader consequencesour results suggest a substantial rethinking of several widely credited hypotheses in molecular evolution first most variation in evolutionary rates appears to be attributable to regulatory rather than functional differences and translation often ignored may play a central role second synonymous changes are not silent in any of the organisms studied and as a direct consequence the common practice of using dn ds equivalently ka ks as a measure of protein divergence controlled for neutral divergence should be reconsidered if as our results strongly suggest both dn and ds reflect translational selection in most organisms then analyses of brain evolution that ascribe evolutionary significance to increases in this ratio dorus etal should be revisited finally in place of common claims that sequence evolution in multicellular organisms differs in qualitative ways from that in microbes our results rather suggest a continuum c elegans is a behaving multicellular eukaryotic animal with multiple tissues and yet on the dimensions analyzed in the present work its genes evolve almost indistinguishably from those of e coli if infidelity of the evolutionarily ancient translational apparatus indeed governs sequence change the model organism paradigm may be exploited with unusual confidence in efforts to uncover the relevant cell biology experimental procedures genome and transcriptome data and evolutionary rate measurementsdetailed methods data sources and gene count data are listed in the supplemental experimental procedures in brief coding dna sequences were built from coding exon sequences that were extracted from chromosomal dna sequences protein alignments of translated orthologous gene sequences were generated with muscle edgar and used to align nucleotide sequences a single cdna per gene was randomly chosen from each gene that showed evidence of alternative splicing only cdnas with alignment to their ortholog ds except as noted and at least codons were retained whole organism and per tissue mrna level data were downloaded and for human mouse and fly combined with a geometric mean across tissues detailed procedures in the supplemental data evolutionary rates and transition transversion rate ratios were computed by maximum likelihood with paml yang and yang with a physical sites definition bierne and eyre walker operating on codons codeml program with the codon frequency model one dn ds ratio per branch model and an arbitrary seed ts tv rate ratio of ts tv ratios were computed by counting of transitions and transversions separating orthologous sequences adding to each laplace estimation and taking their ratio distributions of dn and ds for all organisms are shown in the fraction of optimal codons fop was calculated as described duret and mouchiroud with published optimal codons except in mouse cf table translational accuracy selection was first tested exactly as described with akashi s test akashi sites with the same amino acid at the aligned position in the orthologous gene or for the simulation in all ancestral proteins on the line of descent were designated conserved in a second test significance of the optimal conserved association randomized over the choice of optimal codon set was assessed by computation of the odds ratio for all possible alternate optimal codon sets that preserve the number of optimal codons per synonymous family in the naturally occurring set simulation protocolfive hundred in silico nucleotide sequences of length l encoding model polypeptides that folded into a specific native structure with a stability free energy of unfolding of at least kcal mol were found and folded as described wilke evolution of each gene at a rate of mutations site generation proceeded in parallel in a population of genes parallel evolution employed to make large scale evolution tractable rests on the assumptions of no linkage independent mutations plausible because ne l and fitness as defined below nucleotide mutations were equally likely e g no mutational transition bias for simulation of regulated expression polypeptides translated from a gene were folded until a target number of folded proteins the gene s expression level x was obtained the translation error spectrum was implemented as described freeland and hurst translation at codons identified as optimal for yeast sharp and cowe proceeded with fold higher accuracy on average than at nonoptimal codons and roughly of low expression polypeptides contained an error consistent with biological expectations drummond etal misfolding resulted from mistranslation leading to truncation adoption of a nonnative conformation or native state stability of less than kcal mol toxicity of misfolded proteins was assumed to derive from exposure of interaction prone buried residues bucciantini etal and aggregation was not modeled explicitly the likelihood of reproduction was proportional to organism fitness wright fisher sampling which was in turn determined by the number of toxic misfolded proteins m according to fitness m ecm see the supplemental results where cis a positive constant and m is the amount of misfolded protein if f is the fraction of folded proteins translated then m x f we chose c altering c is equivalent to raising or lowering all expression levels assesses the fitness disadvantage of codon changes as a function of expression level for perspective an alternate fitness function incorporating protein importance or dispensability was also used in which fitness es where sis the additive growth rate effect of gene deletion i e f s ln deletion strain growth rate max growth rate see the supplemental experimental procedures source code is available upon request acknowledgmentswe are grateful to f h arnold for invaluable guidance and support in developing this work we thank c adami for support s eddy for trna gene count data j cuff and b baer for computational assistance and b stern e o shea a murray m laub j plotkin a kieu and members of the fas center for systems biology for helpful discussions this work was supported by grants from the national institutes of health nih c o w and an nih center grant to the fas center for systems biology d a d conceived of the study and performed the analyses and d a d and c o w designed the research constructed the simulation and wrote paper
networks and the epidemiology of directly transmitted infectious diseases are fundamentally linked the foundations of epidemiology and early epidemiological models were based on population wide random mixing but in practice each individual has a finite set of contacts to whom they can pass infection the ensemble of all such contacts forms a mixing network knowledge of the structure of the network allows models to compute the epidemic dynamics at the population scale from the individual level behaviour of infections therefore characteristics of mixing networksand how these deviate from the random mixing normhave become important applied concerns that may enhance the understanding and prediction of epidemic patterns and intervention measures here we review the basis of epidemiological theory based on random mixing models and network theory based on work from the social sciences and graph theory we then describe a variety of methods that allow the mixing network or an approximation to the network to be ascertained it is often the case that time and resources limit our ability to accurately find all connections within a network and hence a generic understanding of the relationship between network structure and disease dynamics is needed therefore we review some of the variety of idealized network types and approximation techniques that have been utilized to elucidate this link finally we look to the future to suggest how the two fields of network theory and epidemiological modelling can deliver an improved understanding of disease dynamics and better public health through effective control
backgroundthe design and construction of novel biological systems by combining basic building blocks represents a dominant paradigm in synthetic biology creating and maintaining a database of these building blocks is a way to streamline the fabrication of complex constructs the registry of standard biological parts registry is the most advanced implementation of this idea methods principal findingsby analyzing inclusion relationships between the sequences of the registry entries we build a network that can be related to the registry abstraction hierarchy the distribution of entry reuse and complexity was extracted from this network the collection of clones associated with the database entries was also analyzed the plasmid inserts were amplified and sequenced the sequences of inserts could be confirmed experimentally but unexpected discrepancies have also been identified conclusions significanceorganizational guidelines are proposed to help design and manage this new type of scientific resources in particular it appears necessary to compare the cost of ensuring the integrity of database entries and associated biological samples with their value to the users the initial strategy that permits including any combination of parts irrespective of its potential value leads to an exponential and economically unsustainable growth that may be detrimental to the quality and long term value of the resource to users
the distribution of species body size within taxonomic groups exhibits a heavy right tail extending over many orders of magnitude where most species are much larger than the smallest species we provide a simple model of cladogenetic diffusion over evolutionary time that omits explicit mechanisms for interspecific competition and other microevolutionary processes yet fully explains the shape of this distribution we estimate the model s parameters from fossil data and find that it robustly reproduces the distribution of mammal species from the late quaternary the observed fit suggests that the asymmetric distribution arises from a fundamental trade off between the short term selective advantages cope s rule and long term selective risks of increased species body size in the presence of a taxon specific lower limit on body science
online journals promise to serve more information to more dispersed audiences and are more efficiently searched and recalled but because they are used differently than print scientists and scholars tend to search electronically and follow hyperlinks rather than browse or peruse electronically available journals may portend an ironic change for science using a database of million articles their citations to and online availability to i show that as more journal issues came online the articles referenced tended to be more recent fewer journals and articles were cited and more of those citations were to fewer journals and articles the forced browsing of print archives may have stretched scientists and scholars to anchor findings deeply into past and present scholarship searching online is more efficient and following hyperlinks quickly puts researchers in touch with prevailing opinion but this may accelerate consensus and narrow the range of findings and ideas built science
this publication presents the american heart association aha guidelines for cardiopulmonary resuscitation cpr and emergency cardiovascular care ecc of the pediatric patient and the american academy of pediatrics aha guidelines for cpr and ecc of the neonate the guidelines are based on the evidence evaluation from the international consensus conference on cardiopulmonary resuscitation and emergency cardiovascular care science with treatment recommendations hosted by the american heart association in dallas texas january the aha guidelines for cardiopulmonary resuscitation and emergency cardiovascular care contain recommendations designed to improve survival from sudden cardiac arrest and acute life threatening cardiopulmonary problems the evidence evaluation process that was the basis for these guidelines was accomplished in collaboration with the international liaison committee on resuscitation ilcor the ilcor process is described in more detail in the international consensus on cardiopulmonary resuscitation and emergency cardiovascular care science with treatment recommendations the recommendations in the aha guidelines for cardiopulmonary resuscitation and emergency cardiovascular care confirm the safety and effectiveness of many approaches acknowledge that other approaches may not be optimal and recommend new treatments that have undergone evidence evaluation these new recommendations do not imply that care involving the use of earlier guidelines is unsafe in addition it is important to note that these guidelines will not apply to all rescuers and all victims in all situations the leader of a resuscitation attempt may need to adapt application of the guidelines to unique circumstances the following are the major pediatric advanced life support changes in the guidelines there is further caution about the use of endotracheal tubes laryngeal mask airways are acceptable when used by experienced providers cuffed endotracheal tubes may be used in infants except newborns and children in in hospital settings provided that cuff inflation pressure is kept cm confirmation of tube placement requires clinical assessment and assessment of exhaled carbon dioxide esophageal detector devices may be considered for use in children weighing kg who have a perfusing rhythm correct placement must be verified when the tube is inserted during transport and whenever the patient is moved during cpr with an advanced airway in place rescuers will no longer perform cycles of cpr instead the rescuer performing chest compressions will perform them continuously at a rate of minute without pauses for ventilation the rescuer providing ventilation will deliver to breaths per minute breath approximately every seconds timing of shock cpr and drug administration during pulseless arrest has changed and now is identical to that for advanced cardiac life support routine use of high dose epinephrine is not recommended lidocaine is de emphasized but it can be used for treatment of ventricular fibrillation pulseless ventricular tachycardia if amiodarone is not available induced hypothermia degrees c for hours may be considered if the child remains comatose after resuscitation indications for the use of inodilators are mentioned in the postresuscitation section termination of resuscitative efforts is discussed it is noted that intact survival has been reported following prolonged resuscitation and absence of spontaneous circulation despite doses of epinephrine the following are the major neonatal resuscitation changes in the guidelines supplementary oxygen is recommended whenever positive pressure ventilation is indicated for resuscitation free flow oxygen should be administered to infants who are breathing but have central cyanosis although the standard approach to resuscitation is to use oxygen it is reasonable to begin resuscitation with an oxygen concentration of less than or to start with no supplementary oxygen ie start with room air if the clinician begins resuscitation with room air it is recommended that supplementary oxygen be available to use if there is no appreciable improvement within seconds after birth in situations where supplementary oxygen is not readily available positive pressure ventilation should be administered with room air current recommendations no longer advise routine intrapartum oropharyngeal and nasopharyngeal suctioning for infants born to mothers with meconium staining of amniotic fluid endotracheal suctioning for infants who are not vigorous should be performed immediately after birth a self inflating bag a flow inflating bag or a t piece a valved mechanical device designed to regulate pressure and limit flow can be used to ventilate a newborn an increase in heart rate is the primary sign of improved ventilation during resuscitation exhaled detection is the recommended primary technique to confirm correct endotracheal tube placement when a prompt increase in heart rate does not occur after intubation the recommended intravenous iv epinephrine dose is to mg kg per dose higher iv doses are not recommended and iv administration is the preferred route although access is being obtained administration of a higher dose up to mg kg through the endotracheal tube may be considered it is possible to identify conditions associated with high mortality and poor outcome in which withholding resuscitative efforts may be considered reasonable particularly when there has been the opportunity for parental agreement the following guidelines must be interpreted according to current regional outcomes when gestation birth weight or congenital anomalies are associated with almost certain early death and when unacceptably high morbidity is likely among the rare survivors resuscitation is not indicated examples are provided in the guidelines in conditions associated with a high rate of survival and acceptable morbidity resuscitation is nearly always indicated in conditions associated with uncertain prognosis in which survival is borderline the morbidity rate is relatively high and the anticipated burden to the child is high parental desires concerning initiation of resuscitation should be supported infants without signs of life no heartbeat and no respiratory effort after minutes of resuscitation show either a high mortality rate or severe neurodevelopmental disability after minutes of continuous and adequate resuscitative efforts discontinuation of resuscitation may be justified if there are no signs life
genome structure variation has profound impacts on phenotype in organisms ranging from microbes to humans yet little is known about how natural selection acts on genome arrangement pathogenic bacteria such as yersinia pestis which causes bubonic and pneumonic plague often exhibit a high degree of genomic rearrangement the recent availability of several yersinia genomes offers an unprecedented opportunity to study the evolution of genome structure and arrangement we introduce a set of statistical methods to study patterns of rearrangement in circular chromosomes and apply them to the yersinia we constructed a multiple alignment of eight yersinia genomes using mauve software to identify conserved segments that are internally free from genome rearrangement based on the alignment we applied bayesian statistical methods to infer the phylogenetic inversion history of yersinia the sampling of genome arrangement reconstructions contains seven parsimonious tree topologies each having different histories of inversions topologies with a greater number of inversions also exist but were sampled less frequently the inversion phylogenies agree with results suggested by snp patterns we then analyzed reconstructed inversion histories to identify patterns of rearrangement we confirm an over representation of symmetric inversions inversions with endpoints that are equally distant from the origin of chromosomal replication ancestral genome arrangements demonstrate moderate preference for replichore balance in yersinia we found that all inversions are shorter than expected under a neutral model whereas inversions acting within a single replichore are much shorter than expected we also found evidence for a canonical configuration of the origin and terminus of replication finally breakpoint reuse analysis reveals that inversions with endpoints proximal to the origin of dna replication are nearly three times more frequent our findings represent the first characterization of genome arrangement evolution in a bacterial population evolving outside laboratory conditions insight into the process of genomic rearrangement may further the understanding of pathogen population dynamics and selection on the architecture of circular chromosomes
this paper evaluates the suitability of the mapreduce model for multi core and multi processor systems mapreduce was created by google for application development on data centers with thousands of servers it allows programmers to write functional style code that is automaticatlly parallelized and scheduled in a distributed system we describe phoenix an implementation of mapreduce for shared memory systems that includes a programming api and an efficient runtime system the phoenix run time automatically manages thread creation dynamic task scheduling data partitioning and fault tolerance across processor nodes we study phoenix with multi core and symmetric multiprocessor systems and evaluate its performance potential and error recovery features we also compare mapreduce code to code written in lower level apis such as p threads overall we establish that given a careful implementation mapreduce is a promising model for scalable performance on shared memory systems with simple code
background probabilistic functional gene networks are powerful theoretical frameworks for integrating heterogeneous functional genomics and proteomics data into objective models of cellular systems such networks provide syntheses of millions of discrete experimental observations spanning dna microarray experiments physical protein interactions genetic interactions and comparative genomics the resulting networks can then be easily applied to generate testable hypotheses regarding specific gene functions and associations methodology principal findings we report a significantly improved version v of a probabilistic functional gene network of the baker s yeast saccharomyces cerevisiae we describe our optimization methods and illustrate their effects in three major areas the reduction of functional bias in network training reference sets the application of a probabilistic model for calculating confidences in pair wise protein physical or genetic interactions and the introduction of simple thresholds that eliminate many false positive mrna co expression relationships using the network we predict and experimentally verify the function of the yeast rna binding protein in ribosomal subunit biogenesis conclusions significance yeastnet v constructed using these optimizations together with additional data shows significant reduction in bias and improvements in precision and recall in total covering linkages among yeast proteins of the validated proteome yeastnet is available from http www org
mutation hotspots are commonly observed in genomic sequences and certain human disease loci but general mechanisms for their formation remain elusive here we investigate the distribution of single nucleotide changes around insertions deletions indels in six independent genome comparisons including primates rodents fruitfly rice and yeast in each of these genomic comparisons nucleotide divergence d is substantially elevated surrounding indels and decreases monotonically to near background levels over several hundred bases d is significantly correlated with both size and abundance of nearby indels in comparisons of closely related species derived nucleotide substitutions surrounding indels occur in significantly greater numbers in the lineage containing the indel than in the one containing the ancestral non indel allele the same holds within species for single nucleotide mutations surrounding polymorphic indels we propose that heterozygosity for an indel is mutagenic to surrounding sequences and use yeast genome wide polymorphism data to estimate the increase in mutation rate the consistency of these patterns within and between species suggests that indel associated substitution is a general mechanism
summary next generation sequencing can provide insight into protein dna association events on a genome wide scale and is being applied in an increasing number of applications in genomics and meta genomics research however few software applications are available for interpreting these experiments we present here an efficient application for use with chromatin immunoprecipitation chip seq experimental data that includes novel functionality for identifying areas of gene enrichment and transcription factor binding site locations as well as for estimating dna fragment size distributions in enriched areas the findpeaks application can generate ucsc compatible custom wig track files from aligned read files for short read sequencing technology the software application can be executed on any platform capable of running a java runtime environment memory requirements are proportional to the number of sequencing reads analyzed typically gb permits processing of up to million reads availability the findpeaks package and manual containing algorithm descriptions usage instructions and examples are available at http www bcgsc ca platform bioinfo software findpeaks source files for findpeaks are available for academic use contact afejes bcgsc bioinformatics
ever since their introduction two decades ago single molecule sm fluorescence methods have matured and branched out to address numerous biological questions which were inaccessible via ensemble measurements among the current arsenal sm fluorescence techniques have capabilities of probing the dynamic interactions of nucleic acids and proteins via f rster fluorescence resonance energy transfer fret tracking single particles over microns of distances and deciphering the rotational motion of multisubunit systems in this exciting era of transitioning from in vitro to in vivo and in situ conditions it is anticipated that sm fluorescence methodology will become a common tool of biology
recent technological breakthroughs allow the quantification of hundreds of thousands of genetic interactions gis in saccharomyces cerevisiae the interpretation of these data is often difficult but it can be improved by the joint analysis of gis along with complementary data types here we describe a novel methodology that integrates genetic and physical interaction data we use our method to identify a collection of functional modules related to chromosomal biology and to investigate the relations among them we show how the resulting map of modules provides clues for the elucidation of function both at the level of individual genes and at the level of modules
search engine click logs provide an invaluable source of relevance information but this information is biased because we ignore which documents from the result list the users have actually seen before and after they clicked otherwise we could estimate document relevance by simple counting in this paper we propose a set of assumptions on user browsing behavior that allows the estimation of the probability that a document is seen thereby providing an unbiased estimate of document relevance to train test and compare our model to the best alternatives described in the literature we gather a large set of real data and proceed to an extensive cross validation experiment our solution outperforms very significantly all previous models as a side effect we gain insight into the browsing behavior of users and we can compare it to the conclusions of an eye tracking experiments by joachims et al in particular our findings confirm that a user almost always see the document directly after a clicked document they also explain why documents situated just after a very relevant document are clicked often
novel aspects of human dynamics and social interactions are investigated by means of mobile phone data using extensive phone records resolved in both time and space we study the mean collective behavior at large scales and focus on the occurrence of anomalous events we discuss how these spatiotemporal anomalies can be described using standard percolation theory tools we also investigate patterns of calling activity at the individual level and show that the interevent time of consecutive calls is heavy tailed this finding which has implications for dynamics of spreading phenomena in social networks agrees with results previously reported on other activities
this article aims to provide an introductory survey on quantum random walks starting from a physical effect to illustrate the main ideas we will introduce quantum random walks review some of their properties and outline their striking differences to classical walks we will touch upon both physical effects and computer science applications introducing some of the main concepts and language of present day quantum information science in this context we will mention recent developments in this new area and outline some questions
motivation the exponential growth of sequence databases poses a major challenge to bioinformatics tools for querying alignment and annotation databases there is a pressing need for methods for finding overlapping sequence intervals that are highly scalable to database size query interval size result size and construction updating of the interval database results we have developed a new interval database representation the nested containment list nclist whose query time is o n log n where n is the database size and n is the size of the result set in all cases tested this query algorithm is fold faster than other indexing methods tested in this study such as mysql multi column indexing mysql binning and r tree indexing we provide performance comparisons both in simulated datasets and real world genome alignment databases across a wide range of database sizes and query interval widths we also present an in place nclist construction algorithm that yields database construction times that are fold faster than other methods available the nclist data structure appears to provide a useful foundation for highly scalable interval database applications availability nclist data structure is part of pygr a bioinformatics graph database library available at http sourceforge net projects pygr contact leec chem ucla edu supplementary information supplementary data are available at bioinformatics bioinformatics
although prognostic gene expression signatures for survival in early stage lung cancer have been proposed for clinical application it is critical to establish their performance across different subject populations and in different laboratories here we report a large trainingtesting multi site blinded validation study to characterize the performance of several prognostic models based on gene expression for lung adenocarcinomas the hypotheses proposed examined whether microarray measurements of gene expression either alone or combined with basic clinical covariates stage age sex could be used to predict overall survival in lung cancer subjects several models examined produced risk scores that substantially correlated with actual subject outcome most methods performed better with clinical data supporting the combined use of clinical and molecular information when building prognostic models for early stage lung cancer this study also provides the largest available set of microarray data with extensive pathological and clinical annotation for adenocarcinomas
abstract background cancer diagnosis and clinical outcome prediction are among the most important emerging applications of gene expression microarray technology with several molecular signatures on their way toward clinical deployment use of the most accurate classification algorithms available for microarray gene expression data is a critical ingredient in order to develop the best possible molecular signatures for patient care as suggested by a large body of literature to date support vector machines can be considered best of class algorithms for classification of such data recent work however suggests that random forest classifiers may outperform support vector machines in this domain results in the present paper we identify methodological biases of prior work comparing random forests and support vector machines and conduct a new rigorous evaluation of the two algorithms that corrects these limitations our experiments use diagnostic and prognostic datasets and show that support vector machines outperform random forests often by a large margin our data also underlines the importance of sound research design in benchmarking and comparison of bioinformatics algorithms conclusions we found that both on average and in the majority of microarray datasets random forests are outperformed by support vector machines both in the settings when no gene selection is performed and when several popular gene selection methods used
motivation survival prediction of breast cancer bc patients independently of treatment also known as prognostication is a complex task since clinically similar breast tumors in addition to be molecularly heterogeneous may exhibit different clinical outcomes in recent years the analysis of gene expression profiles by means of sophisticated data mining tools emerged as a promising technology to bring additional insights into bc biology and to improve the quality of prognostication the aim of this work is to assess quantitatively the accuracy of prediction obtained with state of the art data analysis techniques for bc microarray data through an independent and thorough framework results due to the large number of variables the reduced amount of samples and the high degree of noise complex prediction methods are highly exposed to performance degradation despite the use of cross validation techniques our analysis shows that the most complex methods are not significantly better than the simplest one a univariate model relying on a single proliferation gene this result suggests that proliferation might be the most relevant biological process for bc prognostication and that the loss of interpretability deriving from the use of overcomplex methods may be not sufficiently counterbalanced by an improvement of the quality of prediction availability the comparison study is implemented in an r package called survcomp and is available from http www ulb ac be di map bhaibeka software survcomp contact bhaibeka ulb ac be supplementary information supplementary data are available at bioinformatics bioinformatics
winner of the society for cinema and media studies katherine singer kovacs book award convergence culture maps a new territory where old and new media intersect where grassroots and corporate media collide where the power of the media producer and the power of the consumer interact in unpredictable ways henry jenkins one of america s most respected media analysts delves beneath the new media hype to uncover the important cultural transformations that are taking place as media converge he takes us into the secret world of spoilers where avid internet users pool their knowledge to unearth the show s secrets before they are revealed on the air he introduces us to young fans who are writing their own hogwart s tales while executives at warner brothers struggle for control of their franchise he shows us how has pushed transmedia storytelling to new levels creating a fictional world where consumers track down bits of the story across multiple media channels jenkins argues that struggles over convergence will redefine the face of american popular culture industry leaders see opportunities to direct content across many channels to increase revenue and broaden markets at the same time consumers envision a liberated public sphere free of network controls in a decentralized media environment sometimes corporate and grassroots efforts reinforce each other creating closer more rewarding relations between media producers and consumers sometimes these two forces are at war jenkins provides a riveting introduction to the world where every story gets told and every brand gets sold across multiple media platforms he explains the cultural shift that is occurring as consumers fight for control across disparate channels changing the way we do business elect our leaders and educate children
motivation there are several levels of uncertainty involved in the mathematical modelling of biochemical systems there often may be a degree of uncertainty about the values of kinetic parameters about the general structure of the model and about the behaviour of biochemical species which cannot be observed directly the methods of bayesian inference provide a consistent framework for modelling and predicting in these uncertain conditions we present a software package for applying the bayesian inferential methodology to problems in systems biology results described herein is a software package biobayes which provides a framework for bayesian parameter estimation and evidential model ranking over models of biochemical systems defined using ordinary differential equations the package is extensible allowing additional modules to be included by developers there are no other such packages available which provide this functionality availability http www dcs gla ac uk biobayes contact vvv dcs gla ac bioinformatics
statistical dependencies in the responses of sensory neurons govern both the amount of stimulus information conveyed and the means by which downstream neurons can extract it although a variety of measurements indicate the existence of such dependencies their origin and importance for neural coding are poorly understood here we analyse the functional significance of correlated firing in a complete population of macaque parasol retinal ganglion cells using a model of multi neuron spike responses the model with parameters fit directly to physiological data simultaneously captures both the stimulus dependence and detailed spatio temporal correlations in population responses and provides two insights into the structure of the neural code first neural encoding at the population level is less noisy than one would expect from the variability of individual neurons spike times are more precise and can be predicted more accurately when the spiking of neighbouring neurons is taken into account second correlations provide additional sensory information optimal model based decoding that exploits the response correlation structure extracts more information about the visual scene than decoding under the assumption of independence and preserves more visual information than optimal linear decoding this model based approach reveals the role of correlated activity in the retinal coding of visual stimuli and provides a general framework for understanding the importance of correlated activity in populations neurons
in this paper we describe a study of the effectiveness of mobile learning m learning in the form of podcasting for teaching undergraduate students in higher education podcasting involves downloading a series of audio or video broadcasts files onto a digital media player via a computer over a period of weeks these can then be watched or listened to when where and as often as students choose the use of digital media players popularised by apple s ipod tm is widespread amongst undergraduate students a pilot survey of business and management students indicated that over owned some form of digital media player with a further indicating that they intended to purchase one in the next six months whilst podcasting is being utilized as a teaching tool by some educators in the secondary sector its use in higher education and its effectiveness as a learning tool for adults remains to be established in our study a separate group of just under first level students were given a series of revision podcasts after completing a course in information and communications technology and prior to their examination as part of the subscription process they had to complete an online questionnaire about their experience the questionnaire utilized a five point likert scale comparing their attitudes to lectures podcasts notes textbooks and multimedia e learning systems statistical analysis of the results of the study indicates that students believe that podcasts are more effective revision tools than their textbooks and they are more efficient than their own notes in helping them to learn they also indicate that they are more receptive to the learning material in the form of a podcast than a traditional lecture or textbook the study suggests that the use of podcasts as a revision tool has clear benefits as perceived by undergraduate students in terms of the time they take to revise and how much they feel they can learn coupled with the advantages of flexibility in when where and how it is used podcasting appears to have significant potential as an innovative learning tool for adult learners in higher education c elsevier ltd all reserved
for centuries biologists have studied patterns of plant and animal diversity at continental scales until recently similar studies were impossible for microorganisms arguably the most diverse and abundant group of organisms on earth here we present a continental scale description of soil bacterial communities and the environmental factors influencing their biodiversity we collected soil samples from across north and south america and used a ribosomal dna fingerprinting method to compare bacterial community composition and diversity quantitatively across sites bacterial diversity was unrelated to site temperature latitude and other variables that typically predict plant and animal diversity and community composition was largely independent of geographic distance the diversity and richness of soil bacterial communities differed by ecosystem type and these differences could largely be explained by soil ph r and r respectively p in both cases bacterial diversity was highest in neutral soils and lower in acidic soils with soils from the peruvian amazon the most acidic and least diverse in our study our results suggest that microbial biogeography is controlled primarily by edaphic variables and differs fundamentally from the biogeography of organisms
we analyze the social network emerging from the user comment activity on the website slashdot the network presents common features of traditional social networks such as a giant component small average path length and high clustering but differs from them showing moderate reciprocity and neutral assortativity by degree using kolmogorov smirnov statistical tests we show that the degree distributions are better explained by log normal instead of power law distributions we also study the structure of discussion threads using an intuitive radial tree representation threads show strong heterogeneity and self similarity throughout the different nesting levels of a conversation we use these results to propose a simple measure to evaluate the degree of controversy provoked by post
following its invention years ago pcr has been adapted for numerous molecular biology applications gene expression analysis by reverse transcription quantitative pcr rt qpcr has been a key enabling technology of the post genome era since the founding of biotechniques this journal has been a resource for the improvements in qpcr technology experimental design and data analysis qpcr and more specifically real time qpcr has become a routine and robust approach for measuring the expression of genes of interest validating microarray experiments and monitoring biomarkers the use of real time qpcr has nearly supplanted other approaches e g northern blotting rnase protection assays this review examines the current state of qpcr for gene expression analysis now that the method has reached a mature stage of development and implementation specifically the different fluorescent reporter technologies of real time qpcr are discussed as well as the selection of endogenous controls the conceptual framework for data analysis methods is also presented to demystify these analysis techniques the future of qpcr remains bright as the technology becomes more rapid cost effective easier to use and capable of throughput
a wireless sensor network wsn has important applications such as remote environmental monitoring and target tracking this has been enabled by the availability particularly in recent years of sensors that are smaller cheaper and intelligent these sensors are equipped with wireless interfaces with which they can communicate with one another to form a network the design of a wsn depends significantly on the application and it must consider factors such as the environment the application s design objectives cost hardware and system constraints the goal of our survey is to present a comprehensive review of the recent literature since the publication of i f akyildiz w su y sankarasubramaniam e cayirci a survey on sensor networks ieee communications magazine following a top down approach we give an overview of several new applications and then review the literature on various aspects of wsns we classify the problems into three different categories internal platform and underlying operating system communication protocol stack and network services provisioning and deployment we review the major development in these three categories and outline challenges
identifying ecologically differentiated populations within complex microbial communities remains challenging yet is critical for interpreting the evolution and ecology of microbes in the wild here we describe spatial and temporal resource partitioning among vibrionaceae strains coexisting in coastal bacterioplankton a quantitative model adaptml establishes the evolutionary history of ecological differentiation thus revealing populations specific for seasons and life styles combinations of free living particle or zooplankton associations these ecological population boundaries frequently occur at deep phylogenetic levels consistent with named species however recent and perhaps ongoing adaptive radiation is evident in vibrio splendidus which comprises numerous ecologically distinct populations at different levels of phylogenetic differentiation thus environmental specialization may be an important correlate or even trigger of speciation among sympatric science
microorganisms represent the largest reservoir of biodiversity on earth both in numbers and total genetic diversity but it remains unclear whether this biodiversity is organized in discrete units that correspond to ecologically coherent species to further explore this question we examined patterns of genomic diversity in sympatric microbial populations analyses of a total of approximately mb of microbial community genomic dna sequence recovered from m depth in the pacific ocean revealed discrete sequence defined populations of bacteria and archaea with intrapopulation genomic sequence divergence ranging from approximately to approximately the populations appeared to be maintained at least in part by intrapopulation genetic exchange homologous recombination although the frequency of recombination was estimated to be about three times lower than that observed previously in thermoacidophilic archaeal biofilm populations furthermore the genotypes of a given population were clearly distinguishable from their closest co occurring relatives based on their relative abundance in situ the genetic distinctiveness and the matching sympatric abundances imply that these genotypes share similar ecophysiological properties and therefore may represent fundamental units of microbial diversity in the deep sea comparisons to surface dwelling relatives of the sargasso sea revealed that distinct sequence based clusters were not always detectable presumably due to environmental variations further underscoring the important relationship between environmental contexts and genetic mechanisms which together shape and sustain microbial structure
testing one snp at a time does not fully realise the potential of genome wide association studies to identify multiple causal variants which is a plausible scenario for many complex diseases we show that simultaneous analysis of the entire set of snps from a genome wide study to identify the subset that best predicts disease outcome is now feasible thanks to developments in stochastic search methods we used a bayesian inspired penalised maximum likelihood approach in which every snp can be considered for additive dominant and recessive contributions to disease risk posterior mode estimates were obtained for regression coefficients that were each assigned a prior with a sharp mode at zero a non zero coefficient estimate was interpreted as corresponding to a significant snp we investigated two prior distributions and show that the normal exponential gamma prior leads to improved snp selection in comparison with single snp tests we also derived an explicit approximation for type i error that avoids the need to use permutation procedures as well as genome wide analyses our method is well suited to fine mapping with very dense snp sets obtained from re sequencing and or imputation it can accommodate quantitative as well as case control phenotypes covariate adjustment and can be extended to search for interactions here we demonstrate the power and empirical type i error of our approach using simulated case control data sets of up to k snps a real genome wide data set of k snps and a sequence based dataset each of which can be analysed in a few hours on a workstation
chromatin structure plays an important role in modulating the accessibility of genomic dna to regulatory proteins in eukaryotic cells we performed an integrative analysis on dozens of recent datasets generated by deep sequencing and high density tiling arrays and we discovered an array of well positioned nucleosomes flanking sites occupied by the insulator binding protein ctcf across the human genome these nucleosomes are highly enriched for the histone variant z and histone modifications the distances between the center positions of the neighboring nucleosomes are largely invariant and we estimate them to be bp on average surprisingly subsets of nucleosomes that are enriched in different histone modifications vary greatly in the lengths of dna protected from micrococcal nuclease cleavage bp the nucleosomes enriched in those histone modifications previously implicated to be correlated with active transcription tend to contain less protected dna indicating that these modifications are correlated with greater dna accessibility another striking result obtained from our analysis is that nucleosomes flanking ctcf sites are much better positioned than those downstream of transcription start sites the only genomic feature previously known to position nucleosomes genome wide this nucleosome positioning phenomenon is not observed for other transcriptional factors for which we had genome wide binding data we suggest that binding of ctcf provides an anchor point for positioning nucleosomes and chromatin remodeling is an important component of function
in the last decade advances in high throughput technologies such as dna microarrays have made it possible to simultaneously measure the expression levels of tens of thousands of genes and proteins this has resulted in large amounts of biological data requiring analysis and interpretation nonnegative matrix factorization nmf was introduced as an unsupervised parts based learning paradigm involving the decomposition of a nonnegative matrix v into two nonnegative matrices w and h via a multiplicative updates algorithm in the context of a p n gene expression matrix v consisting of observations on p genes from n samples each column of w defines a metagene and each column of h represents the metagene expression pattern of the corresponding sample nmf has been primarily applied in an unsupervised setting in image and natural language processing more recently it has been successfully utilized in a variety of applications in computational biology examples include molecular pattern discovery class comparison and prediction cross platform and cross species analysis functional characterization of genes and biomedical informatics in this paper we review this method as a data analytical and interpretive tool in computational biology with an emphasis on applications
research on personal epistemologies has begun to consider ontology do naive epistemologies take the form of stable unitary beliefs or of fine grained context sensitive resources debates such as this regarding subtleties of cognitive theory however may be difficult to connect to everyday instructional practice our purpose in this article is to make that connection we first review reasons for supporting the latter account of naive epistemologies as made up of fine grained context sensitive resources as part of this argument we note that familiar strategies and curricula tacitly ascribe epistemological resources to students we then present several strategies designed more explicitly to help students tap those resources for learning introductory physics finally we reflect on this work as an example of interplay between modes of inquiry into student thinking that of instruction and that of formal research learning
protein interactions play a vital part in the function of a cell as experimental techniques for detection and validation of protein interactions are time consuming there is a need for computational methods for this task protein interactions appear to form a network with a relatively high degree of local clustering in this paper we exploit this clustering by suggesting a score based on triplets of observed protein interactions the score utilises both protein characteristics and network properties our score based on triplets is shown to complement existing techniques for predicting protein interactions outperforming them on data sets which display a high degree of clustering the predicted interactions score highly against test measures for accuracy compared to a similar score derived from pairwise interactions only the triplet score displays higher sensitivity and specificity by looking at specific examples we show how an experimental set of interactions can be enriched and validated as part of this work we also examine the effect of different prior databases upon the accuracy of prediction and find that the interactions from the same kingdom give better results than from across kingdoms suggesting that there may be fundamental differences between the networks these results all emphasize that network structure is important and helps in the accurate prediction of protein interactions the protein interaction data set and the program used in our analysis and a list of predictions and validations are available at http www stats ox ac uk bioinfo predictinginteractions
pnas the subseafloor marine biosphere may be one of the largest reservoirs of microbial biomass on earth and has recently been the subject of debate in terms of the composition of its microbial inhabitants particularly on sediments from the peru margin a metagenomic analysis was made by using whole genome amplification and pyrosequencing of sediments from ocean drilling program site on the peru margin to further explore the microbial diversity and overall community composition within this environment a total of mb of genetic material was sequenced from sediments at horizons and m below the seafloor these depths include sediments from both primarily sulfate reducing methane generating regions of the sediment column many genes of the annotated genes including those encoding ribosomal proteins corresponded to those from the chloroflexi and euryarchaeota however analysis of the small subunit ribosomal genes suggests that crenarchaeota are the abundant microbial member quantitative pcr confirms that uncultivated crenarchaeota are indeed a major microbial group in these subsurface samples these findings show that the marine subsurface is a distinct microbial habitat and is different from environments studied by metagenomics especially because of the predominance of uncultivated groups
the exploration of the microbial world has been an exciting series of unanticipated discoveries despite being largely uninformed by rational estimates of the magnitude of task confronting us however in the long term more structured surveys can be achieved by estimating the diversity of microbial communities and the effort required to describe them the rates of recovery of new microbial taxa in very large samples suggest that many more taxa remain to be discovered in soils and the oceans we apply a robust statistical method to large gene sequence libraries from these environments to estimate both diversity and the sequencing effort required to obtain a given fraction of that diversity in the upper ocean we predict some phylotypes and a mere fivefold increase in shotgun reads could yield of the metagenome that is all genes from all taxa however at deep ocean hydrothermal vents and diversities in soils can be up to two orders of magnitude larger and hundreds of times the current number of samples will be required just to obtain of the taxonomic diversity based on difference in rdna obtaining of the metagenome will require tens of thousands of times the current sequencing effort although the definitive sequencing of hyperdiverse environments is not yet possible we can using taxa abundance distributions begin to plan and develop the required methods and strategies this would initiate a new phase in the exploration of the world
in most previous work on personalized search algorithms the results for all queries are personalized in the same manner however as we show in this paper there is a lot of variation across queries in the benefits that can be achieved through personalization for some queries everyone who issues the query is looking for the same thing for other queries different people want very different results even though they express their need in the same way we examine variability in user intent using both explicit relevance judgments and large scale log analysis of user behavior patterns while variation in user behavior is correlated with variation in explicit relevance judgments the same query there are many other factors such as result entropy result quality and task that can also affect the variation in behavior we characterize queries using a variety of features of the query the results returned for the query and people s interaction history with the query using these features we build predictive models to identify queries that can benefit personalization
sba this is the era of social networking collective intelligence participation collaborative creation and borderless distribution every day we are bombarded with more publicity about collaborative environments news feeds blogs wikis podcasting webcasting folksonomies social bookmarking social citations collaborative filtering recommender systems media sharing massive multiplayer online games virtual worlds and mash ups this sort of anarchic environment appeals to the digital natives but which of these so called web technologies are going to have a real business impact this paper addresses the impact that issues such as quality control security privacy and bandwidth may have on the implementation of social networking in hide bound large organizations she discusses other such as enterprise which has a lot of similar jargon to web she claims facebook and myspace are used by employers to check up on potential and current employees library of congress attempting to archive creative content in second life dell has launched a site to gather ideas from consumers ideastorm this seems like an invitation to listen in on all sorts of conversations that happen at and away from the place
this paper describes a procedure that makes it possible to design and fabricate including sealing microfluidic systems in an elastomeric material poly dimethylsiloxane pdms in less than h a network of microfluidic channels with width m is designed in a cad program this design is converted into a transparency by a high resolution printer this transparency is used as a mask in photolithography to create a master in positive relief photoresist pdms cast against the master yields a polymeric replica containing a network of channels the surface of this replica and that of a flat slab of pdms are oxidized in an oxygen plasma these oxidized surfaces seal tightly and irreversibly when brought into conformal contact oxidized pdms also seals irreversibly to other materials used in microfluidic systems such as glass silicon silicon oxide and oxidized polystyrene a number of substrates for devices are therefore practical options oxidation of the pdms has the additional advantage that it yields channels whose walls are negatively charged when in contact with neutral and basic aqueous solutions these channels support electroosmotic pumping and can be filled easily with liquids with high surface energies especially water the performance of microfluidic systems prepared using this rapid prototyping technique has been evaluated by fabricating a miniaturized capillary electrophoresis system amino acids charge ladders of positively and negatively charged proteins and dna fragments were separated in aqueous solutions with this system with resolution comparable to that obtained using fused capillaries
biological pathways are abstract and functional visual representations of existing biological knowledge by mapping high throughput data on these representations changes and patterns in biological systems on the genetic metabolic and protein level are instantly assessable many public domain repositories exist for storing biological pathways each applying its own conventions and storage format a pathway based content review of these repositories reveals that none of them are comprehensive to address this issue we apply a general workflow to create curated biological pathways in which we combine three content sources public domain databases literature and experts in this workflow all content of a particular biological pathway is manually retrieved from biological pathway databases and literature after which this content is compared combined and subsequently curated by experts from the curated content new biological pathways can be created for a pathway analysis tool of choice and distributed among its user base we applied this procedure to construct high quality curated biological pathways involved in human fatty metabolism
abstract background functional annotation of proteins remains a challenging task currently the scientific literature serves as the main source for yet uncurated functional annotations but curation work is slow and expensive automatic techniques that support this work are still lacking reliability we developed a method to identify conserved protein interaction graphs and to predict missing protein functions from orthologs in these graphs to enhance the precision of the results we furthermore implemented a procedure that validates all predictions based on findings reported in the literature results using this procedure more than of the go annotations for proteins with highly conserved orthologs that are available in uniprotkb swiss prot could be verified automatically for a subset of proteins we predicted new go annotations that were not available in uniprotkb swiss prot all predictions were correct precision according to the verifications from a trained curator conclusion our method of integrating ccss and literature mining is thus a highly reliable approach to predict go annotations for weakly characterized proteins orthologs
the formation of proteins into stable protein complexes plays a fundamental role in the operation of the cell the study of the degree of evolutionary conservation of protein complexes between species and the evolution of protein protein interactions has been hampered by lack of comprehensive coverage of the high throughput htp technologies that measure the interactome we show that new high throughput datasets on protein co purification in yeast have a substantially lower false negative rate than previous datasets when compared to known complexes these datasets are therefore more suitable to estimate the conservation of protein complex membership than hitherto possible we perform comparative genomics between curated protein complexes from human and the htp data in saccharomyces cerevisiae to study the evolution of co complex memberships this analysis revealed that out of the protein pairs that are part of the same complex in human are absent because both proteins lack an ortholog in s cerevisiae while for the co complex membership is disrupted because one of the two proteins lacks an ortholog for the remaining protein pairs only were never co purified in the large scale experiments this implies a conservation level of co complex membership of when the genes coding for the protein pairs that participate in the same protein complex are also conserved we conclude that the evolutionary dynamics of protein complexes are by and large not the result of network rewiring i e acquisition or loss of co complex memberships but mainly due to genomic acquisition or loss of genes coding for subunits we thus reveal evidence for the tight interrelation of genomic and evolution
in the analysis of the ethicalproblems of online research there is much tobe learned from the work that has already beendone on research ethics in the socialsciences and the humanities i discuss thestructure of norms in the norwegian ethicalguidelines for research in the social scienceswith respect to their relevance for the ethicalissues of internet research a four stepprocedure for the ethical evaluation ofresearch is suggested i argue that eventhough at one level the problems of onlineresearch are very similar to those we find intraditional areas of social scientificresearch there still are some issues that areunique to research online a general model forthe analysis of privacy and data protection issuggested this model is then used tocharacterize the special problems pertaining tothe protection of privacy in online contexts and to argue that one cannot assume a simpledistinction between the private and the publicwhen researching in contexts
midbrain dopamine neurons are activated when reward is greater than predicted and this error signal could teach target neurons both the value of reward and when it will occur we used the dopamine error signal to measure how the expectation of reward was distributed over time animals were trained with fixed duration intervals of s between conditioned stimulus onset and reward in contrast to the weak responses that have been observed after short intervals s activations to reward increased steeply and linearly with the logarithm of the interval results with varied stimulus reward intervals suggest that the neural expectation was substantial after just half an interval had elapsed thus the neural expectation of reward in these experiments was not highly precise and the precision declined sharply with interval duration the neural precision of expectation appeared to be at least qualitatively similar to the precision of anticipatory behavior
semantic search seems to be an elusive and fuzzy target to many researchers one of the reasons is that the task lies in between several areas of specialization in this extended abstract we review some of the ideas we have been investigating while approaching this problem first we present how we understand semantic search the web and the current challenges second how to use shallow semantics to improve web search third how the usage of search engines can capture the implicit semantics encoded in the queries and actions of people to conclude we discuss how these ideas can create virtuous feedback circuit for machine learning and ultimately search
we report on our findings regarding authors use of theory in articles that appeared in six information science is journals from our findings indicate that theory was discussed in of the articles theory incidents per article incidents per article when considering only those articles employing theory the majority of these theories were from the social sciences followed by is the sciences and humanities new is theories whencomparedwithpreviousstudies ourresultssuggestanincreaseintheuse of theory within is however clear discrepancies were evidentintermsofhowresearchersworkingindifferent subfields define theory results from citation analysis indicate that is theory is not heavily cited outside the field exceptbyisauthorspublishinginotherliteratures suggestions for further research are discussed background having atheory is today the mark of research seriousness and respectability theory is of course convenient and helps to organize and communicate unwieldy data and simplify the terrible complexities of the social world matters thatmaywellbemoreimportanttothefieldthanwhetheror not agiven theory is true of false van maanen p xxix it is a well known fact that is lacks good theories hjrland p theories may be expressed or represented in written and graphical form they may well inspire and guide practical achievements of aconcrete form yet atheory remains amental construct agood theory is one that matches well our perception of whatever the theory is about the closer the match the better the theory is buckland p working with conceptual frameworks and empirical research has never been easy p
online chat rooms are a novel communication medium that provide an opportunity to transform classroom learning in unexpected and powerful ways youth are a demographic of highly engaged core members of the always on crowd active users of the internet instant messaging video games and social networking sites numerous studies have documented how young people use instant messaging and online chat rooms in their personal lives some youth today perceive technologies to be entirely new and in the position of setting unprecedented opportunities for interactions online one high school student stated that i can t see how people in the past survived without digital media similarly another asserts that my generation those born in the early s are the first humans to be so profoundly impacted by today s new technology their familiarity with and enthusiasm for these tools suggests a valuable opportunity to examine how such communication media can be transferred into more formal educational settings to enable both formal and informal learning through student discussions and interactions online students can learn from one another through collaborative knowledge sharing while educators can use the tool to gain more insight into what and how their students are learning kyle a high school teenager from wyoming captures many of the most important factors in chat room use when says
abstractsince vassily v nalimov coined the term scientometrics in the this term has grown in popularity and is used to describe the study of science growth structure interrelationships and productivity scientometrics is related to and has overlapping interests with bibliometrics and informetrics the terms bibliometrics scientometrics and informetrics refer to component fields related to the study of the dynamics of disciplines as reflected in the production of their literature areas of study range from charting changes in the output of a scholarly field through time and across countries to the library collection problem of maintaining control of the output and to the low publication productivity of most researchers these terms are used to describe similar and overlapping methodologies the origins and historical survey of the development of each of these terms are presented profiles of the usage of each of these terms over time are presented using an appropriate subject category of databases on the dialog information service various definitions of each of the terms are provided from an examination of the literature the size of the overall literature of these fields is determined and the growth and stabilisation of both the dissertation and non dissertation literature are shown a listing of the top journals in the three fields are given as well as a list of the major reviews and bibliographies that have been published over years
abstract recent analyses of organizational change suggest a growing concern with the tempo of change understood as the characteristic rate rhythm or pattern of work or activity episodic change is contrasted with continuous change on the basis of implied metaphors of organizing analytic frameworks ideal organizations intervention theories and roles for change agents episodic change follows the sequence unfreeze transition refreeze whereas continuous change follows the sequence freeze rebalance unfreeze conceptualizations of inertia are seen to underlie the choice to view change as episodic continuous
organizational learning has many virtues virtues which recent writings in strategic management have highlighted learning processes however are subject to some important limitations as is well known learning has to cope with confusing experience and the complicated problem of balancing the competing goals of developing new knowledge i e exploring and exploiting current competencies in the face of dynamic tendencies to emphasize one or the other we examine the ways organizations approach these problems through simplification and specialization and how those approaches contribute to three forms of learning myopia the tendency to overlook distant times distant places and failures and we identify some ways in which organizations sustain exploration in the face of a tendency to overinvest in exploitation we conclude that the imperfections of learning are not so great as to require abandoning attempts to improve the learning capabilities of organizations but that those imperfections suggest a certain conservatism expectations
pseudo relevance feedback assumes that most frequent terms in the pseudo feedback documents are useful for the retrieval in this study we re examine this assumption and show that it does not hold in reality many expansion terms identified in traditional approaches are indeed unrelated to the query and harmful to the retrieval we also show that good expansion terms cannot be distinguished from bad ones merely on their distributions in the feedback documents and in the whole collection we then propose to integrate a term classification process to predict the usefulness of expansion terms multiple additional features can be integrated in this process our experiments on three trec collections show that retrieval effectiveness can be much improved when term classification is used in addition we also demonstrate that good terms should be identified directly according to their possible impact on the retrieval effectiveness i e using supervised learning instead of learning
as a social service inweb folksonomy provides the users the ability to save and organize their bookmarks online with social annotations or tags social annotations are high quality descriptors of the web pages topics as well as good indicators of web users interests we propose a personal ized search framework to utilize folksonomy for personalized search specifically three properties of folksonomy namely the categorization keyword and structure property are ex plored in the framework the rank of a web page is decided not only by the term matching between the query and the web pages content but also by the topic matching between the users interests and the web pages topics in the evalu ation we propose an automatic evaluation framework based on folksonomy data which is able to help lighten the com mon high cost in personalized search evaluations a series of experiments are conducted using two heterogeneous data sets one crawled from del icio us and the other from do gear extensive experimental results show that our person alized search approach can significantly improve the quality
we measured the elastic properties and intrinsic breaking strength of free standing monolayer graphene membranes by nanoindentation in an atomic force microscope the force displacement behavior is interpreted within a framework of nonlinear elastic stress strain response and yields second and third order elastic stiffnesses of newtons per meter n m minus and minus minus respectively the breaking strength is n m minus and represents the intrinsic strength of a defect free sheet these quantities correspond to a young s modulus of e terapascals third order elastic stiffness of d ndash terapascals and intrinsic strength of int gigapascals for bulk graphite these experiments establish graphene as the strongest material ever measured and show that atomically perfect nanoscale materials can be mechanically tested to deformations well beyond the linear regime copy the american association for the advancement science
survival can depend on the ability to change a current course of action to respond to potentially advantageous or threatening stimuli this reorienting response involves the coordinated action of a right hemisphere dominant ventral frontoparietal network that interrupts and resets ongoing activity and a dorsal frontoparietal network specialized for selecting and linking stimuli and responses at rest each network is distinct and internally correlated but when attention is focused the ventral network is suppressed to prevent reorienting to distracting events these different patterns of recruitment may reflect inputs to the ventral attention network from the locus coeruleus norepinephrine system while originally conceptualized as a system for redirecting attention from one object to another recent evidence suggests a more general role in switching between networks which may explain recent evidence of its involvement in functions such as cognition
interpretations of tf idf are based on binary independence retrieval poisson information theory and language modelling this paper contributes a review of existing interpretations and then tf idf is systematically related to the probabilities p q d and p d q two approaches are explored a space of independent and a space of disjoint terms for independent terms an extreme query non query term assumption uncovers tf idf and an analogy of p d q and the probabilistic odds o r d q mirrors relevance feedback for disjoint terms a relationship between probability theory and tf idf is established through the integral x d x log x this study uncovers components such as divergence from randomness and pivoted document length to be inherent parts of a document query independence dqi measure and interestingly an integral of the dqi over the term occurrence probability leads to idf
how do mobility and presence feature as aspects of social life using a case study of paroled offenders tracked via global positioning system gps we explore the ways that location based technologies frame people s everyday experiences of space in particular we focus on how access and presence are negotiated outside of traditional conceptions of privacy we introduce the notion of accountabilities of presence and suggest that it is a more useful concept than privacy for understanding the relationship between presence sociality
wireless networks are the fastest growing communications technology in history are mobile phones expressions of identity fashionable gadgets tools for lifeor all of the above mobile communication and society looks at how the possibility of multimodal communication from anywhere to anywhere at any time affects everyday life at home at work and at school and raises broader concerns about politics and culture both global and local drawing on data gathered from around the world the authors explore who has access to wireless technology and why and analyze the patterns of social differentiation seen in unequal access they explore the social effects of wireless communicationwhat it means for family life for example when everyone is constantly in touch or for the idea of an office when workers can work anywhere is the technological ability to multitask further compressing time in our already hurried existence the authors consider the rise of a mobile youth culture based on peer to peer networks with its own language of texting and its own values they examine the phenomenon of flash mobs and the possible political implications and they look at the relationship between communication and development and the possibility that developing countries could leapfrog directly to wireless and satellite technology this sweeping bookmoving easily in its analysis from the united states to china from europe to latin america and africaanswers the key questions about our transformation into a mobile society
current research on micromechanical resonators strives for quantum limited detection of the motion of macroscopic objects prerequisite to this goal is the observation of measurement backaction consistent with quantum metrology limits however thermal noise currently dominates measurements and precludes ground state preparation of the resonator here we establish the collective motion of an ultracold atomic gas confined tightly within a fabryperot optical cavity as a system for investigating the quantum mechanics of macroscopic bodies the cavity mode structure selects a particular collective vibrational motion that is measured by the cavity s optical properties actuated by the cavity optical field and subject to backaction by the quantum force fluctuations of this field experimentally we quantify such fluctuations by measuring the cavity light induced heating of the intracavity atomic ensemble these measurements represent the first observation of backaction on a macroscopic mechanical resonator at the quantumlimit
novel sequencing technologies permit the rapid production of large sequence data sets these technologies are likely to revolutionize genetics and biomedical research but a thorough characterization of the ultra short read output is necessary we generated and analyzed two illumina ultra short read data sets i e million reads from a beta vulgaris genomic clone and million from the helicobacter acinonychis genome we found that error rates range from at the beginning of reads to at the end of reads wrong base calls are frequently preceded by base g base substitution error frequencies vary by to fold with a c transversion being among the most frequent and c g transversions among the least frequent substitution errors insertions and deletions of single bases occur at very low rates when simulating re sequencing we found a fold sequencing coverage to be sufficient to compensate errors by correct reads the read coverage of the sequenced regions is biased the highest read density was found in intervals with elevated gc content high solexa quality scores are over optimistic and low scores underestimate the data quality our results show different types of biases and ways to detect them such biases have implications on the use and interpretation of solexa data for de novo sequencing re sequencing the identification of single nucleotide polymorphisms and dna methylation sites as well as for analysis
summary the cellml model repository provides free access to over biological models the vast majority of these models are derived from published peer reviewed papers model curation is an important and ongoing process to ensure the cellml model is able to accurately reproduce the published results as the cellml community grows and more people add their models to the repository model annotation will become increasingly important to facilitate data searches and information retrieval availability the cellml model repository is publicly accessible at http www cellml org models contact c lloyd auckland ac bioinformatics
isolates of salmonella enterica serovar typhi typhi a human restricted bacterial pathogen that causes typhoid show limited genetic variation we generated whole genome sequences for typhi isolates using roche and solexa illumina technologies isolates including the previously sequenced and isolates were selected to represent major nodes in the phylogenetic tree comparative analysis showed little evidence of purifying selection antigenic variation or recombination between isolates rather evolution in the typhi population seems to be characterized by ongoing loss of gene function consistent with a small effective population size the lack of evidence for antigenic variation driven by immune selection is in contrast to strong adaptive selection for mutations conferring antibiotic resistance in typhi the observed patterns of genetic isolation and drift are consistent with the proposed key role of asymptomatic carriers of typhi as the main reservoir of this pathogen highlighting the need for identification and treatment carriers
most diseases are the consequence of the breakdown of cellular processes but the relationships among genetic epigenetic defects the molecular interaction networks underlying them and the disease phenotypes remain poorly understood to gain insights into such relationships here we constructed a bipartite human disease association network in which nodes are diseases and two diseases are linked if mutated enzymes associated with them catalyze adjacent metabolic reactions we find that connected disease pairs display higher correlated reaction flux rate corresponding enzyme encoding gene coexpression and higher comorbidity than those that have no metabolic link between them furthermore the more connected a disease is to other diseases the higher is its prevalence and associated mortality rate the network topology based approach also helps to uncover potential mechanisms that contribute to their shared pathophysiology thus the structure and modeled function of the human metabolic network can provide insights into disease comorbidity with potentially important consequences for disease diagnosis prevention
tags are user generated labels for entities existing research on tag recommendation either focuses on improving its accuracy or on automating the process while ignoring the efficiency issue we propose a highly automated novel framework for real time tag recommendation the tagged training documents are treated as triplets of words docs tags and represented in two bipartite graphs which are partitioned into clusters by spectral recursive embedding sre tags in each topical cluster are ranked by our novel ranking algorithm a two way poisson mixture model pmm is proposed to model the document distribution into mixture components within each cluster and aggregate words into word clusters simultaneously a new document is classified by the mixture model based on its posterior probabilities so that tags are recommended according to their ranks experiments on large scale tagging datasets of scientific documents citeulike and web pages del icio us indicate that our framework is capable of making tag recommendation efficiently and effectively the average tagging time for testing a document is around second with over test documents correctly labeled with the top nine tags suggested
online communities have become popular for publishing and searching content as well as for finding and connecting to other users user generated content includes for example personal blogs bookmarks and digital photos these items can be annotated and rated by different users and these social tags and derived user specific scores can be leveraged for searching relevant content and discovering subjectively interesting items moreover the relationships among users can also be taken into consideration for ranking search results the intuition being that you trust the recommendations of your close friends more than those of your casual acquaintances queries for tag or keyword combinations that compute and rank the top k results thus face a large variety of options that complicate the query processing and pose efficiency challenges this paper addresses these issues by developing an incremental top k algorithm with two dimensional expansions social expansion considers the strength of relations among users and semantic expansion considers the relatedness of different tags it presents a new algorithm based on principles of threshold algorithms by folding friends and related tags into the search space in an incremental on demand manner the excellent performance of the method is demonstrated by an experimental evaluation on three real world datasets crawled from deli cio us flickr librarything
in this paper we look at the social tag prediction prob lem given a set of objects and a set of tags applied to those objects by users can we predict whether a given tag could should be applied to a particular object we inves tigated this question using one of the largest crawls of the social bookmarking system del icio us gathered to date for urls in del icio us we predicted tags based on page text anchor text surrounding hosts and other tags applied to the url we found an entropy based metric which captures the generality of a particular tag and informs an analysis of how well that tag can be predicted we also found that tag based association rules can produce very high precision predictions as well as giving deeper understanding into the relationships between tags our results have implications for both the study of tagging systems as potential information retrieval tools and for the design of systems
a growing number of common traumatic events involve both physical and emotional injuries in contrast to previously held beliefs the rapidly growing body of literature shows quite convincingly that physical injury over and above exposure to the traumatic event itself increases rather than decreases the risk for posttraumatic stress disorder ptsd a pertinent question becomes how bodily injury contributes to the risk of developing ptsd in this article we review contemporary findings regarding the neurobiological and psychological mechanisms by which bodily injury may augment or independently contribute to chronic posttraumatic stress in addition we propose three theoretical pathways through which physical injury can increase the risk for ptsd these pathways are additive unique and recovery impeding finally we highlight unresolved issues pertaining to each one of these pathways and propose directions for future research to them
motivation the effort to build a whole cell model requires the development of new modeling approaches and in particular the integration of models for different types of processes each of which may be best described using different representation flux balance analysis fba has been useful for large scale analysis of metabolic networks and methods have been developed to incorporate transcriptional regulation regulatory fba or rfba of current interest is the integration of these approaches with detailed models based on ordinary differential equations odes results we developed an approach to modeling the dynamic behavior of metabolic regulatory and signaling networks by combining fba with regulatory boolean logic and ordinary differential equations we use this approach called integrated fba or ifba to create an integrated model of escherichia coli which combines a flux balance based central carbon metabolic and transcriptional regulatory model with an ode based detailed model of carbohydrate uptake control we compare the predicted escherichia coli wild type and single gene perturbation phenotypes for diauxic growth on glucose lactose and glucose glucose phosphate with that of the individual models we find that ifba encapsulates the dynamics of three internal metabolites and three transporters inadequately predicted by rfba furthermore we find that ifba predicts different and more accurate phenotypes than the ode model for of single gene perturbation simulations as well for the wild type simulations we conclude that ifba is a significant improvement over the individual rfba and ode modeling paradigms availability all matlab files used in this study are available at http www simtk org home ifba contact covert stanford edu supplementary information supplementary data are available at bioinformatics bioinformatics
electrophysiological data measured by electroencephalography and magnetoencephalography meg are widely used to investigate human brain activity in various cognitive tasks this is typically done by characterizing event related potentials fields or modulations of oscillatory activity e g event related synchronization in response to cognitively relevant stimuli here we provide a link between the two phenomena an essential component of our theory is that peaks and troughs of oscillatory activity fluctuate asymmetrically e g peaks are more strongly modulated than troughs in response to stimuli as a consequence oscillatory brain activity will not average out when multiple trials are averaged using meg we demonstrate that such asymmetric amplitude fluctuations of the oscillatory alpha rhythm explain the generation of slow event related fields furthermore we provide a physiological explanation for the observed asymmetric amplitude fluctuations in particular slow event related components are modulated by a wide range of cognitive tasks hence our findings provide new insight into the physiological basis of cognitive modulation in event related brain jneurosci
animal micrornas mirnas regulate gene expression by inhibiting translation and or by inducing degradation of target messenger rnas it is unknown how much translational control is exerted by mirnas on a genome wide scale we used a new proteomic approach to measure changes in synthesis of several thousand proteins in response to mirna transfection or endogenous mirna knockdown in parallel we quantified mrna levels using microarrays here we show that a single mirna can repress the production of hundreds of proteins but that this repression is typically relatively mild a number of known features of the mirna binding site such as the seed sequence also govern repression of human protein synthesis and we report additional target sequence characteristics we demonstrate that in addition to downregulating mrna levels mirnas also directly repress translation of hundreds of genes finally our data suggest that a mirna can by direct or indirect effects tune protein synthesis from thousands genes
micrornas are endogenous approximately nucleotide rnas that can pair to sites in the messenger rnas of protein coding genes to downregulate the expression from these messages micrornas are known to influence the evolution and stability of many mrnas but their global impact on protein output had not been examined here we use quantitative mass spectrometry to measure the response of thousands of proteins after introducing micrornas into cultured cells and after deleting mir in mouse neutrophils the identities of the responsive proteins indicate that targeting is primarily through seed matched sites located within favourable predicted contexts in untranslated regions hundreds of genes were directly repressed albeit each to a modest degree by individual micrornas although some targets were repressed without detectable changes in mrna levels those translationally repressed by more than a third also displayed detectable mrna destabilization and for the more highly repressed targets mrna destabilization usually comprised the major component of repression the impact of micrornas on the proteome indicated that for most interactions micrornas act as rheostats to make fine scale adjustments to output
the rapid expansion of networking capabilities and growing potential of access to such facilities is stimulating an exponential growth in the interest to develop technological resources to facilitate and enhance the learning experience within higher education thus educational institutions are increasingly being encouraged to experiment with tools that promote collaborative working which are in turn perceived to help in the development of more autonomous responsible learners this paper therefore seeks briefly to explore the theoretical underpinnings that usually prompt the adoption of such tools as asynchronous computer conferencing acc technology for collaborative working in an educational environment the research will then go on to question the traditional approach of the moderated implementation of such technology as well as reporting on some findings gained from fieldwork studies undertaken with campus based undergraduates using acc for supporting computer supported collaborative learning as an integral part of their learning experience within a higher environment
background the distributed annotation system das is a widely adopted protocol for dynamically integrating a wide range of biological data from geographically diverse sources das continues to expand its applicability and evolve in response to new challenges facing integrative bioinformatics results here we describe the various infrastructure components of das and present a new extended version of the das specification version incorporates several recent developments including its extension to serve new data types and an ontology for protein features conclusion our extensions to the das protocol have facilitated the integration of new data types and our improvements to the existing das infrastructure have addressed recent challenges the steadily increasing numbers of available data sources demonstrates further adoption of the das protocol jenkinson et al licensee biomed ltd
humans host complex microbial communities believed to contribute to health maintenance and when in imbalance to the development of diseases determining the microbial composition in patients and healthy controls may thus provide novel therapeutic targets for this purpose high throughput cost effective methods for microbiota characterization are needed we have employed pyrosequencing of a hyper variable region of the rrna gene in combination with sample specific barcode sequences which enables parallel in depth analysis of hundreds of samples with limited sample processing in silico modeling demonstrated that the method correctly describes microbial communities down to phylotypes below the genus level here we applied the technique to analyze microbial communities in throat stomach and fecal samples our results demonstrate the applicability of barcoded pyrosequencing as a high throughput method for comparative ecology
background previous studies of named entity recognition have shown that a reasonable level of recognition accuracy can be achieved by using machine learning models such as conditional random fields or support vector machines however the lack of training data i e annotated corpora makes it difficult for machine learning based named entity recognizers to be used in building practical information extraction systems results this paper presents an active learning like framework for reducing the human effort required to create named entity annotations in a corpus in this framework the annotation work is performed as an iterative and interactive process between the human annotator and a probabilistic named entity tagger unlike active learning our framework aims to annotate all occurrences of the target named entities in the given corpus so that the resulting annotations are free from the sampling bias which is inevitable in active learning approaches conclusion we evaluate our framework by simulating the annotation process using two named entity corpora and show that our approach can reduce the number of sentences which need to be examined by the human annotator the cost reduction achieved by the framework could be drastic when the target named entities sparse
purpose this paper reviews the research literature on electronic health record ehr systems the aim is to find out how electronic health records are defined how the structure of these records is described in what contexts ehrs are used who has access to ehrs which data components of the ehrs are used and studied what is the purpose of research in this field what methods of data collection have been used in the studies reviewed and what are the results of these studies methods a systematic review was carried out of the research dealing with the content of ehrs a literature search was conducted on four electronic databases pubmed medline cinalh eval and cochrane results the concept of ehr comprised a wide range of information systems from files compiled in single departments to longitudinal collections of patient data only very few papers offered descriptions of the structure of ehrs or the terminologies used ehrs were used in primary secondary and tertiary care data were recorded in ehrs by different groups of health care professionals secretarial staff also recorded data from dictation or nurses or physicians manual notes some information was also recorded by patients themselves this information is validated by physicians it is important that the needs and requirements of different users are taken into account in the future development of information systems several data components were documented in ehrs daily charting medication administration physical assessment admission nursing note nursing care plan referral present complaint e g symptoms past medical history life style physical examination diagnoses tests procedures treatment medication discharge history diaries problems findings and immunization in the future it will be necessary to incorporate different kinds of standardized instruments electronic interviews and nursing documentation systems in ehr systems the aspects of information quality most often explored in the studies reviewed were the completeness and accuracy of different data components it has been shown in several studies that the use of an information system was conducive to more complete and accurate documentation by health care professionals the quality of information is particularly important in patient care but ehrs also provide important information for secondary purposes such as health policy planning conclusion studies focusing on the content of ehrs are needed especially studies of nursing documentation or patient self documentation one future research area is to compare the documentation of different health care professionals with the core information about ehrs which has been determined in national health projects the challenge for ongoing national health record projects around the world is to take into account all the different types of ehrs and the needs and requirements of different health care professionals and consumers in the development of ehrs a further challenge is the use of international terminologies in order to achieve interoperability
next generation sequencing is limited to short read lengths and by high error rates we systematically analyzed sources of noise in the illumina genome analyzer that contribute to these high error rates and developed a base caller alta cyclic that uses machine learning to compensate for noise factors alta cyclic substantially improved the number of accurate reads for sequencing runs up to bases and reduced systematic biases facilitating confident identification of variants
natural selection dictates that cells constantly adapt to dynamically changing environments in a context dependent manner gene regulatory networks often mediate the cellular response to and an understanding of cellular adaptation will require experimental approaches aimed at subjecting cells to a dynamic environment that mimics their natural here we monitor the response of saccharomyces cerevisiae metabolic gene regulation to periodic changes in the external carbon source by using a microfluidic platform that allows precise dynamic control over environmental conditions we show that the metabolic system acts as a low pass filter that reliably responds to a slowly changing environment while effectively ignoring fast fluctuations the sensitive low frequency response was significantly faster than in predictions arising from our computational modelling and this discrepancy was resolved by the discovery that two key galactose transcripts possess half lives that depend on the carbon source finally to explore how induction characteristics affect frequency response we compare two s cerevisiae strains and show that they have the same frequency response despite having markedly different induction properties this suggests that although certain characteristics of the complex networks may differ when probed in a static environment the system has been optimized for a robust response to a dynamically environment
this pioneering text describes a rationally consistent basis for instructional design based in cognitive psychology and information processing theory the authors prepare teachers to design and develop a course unit and module of instruction outline the nine stages of instructional design procedure and integrate current research and practice in the movement toward performance systems technology the fifth edition of principles of instructional design emphasizes the social and cultural context of learning learner centered principles and the affordances of new technologies and environments
motivation epigenetic modifications are one of the critical factors to regulate gene expression and genome function among different epigenetic modifications the differential histone modification sites dhmss are of great interest to study the dynamic nature of epigenetic and gene expression regulations among various cell types stages or environmental responses to capture the histone modifications at whole genome scale chip seq technology is becoming a robust and comprehensive approach thus the dhmss are potentially identifiable by comparing two chip seq libraries however little has been addressed on this issue in literature results aiming at identifying dhmss we propose an approach called chipdiff for the genome wide comparison of histone modification sites identified by chip seq based on the observations of chip fragment counts the proposed approach employs a hidden markov model hmm to infer the states of histone modification changes at each genomic location we evaluated the performance of chipdiff by comparing the modification sites between mouse embryonic stem cell esc and neural progenitor cell npc we demonstrated that the dhmss identified by our approach are of high sensitivity specificity and technical reproducibility chipdiff was further applied to uncover the differential and sites between different cell states interesting biological discoveries were achieved from such comparison in our study availability http cmb gis a star edu sg chipseq tools htm contact asflin ntu edu sg sungk gis a star edu sg supplementary information supplementary methods and data are available at bioinformatics bioinformatics
the advent of multicore cpus and manycore gpus means that mainstream processor chips are now parallel systems furthermore their parallelism continues to scale with moore s law the challenge is to develop mainstream application software that transparently scales its parallelism to leverage the increasing number of processor cores much as graphics applications transparently scale their parallelism to manycore gpus with widely varying numbers cores
genome wide scans for positively selected genes psgs in mammals have provided insight into the dynamics of genome evolution the genetic basis of differences between species and the functions of individual genes however previous scans have been limited in power and accuracy owing to small numbers of available genomes here we present the most comprehensive examination of mammalian psgs to date using the six high coverage genome assemblies now available for eutherian mammals the increased phylogenetic depth of this dataset results in substantially improved statistical power and permits several new lineage and clade specific tests to be applied of human genes with high confidence orthologs in at least two other species genes showed significant evidence of positive selection fdr according to a standard likelihood ratio test an additional genes showed evidence of positive selection on particular lineages or clades as in previous studies the identified psgs were enriched for roles in defense immunity chemosensory perception and reproduction but enrichments were also evident for more specific functions such as complement mediated immunity and taste perception several pathways were strongly enriched for psgs suggesting possible co evolution of interacting genes a novel bayesian analysis of the possible selection histories of each gene indicated that most psgs have switched multiple times between positive selection and nonselection suggesting that positive selection is often episodic a detailed analysis of affymetrix exon array data indicated that psgs are expressed at significantly lower levels and in a more tissue specific manner than non psgs genes that are specifically expressed in the spleen testes liver and breast are significantly enriched for psgs but no evidence was found for an enrichment for psgs among brain specific genes this study provides additional evidence for widespread positive selection in mammalian evolution and new genome wide insights into the functional implications of selection
objective to measure the effect of free access to the scientific literature on article downloads and citations design randomised controlled trial setting journals published by the american physiological society participants research articles and reviews main outcome measures article readership measured as downloads of full text pdfs and abstracts and number of unique visitors internet protocol addresses citations to articles were gathered from the institute for scientific information after one year interventions random assignment on online publication of articles published in scientific journals to open access treatment or subscription access control results articles assigned to open access were associated with more full text downloads confidence interval to more pdf downloads to and more unique visitors to but fewer abstract downloads to than subscription access articles in the first six months after publication open access articles were no more likely to be cited than subscription access articles in the first year after publication fifty nine per cent of open access articles of were cited nine to months after publication compared with of of subscription access articles logistic and negative binomial regression analysis of article citation counts confirmed no citation advantage for open access articles conclusions open access publishing may reach more readers than subscription access publishing no evidence was found of a citation advantage for open access articles in the first year after publication the citation advantage from open access reported widely in the literature may be an artefact of other bmj
during the past decade the interaction of light with multiatom ensembles has attracted much attention as a basic building block for quantum information processing and quantum state engineering the field started with the realization that optically thick free space ensembles can be efficiently interfaced with quantum optical fields by now the atomic ensemble light interfaces have become a powerful alternative to the cavity enhanced interaction of light with single atoms various mechanisms used for the quantum interface are discussed including quantum nondemolition or faraday interaction quantum measurement and feedback raman interaction photon echo and electromagnetically induced transparency this review provides a common theoretical frame for these processes describes basic experimental techniques and media used for quantum interfaces and reviews several key experiments on quantum memory for light quantum entanglement between atomic ensembles and light and quantum teleportation with atomic ensembles the two types of quantum measurements which are most important for the interface are discussed homodyne detection and photon counting this review concludes with an outlook on the future of atomic ensembles as an enabling technology in quantum processing
organizational researchers cite simon and polanyi without adequately acknowledging and dealing with the disparities in their perspectives on rationality and knowledge this study explains the views of simon and polanyi and traces their disparities to differing underlying philosophical perspectives simon s research emphasized cognition over action explicit knowledge over tacit knowledge mechanistic information processing over human judgment and means over ends polanyi provides counterbalancing emphases for each of these orientations in his explanation of skillful performance the latter portion of this study draws conclusions about the perspectives of simon and polanyi and identifies implications for research on rationality and management
many scientists lack the background to fully utilize the wealth of solved three dimensional biomacromolecule structures thus a resource is needed to present structure function information in a user friendly manner to a broad scientific audience proteopedia http www proteopedia org is an interactive wiki web resource whose pages have embedded three dimensional structures surrounded by descriptive text containing hyperlinks that change the appearance view representations colors labels of the adjacent three dimensional structure to reflect the concept explained in text
motivation functional characterization of genes is of great importance for the understanding of complex cellular processes valuable information for this purpose can be obtained from pathway databases like kegg however only a small fraction of genes is annotated with pathway information up to now in contrast information on contained protein domains can be obtained for a significantly higher number of genes e g from the interpro database results we present a classification model which for a specific gene of interest can predict the mapping to a kegg pathway based on its domain signature the classifier makes explicit use of the hierarchical organization of pathways in the kegg database furthermore we take into account that a specific gene can be mapped to different pathways at the same time the classification method produces a scoring of all possible mapping positions of the gene in the kegg hierarchy evaluations of our model which is a combination of a svm and ranking perceptron approach show a high prediction performance moreover for signaling pathways we reveal that it is even possible to forecast accurately the membership to individual pathway components availability the r package is a supplement to this paper contact h froehlich dkfz heidelberg de t beissbarth dkfz heidelberg bioinformatics
forward checking fc is a highly regarded complete search algorithm used to solve constraint satisfaction problems in this paper a lazy variant of fc called minimal forward checking mfc is introduced mfc is a natural marriage of incremental fc and backchecking given a variable selection heuristic which does not depend on domain size mfc s worst case performance on any csp instance is the number of constraint checks performed by fc experiments using hard random problems are presented which show that mfc outperforms fc especially for problems with large domain sizes and or a large number of variables introduction many problems in artificial intelligence and operations research can be expressed as constraint satisfaction problems csps a csp is represented with a set of variables a set of finite discrete domains for those variables and a set of constraints over those variables in this paper we restrict our attention to binary csp s where all the constraints o
the centrality lethality rule which notes that high degree nodes in a protein interaction network tend to correspond to proteins that are essential suggests that the topological prominence of a protein in a protein interaction network may be a good predictor of its biological importance even though the correlation between degree and essentiality was confirmed by many independent studies the reason for this correlation remains illusive several hypotheses about putative connections between essentiality of hubs and the topology of proteinprotein interaction networks have been proposed but as we demonstrate these explanations are not supported by the properties of protein interaction networks to identify the main topological determinant of essentiality and to provide a biological explanation for the connection between the network topology and essentiality we performed a rigorous analysis of six variants of the genomewide protein interaction network for saccharomyces cerevisiae obtained using different techniques we demonstrated that the majority of hubs are essential due to their involvement in essential complex biological modules a group of densely connected proteins with shared biological function that are enriched in essential proteins moreover we rejected two previously proposed explanations for the centrality lethality rule one relating the essentiality of hubs to their role in the overall network connectivity and another relying on the recently published essential protein model
whole genome sequencing is a powerful technique for obtaining the reference sequence information of multiple organisms its use can be dramatically expanded to rapidly identify genomic variations which can be linked with phenotypes to obtain biological insights we explored these potential applications using the emerging next generation sequencing platform solexa genome analyzer and the well characterized model bacterium bacillus subtilis combining sequencing with experimental verification we first improved the accuracy of the published sequence of the b subtilis reference strain then obtained sequences of multiple related laboratory strains and different isolates of each strain this provides a framework for comparing the divergence between different laboratory strains and between their individual isolates we also demonstrated the power of solexa sequencing by using its results to predict a defect in the citrate signal transduction pathway of a common laboratory strain which we verified experimentally finally we examined the molecular nature of spontaneously generated mutations that suppress the growth defect caused by deletion of the stringent response mediator rela using whole genome sequencing we rapidly mapped these suppressor mutations to two small homologs of rela interestingly stable suppressor strains had mutations in both genes with each mutation alone partially relieving the rela growth defect this supports an intriguing three locus interaction module that is not easily identifiable through traditional suppressor mapping we conclude that whole genome sequencing can drastically accelerate the identification of suppressor mutations and complex genetic interactions and it can be applied as a standard tool to investigate the genetic traits of organisms
pnas algorithms for finding structure in data have become increasingly important both as tools for scientific data analysis and as models of human learning yet they suffer from a critical limitation scientists discover qualitatively new forms of structure in observed data for instance linnaeus recognized the hierarchical organization of biological species and mendeleev recognized the periodic structure of the chemical elements analogous insights play a pivotal role in cognitive development children discover that object category labels can be organized into hierarchies friendship networks are organized into cliques and comparative relations e g bigger than or better than respect a transitive order standard algorithms however can only learn structures of a single form that must be specified in advance for instance algorithms for hierarchical clustering create tree structures whereas algorithms for dimensionality reduction create low dimensional spaces here we present a computational model that learns structures of many different forms and that discovers which form is best for a given dataset the model makes probabilistic inferences over a space of graph grammars representing trees linear orders multidimensional spaces rings dominance hierarchies cliques and other forms and successfully discovers the underlying structure of a variety of physical biological and social domains our approach brings structure learning methods closer to human abilities and may lead to a deeper computational understanding of development
this editorial lead article for the i journal of location based services i surveys this complex and multi disciplinary field and identifies the key research issues although this field has produced early commercial disappointments the inevitability that pervasive location aware services on mobile devices will emerge means that much research is needed to inform these developments the article reviews firstly the science and technology of positioning geographic information science mobile cartography spatial cognition and interfaces information science ubiquitous computing and secondly the business content and legal social and ethics aspects before synthesising the key issues for this field
the aim of the present paper is to describe the role played by three dimensional d virtual worlds in ehealth applications addressing some potential advantages and issues related to the use of this emerging medium in clinical practice due to the enormous diffusion of the world wide web www telepsychology and telehealth in general have become accepted and validated methods for the treatment of many different health care concerns the introduction of the web has facilitated the development of new forms of collaborative interaction between multiple users based on d virtual worlds this paper describes the development and implementation of a form of tailored immersive e therapy called p health whose key factor is interreality that is the creation of a hybrid augmented experience merging physical and virtual worlds we suggest that compared with conventional telehealth applications such as emails chat and videoconferences the interaction between real and d virtual worlds may convey greater feelings of presence facilitate the clinical communication process positively influence group processes and cohesiveness in group based therapies and foster higher levels of interpersonal trust between therapists and patients however challenges related to the potentially addictive nature of such virtual worlds and questions related to privacy and personal safety will also discussed
this paper presents a many core visual computing architecture code named larrabee a new software rendering pipeline a manycore programming model and performance analysis for several applications larrabee uses multiple in order cpu cores that are augmented by a wide vector processor unit as well as some fixed function logic blocks this provides dramatically higher performance per watt and per unit of area than out of order cpus on highly parallel workloads it also greatly increases the flexibility and programmability of the architecture as compared to standard gpus a coherent on die nd level cache allows efficient inter processor communication and high bandwidth local data access by cpu cores task scheduling is performed entirely with software in larrabee rather than in fixed function logic the customizable software graphics rendering pipeline for this architecture uses binning in order to reduce required memory bandwidth minimize lock contention and increase opportunities for parallelism relative to standard gpus the larrabee native programming model supports a variety of highly parallel applications that use irregular data structures performance analysis on those applications demonstrates larrabee s potential for a broad range of computation
the fast multipole method allows the rapid approximate evaluation of sums of radial basis functions for a specified accuracy e the method scales as o n in both time and memory compared to the direct method with complexity o n which allows the solution of larger problems with given resources graphical processing units gpu are now increasingly viewed as data parallel compute coprocessors that can provide significant computational performance at low price we describe acceleration of the fmm using the data parallel gpu architecture the fmm has a complex hierarchical adaptive structure which is not easily implemented on data parallel processors we described strategies for parallelization of all components of the fmm develop a model to explain the performance of the algorithm on the gpu architecture and determined optimal settings for the fmm on the gpu these optimal settings are different from those on usual cpus some innovations in the fmm algorithm including the use of modified stencils real polynomial basis functions for the laplace kernel and decompositions of the translation operators are also described we obtained accelerations of the laplace kernel fmm on a single nvidia geforce gtx gpu in the range of compared to a serial cpu fmm implementation for a problem with a million sources the summations involved are performed in approximately one second this performance is equivalent to solving of the same problem at a teraflop rate if we use summation
an thorough introduction is given at an introductory level to the field of quantitative complex system science with special emphasis on emergence in dynamical systems based on network topologies subjects treated include graph theory and small world networks a generic introduction to the concepts of dynamical system theory random boolean networks cellular automata and self organized criticality the statistical modeling of darwinian evolution synchronization phenomena and an introduction to the theory of systems
these lectures deal with the problem of inductive inference that is the problem of reasoning under conditions of incomplete information is there a general method for handling uncertainty or at least are there rules that could in principle be followed by an ideally rational mind when discussing scientific matters what makes one statement more plausible than another how much more plausible and then when new information is acquired how do we change our minds or to put it differently are there rules for learning are there rules for processing information that are objective and consistent are they unique and come to think of it what after all is information it is clear that data contains or conveys information but what does this precisely mean can information be conveyed in other ways is information physical can we measure amounts of information do we need to our goal is to develop the main tools for inductive inference probability and entropy from a thoroughly bayesian point of view and to illustrate their use in physics with examples borrowed from the foundations of classical physics
background the term web became popular following the oreilly media web conference in however there are difficulties in its application to health and medicine principally the definition published by oreilly is criticized for being too amorphous where other authors claim that web does not really exist despite this skepticism the online community using web tools for health continues to grow and the term medicine has entered popular nomenclature objective this paper aims to establish a clear definition for medicine and delineate literature that is specific to the field in addition we propose a framework for categorizing the existing medicine literature and identify key research themes underdeveloped research areas as well as the underlying tensions or controversies in medicine diverse interest groups methods in the first phase we employ a thematic analysis of online definitions that is the most important linked papers websites or blogs in the medicine community itself in a second phase this definition is then applied across a series of academic papers to review medicine core literature base delineating it from a wider concept of ehealth results the terms medicine and health were found to be very similar and subsume five major salient themes the participants involved doctors patients etc its impact on both traditional and collaborative practices in medicine its ability to provide personalized health care its ability to promote ongoing medical education and its associated method and tool related issues such as potential inaccuracy in enduser generated content in comparing definitions of medicine to ehealth key distinctions are made by the collaborative nature of medicine and its emphasis on personalized health care however other elements such as health or medical education remain common for both categories in addition this emphasis on personalized health care is not a salient theme within the academic literature of papers originally identified as potentially relevant we found articles that were exclusively focused on medicine as opposed to wider ehealth discussions four major tensions or debates between stakeholders were found in this literature including the lack of clear medicine definitions tension due to the loss of control over information as perceived by doctors the safety issues of inaccurate information and ownership and privacy issues with the growing body of information created by medicine conclusion this paper is distinguished from previous reviews in that earlier studies mainly introduced specific medicine tools in addressing the fields definition via empirical online data it establishes a literature base and delineates key topics for future research into medicine distinct to that ehealth
the coupling of mechanical and optical degrees of freedom via radiation pressure has been a subject of early research in the context of gravitational wave detection recent experimental advances have allowed studying for the first time the modifications of mechanical dynamics provided by radiation pressure this paper reviews the consequences of back action of light confined in whispering gallery dielectric microcavities and presents a unified treatment of its two manifestations notably the parametric instability mechanical amplification and oscillation and radiation pressure back action cooling parametric instability offers a novel photonic clock which is driven purely by the pressure of light in contrast radiation pressure cooling can surpass existing cryogenic technologies and offers cooling to phonon occupancies below unity and provides a route towards cavity optomechanics
biological systems are robust in that they can maintain stable phenotypes under varying conditions or attacks biological systems are also complex being organized into many functional modules that communicate through interlocking pathways and feedback mechanisms in these systems robustness and complexity are linked because both qualities arise from the same underlying mechanisms when perturbed by multiple attacks such complex systems become fragile in both theoretical and experimental studies and this fragility depends on the number of agents applied we explore how this relationship can be used to study the functional robustness of a biological system using systematic high order combination experiments this presents a promising approach toward many biomedical and bioengineering challenges for example high order experiments could determine the point of fragility for pathogenic bacteria and might help identify optimal treatments against multi drug resistance such studies would also reinforce the growing appreciation that biological systems are best manipulated not by targeting a single protein but by modulating the set of many nodes that can selectively control a system s state
crowded intracellular environments present a challenge for proteins to form functional specific complexes while reducing non functional interactions with promiscuous non functional partners here we show how the need to minimize the waste of resources to non functional interactions limits the proteome diversity and the average concentration of co expressed and co localized proteins using the results of high throughput yeast hybrid experiments we estimate the characteristic strength of non functional proteinprotein interactions by combining these data with the strengths of specific interactions we assess the fraction of time proteins spend tied up in non functional interactions as a function of their overall concentration this allows us to sketch the phase diagram for baker s yeast cells using the experimentally measured concentrations and subcellular localization of their proteins the positions of yeast compartments on the phase diagram are consistent with our hypothesis that the yeast proteome has evolved to operate closely to the upper limit of its size whereas keeping individual protein concentrations sufficiently low to reduce non functional interactions these findings have implication for conceptual understanding of intracellular compartmentalization multicellularity differentiation
large scale genetic interaction studies provide the basis for defining gene function and pathway architecture recent advances in the ability to generate double mutants en masse in saccharomyces cerevisiae have dramatically accelerated the acquisition of genetic interaction information and the biological inferences that follow here we describe a method based on f factor driven conjugation which allows for high throughput generation of double mutants in escherichia coli this method termed genetic interaction analysis technology for e coli giant coli permits us to systematically generate and array double mutant cells on solid media in high density arrays we show that colony size provides a robust and quantitative output of cellular fitness and that giant coli can recapitulate known synthetic interactions and identify previously unidentified negative synthetic sickness or lethality and positive suppressive or epistatic relationships finally we describe a complementary strategy for genome wide suppressor mutant identification together these methods permit rapid large scale genetic interaction studies in coli
chip seq which combines chromatin immunoprecipitation chip with ultra high throughput massively parallel sequencing is increasingly being used for mapping protein dna interactions in vivo on a genome scale typically short sequence reads from chip seq are mapped to a reference genome for further analysis although genomic regions enriched with mapped reads could be inferred as approximate binding regions short read lengths approximately nt pose challenges for determining the exact binding sites within these regions here we present sissrs site identification from short sequence reads a novel algorithm for precise identification of binding sites from short reads generated from chip seq experiments the sensitivity and specificity of sissrs are demonstrated by applying it on chip seq data for three widely studied and well characterized human transcription factors ctcf ccctc binding factor nrsf neuron restrictive silencer factor and signal transducer and activator of transcription protein we identified and binding sites for ctcf nrsf and proteins respectively which is and more than that inferred previously for the respective proteins motif analysis revealed that an overwhelming majority of the identified binding sites contained the previously established consensus binding sequence for the respective proteins thus attesting for sissrs accuracy sissrs sensitivity and precision facilitated further analyses of chip seq data revealing interesting insights which we believe will serve as guidance for designing chip seq experiments to map in vivo protein dna interactions we also show that tag densities at the binding sites are a good indicator of protein dna binding affinity which could be used to distinguish and characterize strong and weak binding sites using tag density as an indicator of dna binding affinity we have identified core residues within the nrsf and ctcf binding sites that are critical for a stronger binding
identification of lineage specific innovations in genomic control elements is critical for understanding transcriptional regulatory networks and phenotypic heterogeneity we analyzed from an evolutionary perspective the binding regions of seven mammalian transcription factors myc rela and ctcf identified on a genome wide scale by different chromatin immunoprecipitation approaches and found that only a minority of sites appear to be conserved at the sequence level instead we uncovered a pervasive association with genomic repeats by showing that a large fraction of the bona fide binding sites for five of the seven transcription factors and ctcf are embedded in distinctive families of transposable elements using the age of the repeats we established that these repeat associated binding sites rabs have been associated with significant regulatory expansions throughout the mammalian phylogeny we validated the functional significance of these rabs by showing that they are over represented in proximity of regulated genes and that the binding motifs within these repeats have undergone evolutionary selection our results demonstrate that transcriptional regulatory networks are highly dynamic in eukaryotic genomes and that transposable elements play an important role in expanding the repertoire of sites
abstract background there has been a dramatic increase in the amount of quantitative data derived from the measurement of changes at different levels of biological complexity during the post genomic era however there are a number of issues associated with the use of computational tools employed for the analysis of such data for example computational tools such as r and matlab require prior knowledge of their programming languages in order to implement statistical analyses on data combining two or more tools in an analysis may also be problematic since data may have to be manually copied and pasted between separate user interfaces for each tool furthermore this transfer of data may require a reconciliation step in order for there to be interoperability between computational tools results developments in the taverna workflow system have enabled pipelines to be constructed and enacted for generic and ad hoc analyses of quantitative data here we present an example of such a workflow involving the statistical identification of differentially expressed genes from microarray data followed by the annotation of their relationships to cellular processes this workflow makes use of customised maxdbrowse web services a system that allows taverna to query and retrieve gene expression data from the microarray database these data are then analysed by r to identify differentially expressed genes using the taverna rshell processor which has been developed for invoking this tool when it has been deployed as a service using the rserve library in addition the workflow uses beanshell scripts to reconcile mismatches of data between services as well as to implement a form of user interaction for selecting subsets of microarray data for analysis as part of the workflow execution a new plugin system in the taverna software architecture is demonstrated by the use of renderers for displaying pdf files and csv formatted data within the taverna workbench conclusions taverna can be used by data analysis experts as a generic tool for composing ad hoc analyses of quantitative data by combining the use of scripts written in the r programming language with tools exposed as services in workflows when these workflows are shared with colleagues and the wider scientific community they provide an approach for other scientists wanting to use tools such as r without having to learn the corresponding programming language to analyse their data
biological gene networks appear to be dynamically robust to mutation stochasticity and changes in the environment and also appear to be sparsely connected studies with computational models however have suggested that denser gene networks evolve to be more dynamically robust than sparser networks we resolve this discrepancy by showing that misassumptions about how to measure robustness in artificial networks have inadvertently discounted the costs of network complexity we show that when the costs of complexity are taken into account that robustness implies a parsimonious network structure that is sparsely connected and not unnecessarily complex and that selection will favor sparse networks when network topology is free to evolve because a robust system of heredity is necessary for the adaptive evolution of complex phenotypes the maintenance of frugal network complexity is likely a crucial design constraint that underlies organization
we introduce an experimental paradigm for studying the cumulative cultural evolution of language in doing so we provide the first experimental validation for the idea that cultural transmission can lead to the appearance of design without a designer our experiments involve the iterated learning of artificial languages by human participants we show that languages transmitted culturally evolve in such a way as to maximize their own transmissibility over time the languages in our experiments become easier to learn and increasingly structured furthermore this structure emerges purely as a consequence of the transmission of language over generations without any intentional design on the part of individual language learners previous computational and mathematical models suggest that iterated learning provides an explanation for the structure of human language and link particular aspects of linguistic structure with particular constraints acting on language during its transmission the experimental work presented here shows that the predictions of these models and models of cultural evolution more generally can be tested in laboratory
we argue that category theory should become a part of the daily practice of the physicist and more specific the quantum physicist and or informatician the reason for this is not that category theory is a better way of doing mathematics but that monoidal categories constitute the actual algebra of practicing physics we will not provide rigorous definitions or anything resembling a coherent mathematical theory but we will take the reader for a journey introducing concepts which are part of category theory in a manner that the physicist will them
although it is well established that neandertals are the hominid form most closely related to present day humans their exact relationship with modern humans remains a topic of debate hublin et al soficaru et al harvati et al molecular genetic data first spoke to this issue in when a base pair section of the hypervariable region i hvri of the mitochondrial genome mtdna was determined from the neandertal type specimen found in in neander valley near dsseldorf germany krings et al since then a total of complete or partial neandertal hvri sequences as well as two hvrii sequences krings et al krings et al have been described phylogenetic analyses of these suggest that neandertal mtdna falls outside the variation of modern human mtdna since the mtdna genome is maternally inherited without recombination these results indicate that neandertals made no lasting contribution to the modern human mtdna gene pool krings et al currat et al serre et al high throughput sequencing techniques have recently been applied to ancient dna green et al poinar et al stiller et al these methods open new possibilities for the retrieval of ancient dna that has hitherto relied either on the cloning of random molecules in bacteria higuchi et al pbo noonan et al noonan et al or on the pcr amplification of individual dna sequences of interest pbo et al pbo et al the main benefit of the sequencing technique is the sheer volume of sequence data that make it practical to undertake genome scale ancient dna sequencing projects this is particularly feasible for mitochondrial genomes gilbert et al given their smaller size relative to the nuclear genome and their abundance in cells where typically several hundred mtdnas per nuclear genome exist the sequence data from ancient dna have also allowed an increased understanding of dna diagenesis i e how dna is modified during deposition in a burial context in particular they have allowed a quantitative model of how dna degradation and chemical modification occurs and how the effects of these processes interact with the molecular manipulations used to generate sequencing libraries briggs et al notably although it was previously known that a high rate of cytosine deamination occurs in ancient dna hofreiter et al it has become clear that this is particularly prevalent in the ends of the ancient molecules presumably because these are often single stranded briggs et al deamination of cytosine residues results in uracil residues that are read as thymine by dna polymerases leading to a high rate of c to t transitions a high rate of g to a transitions observed near the ends of sequence reads is thought to be caused by deaminated cytosine residues on the complementary strands used as templates during the fill in reaction to create blunt ends when sequencing libraries are constructed briggs et al by sequencing we have generated fold coverage of the neandertal mtdna genome from a neandertal bone vindija bone excavated in from vindija cave croatia malez et al it has been dated to years before present serre et al previously the mtdna hvri sequence of this bone has been determined serre et al as well as bp of mtdna sequences by sequencing green et al here we present its complete mtdna sequence as well as the insights it allows into recent human and neandertal evolution
contemporary theories of politics tend to portray politics as a reflection of society political phenomena as the aggregate consequences of individual behavior action as the result of choices based on calculated self interest history as efficient in reaching unique and appropriate outcomes and decision making and the allocation of resources as the central foci of political life some recent theoretical thought in political science however blends elements of these theoretical styles into an older concern with institutions this new institutionalism emphasizes the relative autonomy of political institutions possibilities for inefficiency in history and the importance of symbolic action to an understanding of politics such ideas have a resonable empirical basis but they are not characterized by powerful theoretical forms some directions for theoretical research may however be identified in institutionalist conceptions of order
motivation the next generation sequencing technologies are generating billions of short reads daily resequencing and personalized medicine need much faster software to map these deep sequencing reads to a reference genome to identify snps or rare transcripts results we present a framework for how full sensitivity mapping can be done in the most efficient way via spaced seeds using the framework we have developed software called zoom which is able to map the illumina solexa reads of coverage of a human genome to the reference human genome in one cpu day allowing two mismatches at full sensitivity availability zoom is freely available to non commercial users at http www bioinfor com zoom contact bma csd uwo ca mli uwaterloo bioinformatics
micrornas mirnas are short noncoding rnas that down regulate gene expression by silencing specific target mrnas while many mirnas are transcribed from their own genes nearly half map within introns of host genes the significance of which remains unclear we report that transcriptional activation of apoptosis associated tyrosine kinase aatk essential for neuronal differentiation also generates mir from an aatk gene intron that silences a family of mrnas whose protein products are negative regulators of neuronal differentiation we conclude that an intronic mirna transcribed together with the host gene mrna may serve the interest of its host gene by silencing a cohort of genes that are functionally antagonistic to the host itself
summary the tumor suppressor is activated upon genotoxic and oxidative stress and in turn inhibits cell proliferation and growth through induction of specific target genes cell growth is positively regulated by mtor whose activity is inhibited by the complex although genotoxic stress has been suggested to inhibit mtor via mediated activation of mtor inhibitors the precise mechanism of this link was unknown we now demonstrate that the products of two target genes and activate the amp responsive protein kinase ampk and target it to phosphorylate and stimulate its gap activity thereby inhibiting mtor correspondingly deficient mice fail to inhibit mtor signaling upon genotoxic challenge and therefore provide an important link between genotoxic stress and the mtor pathway
background both microarrays and quantitative real time pcr are convenient tools for studying the transcriptional levels of genes the former is preferable for large scale studies while the latter is a more targeted technique because of platform dependent systematic effects simple comparisons or merging of datasets obtained by these technologies are difficult even though they may often be desirable these difficulties are exacerbated if there is only partial overlap between the experimental conditions and genes probed in the two datasets results we show here that the generalized singular value decomposition provides a practical tool for merging a small targeted dataset obtained by quantitative real time pcr of specific genes with a much larger microarray dataset the technique permits for the first time the identification of genes present in only one dataset co expressed with a target gene present exclusively in the other dataset even when experimental conditions for the two datasets are not identical with the rapidly increasing number of publically available large scale microarray datasets the latter is frequently the case the method enables us to discover putative candidate genes involved in the biosynthesis of the beta d glucan polysaccharide found in plant cell walls conclusion we show that the generalized singular value decomposition provides a viable tool for a combined analysis of two gene expression datasets with only partial overlap of both gene sets and experimental conditions we illustrate how the decomposition can be optimized self consistently by using a judicious choice of genes to define it the ability of the technique to seamlessly define a concept of co expression across both datasets provides an avenue for meaningful data integration we believe that it will prove to be particularly useful for exploiting large publicly available microarray datasets for species with unsequenced genomes by complementing them with more limited in house measurements
web promises rich opportunities for information sharing electronic commerce and new modes of social interaction all centered around the social web of user contributed content social annotations and person to person social connections but the increasing reliance on this social web also places individuals and their computer systems at risk creating opportunities for malicious participants to exploit the tight social fabric of these networks with these problems in mind we propose the socialtrust framework for tamper resilient trust establishment in online communities socialtrust provides community users with dynamic trust values by i distinguishing relationship quality from trust ii incorporating a personalized feedback mechanism for adapting as the community evolves and iii tracking user behavior we experimentally evaluate the socialtrust framework using real online social networking data consisting of millions of myspace profiles and relationships we find that socialtrust supports robust trust establishment even in the presence of large scale collusion by participants
motivation many multiple sequence alignment tools have been developed in the past progressing either in speed or alignment accuracy given the importance and wide spread use of alignment tools progress in both categories is a contribution to the community and has driven research in the field so far results we introduce a graph based extension to the consistency based progressive alignment strategy we apply the consistency notion to segments instead of single characters the main problem we solve in this context is to define segments of the sequences in such a way that a graph based alignment is possible we implemented the algorithm using the seqan library and report results on amino acid and dna sequences the benefit of our approach is threefold sequences with conserved blocks can be rapidly aligned the implementation is conceptually easy generic and fast and the consistency idea can be extended to align multiple genomic sequences availability the segment based multiple sequence alignment tool can be downloaded from http www seqan de projects msa html a novel version of t coffee interfaced with the tool is available from http www tcoffee org the usage of the tool is described in both documentations contact rausch inf fu berlin bioinformatics
multiprotein complexes partake in nearly all cell functions thus the characterization and visualization of protein protein interactions in living cells constitute an important step in the study of a large array of cellular mechanisms recently noninvasive fluorescence based methods using resonance energy transfer ret namely bioluminescence ret bret and fluorescence ret fret and those centered on protein fragment complementation such as bimolecular fluorescence complementation bifc have been successfully used in the study of protein interactions these new technologies are nowadays the most powerful approaches for visualizing the interactions occurring within protein complexes in living cells thus enabling the investigation of protein behavior in their normal milieu here we address the individual strengths and weaknesses of these methods when applied to the study of protein interactions
themeriver is a prototype system that visualizes thematic variations over time within a large collection of documents the river flows from left to right through time changing width to depict changes in thematic strength of temporally associated documents colored currents flowing within the river narrow or widen to indicate decreases or increases in the strength of an individual topic or a group of topics in the associated documents the river is shown within the context of a timeline and a corresponding textual presentation of events
motivation a typical metagenome dataset generated using a pyrosequencing platform consists of short reads sampled from the collective genome of a microbial community the amount of sequence in such datasets is usually insufficient for assembly and traditional gene prediction cannot be applied to unassembled short reads as a result analysis of such datasets usually involves comparisons in terms of relative abundances of various protein families the latter requires assignment of individual reads to protein families which is hindered by the fact that short reads contain only a fragment usually small of a protein results we have considered the assignment of pyrosequencing reads to protein families directly using rps blast against cog and pfam databases and indirectly via proxygenes that are identified using blastx searches against protein sequence databases using simulated metagenome datasets as benchmarks we show that the proxygene method is more accurate than the direct assignment we introduce a clustering method which significantly reduces the size of a metagenome dataset while maintaining a faithful representation of its functional and taxonomic content contact vmmarkowitz lbl bioinformatics
motivation protein protein interactions are commonly mediated by the physical contact of distinct protein regions computational identi cation of interacting protein regions aids in the detailed understanding of protein networks and supports the prediction of novel protein interactions and the reconstruction of protein complexes results we introduce an integrative approach for predicting protein region interactions using a probabilistic model tted to an observed protein network in particular we consider globular domains short linear motifs and coiled coil regions as potential protein binding regions possible cooperations between multiple regions within the same protein are taken into account a negrained con dence system allows for varying the impact of speci c protein interactions and region annotations on the modeling process we apply our prediction approach to a large training set using a maximum likelihood method compare different scoring functions for region interactions and validate the predicted interactions against a collection of experimentally observed interactions in addition we analyze prediction performance with respect to the inclusion of different region types the incorporation of con dence values for training data and the utilization of predicted protein interactions contact mario albrecht mpi inf mpg de supplementary information supplementary data are available at bioinformatics bioinformatics
motivation current computational methods for the prediction of function from structure are restricted to the detection of similarities and subsequent transfer of functional annotation in a significant minority of cases global sequence or structural fold similarities do not provide clues about protein function in these cases one alternative is to detect local binding site similarities these may still reflect more distant evolutionary relationships as well as unique physico chemical constraints necessary for binding similar ligands thus helping pinpoint the function in the present work we ask the following question is it possible to discriminate within a dataset of non homologous proteins those that bind similar ligands based on their binding site similarities methods we implement a graph matching based method for the detection of atomic similarities introducing some simplifications that allow us to extend its applicability to the analysis of large allatom binding site models this method called isocleft does not require atoms to be connected either in sequence or space we apply the method to a cognate ligand bound dataset of non homologous proteins we define a family of binding site models with decreasing knowledge about the identity of the ligand interacting atoms to uncouple the questions of predicting the location of the binding site and detecting binding site similarities furthermore we calculate the individual contributions of binding site size chemical composition and geometry to prediction performance results we find that it is possible to discriminate between different ligand binding sites in other words there is a certain uniqueness in the set of atoms that are in contact to specific ligand scaffolds this uniqueness is restricted to the atoms in close proximity of the ligand in which case size and chemical composition alone are sufficient to discriminate binding sites discrimination ability decreases with decreasing knowledge about the identity of the ligand interacting binding site atoms the decrease is quite abrupt when considering size and chemical composition alone but much slower when including geometry we also observe that certain ligands are easier to discriminate interestingly the subset of binding site atoms belonging to highly conserved residues is not sufficient to discriminate binding sites implying that convergently evolved binding sites arrived at dissimilar solutions availability isocleft can be obtained from the authors contact rafael najmanovich ebi ac bioinformatics
humans and other animals must often make decisions on the basis of imperfect evidence statisticians use measures such as p values to assign degrees of confidence to propositions but little is known about how the brain computes confidence estimates about decisions we explored this issue using behavioural analysis and neural recordings in rats in combination with computational modelling subjects were trained to perform an odour categorization task that allowed decision confidence to be manipulated by varying the distance of the test stimulus to the category boundary to understand how confidence could be computed along with the choice itself using standard models of decision making we defined a simple measure that quantified the quality of the evidence contributing to a particular decision here we show that the firing rates of many single neurons in the orbitofrontal cortex match closely to the predictions of confidence models and cannot be readily explained by alternative mechanisms such as learning stimulus outcome associations moreover when tested using a delayed reward version of the task we found that rats willingness to wait for rewards increased with confidence as predicted by the theoretical model these results indicate that confidence estimates previously suggested to require metacognition and conscious awareness are available even in the rodent brain can be computed with relatively simple operations and can drive adaptive behaviour we suggest that confidence estimation may be a fundamental and ubiquitous component of making
social networking is beginning to make an impact on the drug discovery process while bioinformatics and chemoinformatics underpin research at a scientific level rapid communication between individual researchers across continents now allows the global exchange of ideas tools and technologies networking at this level of speed and reach is quite a recent phenomenon it facilitates the development of common interests accelerates technology transfer and increases cooperative and competitive behaviour in this review we critically evaluate different web based networking approaches as effective resources for the drug discovery scientist we also ask whether social networking sites will evolve into serious and credible resources for the drug community
a fundamental open question in the analysis of social networks is to understand the interplay between similarity and social ties people are similar to their neighbors in a social network for two distinct reasons first they grow to resemble their current friends due to social influence and second they tend to form new links to others who are already like them a process often termed selection by sociologists while both factors are present in everyday social processes they are in tension social influence can push systems toward uniformity of behavior while selection can lead to fragmentation as such it is important to understand the relative effects of these forces and this has been a challenge due to the difficulty of isolating and quantifying them in settings
scholars have long recognized the potential of internet based communication technologies for improving network research potential that to date remains largely underexploited in the first half of this paper we introduce a new public dataset based on manipulations and embellishments of a popular social network site facebook com we emphasize five distinctive features of this dataset and highlight its advantages and limitations vis a vis other kinds of network data in the second half of this paper we present descriptive findings from our first wave of data subgroups defined by gender race ethnicity and socioeconomic status are characterized by distinct network behaviors and students sharing social relationships as well as demographic traits tend to share a significant number of cultural preferences these findings exemplify the scientific and pedagogical potential of this new network resource and provide a starting point for future analyses c elsevier b v all reserved
given a graph in the maximum clique problem one desires to find the largest number of vertices any two of which are adjacent a branch and bound algorithm for the maximum clique problemwhich is computationally equivalent to the maximum independent stable set problemis presented with the vertex order taken from a coloring of the vertices and with a new pruning strategy the algorithm performs successfully for many instances when applied to random graphs and dimacs graphs
background the production of peroxide and superoxide is an inevitable consequence of aerobic metabolism and while these particular reactive oxygen species ross can exhibit a number of biological effects they are not of themselves excessively reactive and thus they are not especially damaging at physiological concentrations however their reactions with poorly liganded iron species can lead to the catalytic production of the very reactive and dangerous hydroxyl radical which is exceptionally damaging and a major cause of chronic inflammation review we review the considerable and wide ranging evidence for the involvement of this combination of su peroxide and poorly liganded iron in a large number of physiological and indeed pathological processes and inflammatory disorders especially those involving the progressive degradation of cellular and organismal performance these diseases share a great many similarities and thus might be considered to have a common cause i e iron catalysed free radical and especially hydroxyl radical generation the studies reviewed include those focused on a series of cardiovascular metabolic and neurological diseases where iron can be found at the sites of plaques and lesions as well as studies showing the significance of iron to aging and longevity the effective chelation of iron by natural or synthetic ligands is thus of major physiological and potentially therapeutic importance as systems properties we need to recognise that physiological observables have multiple molecular causes and studying them in isolation leads to inconsistent patterns of apparent causality when it is the simultaneous combination of multiple factors that is responsible this explains for instance the decidedly mixed effects of antioxidants that have been observed since in some circumstances especially the presence of poorly liganded iron molecules that are nominally antioxidants can actually act as pro oxidants the reduction of redox stress thus requires suitable levels of both antioxidants and effective iron chelators some polyphenolic antioxidants may serve both roles understanding the exact speciation and liganding of iron in all its states is thus crucial to separating its various pro and anti inflammatory activities redox stress innate immunity and pro and some anti inflammatory cytokines are linked in particular via signalling pathways involving nf kappab and with the oxidative roles of iron here seemingly involved upstream of the ikappab kinase ikk reaction in a number of cases it is possible to identify mechanisms by which ross and poorly liganded iron act synergistically and autocatalytically leading to runaway reactions that are hard to control unless one tackles multiple sites of action simultaneously some molecules such as statins and erythropoietin not traditionally associated with anti inflammatory activity do indeed have pleiotropic anti inflammatory effects that may be of benefit here conclusion overall we argue by synthesising a widely dispersed literature that the role of poorly liganded iron has been rather underappreciated in the past and that in combination with peroxide and superoxide its activity underpins the behaviour of a great many physiological processes that degrade over time understanding these requires an integrative systems level approach that may lead to novel targets
motivation next generation sequencing technologies open exciting new possibilities for genome and transcriptome sequencing while reads produced by these technologies are relatively short and error prone compared to the sanger method their throughput is several magnitudes higher to utilize such reads for transcriptome sequencing and gene structure identification one needs to be able to accurately align the sequence reads over intron boundaries this represents a significant challenge given their short length and inherent high error rate results we present a novel approach called qpalma for computing accurate spliced alignments which takes advantage of the read s quality information as well as computational splice site predictions our method uses a training set of spliced reads with quality information and known alignments it uses a large margin approach similar to support vector machines to estimate its parameters to maximize alignment accuracy in computational experiments we illustrate that the quality information as well as the splice site predictions help to improve the alignment quality finally to facilitate mapping of massive amounts of sequencing data typically generated by the new technologies we have combined our method with a fast mapping pipeline based on enhanced suffix arrays our algorithms were optimized and tested using reads produced with the illumina genome analyzer for the model plant arabidopsis thaliana availability datasets for training and evaluation additional results and a stand alone alignment tool implemented in c and python are available at http www fml mpg de raetsch projects qpalma contact gunnar raetsch tuebingen mpg bioinformatics
motivation biojava is a mature open source project that provides a framework for processing of biological data biojava contains powerful analysis and statistical routines tools for parsing common file formats and packages for manipulating sequences and structures it enables rapid bioinformatics application development in the java programming language availability biojava is an open source project distributed under the lesser gpl lgpl biojava can be downloaded from the biojava website http www biojava org biojava requires java or higher contact all queries should be directed to the biojava mailing lists details are available at http biojava org wiki biojava mailinglists supplementary information full documentation can be found at the biojava bioinformatics
scivee understanding the molecular mechanisms responsible for the regulation of the transcriptome present in eukaryotic cells is one of the most challenging tasks in the postgenomic era in this regard alternative splicing as is a key phenomenon contributing to the production of different mature transcripts from the same primary rna sequence as a plethora of different transcript forms is available in databases a first step to uncover the biology that drives as is to identify the different types of reflected splicing variation in this work we present a general definition of the as event along with a notation system that involves the relative positions of the splice sites this nomenclature univocally and dynamically assigns a specific as code to every possible pattern of splicing variation on the basis of this definition and the corresponding codes we have developed a computational tool astalavista that automatically characterizes the complete landscape of as events in a given transcript annotation of a genome thus providing a platform to investigate the transcriptome diversity across genes chromosomes and species our analysis reveals that a substantial partin human more than a quarterof the observed splicing variations are ignored in common classification pipelines we have used astalavista to investigate and to compare the as landscape of different reference annotation sets in human and in other metazoan species and found that proportions of as events change substantially depending on the annotation protocol species specific attributes and coding constraints acting on the transcripts the astalavista system therefore provides a general framework to conduct specific studies investigating the occurrence impact and regulation as
metamaterials are artificially engineered structures that have properties such as a negative refractive not attainable with naturally occurring materials negative index metamaterials nims were first demonstrated for microwave but it has been challenging to design nims for optical frequencies and they have so far been limited to optically thin samples because of significant fabrication challenges and strong energy dissipation in such thin structures are analogous to a monolayer of atoms making it difficult to assign bulk properties such as the index of refraction negative refraction of surface plasmons was recently demonstrated but was confined to a two dimensional three dimensional optical metamaterials have come into focus recently including the realization of negative refraction by using layered semiconductor metamaterials and a magnetic metamaterial in the infrared frequencies however neither of these had a negative index of here we report a optical metamaterial having negative refractive index with a very high figure of merit of that is low loss this metamaterial is made of cascaded fishnet structures with a negative index existing over a broad spectral range moreover it can readily be probed from free space making it functional for optical devices we construct a prism made of this optical nim to demonstrate negative refractive index at optical frequencies resulting unambiguously from the negative phase evolution of the wave propagating inside the metamaterial bulk optical metamaterials open up prospects for studies of optical effects and applications associated with nims and zero index materials such as reversed doppler effect superlenses optical tunnelling compact resonators and directional
despite the widely recognised importance of knowledge as a vital source of competitive advantage there is little understanding of how organisations actually create and manage knowledge dynamically nonaka toyama and konno start from the view of an organisation as an entity that creates knowledge continuously and their goal in this article is to understand the dynamic process in which an organisation creates maintains and exploits knowledge they propose a model of knowledge creation consisting of three elements i the seci process knowledge creation through the conversion of tacit and explicit knowledge ii ba the shared context for knowledge creation and iii knowledge assets the inputs outputs and moderators of the knowledge creating process the knowledge creation process is a spiral that grows out of these three elements the key to leading it is dialectical thinking the role of top management in articulating the organisation s knowledge vision is emphasised as is the important role of middle management knowledge producers in energising ba in summary using existing knowledge assets an organisation creates new knowledge through the seci process that takes place in ba where new knowledge once created becomes in turn the basis for a new spiral of creation
just as vision scientists study visual art and illusions to elucidate the workings of the visual system so too can cognitive scientists study cognitive illusions to elucidate the underpinnings of cognition magic shows are a manifestation of accomplished magic performers deep intuition for and understanding of human attention and awareness by studying magicians and their techniques neuroscientists can learn powerful methods to manipulate attention and awareness in the laboratory such methods could be exploited to directly study the behavioural and neural basis of consciousness itself for instance through the use of brain imaging and other neural techniques
summary seqmap is a tool for mapping large amount of short sequences to the genome it is designed for finding all the places in a reference genome where each sequence may come from this task is essential to the analysis of data from ultra high throughput sequencing machines with a carefully designed index filtering algorithm and an efficient implementation seqmap can map tens of millions of short sequences to a genome of several billions of nucleotides multiple substitutions and insertions deletions of the nucleotide bases in the sequences can be tolerated and therefore detected seqmap supports fasta input format and various output formats and provides command line options for tuning almost every aspect of the mapping process a typical mapping can be done in a few hours on a desktop pc parallel use of seqmap on a cluster is also very straightforward contact whwong stanford bioinformatics
abstract background inventories of small subgraphs in biological networks have identified commonly recurring patterns called motifs the inference that these motifs have been selected for function rests on the idea that their occurrences are significantly more frequent than random results our analysis of several large biological networks suggests in contrast that the frequencies of appearance of common subgraphs are similar in natural and corresponding random networks conclusions indeed certain topological features of biological networks give rise naturally to the common appearance of the motifs we therefore question whether frequencies of occurrences are reasonable evidence that the structures of motifs have been selected for their functional contribution to the operation networks
quantitative models of biochemical networks signal transduction cascades metabolic pathways gene regulatory circuits are a central component of modern systems biology building and managing these complex models is a major challenge that can benefit from the application of formal methods adopted from theoretical computing science here we provide a general introduction to the field of formal modelling which emphasizes the intuitive biochemical basis of the modelling process but is also accessible for an audience with a background in computing science and or model engineering we show how signal transduction cascades can be modelled in a modular fashion using both a qualitative approach qualitative petri nets and quantitative approaches continuous petri nets and ordinary differential equations odes we review the major elementary building blocks of a cellular signalling model discuss which critical design decisions have to be made during model building and present a number of novel computational tools that can help to explore alternative modular models in an easy and intuitive manner these tools which are based on petri net theory offer convenient ways of composing hierarchical ode models and permit a qualitative analysis of their behaviour we illustrate the central concepts using signal transduction as our main example the ultimate aim is to introduce a general approach that provides the foundations for a structured formal engineering of large scale models of biochemical bib
abstract a new web portal for the charmm macromolecular modeling package charmming charmm interface and graphics http www charmming org is presented this tool provides a user friendly interface for the preparation submission monitoring and visualization of molecular simulations i e energy minimization solvation and dynamics the infrastructure used to implement the web application is described two additional programs have been developed and integrated with charmming genrtf which is employed to define structural features not supported by the standard charmm force field and a job broker which is used to provide a portable method for using grid and cluster computing with charmming the use of the program is described with three proteins and source code is provided allowing charmming to be downloaded installed and used by supercomputing centers and research groups that have a charmm license although no software can replace a scientist s own judgment and experience charmming eases the introduction of newcomers to the molecular modeling discipline by providing a graphical method for simulations
in bioinformatics studies supervised classification with high dimensional input variables is frequently encountered examples routinely arise in genomic epigenetic and proteomic studies feature selection can be employed along with classifier construction to avoid over fitting to generate more reliable classifier and to provide more insights into the underlying causal relationships in this article we provide a review of several recently developed penalized feature selection and classification techniques which belong to the family of embedded feature selection methods for bioinformatics studies with high dimensional input classification objective functions penalty functions and computational algorithms are discussed our goal is to make interested researchers aware of these feature selection and classification methods that are applicable to high dimensional data
although the problem of determining the minimum cost path through a graph arises naturally in a number of interesting applications there has been no underlying theory to guide the development of efficient search procedures moreover there is no adequate conceptual framework within which the various ad hoc search strategies proposed to date can be compared this paper describes how heuristic information from the problem domain can be incorporated into a formal mathematical theory of graph searching and demonstrates an optimality property of a class of strategies
lsquo spooky action at a distance rsquo correlations are generally described by one of two mechanisms either a first event influences a second one by sending information encoded in bosons or other physical carriers or the correlated events have some common causes in their shared history quantum physics predicts an entirely different kind of cause for some correlations named entanglement this reveals itself in correlations that violate bell inequalities implying that they cannot be described by common causes between space like separated events implying that they cannot be described by classical communication many bell tests have been and loopholes related to and have been closed in several independent experiments it is still possible that a first event could influence a second but the speed of this hypothetical influence einstein s spooky action at a distance would need to be defined in some universal privileged reference frame and be greater than the speed of light here we put stringent experimental bounds on the speed of all such hypothetical influences we performed a bell test over more than hours between two villages separated by and approximately eastwest oriented with the source located precisely in the middle we continuously observed two photon interferences well above the bell inequality threshold taking advantage of the earth s rotation the configuration of our experiment allowed us to determine for any hypothetically privileged frame a lower bound for the speed of the influence for example if such a privileged reference frame exists and is such that the earth s speed in this frame is less than times that of the speed of light then the speed of the influence would have to exceed that of light by at least four orders magnitude
pnas allostery the coupling between ligand binding and protein conformational change is the heart of biological network and it has often been explained by two representative models the induced fit and the population shift models here we clarified for what systems one model fits better than the other by performing molecular simulations of coupled binding and conformational change based on the dynamic energy landscape view we developed an implicit ligand binding model combined with the double basin hamiltonian that describes conformational change from model simulations performed for a broad range of parameters we uncovered that each of the two models has its own range of applicability stronger and longer ranged interaction between ligand and protein favors the induced fit model and weaker and shorter ranged interaction leads to the population shift model we further postulate that the protein binding to small ligand tends to proceed via the population shift model whereas the protein docking to macromolecules such as dna tends to fit the induced model
background data data everywhere the diversity and magnitude of the data generated in the life sciences defies automated articulation among complementary efforts the additional need in this field for managing property and access permissions compounds the difficulty very significantly this is particularly the case when the integration involves multiple domains and disciplines even more so when it includes clinical and high throughput molecular data methodology principal findings the emergence of semantic web technologies brings the promise of meaningful interoperation between data and analysis resources in this report we identify a core model for biomedical knowledge engineering applications and demonstrate how this new technology can be used to weave a management model where multiple intertwined data structures can be hosted and managed by multiple authorities in a distributed management infrastructure specifically the demonstration is performed by linking data sources associated with the lung cancer spore awarded to the university of texas md anderson cancer center at houston and the southwestern medical center at dallas a software prototype available with open source at www org was developed and its proposed design has been made publicly available as an open source instrument for shared distributed data management conclusions significance the semantic web technologies have the potential to addresses the need for distributed and evolvable representations that are critical for systems biology and translational biomedical research as this technology is incorporated into application development we can expect that both general purpose productivity software and domain specific software installed on our personal computers will become increasingly integrated with the relevant remote resources in this scenario the acquisition of a new dataset should automatically trigger the delegation of analysis
captchas completely automated public turing test to tell computers and humans apart are widespread security measures in the world wide web that prevent automated programs from abusing online services they do so by asking humans to perform a task that computers cannot yet perform such as deciphering distorted characters our research explored whether such human effort can be channeled into a useful purpose helping to digitize old printed material by asking users to decipher scanned words from books that computerized optical character recognition ocr failed to recognize we showed that this method can transcribe text with word accuracy over matching the guarantee of professional human transcribers our apparatus is deployed in over web sites and has transcribed over million science
changes in gene regulation may be important in evolution however the evolutionary properties of regulatory mutations are currently poorly understood this is partly the result of an incomplete annotation of functional regulatory dna in many species for example transcription factor binding sites tfbss a major component of eukaryotic regulatory architecture are typically short degenerate and therefore difficult to differentiate from randomly occurring nonfunctional sequences furthermore although sites such as tfbss can be computationally predicted using evolutionary conservation as a criterion estimates of the true level of selective constraint defined as the fraction of strongly deleterious mutations occurring at a locus in regulatory regions will by definition be upwardly biased in datasets that are a priori evolutionarily conserved here we investigate the fitness effects of regulatory mutations using two complementary datasets of human tfbss that are likely to be relatively free of ascertainment bias with respect to evolutionary conservation but importantly are supported by experimental data the first is a collection of almost human tfbss drawn from the literature in the transfac database and the second is derived from several recent high throughput chromatin immunoprecipitation coupled with genomic microarray chip chip analyses we also define a set of putative cis regulatory modules pcrms by spatially clustering multiple tfbss that regulate the same gene we find that a relatively high proportion of mutations at tfbss are strongly deleterious similar to that at a fold degenerate protein coding site however constraint is significantly reduced in human and chimpanzee pcrms and chip chip sequences relative to macaques we estimate that the fraction of regulatory mutations that have been driven to fixation by positive selection in humans is not significantly different from zero we also find that the level of selective constraint in our tfbss pcrms and chip chip sequences is negatively correlated with the expression breadth of the regulated gene whereas the opposite relationship holds at that gene s nonsynonymous and synonymous sites finally we find that the rate of protein evolution in a transcription factor appears to be positively correlated with the breadth of expression of the gene it regulates our study suggests that strongly deleterious regulatory mutations are considerably more likely fold to occur in tissue specific than in housekeeping genes implying that there is a fitness cost to increasing complexity of expression
this study follows up on last year s publication of the complete sequence of venter s genome this time reporting a detailed analysis of a small but quite informative fraction of the genome there is much interest in characterizing the variation in a human individual because this may elucidate what contributes significantly to a person s phenotype thereby enabling personalized genomics we focus here on the variants in a person s exome which is the set of exons in a genome because the exome is believed to harbor much of the functional variation we provide an analysis of the variants that affect the protein coding portion of an individual s genome we identified nonsynonymous single nucleotide polymorphisms nssnps in this individual of which are rare in the human population we predict nssnps affect protein function and these tend be heterozygous rare or novel of the coding indels approximately half tend to have lengths that are a multiple of three which causes insertions deletions of amino acids in the corresponding protein rather than introducing frameshifts coding indels also occur frequently at the termini of genes so even if an indel causes a frameshift an alternative start or stop site in the gene can still be used to make a functional protein in summary we reduced the set of nonsilent coding variants by fold to a set of variants that are most likely to have major effects on their proteins functions this is our first glimpse of an individual s exome and a snapshot of the current state of personalized genomics the majority of coding variants in this individual are common and appear to be functionally neutral our results also indicate that some variants can be used to improve the current ncbi human reference genome as more genomes are sequenced many rare variants and non snp variants will be discovered we present an approach to analyze the coding variation in humans by proposing multiple bioinformatic methods to hone in on possible variation
the merging of network theory and microarray data analysis techniques has spawned a new field gene coexpression network analysis while network methods are increasingly used in biology the network vocabulary of computational biologists tends to be far more limited than that of say social network theorists here we review and propose several potentially useful network concepts we take advantage of the relationship between network theory and the field of microarray data analysis to clarify the meaning of and the relationship among network concepts in gene coexpression networks network theory offers a wealth of intuitive concepts for describing the pairwise relationships among genes which are depicted in cluster trees and heat maps conversely microarray data analysis techniques singular value decomposition tests of differential expression can also be used to address difficult problems in network theory we describe conditions when a close relationship exists between network analysis and microarray data analysis techniques and provide a rough dictionary for translating between the two fields using the angular interpretation of correlations we provide a geometric interpretation of network theoretic concepts and derive unexpected relationships among them we use the singular value decomposition of module expression data to characterize approximately factorizable gene coexpression networks i e adjacency matrices that factor into node specific contributions high and low level views of coexpression networks allow us to study the relationships among modules and among module genes respectively we characterize coexpression networks where hub genes are significant with respect to a microarray sample trait and show that the network concept of intramodular connectivity can be interpreted as a fuzzy measure of module membership we illustrate our results using human mouse and yeast microarray gene expression data the unification of coexpression network methods with traditional data mining methods can inform the application and development of systems methods
abstract nbsp nbsp knowledge sharing has been the focus of research for more than a decade and it is widely recognized that it can contribute to the success of an organisation however in comparison with other countries relatively little work on this topic has been done in the chinese context knowledge sharing is particularly interesting to study in the chinese context at the individual level given the unique social and cultural characteristics of this environment in this paper we develop a theoretical model to explain how personal factors would affect peoples intention to share their knowledge the theory of reasoned action and social exchange theory are used in this paper as are the time dimension of national culture face and guanxi a survey methodology is used to test the model face and guanxi orientation both exert a significant effect on the intention to share knowledge theoretical and practical implications as well as directions for future research discussed
the synergizer is a database and web service that provides translations of biological database identifiers it is accessible both programmatically and interactively availability the synergizer is freely available to all users inter actively via a web application http llama med harvard edu synergizer translate and programmatically via a web service clients implementing the synergizer application programming interface api are also freely available please visit http llama med harvard edu synergizer doc details
micro rna s mi rna s are crucial for normal embryonic stem es cell self renewal and cellular differentiation but how mi rna gene expression is controlled by the key transcriptional regulators of es cells has not been established we describe here the transcriptional regulatory circuitry of es cells that incorporates protein coding and mi rna genes based on high resolution chip seq data systematic identification of mi rna promoters and quantitative sequencing of short transcripts in multiple cell types we find that the key es cell transcription factors are associated with promoters for mi rna s that are preferentially expressed in es cells and with promoters for a set of silent mi rna genes this silent set of mi rna genes is co occupied by polycomb group proteins in es cells and shows tissue specific expression in differentiated cells these data reveal how key es cell transcription factors promote the es cell mi rna expression program and integrate mi rna s into the regulatory circuitry controlling es identity
a new generation of sequencing technologies from illumina solexa abi solid roche and helicos has provided unprecedented opportunities for high throughput functional genomic research to date these technologies have been applied in a variety of contexts including whole genome sequencing targeted resequencing discovery of transcription factor binding sites and noncoding rna expression profiling this review discusses applications of next generation sequencing technologies in functional genomics research and highlights the transforming potential these offer
background the prevalence of smoking has decreased substantially in the united states over the past years we examined the extent of the person to person spread of smoking behavior and the extent to which groups of widely connected people quit together methods we studied a densely interconnected social network of people assessed repeatedly from to as part of the framingham heart study we used network analytic methods and longitudinal statistical models results discernible clusters of smokers and nonsmokers were present in the network and the clusters extended to three degrees of separation despite the decrease in smoking in the overall population the size of the clusters of smokers remained the same across time suggesting that whole groups of people were quitting in concert smokers were also progressively found in the periphery of the social network smoking cessation by a spouse decreased a person s chances of smoking by confidence interval ci to smoking cessation by a sibling decreased the chances by ci to smoking cessation by a friend decreased the chances by ci to among persons working in small firms smoking cessation by a coworker decreased the chances by ci to friends with more education influenced one another more than those with less education these effects were not seen among neighbors in the immediate geographic area conclusions network phenomena appear to be relevant to smoking cessation smoking behavior spreads through close and distant social ties groups of interconnected people stop smoking in concert and smokers are increasingly marginalized socially these findings have implications for clinical and public health interventions to reduce and smoking
motivation living cells are the product of gene expression programs that involve the regulated transcription of thousands of genes the elucidation of transcriptional regulatory networks is thus needed to understand the cell s working mechanism and can for example be useful for the discovery of novel therapeutic targets although several methods have been proposed to infer gene regulatory networks from gene expression data a recent comparison on a large scale benchmark experiment revealed that most current methods only predict a limited number of known regulations at a reasonable precision level results we propose sirene supervised inference of regulatory networks a new method for the inference of gene regulatory networks from a compendium of expression data the method decomposes the problem of gene regulatory network inference into a large number of local binary classification problems that focus on separating target genes from non targets for each transcription factor sirene is thus conceptually simple and computationally efficient we test it on a benchmark experiment aimed at predicting regulations in escherichia coli and show that it retrieves of the order of times more known regulations than other state of the art inference methods availability all data and programs are freely available at http cbio ensmp fr sirene contact fantine mordelet fr
background as biomedical research projects become increasingly interdisciplinary and complex collaboration with appropriate individuals teams and institutions becomes ever more crucial to project success while social networks are extremely important in determining how scientific collaborations are formed social networking technologies have not yet been studied as a tool to help form scientific collaborations many currently emerging expertise locating systems include social networking technologies but it is unclear whether they make the process of finding collaborators more efficient and effective objective this study was conducted to answer the following questions which requirements should systems for finding collaborators in biomedical science fulfill and which information technology services can address these requirements methods the background research phase encompassed a thorough review of the literature affinity diagramming contextual inquiry and semistructured interviews this phase yielded five themes suggestive of requirements for systems to support the formation of collaborations in the next phase the generative phase we brainstormed and selected design ideas for formal concept validation with end users then three related well validated ideas were selected for implementation and evaluation in a prototype results five main themes of systems requirements emerged beyond expertise successful collaborations require compatibility with respect to personality work style productivity and many other factors compatibility finding appropriate collaborators requires the ability to effectively search in domains other than your own using information that is comprehensive and descriptive communication social networks are important for finding potential collaborators assessing their suitability and compatibility and establishing contact with them intermediation information profiles must be complete correct up to date and comprehensive and allow fine grained control over access to information by different audiences information quality and access keeping online profiles up to date should require little or no effort and be integrated into the scientists existing workflow motivation based on the requirements design ideas underwent formal validation with end users of those three were chosen to be implemented and evaluated in a system prototype digital vita maintaining formatting and semi automated updating of biographical information searching for experts and building and maintaining the social network and managing document flow conclusions in addition to quantitative and factual information about potential collaborators social connectedness personal and professional compatibility and power differentials also influence whether collaborations are formed current systems only partially model these requirements services in digital vita combine an existing workflow maintaining and formatting biographical information with collaboration searching functions in a novel way several barriers to the adoption of systems such as digital vita exist such as potential adoption asymmetries between junior and senior researchers and the tension between public and private information developers and researchers may consider one or more of the services described in this paper for implementation in their own expertise systems
we combined psychophysical and transcranial magnetic stimulation studies to investigate the dynamics of action anticipation and its underlying neural correlates in professional basketball players athletes predicted the success of free shots at a basket earlier and more accurately than did individuals with comparable visual experience coaches or sports journalists and novices moreover performance between athletes and the other groups differed before the ball was seen to leave the model s hands suggesting that athletes predicted the basket shot s fate by reading the body kinematics both visuo motor and visual experts showed a selective increase of motor evoked potentials during observation of basket shots however only athletes showed a time specific motor activation during observation of erroneous basket throws results suggest that achieving excellence in sports may be related to the fine tuning of specific anticipatory resonance mechanisms that endow elite athletes brains with the ability to predict others actions ahead of realization
aimto measure the frequency and content of online social networking among medical students and residents methodsusing the online network facebook we evaluated online profiles of all medical students n and residents n at the university of florida gainesville objective measures included the existence of a profile whether it was made private and any personally identifiable information subjective outcomes included photographic content affiliated social groups and personal information not generally disclosed in a doctorpatient encounter resultssocial networking with facebook is common among medical trainees with having an account medical students used it frequently and residents less frequently p lt the majority of accounts listed at least form of personally identifiable information only a third were made private and some accounts displayed potentially unprofessional material there was a significant decline in utilization of facebook as trainees approached medical or residency graduation first year as referent years and p lt discussionwhile social networking in medical trainees is common in the current culture of emerging professionals a majority of users allow anyone to view their profile with a significant proportion having subjectively inappropriate content acgme competencies in professionalism must include instruction on the intersection of personal and identities
an important part of our information gathering behavior has always been to find out what other people think with the growing availability and popularity of opinion rich resources such as online review sites and personal blogs new opportunities and challenges arise as people can and do actively use information technologies to seek out and understand the opinions of others the sudden eruption of activity in the area of opinion mining and sentiment analysis which deals with the computational treatment of opinion sentiment and subjectivity in text has thus occurred at least in part as a direct response to the surge of interest in new systems that deal directly with opinions as a first class object opinion mining and sentiment analysis covers techniques and approaches that promise to directly enable opinion oriented information seeking systems the focus is on methods that seek to address the new challenges raised by sentiment aware applications as compared to those that are already present in more traditional fact based analysis the survey includes an enumeration of the various applications a look at general challenges and discusses categorization extraction and summarization finally it moves beyond just the technical issues devoting significant attention to the broader implications that the development of opinion oriented information access services have questions of privacy vulnerability to manipulation and whether or not reviews can have measurable economic impact to facilitate future work a discussion of available resources benchmark datasets and evaluation campaigns is also provided opinion mining and sentiment analysis is the first such comprehensive survey of this vibrant and important research area and will be of interest to anyone with an interest in opinion oriented information systems
wiki pages and commentingbiology is an information driven science large scale data sets from genomics physiology population genetics and imaging are driving research at a dizzying rate simultaneously interdisciplinary collaborations among experimental biologists theorists statisticians and computer scientists have become the key to making effective use of these data sets however too many biologists have trouble accessing and using these electronic data sets and tools effectively a cyberinfrastructure is a combination of databases network protocols and computational services that brings people information and computational tools together to perform science in this information driven world this article reviews the components of a biological cyberinfrastructure discusses current and pending implementations and notes the many challenges that ahead
motivation more and more genomes are being sequenced and to keep up with the pace of sequencing projects automated annotation techniques are required one of the most challenging problems in genome annotation is the identification of the core promoter because the identification of the transcription initiation region is such a challenging problem it is not yet a common practice to integrate transcription start site prediction in genome annotation projects nevertheless better core promoter prediction can improve genome annotation and can be used to guide experimental work results comparing the average structural profile based on base stacking energy of transcribed promoter and intergenic sequences demonstrates that the core promoter has unique features that cannot be found in other sequences we show that unsupervised clustering by using self organizing maps can clearly distinguish between the structural profiles of promoter sequences and other genomic sequences an implementation of this promoter prediction program called prosom is available and has been compared with the state of the art we propose an objective accurate and biologically sound validation scheme for core promoter predictors prosom performs at least as well as the software currently available but our technique is more balanced in terms of the number of predicted sites and the number of false predictions resulting in a better all round performance additional tests on the encode regions of the human genome show that of all predictions made by prosom can be associated with transcriptionally active regions which demonstrates the high precision availability predictions for the human genome the validation datasets and the program prosom are available upon request contact yves vandepeer psb ugent bioinformatics
tagging is an important way for users to succinctly describe the content they upload to the internet however most tag suggestion systems recommend words that are highly correlated with the existing tag set and thus add little information to a user s contribution this paper describes a means to determine the ambiguity of a set of user contributed tags and suggests new tags that disambiguate the original tags we introduce a probabilistic framework that allows us to find two tags that appear in different contexts but are both likely to co occur with the original tag set if such tags can be found the current description is considered ambiguous and the two tags are recommended to the user for further clarification in contrast to previous work we only query the user when information is most needed and good suggestions are available we verify the efficacy of our approach using geographical temporal and semantic metadata and a user study we built our system using statistics from a large database of images and tags
implicit solvation models are popular alternatives to explicit solvent methods due to their ability to pre average solvent behavior and thus reduce the need for computationally expensive sampling previously we have demonstrated that poisson boltzmann models for polar solvation and integral based models for nonpolar solvation can reproduce explicit solvation forces in a low charge density protein system in the present work we examine the ability of these continuum models to describe solvation forces at the surface of a rna hairpin while these models do not completely describe all of the details of solvent behavior at this highly charged biomolecular interface they do provide a reasonable description of average solvation forces and therefore show significant promise for developing more robust implicit descriptions of solvent around nucleic acid systems for use in biomolecular simulation and modeling additionally we observe fairly good transferability in the nonpolar model parameters optimized for protein systems suggesting its robustness for modeling general nonpolar solvation phenomena in systems
for centuries people have aspired to understand and control the functions of the mind and brain it has now become possible to image the functioning of the human brain in real time using functional mri fmri and thereby to access both sides of the mindbrain interface subjective experience that is one s mind and objective observations that is external quantitative measurements of one s brain activity simultaneously developments in neuroimaging are now being translated into many new potential practical applications including the reading of brain states braincomputer interfaces communicating with locked in patients lie detection and learning control over brain activation to modulate cognition or even disease
abstract nbsp nbsp label ranking studies the problem of learning a mapping from instances to rankings over a predefined set of labels hitherto existing approaches to label ranking implicitly operate on an underlying utility scale which is not calibrated in the sense that it lacks a natural zero point we propose a suitable extension of label ranking that incorporates the calibrated scenario and substantially extends the expressive power of these approaches in particular our extension suggests a conceptually novel technique for extending the common learning by pairwise comparison approach to the multilabel scenario a setting previously not being amenable to the pairwise decomposition technique the key idea of the approach is to introduce an artificial calibration label that in each example separates the relevant from the irrelevant labels we show that this technique can be viewed as a combination of pairwise preference learning and the conventional relevance classification technique where a separate classifier is trained to predict whether a label is relevant or not empirical results in the area of text categorization image classification and gene analysis underscore the merits of the calibrated model in comparison to state of the art multilabel methods
in functional brain mapping pattern recognition methods allow detecting multivoxel patterns of brain activation which are informative with respect to a subject s perceptual or cognitive state the sensitivity of these methods however is greatly reduced when the proportion of voxels that convey the discriminative information is small compared to the total number of measured voxels to reduce this dimensionality problem previous studies employed univariate voxel selection or region of interest based strategies as a preceding step to the application of machine learning algorithms here we employ a strategy for classifying functional imaging data based on a multivariate feature selection algorithm recursive feature elimination rfe that uses the training algorithm support vector machine recursively to eliminate irrelevant voxels and estimate informative spatial patterns generalization performances on test data increases while features voxels are pruned based on their discrimination ability in this article we evaluate rfe in terms of sensitivity of discriminative maps receiver operative characteristic analysis and generalization performances and compare it to previously used univariate voxel selection strategies based on activation and discrimination measures using simulated fmri data we show that the recursive approach is suitable for mapping discriminative patterns and that the combination of an initial univariate activation based f test reduction of voxels and multivariate recursive feature elimination produces the best results especially when differences between conditions have a low contrast to noise ratio furthermore we apply our method to high resolution x x mm data from an auditory fmri experiment in which subjects were stimulated with sounds from four different categories with these real data our recursive algorithm proves able to detect and accurately classify multivoxel spatial patterns highlighting the role of the superior temporal gyrus in encoding the information of sound categories in line with the simulation results our method outperforms univariate statistical analysis and statistical learning without selection
pnas a change in climate would be expected to shift plant distribution as species expand in newly favorable areas and decline in increasingly hostile locations we compared surveys of plant cover that were made in and along a m elevation gradient in southern california s santa rosa mountains southern california s climate warmed at the surface the precipitation variability increased and the amount of snow decreased during the year period preceding the second survey we found that the average elevation of the dominant plant species rose by m between the surveys this shift cannot be attributed to changes in air pollution or fire frequency and appears to be a consequence of changes in climate
new sequencing technologies promise a new era in the use of dna sequence however some of these technologies produce very short reads typically of a few tens of base pairs and to use these reads effectively requires new algorithms and software in particular there is a major issue in efficiently aligning short reads to a reference genome and handling ambiguity or lack of accuracy in this alignment here we introduce the concept of mapping quality a measure of the confidence that a read actually comes from the position it is aligned to by the mapping algorithm we describe the software maq that can build assemblies by mapping shotgun short reads to a reference genome using quality scores to derive genotype calls of the consensus sequence of a diploid genome e g from a human sample maq makes full use of mate pair information and accurately estimates the error probability of each read alignment error probabilities are also derived for the final genotype calls using a bayesian statistical model that incorporates the mapping qualities error probabilities from the raw sequence quality scores sampling of the two haplotypes and an empirical model for correlated errors at a site both read mapping and genotype calling are evaluated on simulated data and real data maq is accurate efficient versatile and user friendly it is freely available at http maq net
the genome sequencer flx system gs flx powered by sequencing is a next generation dna sequencing technology featuring a unique mix of long reads exceptional accuracy and ultra high throughput it has been proven to be the most versatile of all currently available next generation sequencing technologies supporting many high profile studies in over seven applications categories gs flx users have pursued innovative research in de novo sequencing re sequencing of whole genomes and target dna regions metagenomics and rna analysis sequencing is a powerful tool for human genetics research having recently re sequenced the genome of an individual human currently re sequencing the complete human exome and targeted genomic regions using the nimblegen sequence capture process and detected low frequency somatic mutations linked cancer
animal studies have shown robust electrophysiological activity in the sensory cortex in the absence of stimuli or tasks similarly recent human functional magnetic resonance imaging fmri revealed widespread spontaneously emerging cortical fluctuations however it is unknown what neuronal dynamics underlie this spontaneous activity in the human brain here we studied this issue by combining bilateral single unit local field potentials lfps and intracranial electrocorticography ecog recordings in individuals undergoing clinical monitoring we found slow lt hz following f like profiles spontaneous fluctuations of neuronal activity with significant interhemispheric correlations these fluctuations were evident mainly in neuronal firing rates and in gamma hz lfp power modulations notably the interhemispheric correlations were enhanced during rapid eye movement and stage sleep multiple intracranial ecog recordings revealed clear selectivity for functional networks in the spontaneous gamma lfp power modulations our results point to slow spontaneous modulations in firing rate and gamma lfp as the likely correlates of spontaneous fmri fluctuations in the human cortex
the role of primary visual cortex in determining the contents of perception is controversial human functional magnetic resonance imaging fmri studies of perceptual suppression have revealed a robust drop in activity when a stimulus is subjectively invisible in contrast monkey single unit recordings have failed to demonstrate such perception locked changes in to investigate the basis of this discrepancy we measured both the blood oxygen leveldependent bold response and several electrophysiological signals in two behaving monkeys we found that all signals were in good agreement during conventional stimulus presentation showing strong visual modulation to presentation and removal of a stimulus during perceptual suppression however only the bold response and the low frequency local field potential lfp power showed decreases whereas the spiking and high frequency lfp power were unaffected these results demonstrate that the coupling between the bold and electrophysiological signals in is context dependent with a marked dissociation occurring during suppression
negative relevance feedback is a special case of relevance feedback where we do not have any positive example this often happens when the topic is difficult and the search results are poor although in principle any standard relevance feedback technique can be applied to negative relevance feedback it may not perform well due to the lack of positive examples in this paper we conduct a systematic study of methods for negative relevance feedback we compare a set of representative negative feedback methods covering vector space models and language models as well as several special heuristics for negative feedback evaluating negative feedback methods requires a test set with sufficient difficult topics but there are not many naturally difficult topics in the existing test collections we use two sampling strategies to adapt a test collection with easy topics to evaluate negative feedback experiment results on several trec collections show that language model based negative feedback methods are generally more effective than those based on vector space models and using multiple negative models is an effective heuristic for negative feedback our results also show that it is feasible to adapt test collections with easy topics for evaluating negative feedback methods sampling
the middleware market represents a sizable segment of the overall information and communication technology market in the annual middleware license revenue was reported by gartner to be in the region of dollar billion in this article we address the question whether research had any involvement in the creation of the technology that is being sold in this market quest we attempt a scholarly discourse we present the research method that we have applied to answer this question we then present a brief introduction into the key middleware concepts that provide the foundation for this market it would not be feasible to investigate any possible impact that research might have had instead we select a few very successful technologies that are representative for the middleware market as a whole and show the existence of impact of research results in the creation of these technologies we investigate the origins of web services middleware distributed transaction processing middleware message oriented middleware distributed object middleware and remote procedure call systems for each of these technologies we are able to show ample influence of research and conclude that without the research conducted by phd students and researchers in university computer science labs at brown cmu cambridge newcastle mit vrije and university of washington as well as research in industrial labs at apm at t bell labs dec systems research hp labs ibm research and xerox parc we would not have middleware technology in its current form we summarise the article by distilling lessons that can be learnt from this evidenced impact for future technology undertakings
this paper addresses the repeated acquisition of labels for data items when the labeling is imperfect we examine the improvement or lack thereof in data quality via repeated labeling and focus especially on the improvement of training labels for supervised induction with the outsourcing of small tasks becoming easier for example via rent a coder or amazon s mechanical turk it often is possible to obtain less than expert labeling at low cost with low cost labeling preparing the unlabeled part of the data can become considerably more expensive than labeling we present repeated labeling strategies of increasing complexity and show several main results i repeated labeling can improve label quality and model quality but not always ii when labels are noisy repeated labeling can be preferable to single labeling even in the traditional setting where labels are not particularly cheap iii as soon as the cost of processing the unlabeled data is not free even the simple strategy of labeling everything multiple times can give considerable advantage iv repeatedly labeling a carefully chosen set of points is generally preferable and we present a robust technique that combines different notions of uncertainty to select data points for which quality should be improved the bottom line the results show clearly that when labeling is not perfect selective acquisition of multiple labels is a strategy that data miners should have in their repertoire for certain label quality cost regimes the benefit substantial
abstract online peer production systems have enabled people to coactively create share classify and rate content on an unprecedented scale this paper describes strong macroscopic regularities in how people contribute to peer production systems and shows how these regularities arise from simple dynamical rules first it is demonstrated that the probability a person stops contributing varies inversely with the number of contributions he has made this rule leads to a power law distribution for the number of contributions per person in which a small number of very active users make most of the contributions the rule also implies that the power law exponent is proportional to the effort required to contribute as justified by the data second the level of activity per topic is shown to follow a lognormal distribution generated by a stochastic reinforcement mechanism a small number of very popular topics thus accumulate the vast majority of contributions these trends are demonstrated to hold across hundreds of millions of contributions to four disparate peer production systems of differing scope interface style purpose
graphics processors gpus provide a vast number of simple data parallel deeply multithreaded cores and high memory bandwidths gpu architectures are becoming increasingly programmable offering the potential for dramatic speedups for a variety of general purpose applications compared to contemporary general purpose processors cpus this paper uses nvidias c like cuda language and an engineering sample of their recently introduced gtx gpu to explore the effectiveness of gpus for a variety of application types and describes some specific coding idioms that improve their performance on the gpu gpu performance is compared to both single core and multicore cpu performance with multicore cpu implementations written using openmp the paper also discusses advantages and inefficiencies of the cuda programming model and some desirable features that might allow for greater ease of use and also more readily support a larger body applications
molecular interactions between protein complexes and dna mediate essential gene regulatory functions uncovering such interactions by chromatin immunoprecipitation coupled with massively parallel sequencing chip seq has recently become the focus of intense interest we here introduce quantitative enrichment of sequence tags quest a powerful statistical framework based on the kernel density estimation approach which uses chip seq data to determine positions where protein complexes contact dna using quest we discovered several thousand binding sites for the human transcription factors srf gabp and nrsf at an average resolution of about base pairs meme motif discovery toolbased analyses of the quest identified sequences revealed dna binding by cofactors of srf providing evidence that cofactor binding specificity can be obtained from chip seq data by combining quest analyses with gene ontology go annotations and expression data we illustrate how general functions of transcription factors can inferred
summary recent developments in high throughput sequencing technologies have generated considerable demand for tools to analyse large datasets of small rna sequences here we describe a suite of web based tools for processing plant small rna datasets our tools can be used to identify micro rnas and their targets compare expression levels in srna loci and find putative trans acting sirna loci availability the tools are freely available for use at http srna tools cmp uea ac uk contact vincent moulton cmp uea ac bioinformatics
autonomic computing is a concept that brings together many fields of computing with the purpose of creating computing systems that self manage in its early days it was criticised as being a ldquo hype topic rdquo or a rebadging of some multi agent systems work in this survey we hope to show that this was not indeed lsquo hype rsquo and that though it draws on much work already carried out by the computer science and control communities its innovation is strong and lies in its robust application to the specific self management of computing systems to this end we first provide an introduction to the motivation and concepts of autonomic computing and describe some research that has been seen as seminal in influencing a large proportion of early work taking the components of an established reference model in turn we discuss the works that have provided significant contributions to that area we then look at larger scaled systems that compose autonomic systems illustrating the hierarchical nature of their architectures autonomicity is not a well defined subject and as such different systems adhere to different degrees of autonomicity therefore we cross slice the body of work in terms of these degrees from this we list the key applications of autonomic computing and discuss the research work that is missing and what we believe the community should considering
current yeast interactome network maps contain several hundred molecular complexes with limited and somewhat controversial representation of direct binary interactions we carried out a comparative quality assessment of current yeast interactome data sets demonstrating that high throughput yeast two hybrid screening provides high quality binary interaction information because a large fraction of the yeast binary interactome remains to be mapped we developed an empirically controlled mapping framework to produce a second generation high quality high throughput data set covering textasciitilde of all yeast binary interactions both and affinity purification followed by mass spectrometry ap ms data are of equally high quality but of a fundamentally different and complementary nature resulting in networks with different topological and biological properties compared to co complex interactome models this binary map is enriched for transient signaling interactions and intercomplex connections with a highly significant clustering between essential proteins rather than correlating with essentiality protein connectivity correlates with pleiotropy
we propose a method to find the community structure in complex networks based on an extremal optimization of the value of modularity the method outperforms the optimal modularity found by the existing algorithms in the literature giving a better understanding of the community structure we present the results of the algorithm for computer simulated and real networks and compare them with other approaches the efficiency and accuracy of the method make it feasible to be used for the accurate identification of community structure in large networks
doi a central idea in marketing and diffusion research is that influentialsa minority of individuals who influence an exceptional number of their peersare important to the formation of public opinion here we examine this idea which we call the influentials hypothesis using a series of computer simulations of interpersonal influence processes under most conditions that we consider we find that large cascades of influence are driven not by influentials but by a critical mass of easily influenced individuals although our results do not exclude the possibility that influentials can be important they suggest that the influentials hypothesis requires more careful specification and testing than it received
two natural and widely used representations for the community structure of networks are clusterings which partition the vertex set into disjoint subsets and layouts which assign the vertices to positions in a metric space this paper unifies prominent characterizations of layout quality and clustering quality by showing that energy models of pairwise attraction and repulsion subsume newman and girvan s modularity measure layouts with optimal energy are relaxations of and are thus consistent with clusterings with optimal modularity which is of practical relevance because both representations are complementary and often together
pnas although information news and opinions continuously circulate in the worldwide social network the actual mechanics of how any single piece of information spreads on a global scale have been a mystery here we trace such information spreading processes at a person by person level using methods to reconstruct the propagation of massively circulated internet chain letters we find that rather than fanning out widely reaching many people in very few steps according to small world principles the progress of these chain letters proceeds in a narrow but very deep tree like pattern continuing for several hundred steps this suggests a new and more complex picture for the spread of information through a social network we describe a probabilistic model based on network clustering and asynchronous response times that produces trees with this characteristic structure on social data
synthetic biology is a rapidly growing field that has emerged in a global multidisciplinary effort among biologists chemists engineers physicists and mathematicians broadly the field has two complementary goals to improve understanding of biological systems through mimicry and to produce bio orthogonal systems with new functions here we review the area specifically with reference to the concept of synthetic biology space that is a hierarchy of components for and approaches to generating new synthetic and functional systems to test advance and apply our understanding of biological systems in keeping with this issue of current opinion in structural biology we focus largely on the design and engineering of biomolecule based components systems
chip sequencing chip seq is a new method for genomewide mapping of protein binding sites on dna it has generated much excitement in functional genomics to score data and determine adequate sequencing depth both the genomic background and the binding sites must be properly modeled to develop a computational foundation to tackle these issues we first performed a study to characterize the observed statistical nature of this new type of high throughput data by linking sequence tags into clusters we show that there are two components to the distribution of tag counts observed in a number of recent experiments an initial power law distribution and a subsequent long right tail then we develop in silico chip seq a computational method to simulate the experimental outcome by placing tags onto the genome according to particular assumed distributions for the actual binding sites and for the background genomic sequence in contrast to current assumptions our results show that both the background and the binding sites need to have a markedly nonuniform distribution in order to correctly model the observed chip seq data with for instance the background tag counts modeled by a gamma distribution on the basis of these results we extend an existing scoring approach by using a more realistic genomic background model this enables us to identify transcription factor binding sites in chip seq data in a statistically fashion
enbio zip http ucsd edu background microarray experiments measure changes in the expression of thousands of genes the resulting lists of genes with changes in expression are then searched for biologically related sets using several divergent methods such as the fisher exact test as used in multiple go enrichment tools parametric analysis of gene expression page gene set enrichment analysis gsea and the connectivity map results we describe an analytical method geneva gene vector analysis to relate genes to biological properties and to other similar experiments in a uniform way this new method works on both gene sets and on gene lists vectors as input queries and can effectively query databases consisting of sets of biologically related sets or of results from other microarray experiments we also present an improvement to the null model estimate by using the empirical background distribution drawn from previous experiments we validated geneva by rediscovering a number of previous findings and by finding significant relationships within microarrays in the geo repository conclusions provided a reasonable corpus of previous experiments is available this method is more accurate than the class label permutation model especially for data sets with limited number of replicates geneva is moreover computationally faster because the background distributions can be precomputed we also provide a standard evaluation data set based on pairs of related experiments that should share similar functional relationships and pairs of unrelated experiments from geo discovering relationships amongst geo data sets has implications for drug repositioning and understanding relationships between diseases drugs
we use patent data from the worldwide semiconductor industry from to to study the effect of the structure of organizational knowledge bases or the patterns of coupling between their elements of technical knowledge on the usefulness of inventions and knowledge base malleability we argue that organizational variations in coupling patterns between knowledge elements can be reflected in a spectrum of knowledge base structures varying from fully decomposable the knowledge base is composed of distinct clusters of knowledge elements coupled together with no significant ties between clusters through nearly decomposable knowledge clusters are discernable but are connected through cross cluster couplings to non decomposable no knowledge clusters emerge as the couplings are pervasively distributed and that organizations may differ in the way they use their knowledge because of variations in their knowledge base structure rather than because of differences in knowledge
whether as team members brainstorming or cultures experimenting with new technologies problem solvers communicate and share ideas this paper examines how the structure of communication networks among actors can affect system level performance we present an agent based computer simulation model of information sharing in which the less successful emulate the more successful results suggest that when agents are dealing with a complex problem the more efficient the network at disseminating information the better the short run but the lower the long run performance of the system the dynamic underlying this result is that an inefficient network maintains diversity in the system and is thus better for exploration than an efficient network supporting a more thorough search for solutions in the long run for intermediate time frames there is an inverted u relationship between connectedness and performance in which both poorly and well connected systems perform badly and moderately connected systems perform best this curvilinear relationship between connectivity and group performance can be seen in several diverse instances of organizational and behavior
the field of inductive logic programming ilp has made steady progress since the first ilp workshop in based on a balance of developments in theory implementations and applications more recently there has been an increased emphasis on probabilistic ilp and the related fields of statistical relational learning srl and structured prediction the goal of the current paper is to consider these emerging trends and chart out the strategic directions and open problems for the broader area of structured machine learning for the years
summary recent studies have revealed that alternative splicing plays an important role in the observed protein and interaction diversity special microarrays allow for measuring gene expression at the exon level and thus for studying alternative transcripts and their corresponding protein domain architecture we have developed the cytoscape plugin domaingraph that enables the visualization and detailed study of domain domain interactions forming protein interaction networks in addition the integration of exon expression data supports the analysis of alternative splicing events and the characterization of their effects on the protein and domain interaction network different expression patterns between human tissues or cells can be identified by comparing the generated domain graphs availability the plugin domaingraph and the online documentation are available at http domaingraph bioinf mpi inf mpg de contact mario albrecht mpi inf mpg bioinformatics
motivation the computational identification of transcription factor binding sites is a major challenge in bioinformatics and an important complement to experimental approaches results we describe a novel exact discriminative seeding dna motif discovery algorithm designed for fast and reliable prediction of cis regulatory elements in eukaryotic promoters the algorithm is tested on biological benchmark data and shown to perform equally or better than other motif discovery tools the algorithm is applied to the analysis of plant tissue specific promoter sequences and successfully identifies key regulatory elements availability the seeder perl distribution includes four modules it is available for download on the comprehensive perl archive network cpan at http www cpan org supplementary information supplementary information is available at bioinformatics online contact martina stromvik ca
in late may a group of database researchers architects users and pundits met at the claremont resort in berkeley california to discuss the state of the research field and its impacts on practice this was the seventh meeting of this sort in twenty years and was distinguished by a broad consensus that we are at a turning point in the history of the field due both to an explosion of data and usage scenarios and to major shifts in computing hardware and platforms given these forces we are at a time of opportunity for research impact with an unusually large potential for influential results across computing the sciences and society this report details that discussion and highlights the group s consensus view of new focus areas including new database engine architectures declarative programming languages the interplay of structured and unstructured data cloud data services and mobile and virtual worlds we also report on discussions of the community s growth including suggestions for changes in community processes to move the research agenda forward and to enhance impact on a audience
summary investigation of transcription factors tfs and their downstream regulated genes targets is a significant issue in post genome era which can provide a brand new vision for some vital biological process however information of tfs and their targets in mammalian is far from sufficient here we developed an integrated tf platform itfp which included abundant tfs and their targets of mammalian in current release itfp includes putative tfs and potential tf target pairs for human putative tfs and potential tf target pairs for mouse and putative tfs and potential tf target pairs for rat in short itfp will serve as an important resource for the research community of transcription and provide strong support for regulatory network study availability itfp can be accessed at http itfp biosino org itfp contact yyzhu fudan edu cn yxli sibs ac bioinformatics
in a very significant development for ehealth a broad adoption of web technologies and approaches coincides with the more recent emergence of personal health application platforms and personally controlled health records such as google health microsoft healthvault and dossia medicine applications services and tools are defined as web based services for health care consumers caregivers patients health professionals and biomedical researchers that use web technologies and or semantic web and virtual reality approaches to enable and facilitate specifically social networking participation apomediation openness and collaboration within and between these user groups the journal of medical internet research jmir publishes a medicine theme issue and sponsors a conference on how social networking and web changes health health care medicine and biomedical research to stimulate and encourage research in these areas
motivation simulations are an essential tool when analyzing biochemical networks researchers and developers seeking to refine simulation tools or develop new ones would benefit greatly from being able to compare their simulation results summary we present an approach to compare simulation results between several sbml capable simulators and provide a website for the community to share simulation results availability the website with simulation results and additional material can be found under http sys bio org sbwwiki compare the software used to generate the simulation results is available on the website for download contact fbergman u washington bioinformatics
stem cells are defined as self renewing cell populations that can differentiate into multiple distinct cell types however hundreds of different human cell lines from embryonic fetal and adult sources have been called stem cells even though they range from pluripotent cellstypified by embryonic stem cells which are capable of virtually unlimited proliferation and differentiationto adult stem cell lines which can generate a far more limited repertoire of differentiated cell types the rapid increase in reports of new sources of stem cells and their anticipated value to regenerative has highlighted the need for a general reproducible method for classification of these we report here the creation and analysis of a database of global gene expression profiles which we call the stem cell matrix that enables the classification of cultured human stem cells in the context of a wide variety of pluripotent multipotent and differentiated cell types using an unsupervised clustering to categorize a collection of cell samples we discovered that pluripotent stem cell lines group together whereas other cell types including brain derived neural stem cell lines are very diverse using further bioinformatic we uncovered a proteinprotein network plurinet that is shared by the pluripotent cells embryonic stem cells embryonal carcinomas and induced pluripotent cells analysis of published data showed that the plurinet seems to be a common characteristic of pluripotent cells including mouse embryonic stem and induced pluripotent cells and human oocytes our results offer a new strategy for classifying stem cells and support the idea that pluripotency and self renewal are under tight control by specific networks
phyml online is a web interface to phyml a software that implements a fast and accurate heuristic for estimating maximum likelihood phylogenies from dna and protein sequences this tool provides the user with a number of options e g nonparametric bootstrap and estimation of various evolutionary parameters in order to perform comprehensive phylogenetic analyses on large datasets in reasonable computing time the server and its documentation are available at http atgc lirmm fr nar
the repeated occurrence of genes in each other s neighbourhood on genomes has been shown to indicate a functional association between the proteins they encode here we introduce string search tool for recurring instances of neighbouring genes a tool to retrieve and display the genes a query gene repeatedly occurs with in clusters on the genome the tool performs iterative searches and visualises the results in their genomic context by finding the genomically associated genes for a query it delineates a set of potentially functionally associated genes the usefulness of string is illustrated with an example that suggests a functional context for an rna methylase with specificity
pnas an important issue in the study of cities is defining a metropolitan area because different definitions affect conclusions regarding the statistical distribution of urban activity a commonly employed method of defining a metropolitan area is the metropolitan statistical areas msas based on rules attempting to capture the notion of city as a functional economic region and it is performed by using experience the construction of msas is a time consuming process and is typically done only for a subset a few hundreds of the most highly populated cities here we introduce a method to designate metropolitan areas denoted city clustering algorithm cca the cca is based on spatial distributions of the population at a fine geographic scale defining a city beyond the scope of its administrative boundaries we use the cca to examine gibrat s law of proportional growth which postulates that the mean and standard deviation of the growth rate of cities are constant independent of city size we find that the mean growth rate of a cluster by utilizing the cca exhibits deviations from gibrat s law and that the standard deviation decreases as a power law with respect to the city size the cca allows for the study of the underlying process leading to these deviations which are shown to arise from the existence of long range spatial correlations in population growth these results have sociopolitical implications for example for the location of new economic development in cities of size
the first generation of digital natives children who were born into and raised in the digital world are coming of age and soon our world will be reshaped in their image our economy our politics our culture and even the shape of our family life will be forever transformed but who are these digital natives how are they different from older generations or digital immigrants and what is the world they re creating going to look like in born digital leading internet and technology experts john palfrey and urs gasser offer a sociological portrait of these young people who can seem even to those merely a generation older both extraordinarily sophisticated and strangely narrow based on extensive original research including interviews with digital natives around the world born digital explores a broad range of issues from the highly philosophical to the purely practical what does identity mean for young people who have dozens of online profiles and avatars should we worry about privacy issues or is privacy even a relevant concern for digital natives how does the concept of safety translate into an increasingly virtual world are online games addictive and how do we need to worry about violent video games what is the internet s impact on creativity and learning what lies ahead socially professionally and psychologically for this generation a smart practical guide to a brave new world and its complex inhabitants born digital will be essential reading for parents teachers and the myriad of confused adults who want to understand the digital present and shape the future
motivation as the blueprints of cellular actions biological pathways characterize the roles of genomic entities in various cellular mechanisms and as such their availability manipulation and queriability over the web is important to facilitate ongoing biological research results in this article we present the new features of pathcase a system to store query visualize and analyze metabolic pathways at different levels of genetic molecular biochemical and organismal detail the new features include i a web based system with a new architecture containing a server side and a client side and promoting scalability and flexible and easy adaptation of different pathway databases ii an interactive client side visualization tool for metabolic pathways with powerful visualization capabilities and with integrated gene and organism viewers iii two distinct querying capabilities an advanced querying interface for computer savvy users and built in queries for ease of use that can be issued directly from pathway visualizations and iv a pathway functionality analysis tool pathcase is now available for three different datasets namely kegg pathways data sample pathways from the literature and biocyc pathways for humans availability available online at http nashua case pathways
pnas we present a genomewide cross species analysis of regulation for broad acting transcription factors in yeast our model for binding site evolution is founded on biophysics the binding energy between transcription factor and site is a quantitative phenotype of regulatory function and selection is given by a fitness landscape that depends on this phenotype the model quantifies conservation as well as loss and gain of functional binding sites in a coherent way its predictions are supported by direct cross species comparison between four yeast species we find ubiquitous compensatory mutations within functional sites such that the energy phenotype and the function of a site evolve in a significantly more constrained way than does its sequence we also find evidence for substantial evolution of regulatory function involving point mutations as well as sequence insertions and deletions within binding sites genes lose their regulatory link to a given transcription factor at a rate similar to the neutral point mutation rate from which we infer a moderate average fitness advantage of functional over nonfunctional sites in a wider context this study provides an example of inference of selection acting on a quantitative trait
abstract nbsp nbsp the diagnostic role of breast ultrasound has been expanded along with the improvement of high frequency transducers and digital technology vascular assessment has progressed enough to depict normal vascular anatomy of the breast and the lymph nodes pathologic vessels are seen in almost all the tumors thus improving us sensitivity for nonpalpable carcinomas new contrast agents will recirculate enough to search for vascular foci during a thorough investigation of both breasts and nodal stations the us role in screening might be now revised many factors are now in favor of targeted us screening in dense and complex breasts and in high risk patients screening sensitivity is significantly increased most of these us detected tumors are small enough to be curable mammography and sonography together are a unique problem solving and cost effective tool they can easily guide fine aspirations or larger biopsies reducing the cost of unnecessary surgical procedures accurate us investigations facilitate the surgical approach to a very conservative and cosmetic operation high resolution sonography can demonstrate the intraductal spread of tumors and their multiple foci more easily than mammography but us diagnosis is less sensitive than magnetic resonance mammography in the evaluation of the real tumoral extent ductal branching has a complex pattern therefore intraductal spread and multifocal nodes are better demonstrated by multiplanar analysis of ultrasound data volumes sonography can easily explore the different nodal chains metastatic disease is indicated by an enlarged and round shape and the absence of the echogenic hilum irregularities in the cortex are a very useful sign in metastatic nodes without total replacement of lymphoid tissue by neoplastic cells these signs are very specific a time consuming radiation emitting and costly sentinel biopsy may be avoided in one of every five clinically node negative patients but preoperative us assessment is also important as sonography is very sensitive in patients with extensive nodal involvement that might result negative at the sentinel node procedure new technologies and contrast agents allow perfusional studies that enhance the contrast resolution and will increase the sensitivity of us for small metastases
background although recent advances have been made in identifying and analyzing instances of microrna mediated gene regulation it remains unclear by what mechanisms attenuation of transcript expression through micrornas becomes an integral part of post transcriptional modification and it is even less clear to what extent this process occurs for mammalian gene duplicates paralogs specifically while mammalian paralogs are known to overcome their initial complete functional redundancy through variation in regulation and expression the potential involvement of micrornas in this process has not been investigated results we comprehensively investigated the impact of microrna mediated post transcriptional regulation on duplicated genes in human and mouse using predicted targets derived from several analysis methods we report the following observations microrna targets are significantly enriched for duplicate genes implying their roles in the differential regulation of paralogs on average duplicate microrna target genes have longer untranslated regions than singleton targets and are regulated by more microrna species suggesting a more sophisticated mode of regulation ancient duplicates were more likely to be regulated by micrornas and on average have greater expression divergence than recent duplicates and ancient duplicate genes share fewer ancestral microrna regulators and recent duplicate genes share more common regulating micrornas conclusion collectively these results demonstrate that micrornas comprise an important element in evolving the regulatory patterns of mammalian paralogs we further present an evolutionary model in which micrornas not only adjust imbalanced dosage effects created by gene duplication but also help maintain long term buffering of the phenotypic consequences of gene deletion ablation
wikigenes is the first wiki system to combine the collaborative and largely altruistic possibilities of wikis with explicit authorship in view of the extraordinary success of wikipedia there remains no doubt about the potential of collaborative publishing yet its adoption in science has been limited here i discuss a dynamic collaborative knowledge base for the life sciences that provides authors with due credit and that can evolve via continual revision and traditional peer review into a rigorous tool
the construction of metagenomic libraries has permitted the study of microorganisms resistant to isolation and the analysis of rdna sequences has been used for over two decades to examine bacterial biodiversity here we show that the analysis of random sequence reads rsrs instead of is a suitable shortcut to estimate the biodiversity of a bacterial community from metagenomic libraries we generated rsrs from a metagenomic library of microorganisms found in human faecal samples then searched them using the program blastn against a prokaryotic sequence database to assign a taxon to each rsr the results were compared with those obtained by screening and analysing the clones containing rdna sequences in the whole library we found that the biodiversity observed by rsr analysis is consistent with that obtained by rdna we also show that rsrs are suitable to compare the biodiversity between different metagenomic libraries rsrs can thus provide a good estimate of the biodiversity of a metagenomic library and as an alternative to this approach is both faster and nar
in humans most meiotic crossover events are clustered into short regions of the genome known as recombination hot spots we have previously identified dna motifs that are enriched in hot spots particularly the mer cctccct here we use the increased hot spot resolution afforded by the phase hapmap and novel search methods to identify an extended family of motifs based around the degenerate mer ccnccntnnccnc which is critical in recruiting crossover events to at least of all human hot spots and which operates on diverse genetic backgrounds in both sexes furthermore these motifs are found in hypervariable minisatellites and are clustered in the breakpoint regions of both disease causing nonallelic homologous recombination hot spots and common mitochondrial deletion hot spots implicating the motif as a driver of instability
organellar dna sequences are widely used in evolutionary and population genetic studies however the conservative nature of chloroplast gene and genome evolution often limits phylogenetic resolution and statistical power to gain maximal access to the historical record contained within chloroplast genomes we have adapted multiplex sequencing by synthesis msbs to simultaneously sequence multiple genomes using the illumina genome analyzer we pcr amplified kb plastomes from eight species seven pinus one picea in reactions pooled products were ligated to modified adapters that included bp indexing tags and samples were multiplexed at four genomes per lane tagged microreads were assembled by de novo and reference guided assembly methods using previously published pinus plastomes as surrogate references assemblies for these eight genomes are estimated at complete with an average sequence depth of to mononucleotide repeats interrupt contig assembly with increasing repeat length and we estimate that the limit for their assembly is bp comparisons to kb of sanger sequence show a validated error rate of and conspicuous errors are evident from the assembly process this efficient sequencing approach yields high quality draft genomes and should have immediate applicability to genomes with comparable nar
objectives to review the issues that have arisen with the advent of translational research in terms of integration of data and knowledge and survey current efforts to address these issues methods using examples form the biomedical literature we identified new trends in biomedical research and their impact on bioinformatics we analyzed the requirements for effective knowledge repositories and studied issues in the integration of biomedical knowledge results new diagnostic and therapeutic approaches based on gene expression patterns have brought about new issues in the statistical analysis of data and new workflows are needed are needed to support translational research interoperable data repositories based on standard annotations infrastructures and services are needed to support the pooling and meta analysis of data as well as their comparison to earlier experiments high quality integrated ontologies and knowledge bases serve as a source of prior knowledge used in combination with traditional data mining techniques and contribute to the development of more effective data analysis strategies conclusion as biomedical research evolves from traditional clinical and biological investigations towards omics sciences and translational research specific needs have emerged including integrating data collected in research studies with patient clinical data linking omics knowledge with medical knowledge modeling the molecular basis of diseases and developing tools that support in depth analysis of research data as such translational research illustrates the need to bridge the gap between bioinformatics and medical informatics and opens new avenues for biomedical research
cognizing i e thinking understanding knowing and having the capacity to do what cognizers can do is a mental state systems without mental states such as cognitive technology can sometimes also do some of what cognizers can do but that does not make them cognizers cognitive technology allows cognizers to offload some of the functions they would otherwise have had to execute with their own brains and bodies alone it also extends cognizers performance powers beyond those of brains and bodies alone language itself is a form of cognitive technology that allows cognizers to offload some of their brain functions onto the brains of other cognizers language also extends cognizers individual and joint performance powers distributing the load through interactive and collaborative cognition reading writing print telecommunications and computing further extend cognizers capacities and now the web with its distributed network of cognizers digital databases and sofware agents has become the cognitive commons in which cognizers and cognitive technology can interact globally with a speed scope and degree of interactivity that yield performance powers inconceivable with unaided individual alone
suitable labels are at the core of luminescence and fluorescence imaging and sensing one of the most exciting yet also controversial advances in label technology is the emerging development of quantum dots qds inorganic nanocrystals with unique optical and chemical properties but complicated surface chemistryas in vitro and in vivo fluorophores here we compare and evaluate the differences in physicochemical properties of common fluorescent labels focusing on traditional organic dyes and qds our aim is to provide a better understanding of the advantages and limitations of both classes of chromophores to facilitate label choice and to address future challenges in the rational design and manipulation of labels
background with a whole genome duplication event and wealth of biological data salmonids are excellent model organisms for studying evolutionary processes fates of duplicated genes and genetic and physiological processes associated with complex behavioral phenotypes it is surprising therefore that no salmonid genome has been sequenced atlantic salmon salmo salar is a good representative salmonid for sequencing given its importance in aquaculture and the genomic resources available however the size and complexity of the genome combined with the lack of a sequenced reference genome from a closely related fish makes assembly challenging given the cost and time limitations of sanger sequencing as well as recent improvements to next generation sequencing technologies we examined the feasibility of using the genome sequencer gs flx pyrosequencing system to obtain the sequence of a salmonid genome eight pooled bacs belonging to a minimum tiling path covering mb of the atlantic salmon genome were sequenced by gs flx shotgun and long paired end sequencing and compared with a ninth bac sequenced by sanger sequencing of a shotgun library results an initial assembly using only gs flx shotgun sequences average read length bp with coverage allowed gene identification but was incomplete even when sanger generated bac end sequences coverage were incorporated the addition of paired end sequencing reads additional coverage produced a final assembly comprising contigs assembled into four scaffolds with gaps sanger sequencing of the ninth bac coverage produced nine contigs and two scaffolds the number of scaffolds produced by the gs flx assembly was comparable to sanger generated sequencing however the number of gaps was much higher in the gs flx assembly conclusion these results represent the first use of gs flx paired end reads for de novo sequence assembly our data demonstrated that this improved the gs flx assemblies however with respect to de novo sequencing of complex genomes the gs flx technology is limited to gene mining and establishing a set of ordered sequence contigs currently for a salmonid reference sequence it appears that a substantial portion of sequencing should be done using technology
acquisition of metastatic ability by tumor cells is considered a late event in the evolution of malignant tumors here we report that untransformed mouse mammary cells engineered to express inducible oncogenic transgenes myc and or polyoma middle t and introduced into the systemic circulation of a mouse can bypass transformation at the primary site and develop into metastatic pulmonary lesions upon immediate or delayed oncogene induction therefore previously untransformed mammary cells may establish residence in the lung once they have entered the bloodstream and may assume malignant growth upon oncogene activation mammary cells lacking oncogenic transgenes displayed a similar capacity for long term residence in the lungs but did not form ectopic science
we use high density single nucleotide polymorphism snp genotyping microarrays to demonstrate the ability to accurately and robustly determine whether individuals are in a complex genomic dna mixture we first develop a theoretical framework for detecting an individual s presence within a mixture then show through simulations the limits associated with our method and finally demonstrate experimentally the identification of the presence of genomic dna of specific individuals within a series of highly complex genomic mixtures including mixtures where an individual contributes less than of the total genomic dna these findings shift the perceived utility of snps for identifying individual trace contributors within a forensics mixture and suggest future research efforts into assessing the viability of previously sub optimal dna sources due to sample contamination these findings also suggest that composite statistics across cohorts such as allele frequency or genotype counts do not mask identity within genome wide association studies the implications of these findings discussed
in this chapter we survey some particular topics in category theory in a somewhat unconventional manner our main focus will be on monoidal categories mostly symmetric ones for which we propose a physical interpretation these are particularly relevant for quantum foundations and for quantum informatics special attention is given to the category which has finite dimensional hilbert spaces as objects linear maps as morphisms and the tensor product as its monoidal structure fdhilb we also provide a detailed discussion of the category which has sets as objects relations as morphisms and the cartesian product as its monoidal structure rel and thirdly categories with manifolds as objects and cobordisms between these as morphisms while sets hilbert spaces and manifolds do not share any non trivial common structure these three categories are in fact structurally very similar shared features are diagrammatic calculus compact closed structure and particular kinds of internal comonoids which play an important role in each of them the categories fdhilb and rel moreover admit a categorical matrix calculus together these features guide us towards topological quantum field theories we also discuss posetal categories how group representations are in fact categorical constructs and what strictification and coherence of monoidal categories is all about in our attempt to complement the existing literature we omitted some very basic topics for these we refer the reader to other sources
abstract the cortex is a complex system characterized by its dynamics and architecture which underlie many functions such as action perception learning language and cognition its structural architecture has been studied for more than a hundred years however its dynamics have been addressed much less thoroughly in this paper we review and integrate in a unifying framework a variety of computational approaches that have been used to characterize the dynamics of the cortex as evidenced at different levels of measurement computational models at different space time scales help us understand the fundamental mechanisms that underpin neural processes and relate these processes to neuroscience data model ing at the single neuron level is necessary because this is the level at which information is exchanged between the computing elements of the brain the neurons mesoscopic models tell us how neural elements interact to yield emergent behavior at the level of microcolumns and cortical columns macroscopic models can inform us about whole brain dynamics and interactions between large scale neural systems such as cortical regions the thalamus and brain stem each level of description relates uniquely to neuroscience data from single unit record ings through local field potentials to functional magnetic resonance imaging fmri electroencephalogram eeg and magnetoencephalogram meg models of the cortex can establish which types of large scale neuronal net works can perform computations and characterize their emergent properties mean field and related formulations of dynamics also play an essential and complementary role as forward models that can be inverted given empirical data this makes dynamic models critical in integrating theory and experiments we argue that elaborating principled and informed models is a prerequisite for grounding empirical neuroscience in a cogent theoretical framework commensurate with the achievements in the sciences
collaborative tagging used in online social content systems is naturally characterized by many synonyms causing low precision retrieval we propose a mechanism based on user preference profiles to identify synonyms that can be used to retrieve more relevant documents by expanding the users query using a popular online book catalog we discuss the effectiveness of our method over usual similarity based ex methods
the abundance and identity of functional variation segregating in natural populations is paramount to dissecting the molecular basis of quantitative traits as well as human genetic diseases genome sequencing of multiple organisms of the same species provides an efficient means of cataloging rearrangements insertion or deletion polymorphisms indels and single nucleotide polymorphisms snps while inbreeding depression and heterosis imply that a substantial amount of polymorphism is deleterious distinguishing deleterious from neutral polymorphism remains a significant challenge to identify deleterious and neutral dna sequence variation within saccharomyces cerevisiae we sequenced the genome of a vineyard and oak tree strain and compared them to a reference genome among these three strains of the genome is variable mostly attributable to variation in genome content that results from large indels out of the polymorphisms identified are snps and a small but significant fraction can be attributed to recent interspecific introgression and ectopic gene conversion in comparison to the reference genome there is substantial evidence for functional variation in gene content and structure that results from large indels frame shifts and polymorphic start and stop codons comparison of polymorphism to divergence reveals scant evidence for positive selection but an abundance of evidence for deleterious snps we estimate that of coding and of noncoding snps are deleterious based on divergence among yeast species we identified nonsynonymous snps that disrupt conserved amino acids and noncoding snps that disrupt conserved noncoding motifs the deleterious coding snps include those known to affect quantitative traits and a subset of the deleterious noncoding snps occurs in the promoters of genes that show allele specific expression implying that some cis regulatory snps are deleterious our results show that the genome sequences of both closely and distantly related species provide a means of identifying deleterious polymorphisms that disrupt functionally conserved coding and sequences
in this paper we propose the merge framework a general purpose programming model for heterogeneous multi core systems the merge framework replaces current ad hoc approaches to parallel programming on heterogeneous platforms with a rigorous library based methodology that can automatically distribute computation across heterogeneous cores to achieve increased energy and performance efficiency the merge framework provides a predicate dispatch based library system for managing and invoking function variants for multiple architectures a high level library oriented parallel language based on map reduce and a compiler and runtime which implement the map reduce language pattern by dynamically selecting the best available function implementations for a given input and machine configuration using a generic sequencer architecture interface for heterogeneous accelerators the merge framework can integrate function variants for specialized accelerators offering the potential for to the metal performance for a wide range of heterogeneous architectures all transparent to the user the merge framework has been prototyped on a heterogeneous platform consisting of an intel core duo cpu and an core thread intel graphics and media accelerator and a homogeneous way unisys smp system with intel xeon processors we implemented a set of benchmarks using the merge framework and enhanced the library with specific implementations achieving speedups of using the and using the way system relative to the straight c reference implementation on a core
definition resource based view propositions resources types can be identified that lead to high profits strategy ivolves striking a balance between exploitation of existing resourrces and building new ones by basing firm acquisition on a rare resource one can maximize market imperfection dynamic resource management first mover advantage in an attractive resource should yield high returns in the markets where the resources in question is dominant in a uncertain setting versatile multibusiness flexible resources are not necessarily more attractive than specialized resources although veratile resources give more options one would expect more and bigger competition in them definition by a resource is meant anything which could be thought of as a strength or weakness of a given firm examples are machine capacity customer loyalty productions experience leads
benchmarking is critical when evaluating performance but is especially difficult for file and storage systems complex interactions between i o devices caches kernel daemons and other os components result in behavior that is rather difficult to analyze moreover systems have different features and optimizations so no single benchmark is always suitable the large variety of workloads that these systems experience in the real world also add to this difficulty in this article we survey file system and storage benchmarks from recent papers we found that most popular benchmarks are flawed and many research papers do not provide a clear indication of true performance we provide guidelines that we hope will improve future performance evaluations to show how some widely used benchmarks can conceal or over emphasize overheads we conducted a set of experiments as a specific example slowing down read operations on by a factor of resulted in only a wall clock slowdown in a popular compile benchmark finally we discuss future work to improve file system and benchmarking
advances over the past few years have begun to enable prediction and design of macromolecular structures at near atomic accuracy progress has stemmed from the development of reasonably accurate and efficiently computed all atom potential functions as well as effective conformational sampling strategies appropriate for searching a highly rugged energy landscape both driven by feedback from structure prediction and design tests a unified energetic and kinematic framework in the rosetta program allows a wide range of molecular modeling problems from fibril structure prediction to rna folding to the design of new protein interfaces to be readily investigated and highlights areas for improvement the methodology enables the creation of novel molecules with useful functions and holds promise for accelerating experimental structural inference emerging connections to crystallographic phasing nmr modeling and lower resolution approaches are described and assessed
abstract background the methodologies we use both enable and help define our research however as experimental complexity has increased the choice of appropriate methodologies has become an increasingly difficult task this makes it difficult to keep track of available bioinformatics software let alone the most suitable protocols in a specific research area to remedy this we present an approach for capturing methodology from literature in order to identify and thus define best practice within a field results our approach is to implement data extraction techniques on the full text of scientific articles to obtain the set of experimental protocols used by an entire scientific discipline molecular phylogenetics our methodology for identifying methodologies could in principle be applied to any scientific discipline whether or not computer based we find a number of issues related to the nature of best practice as opposed to community practice we find that there is much heterogeneity in the use of molecular phylogenetic methods and software some of which is related to poor specification of protocols we also find that phylogenetic practice exhibits field specific tendencies that have increased through time despite the generic nature of the available software we used the practice of highly published and widely collaborative researchers expert researchers to analyse the influence of authority on community practice we find expert authors exhibit patterns of practice common to their field and therefore act as useful field specific practice indicators conclusions we have identified a structured community of phylogenetic researchers performing analyses that are customary in their own local community and significantly different from those in other areas best practice information can help to bridge such subtle differences by increasing communication of protocols to a wider audience we propose that the practice of expert authors from the field of evolutionary biology is the closest to contemporary best practice in phylogenetic experimental design capturing best practice is however a complex task and should also acknowledge the differences between fields such as the specific context of analysis
coarse grained cg models of biomolecules have recently attracted considerable interest because they enable the simulation of complex biological systems on length scales and timescales that are inaccessible for atomistic molecular dynamics simulation a cg model is defined by a map that transforms an atomically detailed configuration into a cg configuration for cg models of relatively small biomolecules or in cases that the cg and all atom models have similar resolution the construction of this map is relatively straightforward and can be guided by chemical intuition however it is more challenging to construct a cg map when large and complex domains of biomolecules have to be represented by relatively few cg sites this work introduces a new and systematic methodology called essential dynamics coarse graining ed cg this approach constructs a cg map of the primary sequence at a chosen resolution for an arbitrarily complex biomolecule in particular the resulting ed cg method variationally determines the cg sites that reflect the essential dynamics characterized by principal component analysis of an atomistic molecular dynamics trajectory numerical calculations illustrate this approach for the hiv ca protein dimer and atp bound g actin importantly since the cg sites are constructed from the primary sequence of the biomolecule the resulting ed cg model may be better suited to appropriately explore protein conformational space than those from other cg methods at the same degree resolution
in p w anderson suggested that more is different meaning that complex physical systems may exhibit behavior that cannot be understood only in terms of the laws governing their microscopic constituents we strengthen this claim by proving that many macroscopic observable properties of a simple class of physical systems the infinite periodic ising lattice cannot in general be derived from a microscopic description this provides evidence that emergent behavior occurs in such systems and indicates that even if a theory of everything governing all microscopic interactions were discovered the understanding of macroscopic order is likely to require insights
understanding the relationship between robustness and evolvability is key to understand how living things can withstand mutations while producing ample variation that leads to evolutionary innovations mutational robustness and evolvability a system s ability to produce heritable variation harbour a paradoxical tension on one hand high robustness implies low production of heritable phenotypic variation on the other hand both experimental and computational analyses of neutral networks indicate that robustness enhances evolvability i here resolve this tension using rna genotypes and their secondary structure phenotypes as a study system to resolve the tension one must distinguish between robustness of a genotype and a phenotype i confirm that genotype sequence robustness and evolvability share an antagonistic relationship in stark contrast phenotype structure robustness promotes structure evolvability a consequence is that finite populations of sequences with a robust phenotype can access large amounts of phenotypic variation while spreading through a neutral network population level processes and phenotypes rather than individual sequences are key to understand the relationship between robustness and evolvability my observations may apply to other genetic systems where many connected genotypes produce the phenotypes
abstract nbsp nbsp the classical probability ranking principle prp forms the theoretical basis for probabilistic information retrieval ir models which are dominating ir theory since about nbsp years however the assumptions underlying the prp often do not hold and its view is too narrow for interactive information retrieval iir in this article a new theoretical framework for interactive retrieval is proposed the basic idea is that during iir a user moves between situations in each situation the system presents to the user a list of choices about which s he has to decide and the first positive decision moves the user to a new situation each choice is associated with a number of cost and probability parameters based on these parameters an optimum ordering of the choices can the derivedthe prp for iir the relationship of this rule to the classical prp is described and issues of further research are out
we present a novel method for deriving network models from molecular profiles of perturbed cellular systems the network models aim to predict quantitative outcomes of combinatorial perturbations such as drug pair treatments or multiple genetic alterations mathematically we represent the system by a set of nodes representing molecular concentrations or cellular processes a perturbation vector and an interaction matrix after perturbation the system evolves in time according to differential equations with built in nonlinearity similar to hopfield networks capable of representing epistasis and saturation effects for a particular set of experiments we derive the interaction matrix by minimizing a composite error function aiming at accuracy of prediction and simplicity of network structure to evaluate the predictive potential of the method we performed drug pair treatment experiments in a human breast cancer cell line with observation of phospho proteins and cell cycle markers the best derived network model rediscovered known interactions and contained interesting predictions possible applications include the discovery of regulatory interactions the design of targeted combination therapies and the engineering of molecular networks
the relation between the activity of a single neocortical neuron and the dynamics of the network in which it is embedded was explored by single unit recordings and real time optical imaging the firing rate of a spontaneously active single neuron strongly depends on the instantaneous spatial pattern of ongoing population activity in a large cortical area very similar spatial patterns of population activity were observed both when the neuron fired spontaneously and when it was driven by its optimal stimulus the evoked patterns could be used to reconstruct the spontaneous activity of neurons
we describe a new ab initio algorithm genemark es version that identifies protein coding genes in fungal genomes the algorithm does not require a predetermined training set to estimate parameters of the underlying hidden markov model hmm instead the anonymous genomic sequence in question is used as an input for iterative unsupervised training the algorithm extends our previously developed method tested on genomes of arabidopsis thaliana caenorhabditis elegans and drosophila melanogaster to better reflect features of fungal gene organization we enhanced the intron submodel to accommodate sequences with and without branch point sites this design enables the algorithm to work equally well for species with the kinds of variations in splicing mechanisms seen in the fungal phyla ascomycota basidiomycota and zygomycota upon self training the intron submodel switches on in several steps to reach its full complexity we demonstrate that the algorithm accuracy both at the exon and the whole gene level is favorably compared to the accuracy of gene finders that employ supervised training application of the new method to known fungal genomes indicates substantial improvement over existing annotations by eliminating the effort necessary to build comprehensive training sets the new algorithm can streamline and accelerate the process of annotation in a large number of fungal genome projects
summary phylogenetic analyses today involve dealing with computer files in different formats and often several computer programs although some widely used applications have integrated important functionalities for such analyses they still work with local resources only input output files users have to manage them and local computing users have sometimes to leave their programs on their desktop computers running for extended periods of time to address these problems we have developed bosque a multi platform client server software that performs standard phylogenetic tasks either locally or remotely on servers and integrates the results on a local relational database bosque performs sequence alignments and graphical visualization and editing of trees thus providing a powerful environment that integrates all the steps of phylogenetic analyses availability http bosque udec cl contact sram profc udec bioinformatics
the availability of genome wide data provides unprecedented opportunities for resolving difficult phylogenetic relationships and for studying population genetic processes of mutation selection and recombination on a genomic scale the use of appropriate statistical models becomes increasingly important when we are faced with very large datasets which can lead to improved precision but not necessarily improved accuracy if the analytical methods have systematic biases this review provides a critical examination of methods for analyzing genomic datasets from multiple loci including concatenation separate gene by gene analyses and statistical models that accommodate heterogeneity in different aspects of the evolutionary process among data partitions we discuss factors that may cause the gene tree to differ from the species tree as well as strategies for estimating species phylogenies in the presence of gene tree conflicts genomic datasets provide computational and statistical challenges that are likely to be a focus of research for years come
understanding the genetic structure of human populations is of fundamental interest to medical forensic and anthropological sciences advances in high throughput genotyping technology have markedly improved our understanding of global patterns of human genetic variation and suggest the potential to use large samples to uncover variation among closely spaced here we characterize genetic variation in a sample of european individuals genotyped at over half a million variable dna sites in the human genome despite low average levels of genetic differentiation among europeans we find a close correspondence between genetic and geographic distances indeed a geographical map of europe arises naturally as an efficient two dimensional summary of genetic variation in europeans the results emphasize that when mapping the genetic basis of a disease phenotype spurious associations can arise if genetic structure is not properly accounted for in addition the results are relevant to the prospects of genetic ancestry an individual s dna can be used to infer their geographic origin with surprising accuracyoften to within a few kilometres
normal tissue cells are generally not viable when suspended in a fluid and are therefore said to be anchorage dependent such cells must adhere to a solid but a solid can be as rigid as glass or softer than a baby s skin the behavior of some cells on soft materials is characteristic of important phenotypes for example cell growth on soft agar gels is used to identify cancer cells however an understanding of how tissue cells including fibroblasts myocytes neurons and other cell types sense matrix stiffness is just emerging with quantitative studies of cells adhering to gels or to other cells with which elasticity can be tuned to approximate that of tissues key roles in molecular pathways are played by adhesion complexes and the actinmyosin cytoskeleton whose contractile forces are transmitted through transcellular structures the feedback of local matrix stiffness on cell state likely has important implications for development differentiation disease and science
vesicles consisting of a bilayer membrane of amphiphilic lipid molecules are remarkably flexible surfaces that show an amazing variety of shapes of different symmetry and topology owing to the fluidity of the membrane shape transitions such as budding can be induced by temperature changes or the action of optical tweezers thermally excited shape fluctuations are both strong and slow enough to be visible by video microscopy depending on the physical conditions vesicles adhere to and unbind from each other or a substrate this article describes the systematic physical theory developed to understand the static and dynamic aspects of membrane and vesicle configurations the preferred shapes arise from a competition between curvature energy which derives from the bending elasticity of the membrane geometrical constraints such as fixed surface area and fixed enclosed volume and a signature of the bilayer aspect these shapes of lowest energy are arranged into phase diagrams which separate regions of different symmetry by continuous or discontinuous transitions the geometrical constraints affect the fluctuations around these shapes by creating an effective tension for vesicles of non spherical topology the conformal invariance of the curvature energy leads to conformal diffusion which signifies a one fold degeneracy of the ground state unbinding and adhesion transitions arise from the balance between attractive interactions and entropic repulsion or a cost in bending energy respectively both the dynamics of equilibrium fluctuations and the dynamics of shape transformations are governed not only by viscous damping in the surrounding liquid but also by internal friction if the two monolayers slip over each other more complex membranes such as that of the red blood cell exhibit a variety of new phenomena because of coupling between internal degrees of freedom and geometry
theoretical results suggest that in order to learn the kind of complicated functions that can represent high level abstractions e g in vision language and other ai level tasks one may need deep architectures deep architectures are composed of multiple levels of non linear operations such as in neural nets with many hidden layers or in complicated propositional formulae re using many sub formulae searching the parameter space of deep architectures is a difficult task but learning algorithms such as those for deep belief networks have recently been proposed to tackle this problem with notable success beating the state of the art in certain areas this monograph discusses the motivations and principles regarding learning algorithms for deep architectures in particular those exploiting as building blocks unsupervised learning of single layer models such as restricted boltzmann machines used to construct deeper models such as deep networks
changes in gene regulation are thought to have contributed to the evolution of human development however in vivo evidence for uniquely human developmental regulatory function has remained elusive in transgenic mice a conserved noncoding sequence that evolved extremely rapidly in humans acted as an enhancer of gene expression that has gained a strong limb expression domain relative to the orthologous elements from chimpanzee and rhesus macaque this gain of function was consistent across two developmental stages in the mouse and included the presumptive anterior wrist and proximal thumb in vivo analyses with synthetic enhancers in which human specific substitutions were introduced into the chimpanzee enhancer sequence or reverted in the human enhancer to the ancestral state indicated that substitutions clustered in an pair module otherwise highly constrained among terrestrial vertebrates were sufficient to confer the human specific limb domain
direct in vivo investigation of mammalian metabolism is complicated by the distinct metabolic functions of different tissues we present a computational method that successfully describes the tissue specificity of human metabolism on a large scale by integrating tissue specific gene and protein expression data with an existing comprehensive reconstruction of the global human metabolic network we predict tissue specific metabolic activity in ten human tissues this reveals a central role for post transcriptional regulation in shaping tissue specific metabolic activity profiles the predicted tissue specificity of genes responsible for metabolic diseases and tissue specific differences in metabolite exchange with biofluids extend markedly beyond tissue specific differences manifest in enzyme expression data and are validated by large scale mining of tissue specificity data our results establish a computational basis for the genome wide study of normal and abnormal human metabolism in a tissue manner
not only is wikipedia a comprehensive source of quality information it has several kinds of internal structure e g relational summaries known as infoboxes which enable self supervised information extraction while previous efforts at extraction from wikipedia achieve high precision and recall on well populated classes of articles they fail in a larger number of cases largely because incomplete articles and infrequent use of infoboxes lead to insufficient training data this paper presents three novel techniques for increasing recall from wikipedia s long tail of sparse classes shrinkage over an automatically learned subsumption taxonomy a retraining technique for improving the training data and supplementing results by extracting from the broader web our experiments compare design variations and show that used in concert these techniques increase recall by a factor of to while maintaining or precision
the emergence of memory a trace of things past into human consciousness is one of the greatest mysteries of the human mind whereas the neuronal basis of recognition memory can be probed experimentally in human and nonhuman primates the study of free recall requires that the mind declare the occurrence of a recalled memory an event intrinsic to the organism and invisible to an observer here we report the activity of single neurons in the human hippocampus and surrounding areas when subjects first view cinematic episodes consisting of audiovisual sequences and again later when they freely recall these episodes a subset of these neurons exhibited selective firing which often persisted throughout and following specific episodes for as long as seconds verbal reports of memories of these specific episodes at the time of free recall were preceded by selective reactivation of the same hippocampal and entorhinal cortex neurons we suggest that this reactivation is an internally generated neuronal correlate for the subjective experience of spontaneous emergence of human science
a long standing conjecture in neuroscience is that aspects of cognition depend on the brain s ability to self generate sequential neuronal activity we found that reliably and continually changing cell assemblies in the rat hippocampus appeared not only during spatial navigation but also in the absence of changing environmental or body derived inputs during the delay period of a memory task each moment in time was characterized by the activity of a particular assembly of neurons identical initial conditions triggered a similar assembly sequence whereas different conditions gave rise to different sequences thereby predicting behavioral choices including errors such sequences were not formed in control nonmemory tasks we hypothesize that neuronal representations evolved for encoding distance in spatial navigation also support episodic recall and the planning of action science
pnas micrornas mirnas are small noncoding rnas that can contribute to cancer development and progression by acting as oncogenes or tumor suppressor genes recent studies have also linked different sets of mirnas to metastasis through either the promotion or suppression of this malignant process interestingly epigenetic silencing of mirnas with tumor suppressor features by cpg island hypermethylation is also emerging as a common hallmark of human tumors thus we wondered whether there was a mirna hypermethylation profile characteristic of human metastasis we used a pharmacological and genomic approach to reveal this aberrant epigenetic silencing program by treating lymph node metastatic cancer cells with a dna demethylating agent followed by hybridization to an expression microarray among the mirnas that were reactivated upon drug treatment mir mir c and mir were found to undergo specific hypermethylation associated silencing in cancer cells compared with normal tissues the reintroduction of mir and mir c in cancer cells with epigenetic inactivation inhibited their motility reduced tumor growth and inhibited metastasis formation in xenograft models with an associated down regulation of the mirna oncogenic target genes such as c myc and most important the involvement of mir mir c and mir hypermethylation in metastasis formation was also suggested in human primary malignancies because it was significantly associated with the appearance of lymph node metastasis our findings indicate that dna methylation associated silencing of tumor suppressor mirnas contributes to the development of human metastasis
the gene ontology go is extensively used to analyze all types of high throughput experiments however researchers still face several challenges when using go and other functional annotation databases one problem is the large number of multiple hypotheses that are being tested for each study in addition categories often overlap with both direct parents descendents and other distant categories in the hierarchical structure this makes it hard to determine if the identified significant categories represent different functional outcomes or rather a redundant view of the same biological processes to overcome these problems we developed a generative probabilistic model which identifies a small subset of categories that together explain the selected gene set our model accommodates noise and errors in the selected gene set and go using controlled go data our method correctly recovered most of the selected categories leading to dramatic improvements over current methods for go analysis when used with microarray expression data and chip chip data from yeast and human our method was able to correctly identify both general and specific enriched categories which were overlooked by other nar
glioblastoma multiforme gbm is the most common and lethal type of brain cancer to identify the genetic alterations in gbms we sequenced protein coding genes determined the presence of amplifications and deletions using high density oligonucleotide arrays and performed gene expression analyses using next generation sequencing technologies in human tumor samples this comprehensive analysis led to the discovery of a variety of genes that were not known to be altered in gbms most notably we found recurrent mutations in the active site of isocitrate dehydrogenase in of gbm patients mutations in occurred in a large fraction of young patients and in most patients with secondary gbms and were associated with an increase in overall survival these studies demonstrate the value of unbiased genomic analyses in the characterization of human brain cancer and identify a potentially useful genetic alteration for the classification and targeted therapy of science
there are currently few therapeutic options for patients with pancreatic cancer and new insights into the pathogenesis of this lethal disease are urgently needed toward this end we performed a comprehensive genetic analysis of pancreatic cancers we first determined the sequences of transcripts representing protein coding genes in these samples then we searched for homozygous deletions and amplifications in the tumor dna by using microarrays containing probes for approximately single nucleotide polymorphisms we found that pancreatic cancers contain an average of genetic alterations the majority of which are point mutations these alterations defined a core set of cellular signaling pathways and processes that were each genetically altered in to of the tumors analysis of these tumors transcriptomes with next generation sequencing by synthesis technologies provided independent evidence for the importance of these pathways and processes our data indicate that genetically altered core pathways and regulatory processes only become evident once the coding regions of the genome are analyzed in depth dysregulation of these core pathways and processes through mutation can explain the major features of tumorigenesis
mathematical models of the scientific citation process predict a strong first mover effect under which the first papers in a field will essentially regardless of content receive citations at a rate enormously higher than papers published later moreover papers are expected to retain this advantage in perpetuity they should receive more citations indefinitely no matter how many other papers are published after them we test this conjecture against data from a selection of fields and in several cases find a first mover effect of a magnitude similar to that predicted by the theory were we wearing our cynical hat today we might say that the scientist who wants to become famous is better off by a wide margin writing a modest paper in next year s hottest field than an outstanding paper in this year s on the other hand there are some papers albeit only a small fraction that buck the trend and attract significantly more citations than theory predicts despite having relatively late publication dates we suggest that papers of this kind though they often receive comparatively few citations overall are probably worthy of attention
proteindna interactions are crucial for many cellular processes now with the increased availability of structures of proteindna complexes gaining deeper insights into the nature of proteindna interactions has become possible earlier investigations have characterized the interface properties by considering pairwise interactions however the information communicated along the interfaces is rarely a pairwise phenomenon and we feel that a global picture can be obtained by considering a proteindna complex as a network of noncovalently interacting systems furthermore most of the earlier investigations have been carried out from the protein point of view protein centric and the present network approach aims to combine both the protein centric and the dna centric points of view part of the study involves the development of methodology to investigate proteindna graphs networks with the development of key parameters a network representation provides a holistic view of the interacting surface and has been reported here for the first time the second part of the study involves the analyses of these graphs in terms of clusters of interacting residues and the identification of highly connected residues hubs along the proteindna interface a predominance of deoxyriboseamino acid clusters in sheet proteins distinction of the interface clusters in helixturnhelix and the zipper type proteins would not have been possible by conventional pairwise interaction analysis additionally we propose a potential classification scheme for a set of proteindna complexes on the basis of the proteindna interface clusters this provides a general idea of how the proteins interact with the different components of dna in different complexes thus we believe that the present graph based method provides a deeper insight into the analysis of the proteindna recognition mechanisms by throwing more light on the nature and the specificity of interactions
motivation the increasing availability of phylogenetic and trait data for communities of co occurring species has created a need for software that integrates ecological and evolutionary analyses capabilities phylocom calculates numerous metrics of phylogenetic community structure and trait similarity within communities hypothesis testing is implemented using several null models within the same framework it measures phylogenetic signal and correlated evolution for species traits a range of utility functions allow community and phylogenetic data manipulation tree and trait generation and integration into scientific workflows availability open source at http phylodiversity net phylocom contact cwebb oeb harvard bioinformatics
slow wave sleep sws is important for memory consolidation during sleep neural patterns reflecting previously acquired information are replayed one possible reason for this is that such replay exchanges information between hippocampus and neocortex supporting consolidation we recorded neuron ensembles in the rat medial prefrontal cortex mpfc to study memory trace reactivation during sws following learning and execution of cross modal strategy shifts in general reactivation of learning related patterns occurred in distinct highly synchronized transient bouts mostly simultaneous with hippocampal sharp wave ripple complexes spwrs when hippocampal ensemble reactivation and cortico hippocampal interaction is enhanced during sleep following learning of a new rule mpfc neural patterns that appeared during response selection replayed prominently coincident with hippocampal spwrs this was learning dependent as the patterns appeared only after rule acquisition therefore learning or the resulting reliable reward influenced which patterns were most strongly encoded and successively reactivated in the hippocampal network
summary drugviz is a cytoscape plugin that is designed to visualize and analyze small molecules within the framework of the interactome drugviz can import drug target network information in an extended sif file format to cytoscape and display the two dimensional structures of small molecule nodes in a unified visualization environment it also can identify small molecule nodes by means of three different structure searching methods namely isomorphism substructure and fingerprint based similarity searches after selections users can furthermore conduct a two side clustering analysis on drugs and targets which allows for a detailed analysis of the active compounds in the network and elucidate relationships between these drugs and targets drugviz represents a new tool for the analysis of data from chemogenomics metabolomics and systems biology availability drugviz and data set used in application are freely available for download at http software html contact jkshen mail shcnc cn
motivation unraveling the transcriptional regulatory program mediated by transcription factors tfs is a fundamental objective of computational biology yet still remains a challenge method here we present a new methodology that integrates microarray and tf binding data for unraveling transcriptional regulatory networks the algorithm is based on a two stage constrained matrix decomposition model the model takes into account the non linear structure in gene expression data particularly in the tf target gene interactions and the combinatorial nature of gene regulation by tfs the gene expression profile is modeled as a linear weighted combination of the activity profiles of a set of tfs the tf activity profiles are deduced from the expression levels of tf target genes instead directly from tfs themselves the tf target gene relationships are derived from chip chip and other tf binding data the proposed algorithm can not only identify transcriptional modules but also reveal regulatory programs of which tfs control which target genes in which specific ways either activating or inhibiting results in comparison with other methods our algorithm identifies biologically more meaningful transcriptional modules relating to specific tfs we applied the new algorithm on yeast cell cycle and stress response data while known transcriptional regulations were confirmed novel tf gene interactions were predicted and provide new insights into the regulatory mechanisms of the cell contact zhanmi mail nih gov supplementary information supplementary data are available at bioinformatics bioinformatics
micrornas mirnas are generated from long primary pri rna polymerase ii pol ii derived transcripts by two rnase iii processing reactions drosha cleavage of nuclear pri mirnas and dicer cleavage of cytoplasmic pre mirnas here we show that drosha cleavage occurs during transcription acting on both independently transcribed and intron encoded mirnas we also show that both and exonucleases associate with the sites where co transcriptional drosha cleavage occurs promoting intron degradation before splicing we finally demonstrate that mirnas can also derive from flanking transcripts of pol ii genes our results demonstrate that multiple mirna containing transcripts are co transcriptionally cleaved during their synthesis and suggest that exonucleolytic degradation from drosha cleavage sites in pre mrnas may influence the splicing and maturation of mrnas
we present x qui site a scalable system for managing recommendations for social tagging sites like del icio us seamlessly incorporates various user behaviors into the recommendations and aims to recommend not only items of interest but also other relevant information like interesting people and or topics explanations are also provided so that users can obtain a better understanding of the recommendations and decide which recommendations to pursue further we discuss the technical challenges involved in characterizing different user behaviors and in efficiently computing explanations
the interest in statistical classification for critical applications such as diagnoses of patient samples based on supervised learning is rapidly growing to gain acceptance in applications where the subsequent decisions have serious consequences e g choice of cancer therapy any such decision support system must come with a reliable performance estimate tailored for small sample problems cross validation cv and bootstrapping bts have been the most commonly used methods to determine such estimates in virtually all branches of science for the last years here we address the often overlooked fact that the uncertainty in a point estimate obtained with cv and bts is unknown and quite large for small sample classification problems encountered in biomedical applications and elsewhere to avoid this fundamental problem of employing cv and bts until improved alternatives have been established we suggest that the final classification performance always should be reported in the form of a bayesian confidence interval obtained from a simple holdout test or using some other method that yields conservative measures of uncertainty
motivation a central problem in biomarker discovery from large scale gene expression or single nucleotide polymorphism snp data is the computational challenge of taking into account the dependence among all the features methods that ignore the dependence usually identify non reproducible biomarkers across independent datasets we introduce a new graph based semi supervised feature classification algorithm to identify discriminative disease markers by learning on bipartite graphs our algorithm directly classifies the feature nodes in a bipartite graph as positive negative or neutral with network propagation to capture the dependence among both samples and features clinical and genetic variables by exploring bi cluster structures in a graph two features of our algorithm are our algorithm can find a global optimal labeling to capture the dependence among all the features and thus generates highly reproducible results across independent microarray or other high thoughput datasets our algorithm is capable of handling hundreds of thousands of features and thus is particularly useful for biomarker identification from high throughput gene expression and snp data in addition although designed for classifying features our algorithm can also simultaneously classify test samples for disease prognosis diagnosis results we applied the network propagation algorithm to study three large scale breast cancer datasets our algorithm achieved competitive classification performance compared with svms and other baseline methods and identified several markers with clinical or biological relevance with the disease more importantly our algorithm also identified highly reproducible marker genes and enriched functions from the independent datasets availability supplementary results and source code are available at http compbio cs umn edu contact kuang cs umn edu supplementary information supplementary data are available at online
human cancer cells typically harbour multiple chromosomal aberrations nucleotide substitutions and epigenetic modifications that drive malignant transformation the cancer genome atlas tcga pilot project aims to assess the value of large scale multi dimensional analysis of these molecular characteristics in human cancer and to provide the data rapidly to the research community here we report the interim integrative analysis of dna copy number gene expression and dna methylation aberrations in glioblastomasthe most common type of adult brain cancerand nucleotide sequence aberrations in of the glioblastomas this analysis provides new insights into the roles of and uncovers frequent mutations of the phosphatidylinositol oh kinase regulatory subunit gene and provides a network view of the pathways altered in the development of glioblastoma furthermore integration of mutation dna methylation and clinical treatment data reveals a link between mgmt promoter methylation and a hypermutator phenotype consequent to mismatch repair deficiency in treated glioblastomas an observation with potential clinical implications together these findings establish the feasibility and power of tcga demonstrating that it can rapidly expand knowledge of the molecular basis cancer
dissecting the genetic basis of disease risk requires measuring all forms of genetic variation including snps and copy number variants cnvs and is enabled by accurate maps of their locations frequencies and population genetic properties we designed a hybrid genotyping array affymetrix snp to simultaneously measure snps and copy number at million genomic locations by characterizing hapmap samples we developed a map of human cnv at kb breakpoint resolution informed by integer genotypes for copy number polymorphisms cnps that segregate at an allele frequency more than of the sequence in previously reported cnv regions fell outside our estimated cnv boundaries indicating that large kb cnvs affect much less of the genome than initially reported approximately of observed copy number differences between pairs of individuals were due to common cnps with an allele frequency and more than derived from inheritance rather than new mutation most common diallelic cnps were in strong linkage disequilibrium with snps and most low frequency cnvs segregated on specific haplotypes
despite the remarkable thermochemical accuracy of kohnsham densityfunctional theories with gradient corrections for exchangecorrelation see for example a d becke j chem phys we believe that further improvements are unlikely unless exactexchange information is considered arguments to support this view are presented and a semiempirical exchangecorrelation functional containing localspindensity gradient and exactexchange terms is tested on atomization energies ionization potentials proton affinities and total atomic energies of first and secondrow systems this functional performs significantly better than previous functionals with gradient corrections only and fits experimental atomization energies with an impressively small average absolute deviation of mol
most microbes in the biosphere remain whole genome shotgun wgs sequencing of environmental dna metagenomics can be used to study the genetic and metabolic properties of natural microbial however in communities of high complexity metagenomics fails to link specific microbes to specific ecological functions to overcome this limitation we developed a method to target microbial subpopulations by labeling dna through stable isotope probing sip followed by wgs sequencing metagenome analysis of microbes from lake washington in seattle that oxidize single carbon compounds shows specific sequence enrichments in response to different substrates revealing the ecological roles of individual phylotypes we also demonstrate the utility of our approach by extracting a nearly complete genome of a novel methylotroph methylotenera mobilis reconstructing its metabolism and conducting genome wide analyses this high resolution targeted metagenomics approach may be applicable to a wide variety ecosystems
in the scientific research community plagiarism and covert multiple publications of the same data are considered unacceptable because they undermine the public confidence in the scientific integrity yet little has been done to help authors and editors to identify highly similar citations which sometimes may represent cases of unethical duplication for this reason we have made available d j vu a publicly available database of highly similar medline citations identified by the text similarity search engine etblast following manual verification highly similar citation pairs are classified into various categories ranging from duplicates with different authors to sanctioned duplicates d j vu records also contain user provided commentary and supporting information to substantiate each document s categorization d j vu and etblast are available to authors editors reviewers ethicists and sociologists to study intercept annotate and deter questionable publication practices these tools are part of a sustained effort to enhance the quality of medline as the biomedical corpus the d j vu database is freely accessible at http spore swmed edu dejavu the tool etblast is also freely available at http org
bib biomedical researchers have to efficiently explore the scientific literature keeping the focus on their research this goal can only be achieved if the available means for accessing the literature meet the researchers retrieval needs and if they understand how the tools filter the perpetually increasing number of documents we have examined existing web based services for information retrieval in order to give users guidance to improve their everyday practice of literature analysis we propose two dimensions along which the services may be categorized categories of input and output formats and categories of behavioural usage the categorization would be helpful for biologists to understand the differences in the input and output formats and the tasks they fulfil in information retrieval activities also they may inspire future bioinformaticians to further innovative development in field
copy number variation cnv is pervasive in the human genome and can play a causal role in genetic diseases the functional impact of cnv cannot be fully captured through linkage disequilibrium with snps these observations motivate the development of statistical methods for performing direct cnv association studies we show through simulation that current tests for cnv association are prone to false positive associations in the presence of differential errors between cases and controls especially if quantitative cnv measurements are noisy we present a statistical framework for performing case control cnv association studies that applies likelihood ratio testing of quantitative cnv measurements in cases and controls we show that our methods are robust to differential errors and noisy data and can achieve maximal theoretical power we illustrate the power of these methods for testing for association with binary and quantitative traits and have made this software available as the r cnvtools
facta is a text search engine for medline abstracts which is designed particularly to help users browse biomedical concepts e g genes proteins diseases enzymes and chemical compounds appearing in the documents retrieved by the query the concepts are presented to the user in a tabular format and ranked based on the co occurrence statistics unlike existing systems that provide similar functionality facta pre indexes not only the words but also the concepts mentioned in the documents which enables the user to issue a flexible query e g free keywords or boolean combinations of keywords concepts and receive the results immediately even when the number of the documents that match the query is very large the user can also view snippets from medline to get textual evidence of associations between the query terms and the concepts the concept ids and their names synonyms for building the indexes were collected from several biomedical databases and thesauri such as uniprot biothesaurus umls kegg and drugbank availability the system is available at http www nactem ac uk facta
background this paper presents data on alternations in the argument structure of common domain specific verbs and their associated verbal nominalizations in the pennbioie corpus alternation is the term in theoretical linguistics for variations in the surface syntactic form of verbs e g the different forms of stimulate in fsh stimulates follicular development and follicular development is stimulated by fsh the data is used to assess the implications of alternations for biomedical text mining systems and to test the fit of the sublanguage model to biomedical texts methodology principal findings we examined tokens of the ten most common domain specific verbs or their zero related nouns in the pennbioie corpus and labelled them for the presence or absence of three alternations we then annotated the arguments of tokens of the nominalizations related to these verbs and counted alternations related to the presence or absence of arguments and to the syntactic position of non absent arguments we found that alternations are quite common both for verbs and for nominalizations we also found a previously undescribed alternation involving an adjectival present participle conclusions significance we found that even in this semantically restricted domain alternations are quite common and alternations involving nominalizations are exceptionally diverse nonetheless the sublanguage model applies to biomedical language we also report on a previously undescribed alternation involving an adjectival participle
recent developments in programmable highly parallel graphics processing units gpus have enabled high performance implementations of machine learning algorithms we describe a solver for support vector machine training running on a gpu using the sequential minimal optimization algorithm and an adaptive first and second order working set selection heuristic which achieves speedups of over libsvm running on a traditional processor we also present a gpu based system for svm classification which achieves speedups of over libsvm over our own cpu based classifier
considering the natural tendency of people to follow direct or indirect cues of other people s activities collaborative filtering based recommender systems often predict the utility of an item for a particular user according to previous ratings by other similar users consequently effective searching for the most related neighbors is critical for the success of the recommendations in recent years collaborative tagging systems with social bookmarking as their key component from the suite of web technologies allow users to freely bookmark and assign semantic descriptions to various shared resources on the web while the list of favorite web pages indicates the interests or taste of each user the assigned tags can further provide useful hints about what a user thinks of the pages in this paper we propose a new collaborative filtering approach tbcf tag based collaborative filtering based on the semantic distance among tags assigned by different users to improve the effectiveness of neighbor selection that is two users could be considered similar not only if they rated the items similarly but also if they have similar cognitions over these items we tested tbcf on real life datasets and the experimental results show that our approach has significant improvement against the traditional cosine based recommendation method while leveraging user input not explicitly targeting the system
forward genetic mutational studies adaptive evolution and phenotypic screening are powerful tools for creating new variant organisms with desirable traits however mutations generated in the process cannot be easily identified with traditional genetic tools we show that new high throughput massively parallel sequencing technologies can completely and accurately characterize a mutant genome relative to a previously sequenced parental reference strain we studied a mutant strain of pichia stipitis a yeast capable of converting xylose to ethanol this unusually efficient mutant strain was developed through repeated rounds of chemical mutagenesis strain selection transformation and genetic manipulation over a period of seven years we resequenced this strain on three different sequencing platforms surprisingly we found fewer than a dozen mutations in open reading frames all three sequencing technologies were able to identify each single nucleotide mutation given at least fold nominal sequence coverage our results show that detecting mutations in evolved and engineered organisms is rapid and cost effective at the whole genome level using new sequencing technologies identification of specific mutations in strains with altered phenotypes will add insight into specific gene functions and guide further metabolic efforts
query logs record the queries and the actions of the users of search engines and as such they contain valuable information about the interests the preferences and the behavior of the users as well as their implicit feedback to search engine results mining the wealth of information available in the query logs has many important applications including query log analysis user profiling and personalization advertising query recommendation and more the query flow graph is an outcome of query log mining and at the same time a useful tool for it we propose a methodology that builds such a graph by mining time and textual information as well as aggregating queries from different users using this approach we build a real world query flow graph from a large scale query log and we demonstrate its utility in concrete applications namely finding logical sessions and query recommendation we believe however that the usefulness of the query flow graph goes beyond these applications
snp genotyping has emerged as a technology to incorporate copy number variants cnvs into genetic analyses of human traits however the extent to which snp platforms accurately capture cnvs remains unclear using independent sequence based cnv maps we find that commonly used snp platforms have limited or no probe coverage for a large fraction of cnvs despite this in samples we inferred cnvs using illumina snp genotyping data and experimentally validated over two thirds of these we also developed a method snp conditional mixture modeling scimm to robustly genotype deletions using as few as two snp probes we find that hapmap snps are strongly correlated with of common deletions but the newest snp platforms effectively tag about we conclude that currently available genome wide snp assays can capture cnvs accurately but improvements in array designs particularly in duplicated sequences are necessary to facilitate more comprehensive analyses of variation
object recognition is challenging because each object produces myriad retinal images responses of neurons from the inferior temporal cortex it are selective to different objects yet tolerant invariant to changes in object position scale and pose how does the brain construct this neuronal tolerance we report a form of neuronal learning that suggests the underlying solution targeted alteration of the natural temporal contiguity of visual experience caused specific changes in it position tolerance this unsupervised temporal slowness learning utl was substantial increased with experience and was significant in single it neurons after just hour together with previous theoretical work and human object perception experiments we speculate that utl may reflect the mechanism by which the visual stream builds and maintains tolerant object science
this paper reports on a failed experiment to use wiki technology to support student engagement with the subject matter of a third year undergraduate module using qualitative data the findings reveal that in an educational context social technologies such as wiki s are perceived differently compared with ordinary personal use and this discourages student adoption a series of insights are then offered which help he teachers understand the pitfalls of integrating social technologies in educational contexts contains figure tables
background determining the expression levels of micrornas mirnas is of great interest to researchers in many areas of biology given the significant roles these molecules play in cellular regulation two common methods for measuring mirnas in a total rna sample are microarrays and quantitative rt pcr qpcr to understand the results of studies that use these two different techniques to measure mirnas it is important to understand how well the results of these two analysis methods correlate since both methods use total rna as a starting material it is also critical to understand how measurement of mirnas might be affected by the particular method of total rna preparation used results we measured the expression of human mirnas in nine human tissues using agilent microarrays and compared these results to qpcr profiles of mirnas in the same tissues most expressed mirnas correlated well r between the two methods using spiked in synthetic mirnas we further examined the two mirnas with the lowest correlations and found the differences cannot be attributed to differential sensitivity of the two methods we also tested three widely used total rna sample prep methods using mirna microarrays we found that while almost all mirna levels correspond between the three methods there were a few mirnas whose levels consistently differed between the different prep techniques when measured by microarray analysis these differences were corroborated by qpcr measurements conclusion the correlations between agilent mirna microarray results and qpcr results are generally excellent as are the correlations between different total rna prep methods however there are a few mirnas whose levels do not correlate between the microarray and qpcr measurements or between different sample prep methods researchers should therefore take care when comparing results obtained using different analysis or sample methods
understanding the molecular basis of cancer requires characterization of its genetic defects dna microarray technologies can provide detailed raw data about chromosomal aberrations in tumor samples computational analysis is needed to deduce from raw array data actual amplification or deletion events for chromosomal fragments and to distinguish causal chromosomal alterations from functionally neutral ones we present a comprehensive computational approach rae designed to robustly map chromosomal alterations in tumor samples and assess their functional importance in cancer to demonstrate the methodology we experimentally profile copy number changes in a clinically aggressive subtype of soft tissue sarcoma pleomorphic liposarcoma and computationally derive a portrait of candidate oncogenic alterations and their target genes many affected genes are known to be involved in sarcomagenesis others are novel including mediators of adipocyte differentiation and may include valuable therapeutic targets taken together we present a statistically robust methodology applicable to high resolution genomic data to assess the extent and function of copy number alterations cancer
in several species including rodents and fish it has been shown that the major histocompatibility complex mhc influences mating preferences and in some cases that this may be mediated by preferences based on body odour in humans the picture has been less clear several studies have reported a tendency for humans to prefer mhc dissimilar mates a sexual selection that would favour the production of mhc heterozygous offspring who would be more resistant to pathogens but these results are unsupported by other studies here we report analyses of genome wide genotype data from the hapmap ii dataset and hla types in african and european american couples to test whether humans tend to choose mhc dissimilar mates in order to distinguish mhc specific effects from genome wide effects the pattern of similarity in the mhc region is compared to the pattern in the rest of the genome african spouses show no significant pattern of similarity dissimilarity across the mhc region relatedness coefficient r p whereas across the genome they are more similar than random pairs of individuals genome wide r p we discuss several explanations for these observations including demographic effects on the other hand the sampled european american couples are significantly more mhc dissimilar than random pairs of individuals r p and this pattern of dissimilarity is extreme when compared to the rest of the genome both globally genome wide r p and when broken into windows having the same length and recombination rate as the mhc only nine genomic regions exhibit a higher level of genetic dissimilarity between spouses than does the mhc this study thus supports the hypothesis that the mhc influences mate choice in some populations
homologous sets of transcription factors direct conserved tissue specific gene expression yet transcription factor binding events diverge rapidly between closely related species we used hepatocytes from an aneuploid mouse strain carrying human chromosome to determine on a chromosomal scale whether interspecies differences in transcriptional regulation are primarily directed by human genetic sequence or mouse nuclear environment virtually all transcription factor binding locations landmarks of transcription initiation and the resulting gene expression observed in human hepatocytes were recapitulated across the entire human chromosome in the mouse hepatocyte nucleus thus in homologous tissues genetic sequence is largely responsible for directing transcriptional programs interspecies differences in epigenetic machinery cellular environment and transcription factors themselves play secondary science
genetically monomorphic bacteria contain so little sequence diversity that sequencing a few gene fragments yields little or no information as a result our understanding of their evolutionary patterns presents greater technical challenges than exist for genetically diverse microbes these challenges are now being met by analyses at the genomic level for diverse types of genetic variation the most promising of which are single nucleotide polymorphisms many of the most virulent bacterial pathogens are genetically monomorphic and understanding their evolutionary and phylogeographic patterns will help our understanding of the effects of infectious disease on history
collaborative filtering aims at predicting a user s interest for a given item based on a collection of user profiles this article views collaborative filtering as a problem highly related to information retrieval drawing an analogy between the concepts of users and items in recommender systems and queries and documents in text retrieval we present a probabilistic user to item relevance framework that introduces the concept of relevance into the related problem of collaborative filtering three different models are derived namely a user based an item based and a unified relevance model and we estimate their rating predictions from three sources the user s own ratings for different items other users ratings for the same item and ratings from different but similar users for other but similar items to reduce the data sparsity encountered when estimating the probability density function of the relevance variable we apply the nonparametric data driven density estimation technique known as the parzen window method or kernel based density estimation using a gaussian window function the similarity between users and or items would however be based on euclidean distance because the collaborative filtering literature has reported improved prediction accuracy when using cosine similarity we generalize the parzen window method by introducing a projection kernel existing user based and item based approaches correspond to two simplified instantiations of our framework user based and item based collaborative filterings represent only a partial view of the prediction problem where the unified relevance model brings these partial views together under the same umbrella experimental results complement the theoretical insights with improved recommendation accuracy the unified model is more robust to data sparsity because the different types of ratings are used concert
summary tag sequencing using high throughput sequencing technologies are now regularly employed to identify specific sequence features such as transcription factor binding sites chip seq or regions of open chromatin dnase seq to intuitively summarize and display individual sequence data as an accurate and interpretable signal we developed f seq a software package that generates a continuous tag sequence density estimation allowing identification of biologically meaningful sites whose output can be displayed directly in the ucsc genome browser availability the software is written in the java language and is available on all major computing platforms for download at http www genome duke edu labs furey software fseq contact terry furey duke bioinformatics
in adam smith opened the wealth of nations with the observation that the greatest improvements in the productive powers of labour and the greatest part of the skill dexterity and judgement with which it is anywhere directed or applied seem to have been the effects of the division of labour despite the numerous economic advantages thus derived however smith insisted that the division of labor was not itself the effect of any human wisdom or foresight rather it was the necessary albeit very slow and gradual consequence of a certain propensity in human nature textendash the propensity to truck barter and exchange one thing for another common to all men this propensity could be found in no other animals and subsequently encouraged by the recognition of individual self interest it gave rise to differences among men more extensive more important and ultimately more useful than those implied by their natural endowments more than a century later durkheim could observe apparently without exaggeration that economists upheld the division of labor not only as necessary but as the supreme law of human societies and the condition of their progress greater concentrations of productive forces and capital investment seemed to lead modern industry business and agriculture toward greater separation and specialization of occupations and even a greater interdependence among the products themselves and like smith durkheim recognized that this extended beyond the economic world embracing not only political administrative and judicial activities but aesthetic and scientific activities as well even philosophy had been broken into a multitude of special disciplines each of which had its own object method and ideas unlike smith however durkheim viewed this law of the division of labor as applying not only to human societies but to biological organisms generally citing recent speculation in the philosophy of biology see the works of c f wolff k e von baer and h milne edwards durkheim noted the apparent correlation between the functional specialization of the parts of an organism and the extent of that organism s evolutionary development suggesting that this extended the scope of the division of labor so as to make its origins contemporaneous with the origins of life itself this of course eliminated any propensity in human nature as its possible cause and implied that its conditions must be found in the essential properties of all organized matter the division of labor in society was thus no more than a particular form of a process of extreme generality but if the division of labor was thus a natural law then like all natural laws it raised certain moral questions are we to yield to it or resist it is it our duty to become thorough complete self sufficient human beings or are we to be but parts of a whole organs of an organism in other words is this natural law also a moral rule if so why and in what degree in durkheim s opinion the answers of modern societies to these and similar questions had been deeply ambivalent textendash i e on the one hand the division of labor seemed to be increasingly viewed as a moral rule so that in at least one of its aspects the categorical imperative of the modern conscience had become make yourself usefully fulfill a determinate function on the other hand quite aside from such maxims endorsing specialization there were other maxims no less prevalent which called attention to the dangers of over specialization and encouraged all men to realize similar ideals the situation was thus one of moral conflict or antagonism and it was this which durkheim sought first to explain and then to resolve this in turn calls for two final observations first the method of this explanation and resolution was to be that of the so called science of ethics for durkheim was convinced that moral facts like the division of labor were themselves natural phenomena textendash they consisted of certain rules of action imperatively imposed upon conduct which could be recognized observed described classified and explained second this explanation itself was but a preliminary step to the solution of practical social problems for durkheim always conceived of societies as subject to conditions of moral health or illness and the sociologist as a kind of physician who scientifically determined the particular condition of a particular society at a particular time and then prescribed the social medicine necessary to the maintenance or recovery of well being durkheim s problem thus defined his solution fell quite naturally into three principal parts the determination of the function of the division of labor the determination of the causes on which it depended and the determination of those forms of illness which exhibited
pnas life is that which replicates and evolves the origin of life is also the origin of evolution a fundamental question is when do chemical kinetics become evolutionary dynamics here we formulate a general mathematical theory for the origin of evolution all known life on earth is based on biological polymers which act as information carriers and catalysts therefore any theory for the origin of life must address the emergence of such a system we describe prelife as an alphabet of active monomers that form random polymers prelife is a generative system that can produce information prevolutionary dynamics have selection and mutation but no replication life marches in with the ability of replication polymers act as templates for their own reproduction prelife is a scaffold that builds life yet there is competition between life and prelife there is a phase transition if the effective replication rate exceeds a critical value then life outcompetes prelife replication is not a prerequisite for selection but instead there can be selection for replication mutation leads to an error threshold between life prelife
detecting conflicting statements is a foundational text understanding task with applications in information analysis we propose an appropriate definition of contradiction for nlp tasks and develop available corpora from which we construct a typology of contradictions we demonstrate that a system for contradiction needs to make more fine grained distinctions than the common systems for entailment in particular we argue for the centrality of event coreference and therefore incorporate such a component based on topicality we present the first detailed breakdown of performance on this task detecting some types of contradiction requires deeper inferential paths than our system is capable of but we achieve good performance on types arising from negation antonymy
online social networking sites like myspace orkut and flickr are among the most popular sites on the web and continue to experience dramatic growth in their user population the popularity of these sites offers a unique opportunity to study the dynamics of social networks at scale having a proper understanding of how online social networks grow can provide insights into the network structure allow predictions of future growth and enable simulation of systems on networks of arbitrary size however to date most empirical studies have focused on static network snapshots rather than dynamics
accurate and complete measurement of single nucleotide snp and copy number cnv variants both common and rare will be required to understand the role of genetic variation in disease we present birdsuite a four stage analytical framework instantiated in software for deriving integrated and mutually consistent copy number and snp genotypes the method sequentially assigns copy number across regions of common copy number polymorphisms cnps calls genotypes of snps identifies rare cnvs via a hidden markov model hmm and generates an integrated sequence and copy number genotype at every locus for example including genotypes such as a null aab and bbb in addition to aa ab and bb calls such genotypes more accurately depict the underlying sequence of each individual reducing the rate of apparent mendelian inconsistencies the birdsuite software is applied here to data from the affymetrix snp array additionally we describe a method implemented in plink to utilize these combined snp and cnv genotypes for association testing with phenotype
through an analysis of recent data on adults and children s computer use and experiences this datawatch shows that use of computers and the internet is widespread and that significant percentages of the public are already using the internet to get health information the surveys also show that the internet is already a useful vehicle for reaching large numbers of lower income less educated and minority americans however a substantial digital divide continues to characterize computer and internet use with lower income blacks especially affected implications for the future of health communication on the internet also are hlthaff
motivation gene set enrichment analysis gsea can be greatly enhanced by linear model regression diagnostic techniques diagnostics can be used to identify outlying or influential samples and also to evaluate model fit and explore model expansion results we demonstrate this methodology on an adult acute lymphoblastic leukemia all dataset using gsea based on chromosome band mapping of genes individual residuals grouped or aggregated by chromosomal loci indicate problematic samples and potential data entry errors and help identify hyperdiploidy as a factor playing a key role in expression for this dataset subsequent analysis pinpoints suspected dna copy number abnormalities of specific samples and chromosomes most prevalent are chromosomes x and and also reveals significant expression differences between the hyperdiploid and diploid groups on other chromosomes most prominently and differences which are apparently not associated with copy number availability software for the statistical tools demonstrated in this article is available as bioconductor package gsealm supplementary information supplementary data are available at online
gene expression levels appear to be under pervasive stabilizing selection yet the genetic architecture underlying abundant gene expression diversity within and between populations remains elusive here we investigated the role of dominance in the segregation of cis and trans regulation within and between populations we used chromosome substitution lines of drosophila melanogaster to show that i of the genes that are differentially expressed between two homozygous lines are masked in the heterozygous suggesting that one of the substituted chromosomes contains a recessive allele ii such large masking is already obtained with heterozygous chromosomes originating from the same population with the time of divergence between chromosomes in heterozygous lines making only a small but significant contribution to the masking of variation observed in homozygous lines iii variation in gene expression due to trans regulation is biased toward greater deviations from additivity because of recessive and dominant alleles whereas variation due to cis regulation shows higher additivity and iv genetic divergence between second chromosomes is associated with increased cis regulation whereas the level of trans regulation shows little increase over the time scale studied our results indicate that cis acting alleles may be preferentially fixed by positive natural selection because of their higher additivity and that the disruption of gene expression by recessive variation with pervasive trans effects may be important for understanding gene expression variation within populations we suggest that widespread regulatory effects of recessive low frequency homozygous variation may provide a general mechanism mediating disease phenotypes and the genetic load of populations
allowing applications to survive hardware failure is an expensive undertaking which generally involves reengineering software to include complicated recovery logic as well as deploying special purpose hardware this represents a severe barrier to improving the dependability of large or legacy applications we describe the construction of a general and transparent high availability service that allows existing unmodified software to be protected from the failure of the physical machine on which it runs remus provides an extremely high degree of fault tolerance to the point that a running system can transparently continue execution on an alternate physical host in the face of failure with only seconds of downtime while completely preserving host state such as active network connections our approach encapsulates protected software in a virtual machine asynchronously propagates changed state to a backup host at frequencies as high as forty times a second and uses speculative execution to concurrently run the active vm slightly ahead of the replicated state
micrornas mirnas and transcription factors tfs are primary metazoan gene regulators whereas much attention has focused on finding the targets of both mirnas and tfs the transcriptional networks that regulate mirna expression remain largely unexplored here we present the first genome scale caenorhabditis elegans mirna regulatory network that contains experimentally mapped transcriptional tf mirna interactions as well as computationally predicted post transcriptional mirna tf interactions we find that this integrated mirna network contains mirna leftrightarrow tf composite feedback loops in which a tf that controls a mirna is itself regulated by that same mirna by rigorous network randomizations we show that such loops occur more frequently than expected by chance and hence constitute a genuine network motif interestingly mirnas and tfs in such loops are heavily regulated and regulate many targets this high flux capacity suggests that loops provide a mechanism of high information flow for the coordinate and adaptable control of mirna and tf regulons
programmed cell death is an integral component of c elegans development genetic studies in c elegans have led to the identification of more than two dozen genes that are important for the specification of which cells should live or die the activation of the suicide program and the dismantling and removal of dying cells molecular and biochemical studies have revealed the underlying conserved mechanisms that control these three phases of programmed cell death in particular an interplay of transcriptional regulatory cascades and networks involving ces ces hlh hlh tra and other transcriptional regulators is crucial in activating the expression of the key death inducing gene egl in cells destined to die a protein interaction cascade involving egl ced ced and ced results in the activation of the key cell death protease ced the activation of ced initiates the cell disassembly process and nuclear dna fragmentation which is mediated by the release of apoptogenic mitochondrial factors cps and wah and which involves multiple endo and exo nucleases such as nuc and seven crn nucleases the recognition and removal of the dying cell is mediated by two partially redundant signaling pathways involving ced ced and ced in one pathway and ced ced ced ced and psr in the other pathway further studies of programmed cell death in c elegans will continue to advance our understanding of how programmed cell death is regulated activated and executed in organisms
we present model based analysis of chip seq data macs which analyzes data generated by short read sequencers such as solexa s genome analyzer macs empirically models the shift size of chip seq tags and uses it to improve the spatial resolution of predicted binding sites macs also uses a dynamic poisson distribution to effectively capture local biases in the genome allowing for more robust predictions macs compares favorably to existing chip seq peak finding algorithms and is available
micrornas mirnas are short rnas that direct messenger rna degradation or disrupt mrna translation in a sequence dependent for more than a decade attempts to study the interaction of mirnas with their targets were confined to the untranslated regions of fuelling an underlying assumption that these regions are the principal recipients of mirna activity here we focus on the mouse nanog also known as and and demonstrate the existence of many naturally occurring mirna targets in their amino acid coding sequence cds some of the mouse targets analysed do not contain the mirna seed whereas others span exonexon junctions or are not conserved in the human and rhesus genomes mir mir and mir upregulated on retinoic acid induced differentiation of mouse embryonic stem cells target the cds of each transcription factor in various combinations leading to transcriptional and morphological changes characteristic of differentiating mouse embryonic stem cells and resulting in a new phenotype silent mutations at the predicted targets abolish mirna activity prevent the downregulation of the corresponding genes and delay the induced phenotype our findings demonstrate the abundance of cds located mirna targets some of which can be species specific and support an augmented model whereby animal mirnas exercise their control on mrnas through targets that can reside beyond the region
during the earliest stages of caenorhabditis elegans embryogenesis the transcription factor skn initiates development of the digestive system and other mesendodermal tissues postembryonic skn functions have not been elucidated skn binds to dna through a unique mechanism but is distantly related to basic leucine zipper proteins that orchestrate the major oxidative stress response in vertebrates and yeast here we show that despite its distinct mode of target gene recognition skn functions similarly to resist oxidative stress in c elegans during postembryonic stages skn regulates a key phase ii detoxification gene through constitutive and stress inducible mechanisms in the asi chemosensory neurons and intestine respectively skn is present in asi nuclei under normal conditions and accumulates in intestinal nuclei in response to oxidative stress skn mutants are sensitive to oxidative stress and have shortened lifespans skn represents a connection between developmental specification of the digestive system and one of its most basic functions resistance to oxidative and xenobiotic stress this oxidative stress response thus appears to be both widely conserved and ancient suggesting that the mesendodermal specification role of skn was predated by its function in these mechanisms
to date more than genes have been identified in the nematode caenorhabditis elegans which when mutated lead to an increase in lifespan of those tested all confer an increased resistance to oxidative stress in addition the lifespan of c elegans can also be extended by the administration of synthetic superoxide dismutase catalase mimetics these compounds also appear to confer resistance to oxidative damage since they protect against paraquat treatment the protective effects of these compounds are apparent with treatment during either development or adulthood these findings have demonstrated that pharmacological intervention in the aging process is possible and that these compounds can provide important information about the underlying mechanisms to date such interventions have targeted known processes rather than screening compound libraries because of the limitations of assessing lifespan in nematodes however we have recently developed a microplate based assay that allows for a rapid and objective score of nematode survival at rates many times higher than previously possible this system now provides the opportunity to perform high throughput screens for compounds that affect nematode survival in the face of acute oxidative stress and will facilitate the identification of novel drugs that extend lifespan
abstract background the exponential growth of available biological data has caused bioinformatics to be rapidly moving towards a data intensive computational science as a result the computational power needed by bioinformatics applications is growing exponentially as well the recent emergence of accelerator technologies has made it possible to achieve an excellent improvement in execution time for many bioinformatics applications compared to current general purpose platforms in this paper we demonstrate how the playstation r powered by the cell broadband engine can be used as a computational platform to accelerate the smith waterman algorithm results for large datasets our implementation on the playstation r provides a significant improvement in running time compared to other implementations such as ssearch striped smith waterman and cuda our implementation achieves a peak performance of up to mcups conclusions the results from our experiments demonstrate that the playstation r console can be used as an efficient low cost computational platform for high performance sequence applications
abstract the literature on effects of habitat fragmentation on biodiversity is huge it is also very diverse with different authors measuring fragmentation in different ways and as a consequence drawing different conclusions regarding both the magnitude and direction of its effects habitat fragmentation is usually defined as a landscape scale process involving both habitat loss and the breaking apart of habitat results of empirical studies of habitat fragmentation are often difficult to interpret because a many researchers measure fragmentation at the patch scale not the landscape scale and b most researchers measure fragmentation in ways that do not distinguish between habitat loss and habitat fragmentation per se i e the breaking apart of habitat after controlling for habitat loss empirical studies to date suggest that habitat loss has large consistently negative effects on biodiversity habitat fragmentation per se has much weaker effects on biodiversity that are at least as likely to be positive as negative therefore to correctly interpret the influence of habitat fragmentation on biodiversity the effects of these two components of fragmentation must be measured independently more studies of the independent effects of habitat loss and fragmentation per se are needed to determine the factors that lead to positive versus negative effects of fragmentation per se i suggest that the term fragmentation should be reserved for the breaking apart of habitat independent of loss
what makes us human specialists in each discipline respond through the lens of their own expertise in fact anthropogeny explaining the origin of humans requires a transdisciplinary approach that eschews such barriers here we take a genomic and genetic perspective towards molecular variation explore systems analysis of gene expression and discuss an organ systems approach rejecting any genes versus environment dichotomy we then consider genome interactions with environment behaviour and culture finally speculating that aspects of human uniqueness arose because of a primate evolutionary trend towards increasing and irreversible dependence on learned behaviours and culture perhaps relaxing allowable thresholds for large scale diversity
post transcriptional silencing of plant genes using anti sense or co suppression constructs usually results in only a modest proportion of silenced individuals recent work has demonstrated the potential for constructs encoding self complementary hairpin rna hprna to efficiently silence genes in this study we examine design rules for efficient gene silencing in terms of both the proportion of independent transgenic plants showing silencing and the degree of silencing using hprna constructs containing sense anti sense arms ranging from to gave efficient silencing in a wide range of plant species and inclusion of an intron in these constructs had a consistently enhancing effect intron containing constructs ihprna generally gave of independent transgenic plants showing silencing the degree of silencing with these constructs was much greater than that obtained using either co suppression or anti sense constructs we have made a generic vector phannibal that allows a simple single pcr product from a gene of interest to be easily converted into a highly effective ihprna silencing construct we have also created a high throughput vector phellsgate that should facilitate the cloning of gene libraries or large numbers of defined genes such as those in est collections using an in vitro recombinase system this system may facilitate the large scale determination and discovery of plant gene functions in the same way as rnai is being used to examine gene function in elegans
pairwise sequence alignment is a ubiquitous tool for inferring the evolution and function of dna rna and protein sequences it is therefore essential to identify alignments arising by chance alone i e spurious alignments on one hand if an entire alignment is spurious statistical techniques for identifying and eliminating it are well known on the other hand if only a part of the alignment is spurious elimination is much more problematic in practice even the sizes and frequencies of spurious subalignments remain unknown this article shows that some common scoring schemes tend to overextend alignments and generate spurious alignment flanks up to hundreds of base pairs amino acids in length in the ucsc genome database e g spurious flanks probably comprise of the human fugu genome alignment to evaluate the possibility that chance alone generated a particular flank on a particular pairwise alignment we provide a simple overalignment p value the overalignment p value can identify spurious alignment flanks thereby eliminating potentially misleading inferences about evolution and function moreover by explicitly demonstrating the tradeoff between over and under alignment our methods guide the rational choice of scoring schemes for various tasks
background reproducibility is a fundamental requirement in scientific experiments some recent publications have claimed that microarrays are unreliable because lists of differentially expressed genes degs are not reproducible in similar experiments meanwhile new statistical methods for identifying degs continue to appear in the scientific literature the resultant variety of existing and emerging methods exacerbates confusion and continuing debate in the microarray community on the appropriate choice of methods for identifying reliable deg lists results using the data sets generated by the microarray quality control maqc project we investigated the impact on the reproducibility of deg lists of a few widely used gene selection procedures we present comprehensive results from inter site comparisons using the same microarray platform cross platform comparisons using multiple microarray platforms and comparisons between microarray results and those from taqman the widely regarded standard gene expression platform our results demonstrate that previously reported discordance between deg lists could simply result from ranking and selecting degs solely by statistical significance p derived from widely used simple t tests when fold change fc is used as the ranking criterion with a non stringent p value cutoff filtering the deg lists become much more reproducible especially when fewer genes are selected as differentially expressed as is the case in most microarray studies and the instability of short deg lists solely based on p value ranking is an expected mathematical consequence of the high variability of the t values the more stringent the p value threshold the less reproducible the deg list is these observations are also consistent with results from extensive simulation calculations conclusion we recommend the use of fc ranking plus a non stringent p cutoff as a straightforward and baseline practice in order to generate more reproducible deg lists specifically the p value cutoff should not be stringent too small and fc should be as large as possible our results provide practical guidance to choose the appropriate fc and p value cutoffs when selecting a given number of degs the fc criterion enhances reproducibility whereas the p criterion balances sensitivity specificity
the recurrent fixation of newly arising beneficial mutations in a species reduces levels of linked neutral variability models positing frequent weakly beneficial substitutions or alternatively rare strongly selected substitutions predict similar average effects on linked neutral variability if the product of the rate and strength of selection is held constant we propose an approximate bayesian abc polymorphism based estimator that can be used to distinguish between these models and apply it to multi locus data from drosophila melanogaster we investigate the extent to which inference about the strength of selection is sensitive to assumptions about the underlying distributions of the rates of substitution and recombination the strength of selection heterogeneity in mutation rate as well as the population s demographic history we show that assuming fixed values of selection parameters in estimation leads to overestimates of the strength of selection and underestimates of the rate we estimate parameters for an african population of d melanogaster approximately and compare these to previous estimates finally we show that surveying larger genomic regions is expected to lend much more discriminatory power to the approach it will thus be of great interest to apply this method to emerging whole genome polymorphism data sets in taxa
the tragedy of the digital commons does not prevent the copious voluntary production of content that one witnesses in the web we show through an analysis of a massive data set from texttt youtube that the productivity exhibited in crowdsourcing exhibits a strong positive dependence on attention measured by the number of downloads conversely a lack of attention leads to a decrease in the number of videos uploaded and the consequent drop in productivity which in many cases asymptotes to no uploads whatsoever moreover uploaders compare themselves to others when having low productivity and to themselves when exceeding threshold
this paper was first published online by the internet society in december and is being re published in acm sigcomm computer communication review because of its historic import it was written at the urging of its primary editor the late barry leiner he felt that a factual rendering of the events and activities associated with the development of the early internet would be a valuable contribution the contributing authors did their best to incorporate only factual material into this document there are sure to be many details that have not been captured in the body of the document but it remains one of the most accurate renderings of the early period of available
abstract our knowledge on tissue and disease specific functions of human genes is rather limited and highly context specific here we developed a method for the comparison of mrna expression levels of most human genes across affymetrix gene expression array experiments representing normal human tissue types cancer types and other diseases this database of gene expression patterns in normal human tissues and pathological conditions covers million datapoints and is available at the website
oxidative stress is a major contributor to the alterations of various pathological conditions including neurodegenerative and neuropsychiatric problems antioxidative flavonoids ubiquitously included in vegetables fruits and teas are expected to prevent degenerative diseases recently flavonoids have been characterized as neuroprotectants in the treatment of various neurological disorders the present study was designed to investigate protective effects of quercetin a bioflavonoid against acute immobilization induced behavioral and biochemical alterations in mice mice were immobilized for a period of hours quercetin and mg kg i p was administered minutes before subjecting the animals to acute stress behavioral tests mirror chamber actophotometer and tail flick test and biochemical analysis malondialdehyde reduced glutathione catalase nitrite and protein levels were subsequently performed acute immobilization stress for a period of hours caused severe anxiety analgesia and impaired motor activity in mice biochemical analyses revealed an increase in malondialdehyde and nitrite levels as well as partial depletion of reduced glutathione and catalase activity in immobilization stressed brain behavioral and biochemical parameters were significantly altered as compared to naive mice pretreatment with quercetin and mg kg i p significantly reversed immobilized stress induced anxiety and analgesia and reduced locomotor activity biochemically quercetin treatment attenuated malondialdehyde accumulation and nitrite activity and restored the depleted reduced glutathione and catalase activity neuroprotective effects of quercetin were significantly improved as compared to control immobilized stressed animals results suggest that neuroprotective properties of quercetin can be used in the treatment and management of stress and disorders
recent advances in web and cyberinfrastructure have enabled new levels of interactions and interconnections among individuals documents data analytic tools and concepts for communities to be more effective in using these resources it is even more crucial that they have tools help them identify the right expertise or knowledge resources from within this large multidimensional network cyberinfrastructure knowledge networks on the web ci know is a suite of web based tools that facilitates discovery of resources within communities cknow facilitates discovery by implementing a network recommendation system that incorporates social motivations for why we create maintain and dissolve our knowledge network ties the network data is captured by automated harvesting of digital resources using web crawlers text miners tagging tools that automatically generate community oriented metadata and scientometric data such as co authorship and citations based on this knowledge network the ci know recommender system produces personalized search results through two steps identify matching entities according to their metadata and network statistics and select the best fits according to requester s perspectives and connections in the social network integrated with community web portals ci know navigation and auditing portlets provide analysis and visualization tools for community members and serves as a research testbed to test social networks models about individuals motivations for seeking expertise from specific resources people documents datasets etc as a proof of concept this paper demonstrates how ci know has been integrated with the nci supported tobacco informatics grid tobig to facilitate knowledge sharing in the tobacco control community
background random community genomes metagenomes are now commonly used to study microbes in different environments over the past few years the major challenge associated with metagenomics shifted from generating to analyzing sequences high throughput low cost next generation sequencing has provided access to metagenomics to a wide range of researchers results a high throughput pipeline has been constructed to provide high performance computing to all researchers interested in using metagenomics the pipeline produces automated functional assignments of sequences in the metagenome by comparing both protein and nucleotide databases phylogenetic and functional summaries of the metagenomes are generated and tools for comparative metagenomics are incorporated into the standard views user access is controlled to ensure data privacy but the collaborative environment underpinning the service provides a framework for sharing datasets between multiple users in the metagenomics rast all users retain full control of their data and everything is available for download in a variety of formats conclusion the open source metagenomics rast service provides a new paradigm for the annotation and analysis of metagenomes with built in support for multiple data sources and a back end that houses abstract data types the metagenomics rast is stable extensible and freely available to all researchers this service has removed one of the primary bottlenecks in metagenome sequence analysis the availability of high performance computing for annotating the data http metagenomics org
physical activity has been proposed as a behavior intervention that promotes mental health and some of the benefits induced by exercise have been related to the glutamatergic system indeed glutamate is the most abundant excitatory neurotransmitter in brain thus we evaluated if voluntary exercise in mice could modulate glutamatergic synapses at level of postsynaptic density psd through western blot we found that exercise during month increased glutamatergic related protein content in psd from cortex of mice exercise increased the immunocontent of sap grip and in less extent and psd proteins the overall content of nmda subunits and were not altered in mice that had exercised however the phosphorylated nmda subunits phospho and phospho showed a strong increase because exercise increased the content of phosphorylated forms of nmda receptors we evaluated the binding of mk a specific ligand that binds to open nmda channel exercise increased the binding of mk in cortical cellular membranes in altogether our results point to a modulation of glutamatergic synapses by exercise with likely implications in the exercise induced health
in copyright lobbyists succeeded in persuading congress to enact laws greatly expanding copyright owners control over individuals private uses of their works the efforts to enforce these new rights have resulted in highly publicized legal battles between established media and new upstarts in this enlightening and well argued book law professor jessica litman questions whether copyright laws crafted by lawyers and their lobbyists really make sense for the vast majority of us should every interaction between ordinary consumers and copyright protected works be restricted by law is it practical to enforce such laws or expect consumers to obey them what are the effects of such laws on the exchange of information in a free society litman s critique exposes the copyright law as an incoherent patchwork she argues for reforms that reflect common sense and the way people actually behave in their daily digital interactions this paperback edition includes an afterword that comments on recent developments such as the end of the napster story the rise of peer to peer file sharing the escalation of a full fledged copyright war the filing of lawsuits against thousands of individuals and the june supreme court decision in the case
a fundamental task in sequence analysis is to calculate the probability of a multiple alignment given a phylogenetic tree relating the sequences and an evolutionary model describing how sequences change over time however the most widely used phylogenetic models only account for residue substitution events we describe a probabilistic model of a multiple sequence alignment that accounts for insertion and deletion events in addition to substitutions given a phylogenetic tree using a rate matrix augmented by the gap character starting from a continuous markov process we construct a non reversible generative birth death evolutionary model for insertions and deletions the model assumes that insertion and deletion events occur one residue at a time we apply this model to phylogenetic tree inference by extending the program dnaml in phylip using standard benchmarking methods on simulated data and a new concordance test benchmark on real ribosomal rna alignments we show that the extended program dnamlepsilon improves accuracy relative to the usual approach of ignoring gaps while retaining the computational efficiency of the felsenstein algorithm
type diabetes is a debilitating autoimmune disease that results from t cell mediated destruction of insulin producing beta cells its incidence has increased during the past several decades in developed suggesting that changes in the environment including the human microbial environment may influence disease pathogenesis the incidence of spontaneous in non obese diabetic nod mice can be affected by the microbial environment in the animal housing or by exposure to microbial stimuli such as injection with mycobacteria or various microbial here we show that specific pathogen free nod mice lacking protein an adaptor for multiple innate immune receptors that recognize microbial stimuli do not develop the effect is dependent on commensal microbes because germ free negative nod mice develop robust diabetes whereas colonization of these germ free negative nod mice with a defined microbial consortium representing bacterial phyla normally present in human gut attenuates we also find that deficiency changes the composition of the distal gut microbiota and that exposure to the microbiota of specific pathogen free negative nod donors attenuates in germ free nod recipients together these findings indicate that interaction of the intestinal microbes with the innate immune system is a critical epigenetic factor predisposition
the role of prototypes is well established in the field of hci and design a lack of knowledge however about the fundamental nature of prototypes still exists researchers have attempted to identify different types of prototypes such as low vs high fidelity prototypes but these attempts have centered on evaluation rather than support of design exploration there have also been efforts to provide new ways of thinking about the activity of using prototypes such as experience prototyping and paper prototyping but these efforts do not provide a discourse for understanding fundamental characteristics of prototypes in this article we propose an anatomy of prototypes as a framework for prototype conceptualization we view prototypes not only in their role in evaluation but also in their generative role in enabling designers to reflect on their design activities in exploring a design space we base this framework on the findings of two case studies that reveal two key dimensions prototypes as filters and prototypes as manifestations we explain why these two dimensions are important and how this conceptual framework can benefit our field by establishing more solid and systematic knowledge about prototypes prototyping
are there multi neuron computational modules in the c elegans nervous system we attempt to answer this question by applying a systematic statistical approach to the c elegans wiring diagram white et al our approach is to identify multi neuron inter connectivity patterns that are significantly over represented in the actual wiring diagram compared to the randomized wiring diagram which preserves the number of synapses per neuron but not the identity of connections to do this we compute the numbers of occurrences of all n neuron n inter connectivity patterns in the actual and randomized wiring diagrams this statistical approach confirms previous reports of the over abundance of reciprocal connections and triangular connectivity patterns white et al moreover we discover several new four neuron and five neuron inter connectivity patterns that appear significantly more often in c elegans than in randomized wiring diagrams we suggest that these inter connectivity patterns may serve as computational modules that perform functions
abstract background r is the preferred tool for statistical analysis of many bioinformaticians due in part to the increasing number of freely available analytical methods such methods can be quickly reused and adapted to each particular experiment however in experiments where large amounts of data are generated for example using high throughput screening devices the processing time required to analyze data is often quite long a solution to reduce the processing time is the use of parallel computing technologies because r does not support parallel computations several tools have been developed to enable such technologies however these tools require multiple modications to the way r programs are usually written or run although these tools can finally speed up the calculations the time skills and additional resources required to use them are an obstacle for most bioinformaticians results we have designed and implemented an r add on package r parallel that extends r by adding user friendly parallel computing capabilities with r parallel any bioinformatician can now easily automate the parallel execution of loops and benefit from the multicore processor power of today s desktop computers using a single and simple function r parallel can be integrated directly with other existing r packages with no need to change the implemented algorithms the processing time can be approximately reduced n fold n being the number of available processor cores conclusions r parallel saves bioinformaticians time in their daily tasks of analyzing experimental data it achieves this objective on two fronts first by reducing development time of parallel programs by avoiding reimplementation of existing methods and second by reducing processing time by speeding up computations on current desktop computers future work is focused on extending the envelope of r parallel by interconnecting and aggregating the power of several computers both existing office computers and clusters
recent results indicate that the longevity of both invertebrates and vertebrates can be altered through genetic manipulation and pharmacological intervention most of these interventions involve alterations of one or more of the following insulin igf i signaling pathway caloric intake stress resistance and nuclear structure how longevity regulation relates to aging per se is less clear but longevity increases are usually accompanied by extended periods of good health how these results will translate to primate aging and longevity remains to shown
the coupling of optical and mechanical degrees of freedom is the underlying principle of many techniques to measure mechanical displacement from macroscale gravitational wave detectors to microscale cantilevers used in scanning probe microscopy recent experiments have reached a regime where the back action of photons caused by radiation pressure can influence the optomechanical dynamics giving rise to a host of long anticipated phenomena here we review these developments and discuss the opportunities for innovative technology as well as for fundamental science
gene regulatory networks have an important role in every process of life including cell differentiation metabolism the cell cycle and signal transduction by understanding the dynamics of these networks we can shed light on the mechanisms of diseases that occur when these cellular processes are dysregulated accurate prediction of the behaviour of regulatory networks will also speed up biotechnological projects as such predictions are quicker and cheaper than lab experiments computational methods both for supporting the development of network models and for the analysis of their functionality have already proved to be a valuable tool
abstract asi abs wikipedia the free online encyclopedia that anyone can edit is having a huge impact on how a great many people gather information about the world so it is important for epistemologists and information scientists to ask whether people are likely to acquire knowledge as a result of having access to this information source in other words is wikipedia having good epistemic consequences after surveying the various concerns that have been raised about the reliability of wikipedia this article argues that the epistemic consequences of people using wikipedia as a source of information are likely to be quite good according to several empirical studies the reliability of wikipedia compares favorably to the reliability of traditional encyclopedias furthermore the reliability of wikipedia compares even more favorably to the reliability of those information sources that people would be likely to use if wikipedia did not exist viz web sites that are as freely and easily accessible as wikipedia in addition wikipedia has a number of other epistemic virtues e g power speed and fecundity that arguably outweigh any deficiency in terms of reliability even so epistemologists and information scientists should certainly be trying to identify changes or alternatives to wikipedia that will bring about even better epistemic consequences this article suggests that to improve wikipedia we need to clarify what our epistemic values are and to better understand why wikipedia works as well as it does somebody who reads wikipedia is rather in the position of a visitor to a public restroom says mr mchenry britannica s former editor it may be obviously dirty so that he knows to exercise great care or it may seem fairly clean so that he may be lulled into a false sense of security what he certainly does not know is who has used the facilities before him one wonders whether people like mr mchenry would prefer there to be no public lavatories at all the economist vol pp
summary the beneficial effects of polyphenol compounds in fruits and vegetables are mainly extrapolated from in vitro studies or short term dietary supplementation studies due to cost and duration relatively little is known about whether dietary polyphenols are beneficial in whole animals particularly with respect to aging to address this question we examined the effects of blueberry polyphenols on lifespan and aging of the nematode caenorhabditis elegans a useful organism for such a study we report that a complex mixture of blueberry polyphenols increased lifespan and slowed aging related declines in c elegans we also found that these benefits did not just reflect antioxidant activity in these compounds for instance blueberry treatment increased survival during acute heat stress but was not protective against acute oxidative stress the blueberry extract consists of three major fractions that all contain antioxidant activity however only one fraction enriched in proanthocyanidin compounds increased c elegans lifespan and thermotolerance to further determine how polyphenols prolonged c elegans lifespan we analyzed the genetic requirements for these effects prolonged lifespan from this treatment required the presence of a camkii pathway that mediates osmotic stress resistance though not other pathways that affect stress resistance and longevity in conclusion polyphenolic compounds in blueberries had robust and reproducible benefits during aging that were separable from effects
pnas recent years have seen much interest in the study of systems characterized by multiple interacting components a class of statistical models called graphical models in which graphs are used to represent probabilistic relationships between variables provides a framework for formal inference regarding such systems in many settings the object of inference is the network structure itself this problem of network inference is well known to be a challenging one however in scientific settings there is very often existing information regarding network connectivity a natural idea then is to take account of such information during inference this article addresses the question of incorporating prior information into network inference we focus on directed models called bayesian networks and use markov chain monte carlo to draw samples from posterior distributions over network structures we introduce prior distributions on graphs capable of capturing information regarding network features including edges classes of edges degree distributions and sparsity we illustrate our approach in the context of systems biology applying our methods to network inference in signaling
caenorhabditis elegans has recently been developed as a model for microbial pathogenesis yet little is known about its immunological defenses previous work implicated insulin signaling in mediating pathogen resistance in a manner dependent on the transcriptional regulator daf but the mechanism has not been elucidated we present evidence that c elegans like mammalian phagocytes produces reactive oxygen species ros in response to pathogens signs of oxidative stress occur in the intestine the site of the host pathogen interface suggesting that ros release is localized to this tissue evidence includes the accumulation of lipofuscin a pigment resulting from oxidative damage at this site in addition sod a superoxide dismutase regulated by daf is induced in intestinal tissue after exposure to pathogenic bacteria moreover we show that the oxidative stress response genes sod and ctl are required for daf mediated resistance to e faecalis using a c elegans killing assay we propose a model whereby c elegans responds to pathogens by producing ros in the intestine while simultaneously inducing a daf dependent oxidative stress response to protect adjacent tissues because insulin signaling mutants over produce oxidative stress response enzymes the model provides an explanation for their increased resistance pathogens
we formalize the problem of recovering the evolutionary history of a set of genomes that are related to an unseen common ancestor genome by operations of speciation deletion insertion duplication and rearrangement of segments of bases the problem is examined in the limit as the number of bases in each genome goes to infinity in this limit the chromosomes are represented by continuous circles or line segments for such an infinite sites model we present a polynomial time algorithm to find the most parsimonious evolutionary history of any set of related present genomes
the topology of metabolic networks may provide important insights not only into the metabolic capacity of species but also into the habitats in which they evolved here we introduce the concept of a metabolic network s seed setthe set of compounds that based on the network topology are exogenously acquiredand provide a methodological framework to computationally infer the seed set of a given network such seed sets form ecological interfaces between metabolic networks and their surroundings approximating the effective biochemical environment of each species analyzing the metabolic networks of species and identifying the seed set of each species we present a comprehensive large scale reconstruction of such predicted metabolic environments the seed sets composition significantly correlates with several basic properties characterizing the species environments and agrees with biological observations concerning major adaptations species whose environments are highly predictable e g obligate parasites tend to have smaller seed sets than species living in variable environments phylogenetic analysis of the seed sets reveals the complex dynamics governing gain and loss of seeds across the phylogenetic tree and the process of transition between seed and non seed compounds our findings suggest that the seed state is transient and that seeds tend either to be dropped completely from the network or to become non seed compounds relatively fast the seed sets also permit a successful reconstruction of a phylogenetic tree of life the reverse ecology approach presented lays the foundations for studying the evolutionary interplay between organisms and their habitats on a scale
in yeast the transcription factor is dephosphorylated and translocates into the nucleus in response to extracellular calcium here we show using time lapse microscopy that exhibits short bursts of nuclear localization typically lasting min that occur stochastically in individual cells and propagate to the expression of downstream genes strikingly calcium concentration controls the frequency but not the duration of localization bursts using an analytic model we also show that this frequency modulation of bursts ensures proportional expression of multiple target genes across a wide dynamic range of expression levels independent of promoter characteristics we experimentally confirm this theory with natural and synthetic target promoters another stress response transcription factor exhibits similar but largely uncorrelated localization bursts under calcium stress suggesting that frequency modulation regulation of localization bursts may be a general control strategy used by the cell to coordinate multi gene responses to signals
objective to assess the evidence from rigorous clinical trials systematic reviews and meta analyses of complementary and alternative therapies for treating neuropathic and neuralgic pain methods systematic searches were carried out in the databases medline embase amed scopus the cochrane database of systematic reviews natural standard and the natural medicines comprehensive database each database was searched from its respective inception until march to be included trials were required to state that they were randomized systematic reviews and meta analyses were included if based on the results of randomized trials no language restrictions were imposed results five relevant systematic reviews and meta analyses and additional trials met the inclusion criteria and were reviewed data on the following complementary and alternative medicine treatments were identified acupuncture electrostimulation herbal medicine magnets dietary supplements imagery and spiritual healing conclusions on the basis of our findings the evidence is not fully convincing for most complementary and alternative medicine modalities in relieving neuropathic or neuralgic pain however for topically applied capsaicin there is evidence of effectiveness beyond placebo the evidence can be classified as encouraging and warrants further study for cannabis extract magnets carnitine electrostimulation
speculation on the implications of increased use of information and communication technologies in scientific research suggests that use of databases may change the processes and the outcomes of knowledge production most attention focuses on databases as a large scale means of communicating research but they can also be used on a much smaller scale as research tools this paper presents an ethnographic study of the development of a mouse genome mapping resource organized around a database through an examination of the natural social and digital orderings that arise in the construction of the resource it argues that the use of databases in science at least in this kind of project is unlikely to produce wholesale change such changes as do occur in work practices communication regimes and knowledge outcomes are dependent on the orderings that each database embodies and is embedded within instead of imposing its own computer logic the database provides a focus for specifying and tying together particular natural and social orderings the database does not act as an independent agent of change but is an emergent structure that needs to be embedded in an appropriate set of practices
opsin the ligand free form of the g protein coupled receptor rhodopsin at low ph adopts a conformationally distinct active g protein binding state known as ops a synthetic peptide derived from the main binding site of the heterotrimeric g proteinthe carboxy terminus of the subunit gct stabilizes ops here we present the crystal structure of the bovine ops gct peptide complex gct binds to a site in opsin that is opened by an outward tilt of transmembrane helix tm a pairing of and and a restructured kink contacts along the inner surface of and induce an helical conformation in gct with a c terminal reverse turn main chain carbonyl groups in the reverse turn constitute the centre of a hydrogen bonded network which links the two receptor regions containing the conserved e d ry and npxxy x motifs on the basis of the ops gct structure and known conformational changes in g we discuss signal transfer from the receptor to the g protein nucleotide site
web has brought about several new applications that have enabled arbitrary subsets of users to communicate with each other on a social basis such communication increasingly happens not just on facebook and myspace but on several smaller network applications such as twitter and dodgeball we present a detailed characterization of twitter an application that allows users to send short messages we gathered three datasets covering nearly users including constrained crawls of the twitter network using two different methodologies and a sampled collection from the publicly available timeline we identify distinct classes of twitter users and their behaviors geographic growth patterns and current size of the network and compare crawl results obtained under rate constraints
abstract background biological pathways are a useful abstraction of biological concepts and software tools to deal with pathway diagrams can help biological research pathvisio is a new visualization tool for biological pathways that mimics the popular genmapp tool with a completely new java implementation that allows better integration with other open source projects the genmapp mapp file format is replaced by gpml a new xml file format that provides seamless exchange of graphical pathway information among multiple programs results pathvisio can be combined with other bioinformatics tools to open up three possible uses visual compilation of biological knowledge interpretation of high throughput expression datasets and computational augmentation of pathways with interaction information pathvisio is open source software and available at http www pathvisio org conclusion pathvisio is a graphical editor for biological pathways with flexibility and ease of use as goals
solid tumours are an enormous cancer burden and a major therapeutic challenge the cancer stem cell csc hypothesis provides an attractive cellular mechanism to account for the therapeutic refractoriness and dormant behaviour exhibited by many of these tumours there is increasing evidence that diverse solid tumours are hierarchically organized and sustained by a distinct subpopulation of cscs direct evidence for the csc hypothesis has recently emerged from mouse models of epithelial tumorigenesis although alternative models of heterogeneity also seem to apply the clinical relevance of cscs remains a fundamental issue but preliminary findings indicate that specific targeting may possible
summary although prefrontal cortex has been implicated in the cognitive regulation of emotion the cortical subcortical interactions that mediate this ability remain poorly understood to address this issue we identified a right ventrolateral prefrontal region vlpfc whose activity correlated with reduced negative emotional experience during cognitive reappraisal of aversive images we then applied a pathway mapping analysis on subcortical regions to locate mediators of the association between vlpfc activity and reappraisal success i e reductions in reported emotion results identified two separable pathways that together explained not vert of the reported variance in self reported emotion a path through nucleus accumbens that predicted greater reappraisal success and a path through ventral amygdala that predicted reduced reappraisal success i e more negative emotion these results provide direct evidence that vlpfc is involved in both the generation and regulation of emotion through different subcortical pathways suggesting a general role for this region in processes
insulin igf like signaling iis is central to growth and metabolism and has a conserved role in aging in c elegans reductions in iis increase stress resistance and longevity effects that require the iis inhibited foxo protein daf the c elegans transcription factor skn also defends against oxidative stress by mobilizing the conserved phase detoxification response here we show that iis not only opposes daf but also directly inhibits skn in parallel the iis kinases akt and sgk phosphorylate skn and reduced iis leads to constitutive skn nuclear accumulation in the intestine and skn target gene activation skn contributes to the increased stress tolerance and longevity resulting from reduced iis and delays aging when expressed transgenically furthermore skn that is constitutively active increases life span independently of daf our findings indicate that the transcription network regulated by skn promotes longevity and is an important direct target iis
pnas messenger rna molecules are tightly regulated mostly through interactions with proteins and other rnas but the mechanisms that confer the specificity of such interactions are poorly understood it is clear however that this specificity is determined by both the nucleotide sequence and secondary structure of the mrna here we develop rnapromo an efficient computational tool for identifying structural elements within mrnas that are involved in specifying posttranscriptional regulations by analyzing experimental data on mrna decay rates we identify common structural elements in fast decaying and slow decaying mrnas and link them with binding preferences of several rna binding proteins we also predict structural elements in sets of mrnas with common subcellular localization in mouse neurons and fly embryos finally by analyzing pre microrna stemloops we identify structural differences between pre micrornas of animals and plants which provide insights into the mechanism of microrna biogenesis together our results reveal unexplored layers of posttranscriptional regulations in groups of rnas and are therefore an important step toward a better understanding of the regulatory information conveyed within rna molecules our new rna motif discovery tool is online
the rise of digital technologies has the potential to open new directions in ethnography despite the ubiquity of these technologies their infiltration into popular sociological research methods is still limited compared to the insatiable uptake of online scholarly research portals this article argues that social researchers cannot afford to continue this trend building upon pioneering work in digital ethnography i critically examine the possibilities and problems of four new technologies online questionnaires digital video social networking websites and blogs and their potential impacts on the research relationship the article concludes that a balanced combination of physical and digital ethnography not only gives researchers a larger and more exciting array of methods but also enables them to demarginalize the voice of respondents however access to these technologies remains stratified by class race and gender of both researchers respondents
gr whole genome hybridization studies have suggested that the nuclear genomes of accessions natural strains of arabidopsis thaliana can differ by several percent of their sequence to examine this variation and as a first step in the genomes project for this species we produced to fold coverage in illumina sequencing by synthesis sbs reads for the reference accession col and two divergent strains bur and tsu we aligned reads to the reference genome sequence to assess data quality metrics and to detect polymorphisms alignments revealed unique single nucleotide polymorphisms snps and unique to bp indels in the divergent accessions at a specificity of and over potential errors in the reference genome sequence we also identified mb of the bur and tsu genomes as being either extremely dissimilar deleted or duplicated relative to the reference genome to obtain sequences for these regions we incorporated the velvet assembler into a targeted de novo assembly method this approach yielded high confidence contigs that were anchored to flanking sequences and harbored indels as large as bp our methods are broadly applicable for polymorphism discovery in moderate to large genomes even at highly diverged loci and we established by subsampling the illumina sbs coverage depth required to inform a broad range of functional and evolutionary studies our pipeline for aligning reads and predicting snps and indels shore is available for download at org
rna micrornas are short nucleotides noncoding rnas that regulate the stability and translation of mrna targets a number of computational algorithms have been developed to help predict which micrornas are likely to regulate which genes gene expression profiling of biological systems where micrornas might be active can yield hundreds of differentially expressed genes the commonly used public microrna target prediction databases facilitate gene by gene searches however integration of micrornamrna target predictions with gene expression data on a large scale using these databases is currently cumbersome and time consuming for many researchers we have developed a desktop software application which for a given target prediction database retrieves all microrna mrna functional pairs represented by an experimentally derived set of genes furthermore for each microrna the software computes an enrichment statistic for overrepresentation of predicted targets within the gene set which could help to implicate roles for specific micrornas and microrna regulated genes in the system under study currently the software supports searching of results from pictar targetscan and miranda algorithms in addition the software can accept any user defined set of gene to class associations for searching which can include the results of other target prediction algorithms as well as gene annotation or gene to pathway associations a search using our software of genes transcriptionally regulated in vitro by estrogen in breast cancer uncovered numerous targeting associations for specific micrornasabove what could be observed in randomly generated gene listssuggesting a role for micrornas in mediating the estrogen response the software and excel vba source code are freely at
an epistasis map e map was constructed in the fission yeast schizosaccharomyces pombe by systematically measuring the phenotypes associated with pairs of mutations this high density quantitative genetic interaction map focused on various aspects of chromosome function including transcription regulation and dna repair replication the e map uncovered a previously unidentified component of the rna interference rnai machinery and linked the rnai pathway to several other biological processes comparison of the s pombe e map to an analogous genetic map from the budding yeast revealed that whereas negative interactions were conserved between genes involved in similar biological processes positive interactions and overall genetic profiles between pairs of genes coding for physically associated proteins were even more conserved hence conservation occurs at the level of the functional module protein complex but the genetic cross talk between modules can substantially
this case study centers on the blue ridge parkway brp nc and va which is considered one of the most beautiful roads in the united states however very little has been published about the origins of its physical design or the design process this article examines the design attributes of the brp in conjunction with the eighteenth century aesthetic theories of william hogarth and edmund burke the theories proposed that specific principles such as the serpentine line of grace and smoothly transitioned variety were fundamental to beauty these attributes and others are reviewed in conjunction with the brp qualitative methods were used to explore the hypothesis including secondary and primary sources field work and elite interviews with persons associated with the brp design results indicate that the brp appears to embody several of the principles of hogarth and burke a direct link to hogarth and burke was not found that is the brp was not consciously designed according to their theories however links lo the eighteenth century theories were found in the designer s education and apprenticeship twentieth century empirical research appears to support some of hogarth s and burke s propositions this inquiry is unusual in contemporary road related research in that it uses normative art theory as a basis for examining road design it concludes that the fine arts training of landscape architects was important to the design approach and aesthetic success of the brp and suggests ways of incorporating the classic aesthetic principles in future road design abstract author
contact jdwren gmail com as biomedical data accumulates the need to store share and organize it grows consequently the number of internet accessible databases has been rapidly growing on an annual basis bioinformatics regularly publishes descriptions of biomedically relevant databases nucleic acids research has published an annual database issue since and now a new open access journal database the journal of biological databases and curation will soon be launched by oxford university press in http www oxfordjournals org databa since databases can be made publicly available on the internet without publication it is worth considering what factors prioritize publication of database descriptions in a peer reviewed journal in general publication of a database description in a journal advertises it as a valuable resource for scientific research implicitly it is assumed that this resource is publicly available most likely for free and will be maintained however therein lies the problem database papers are simply not of the same nature as regular research articles over time some databases simply become inaccessible some are created but not maintained or updated and some databases are never used galperin thus for database creators reviewers and journal editors there are several additional considerations to judge prior to publication how potentially valuable these new databases may bioinformatics
models and data in is the first truly practical introduction to modern statistical methods for ecology in step by step detail the book teaches ecology graduate students and researchers everything they need to know in order to use maximum likelihood information theoretic and bayesian techniques to analyze their own data using the programming language r drawing on extensive experience teaching these techniques to graduate students in ecology benjamin bolker shows how to choose among and construct statistical models for data estimate their parameters and confidence limits and interpret the results the book also covers statistical frameworks the philosophy of statistical modeling and critical mathematical functions and probability distributions it requires no programming background only basic calculus and statistics practical beginner friendly introduction to modern statistical techniques for ecology using the programming language r step by step instructions for fitting models to messy real world data balanced view of different statistical approaches wide coverage of techniques from simple distribution fitting to complex state space modeling techniques for data manipulation and graphical display companion web site with data and r code for examples
recent improvements in technology have made dna sequencing dramatically faster and more efficient than ever before the new technologies produce highly accurate sequences but one drawback is that the most efficient technology produces the shortest read lengths short read sequencing has been applied successfully to resequence the human genome and those of other species but not to whole genome sequencing of novel organisms here we describe the sequencing and assembly of a novel clinical isolate of pseudomonas aeruginosa strain using very short read technology from reads each nucleotides in length we assembled the genome into one scaffold of ordered contiguous sequences containing nucleotides including one contig spanning nucleotides plus an additional unordered contigs containing nucleotides our method includes a novel gene boosting algorithm that uses amino acid sequences from predicted proteins to build a better assembly this study demonstrates the feasibility of very short read sequencing for the sequencing of bacterial genomes particularly those for which a related species has been sequenced previously and expands the potential application of this new technology to most known species
abstract nbsp nbsp this work examines the application of user adapted technologies to address problems experienced in web based distance education we have proposed an approach to support distance learning instructors by offering advice that points at problems faced by students and suggests possible activities to address these problems the paper describes an original feedback generation framework which utilises student group and class models derived from tracking data in web course management systems and follows a taxonomy of feedback categories to recognise situations that are brought to the instructors attention the results of an empirical study in an online learning course point at benefits of the generated feedback to both instructors and students teachers can get a better understanding of their students by knowing what problems they may be facing when they are behind or ahead of their peers who can help them and how and what roles can be assigned in discussion forums this in turn can have a positive effect on students who can receive feedback tailored to their needs and problems the evaluation study points at issues that can be related in general to planning empirical evaluations of user adapted systems in realistic web based settings
establishing a functional network is invaluable to our understanding of gene function pathways and systems level properties of an organism and can be a powerful resource in directing targeted experiments in this study we present a functional network for the laboratory mouse based on a bayesian integration of diverse genetic and functional genomic data the resulting network includes probabilistic functional linkages among protein coding genes we show that this network can accurately predict novel functional assignments and network components and present experimental evidence for predictions related to nanog homeobox nanog a critical gene in mouse embryonic stem cell pluripotency an analysis of the global topology of the mouse functional network reveals multiple biologically relevant systems level features of the mouse proteome specifically we identify the clustering coefficient as a critical characteristic of central modulators that affect diverse pathways as well as genes associated with different phenotype traits and diseases in addition a cross species comparison of functional interactomes on a genomic scale revealed distinct functional characteristics of conserved neighborhoods as compared to subnetworks specific to higher organisms thus our global functional network for the laboratory mouse provides the community with a key resource for discovering protein functions and novel pathway components as well as a tool for exploring systems level topological and evolutionary features of cellular interactomes to facilitate exploration of this network by the biomedical research community we illustrate its application in function and disease gene discovery through an interactive web based publicly available interface at http mousenet edu
this article discusses the research strategy of theory building from cases particularly multiple cases such a strategy involves using one or more cases to create theoretical constructs propositions and or midrange theory from case based empirical evidence replication logic means that each case serves as a distinct experiment that stands on its own merits as an analytic unit the frequent use of case studies as a research strategy has given rise to some challenges that can be mitigated by the use of very precise wording and thoughtful design
co evolution has an important function in the evolution of species and it is clearly manifested in certain scenarios such as hostparasite and predatorprey interactions symbiosis and mutualism the extrapolation of the concepts and methodologies developed for the study of species co evolution at the molecular level has prompted the development of a variety of computational methods able to predict protein interactions through the characteristics of co evolution particularly successful have been those methods that predict interactions at the genomic level based on the detection of pairs of protein families with similar evolutionary histories similarity of phylogenetic trees mirrortree future advances in this field will require a better understanding of the molecular basis of the co evolution of protein families thus it will be important to decipher the molecular mechanisms underlying the similarity observed in phylogenetic trees of interacting proteins distinguishing direct specific molecular interactions from other general functional constraints in particular it will be important to separate the effects of physical interactions within protein complexes co adaptation from other forces that in a less specific way can also create general patterns of evolution
pnas how protein sequence codes for structure remains a fundamental question in biology one approach to understanding the folding code is to design a pair of proteins with maximal sequence identity but retaining different folds therefore the nonidentities must be responsible for determining which fold topology prevails and constitute a fold specific folding code we recently designed two proteins and with sequence identity but different folds and functions alexander here we describe the detailed structures of these proteins determined in solution by nmr spectroscopy despite a large number of mutations taking the sequence identity level from to and maintain their distinct wild type and folds respectively to our knowledge the structure determination of two proteins with such high sequence identity but different fold topology is unprecedented the geometries of the seven nonidentical residues of total provide insights into the structural basis for switching between and conformations further mutation of a subset of these nonidentities guided by the and structures leads to proteins with even higher levels of sequence identity and different folds thus conformational switching to an alternative monomeric fold of comparable stability can be effected with just a handful of mutations in a small protein this result has implications for understanding not only the folding code but also the evolution of folds
we describe pnuts a massively parallel and geographically distributed database system for yahoo s web applications pnuts provides data storage organized as hashed or ordered tables low latency for large numbers of concurrent requests including updates and queries and novel per record consistency guarantees it is a hosted centrally managed and geographically distributed service and utilizes automated load balancing and failover to reduce operational complexity the first version of the system is currently serving in production we describe the motivation for pnuts and the design and implementation of its table storage and replication layers and then present results
the digitalization and personal use of media technologies have destabilized the traditional dichotomization between mass communication and interpersonal communication and therefore between mass media and personal media e g mobile phones email instant messenger blogs and photo sharing services as private individuals use media technologies to create and share personal expressions through digital networks previous characteristics of mass media as providers of generally accessible information are no longer accurate this article may be situated within a medium theoretical tradition as it elucidates technical and social dimensions of personal media and revises the distinction between mass media and personal media a two dimensional model suggests locating personal media and mass media according to an interactional axis and an institutional professional axis personal media are de institutionalized de professionalized and facilitate mediated interaction the implementation of digital media technologies has important consequences for social networks and fits well within a theoretical discussion of the post self
we present a detailed study of network evolution by analyzing four large online social networks with full temporal information about node and edge arrivals for the first time at such a large scale we study individual node arrival and edge creation processes that collectively lead to macroscopic properties of networks using a methodology based on the maximum likelihood principle we investigate a wide variety of network formation strategies and show that edge locality plays a critical role in evolution of networks our findings supplement earlier network models based on the inherently non local preferential attachment based on our observations we develop a complete model of network evolution where nodes arrive at a prespecified rate and select their lifetimes each node then independently initiates edges according to a gap process selecting a destination for each edge according to a simple triangle closing model free of any parameters we show analytically that the combination of the gap distribution with the node lifetime leads to a power law out degree distribution that accurately reflects the true network in all four cases finally we give model parameter settings that allow automatic evolution and generation of realistic synthetic networks of scale
in many online social systems social ties between users play an important role in dictating their behavior one of the ways this can happen is through social influence the phenomenon that the actions of a user can induce his her friends to behave in a similar way in systems where social influence exists ideas modes of behavior or new technologies can diffuse through the network like an epidemic therefore identifying and understanding social influence is of tremendous interest from both analysis and design points of view this is a difficult task in general since there are factors such as homophily or unobserved confounding variables that can induce statistical correlation between the actions of friends in a social network distinguishing influence from these is essentially the problem of distinguishing correlation from causality a notoriously hard statistical problem in this paper we study this problem systematically we define fairly general models that replicate the aforementioned sources of social correlation we then propose two simple tests that can identify influence as a source of social correlation when the time series of user actions is available we give a theoretical justification of one of the tests by proving that with high probability it succeeds in ruling out influence in a rather general model of social correlation we also simulate our tests on a number of examples designed by randomly generating actions of nodes on a real social network from flickr according to one of several models simulation results confirm that our test performs well on these data finally we apply them to real tagging data on flickr exhibiting that while there is significant social correlation in tagging behavior on this system this correlation cannot be attributed to influence
we developed a generalized framework for multiplexed resequencing of targeted human genome regions on the illumina genome analyzer using degenerate indexed dna bar codes ligated to fragmented dna before sequencing using this method we simultaneously sequenced the dna of multiple hapmap individuals at several encyclopedia of dna elements encode regions we then evaluated the use of bayes factors for discovering and genotyping polymorphisms for polymorphisms that were either previously identified within the single nucleotide polymorphism database dbsnp or visually evident upon re inspection of archived encode traces we observed a false positive rate of using strict thresholds for predicting variants and for lax thresholds conversely false negative rates were with false negatives at stricter cut offs occurring at lower coverage aligned reads these results suggest that of genetic variants are discoverable using multiplexed sequencing provided sufficient coverage at the base
mass spectrometry is a powerful technology for the analysis of large numbers of endogenous however the analytical challenges associated with comprehensive identification and relative quantification of cellular proteomes have so far appeared to be here using advances in computational proteomics instrument performance and sample preparation strategies we compare protein levels of essentially all endogenous proteins in haploid yeast cells to their diploid counterparts our analysis spans more than four orders of magnitude in protein abundance with no discrimination against membrane or low level regulatory proteins stable isotope labelling by amino acids in cell culture silac was very accurate across the proteome as demonstrated by one to one ratios of most yeast proteins key members of the pheromone pathway were specific to haploid yeast but others were unaltered suggesting an efficient control mechanism of the mating response several retrotransposon associated proteins were specific to haploid yeast gene ontology analysis pinpointed a significant change for cell wall components in agreement with geometrical considerations diploid cells have twice the volume but not twice the surface area of haploid cells transcriptome levels agreed poorly with proteome changes overall however after filtering out low confidence microarray measurements messenger rna changes and silac ratios correlated very well for pheromone pathway components systems wide precise quantification directly at the protein level opens up new perspectives in post genomics and biology
we review the study of inhomogeneous perturbations about a homogeneous and isotropic background cosmology we adopt a coordinate based approach but give geometrical interpretations of metric perturbations in terms of the expansion shear and curvature of constant time hypersurfaces and the orthogonal time like vector field we give the gauge transformation rules for metric and matter variables at first and second orders we show how gauge invariant variables are constructed by identifying geometric or matter variables in physically defined coordinate systems and give the relations between many commonly used gauge invariant variables in particular we show how the einstein equations or energymomentum conservation can be used to obtain simple evolution equations at linear order and discuss extensions to non linear order we present evolution equations for systems with multiple interacting fluids and scalar fields identifying adiabatic and entropy perturbations as an application we consider the origin of primordial curvature and isocurvature perturbations from field perturbations during inflation in the very universe
state of the art docking algorithms predict an incorrect binding pose for about of all ligands when only a single fixed receptor conformation is considered in many more cases lack of receptor flexibility results in meaningless ligand binding scores even when the correct pose is obtained incorporating conformational rearrangements of the receptor binding pocket into predictions of both ligand binding pose and binding score is crucial for improving structure based drug design and virtual ligand screening methodologies however direct modeling of protein binding site flexibility remains challenging because of the large conformational space that must be sampled and difficulties remain in constructing a suitably accurate energy function here we show that using multiple fixed receptor conformations either experimentally determined by crystallography or nmr or computationally generated is a practical shortcut that may improve docking calculations in several cases such an approach has led to experimentally predictions
to better understand their perceptions of optimal tools and strategies for success this research analyzed the experiences of learners and instructors in an online distance education environment a qualitative constant comparative analysis methodology supported by an appropriate conceptual framework guided the study data were collected over multiple years and from multiple stakeholders the study identified the following significant conclusions the availability of multiple tools added flexibility to the learning environment technology tools should appeal to multiple learning styles collaboration reflection and building a learning community were important strategies supported by multiple tools and participant satisfaction appropriate prerequisite skills and faculty and administrative involvement ensured programmatic success according to this study optimal distance education environments should address factors identified in the framework
pdz domains are proteinprotein interaction modules that recognize specific c terminal sequences to assemble protein complexes in multicellular organisms by scanning billions of random peptides we accurately map binding specificity for approximately half of the over pdz domains in the human and caenorhabditis elegans proteomes the domains recognize features of the last seven ligand positions and we find distinct specificity classes conserved from worm to human significantly extending the canonical two class system based on position thus most pdz domains are not promiscuous but rather are fine tuned for specific interactions specificity profiling of point mutants of a model pdz domain reveals that the binding site is highly robust as all mutants were able to recognize c terminal peptides however many mutations altered specificity for ligand positions both close and far from the mutated position suggesting that binding specificity can evolve rapidly under mutational pressure our specificity map enables the prediction and prioritization of natural protein interactions which can be used to guide pdz domain cell biology experiments using this approach we predicted and validated several viral ligands for the pdz domains of the scrib polarity protein these findings indicate that many viruses produce pdz ligands that disrupt host protein complexes for their own benefit and that highly pathogenic strains target pdz domains involved in cell polarity growth
pnas the extreme variation in gene content among phylogenetically related microorganisms suggests that gene acquisition expansion and loss are important evolutionary forces for adaptation to new environments accordingly phylogenetically disparate organisms that share a habitat may converge in gene content as they adapt to confront shared challenges this response should be especially pronounced for functional genes that are important for survival in a particular habitat we illustrate this principle by showing that the repertoires of two different types of carbohydrate active enzymes glycoside hydrolases and glycosyltransferases have converged in bacteria and archaea that live in the human gut and that this convergence is largely due to horizontal gene transfer rather than gene family expansion we also identify gut microbes that may have more similar dietary niches in the human gut than would be expected based on phylogeny the techniques used to obtain these results should be broadly applicable to understanding the functional genes and evolutionary processes important for adaptation in many environments and useful for interpreting the large number of reference microbial genome sequences being generated for the international human project
abstract background while text mining and distributed annotation systems both aim at capturing knowledge and presenting it in a standardized form there have been few attempts to investigate potential synergies between these two fields for instance distributed annotation would be very well suited for providing topic focussed expert knowledge enriched text corpora a key limitation for this approach is the availability of literature annotation systems that can be routinely used by groups of collaborating researchers on a day to day basis not distracting from the main focus of their work results for this purpose we have designed bibglimpse features like drop to file svm based automated retrieval of pubmed bibliography for pdf reprints and annotation support make bibglimpse an efficient light weight reprint manager that facilitates distributed literature research for work groups building on an established open search engine full text search and structured queries are supported while at the same time making shared collections of annotated reprints accessible to literature classification and text mining tools conclusions bibglimpse offers scientists a tool that enhances their own literature management moreover it may be used to create content enriched annotated text corpora for research in mining
it is difficult to apply machine learning to new domains because often we lack labeled problem instances in this paper we provide a solution to this problem that leverages domain knowledge in the form of affinities between input features and classes for example in a baseball vs hockey text classification problem even without any labeled data we know that the presence of the word puck is a strong indicator of hockey we refer to this type of domain knowledge as a labeled feature in this paper we propose a method for training discriminative probabilistic models with labeled features and unlabeled instances unlike previous approaches that use labeled features to create labeled pseudo instances we use labeled features directly to constrain the model s predictions on unlabeled instances we express these soft constraints using generalized expectation ge criteria terms in a parameter estimation objective function that express preferences on values of a model expectation in this paper we train multinomial logistic regression models using ge criteria but the method we develop is applicable to other discriminative probabilistic models the complete objective function also includes a gaussian prior on parameters which encourages generalization by spreading parameter weight to unlabeled features experimental results on text classification data sets show that this method outperforms heuristic approaches to training classifiers with labeled features experiments with human annotators show that it is more beneficial to spend limited annotation time labeling features rather than labeling instances for example after only one minute of labeling features we can achieve accuracy on the ibm vs mac text classification problem using ge fl whereas ten minutes labeling documents results in an accuracy only
ccr many clinical studies incorporate genomic experiments to investigate the potential associations between high dimensional molecular data and clinical outcome a critical first step in the statistical analyses of these experiments is that the molecular data are preprocessed this article provides an overview of preprocessing methods including summary algorithms and quality control metrics for microarrays some of the ramifications and effects that preprocessing methods have on the statistical results are illustrated the discussions are centered around a microarray experiment based on lung cancer tumor samples with survival as the clinical outcome of interest the procedures that are presented focus on the array platform used in this study however many of these issues are more general and are applicable to other instruments for genome wide investigation the discussions here will provide insight into the statistical challenges in preprocessing microarrays used in clinical studies of cancer these challenges should not be viewed as inconsequential nuisances but rather as important issues that need to be addressed so that informed conclusions can drawn
abstract nbsp nbsp in the highly competitive world there has been a concomitant increase in the need for the research and planning methodology which can perform an advanced assessment of technological opportunities and an early perception of threats and possibilities of the emerging technology according to the nations economic and social status this research is aiming to provide indicators and visualization methods to measure the latest research trend and aspect underlying scientific and technological documents to researchers and policy planners using co word analysis information security field is a highly prospective market value in this paper we presented an analysis information security co word analysis was employed to reveal patterns and trends in the information security fields by measuring the association strength of terms representatives of relevant publications or other texts produced in the information security field data were collected from sci and the critical keywords could be extracted from the author keywords these extracted keywords were further standardized in order to trace the dynamic changes in the information security field we presented a variety of technology mapping the results showed that the information security field has some established research theme and also rapidly transforms to embrace themes
the development of a safe and effective human immunodeficiency virus hiv vaccine is a critically important global health priority despite recent advances in our understanding of hiv pathogenesis and immunology however major scientific obstacles remain prototype hiv vaccine candidates aimed at eliciting humoral and cellular immune responses have so far failed to protect against hiv infection or to reduce viral loads after infection in clinical efficacy studies a renewed and coordinated commitment to basic discovery research preclinical studies and clinical trials will therefore be required to overcome the hurdles currently facing the field here i review key challenges and future prospects in the quest to develop a prophylactic vaccine
anyone who has ever gamely tried and failed to absorb enjoy and especially understand the complex works of schoenberg mahler strauss or even philip glass will allow themselves a wry smile reading music critic alex ross s outstanding rest is not only does ross manage to give historical biographical and social context to century pieces both major and minor he brings the scores alive in language that s accessible and dramatic take ross s description of schoenberg s second quartet in which he hesitates at a crossroads contemplating various paths forming in front of him the first movement written the previous year still uses a fairly conventional late romantic language the second movement by contrast is a hallucinatory scherzo unlike any other music at the time it contains fragments of the folk song ach du lieber augustin the same tune that held freudian significance for mahler for schoenberg the song seems to represent a bygone world disintegrating the crucial line is alles ist hin all is lost the movement ends in a fearsome sequence of four note figures which are made up of fourths separated by a tritone in them may be discerned traces of the bifurcated scale that begins salome but there is no longer a sense of tonalities colliding instead the very concept of a chord is dissolving into a matrix of intervals p armed with such a detailed aural roadmap even a troglodyte or a heavy metal fan can explore these pivotal works anew but it s not all crashing cymbals honking tubas and somber germans stroking their chins ross also presents the human dramas affairs wars etc behind these sweeping compositions while managing against the odds to discuss c major triads pentatonic scales and b flat dominant sevenths without making our eyes glaze over and he draws a direct link between the beatles and sibelius it s no surprise that the york named rest is one of the best books of music nerds have found their most articulate kim
motivation mathematical modeling and simulation based on biochemical rate equations provide us a rigorous tool for unraveling complex mechanisms of biological pathways to proceed to simulation experiments it is an essential first step to find effective values of model parameters which are difficult to measure from in vivo and in vitro experiments furthermore once a set of hypothetical models has been created any statistical criterion is needed to test the ability of the constructed models and to proceed to model revision results the aim of our research is to present a new statistical technology towards data driven construction of in silico biological pathways the method starts with a knowledge based modeling with hybrid functional petri net it then proceeds to the bayesian learning of model parameters for which experimental data are available this process exploits quantitative measurements of evolving biochemical reactions e g gene expression data another important issue that we consider is statistical evaluation and comparison of the constructed hypothetical pathways for this purpose we have developed a new bayesian information theoretic measure that assesses the predictability and the biological robustness of in silico pathways availability the fortran source codes are available at the url http daweb ism ac jp yoshidar gda supplementary information supplementary data are available at bioinformatics online contact yoshidar ism ac bioinformatics
motivation although metabolic reactions are unquestionably shaped by evolutionary processes the degree to which the overall structure and complexity of their interconnections are linked to the phylogeny of species has not been evaluated in depth here we apply an original metabolome representation termed network of interacting pathways or nip with a combination of graph theoretical and machine learning strategies to address this question nips compress the information of the metabolic network exhibited by a species into much smaller networks of overlapping metabolic pathways where nodes are pathways and links are the metabolites they exchange results our analysis shows that a small set of descriptors of the structure and complexity of the nips combined into regression models reproduce very accurately reference phylogenetic distances derived from rrna sequences fold cross validation correlation coefficient higher than our method also showed better scores than previous work on metabolism based phylogenetic reconstructions as assessed by branch distances score topological similarity and second cousins score thus our metabolome representation as network of overlapping metabolic pathways captures sufficient information about the underlying evolutionary events leading to the formation of metabolic networks and species phylogeny it is important to note that precise knowledge of all of the reactions in these pathways is not required for these reconstructions these observations underscore the potential for the use of abstract modular representations of metabolic reactions as tools in studying the evolution of species supplementary information supplementary data are available at online
motivation microarray expression data reveal functionally associated proteins however most proteins that are associated are not actually in direct physical contact predicting physical interactions directly from microarrays is both a challenging and important task that we addressed by developing a novel machine learning method optimized for this task results we validated our support vector machine based method on several independent datasets at the same levels of accuracy our method recovered more experimentally observed physical interactions than a conventional correlation based approach pairs predicted by our method to very likely interact were close in the overall network of interaction suggesting our method as an aid for functional annotation we applied the method to predict interactions in yeast saccharomyces cerevisiae a gene ontology function annotation analysis and literature search revealed several probable and novel predictions worthy of future experimental validation we therefore hope our new method will improve the annotation of interactions as one component of multi source integrated systems contact columbia edu supplementary information supplementary data are available at bioinformatics bioinformatics
the function of bio macromolecules is determined by both their structure and conformational dynamics these molecules are inherently flexible systems displaying a broad range of dynamics on time scales from picoseconds to seconds nuclear magnetic resonance nmr spectroscopy has emerged as the method of choice for studying both protein structure and dynamics in solution typically nmr experiments are sensitive both to structural features and to dynamics and hence the measured data contain information on both despite major progress in both experimental approaches and computational methods obtaining a consistent view of structure and dynamics from experimental nmr data remains a challenge molecular dynamics simulations have emerged as an indispensable tool in the analysis of data
efficient access to information contained in online scientific literature collections is essential for life science research playing a crucial role from the initial stage of experiment planning to the final interpretation and communication of the results the biological literature also constitutes the main information source for manual literature curation used by expert curated databases following the increasing popularity of web based applications for analyzing biological data new text mining and information extraction strategies are being implemented these systems exploit existing regularities in natural language to extract biologically relevant information from electronic texts automatically the aim of the biocreative challenge is to promote the development of such tools and to provide insight into their performance this review presents a general introduction to the main characteristics and applications of currently available text mining systems for life sciences in terms of the following the type of biological information demands being addressed the level of information granularity of both user queries and results and the features and methods commonly exploited by these applications the current trend in biomedical text mining points toward an increasing diversification in terms of application types and techniques together with integration of domain specific resources such as ontologies additional descriptions of some of the systems discussed here are available on the internet http zope bioinfo cnio webcite
approximate bayesian computation methods can be used to evaluate posterior distributions without having to calculate likelihoods in this paper we discuss and apply an approximate bayesian computation abc method based on sequential monte carlo smc to estimate parameters of dynamical models we show that abc smc gives information about the inferability of parameters and model sensitivity to changes in parameters and tends to perform better than other abc approaches the algorithm is applied to several well known biological systems for which parameters and their credible intervals are inferred moreover we develop abc smc as a tool for model selection given a range of different mathematical descriptions abc smc is able to choose the best model using the standard bayesian model apparatus
supercooled liquids exhibit a pronounced slowdown of their dynamics on without showing any obvious structural or thermodynamic several theories relate this slowdown to increasing spatial however no sign of this is seen in standard static correlation functions despite indirect evidence from considering specific and linear dielectric whereas the dynamic correlation function progressively becomes more non exponential as the temperature is reduced so far no similar signature has been found in static correlations that can distinguish qualitatively between a high temperature and a deeply supercooled glass forming liquid in equilibrium here we show evidence of a qualitative thermodynamic signature that differentiates between the two we show by numerical simulations with fixed boundary conditions that the influence of the boundary propagates into the bulk over increasing length scales on cooling with the increase of this static correlation length the influence of the boundary decays non exponentially such long range susceptibility to boundary conditions is expected within the random first order rfot of the glass transition however a quantitative account of our numerical results requires a generalization of rfot taking into account surface tension betweenstates
arguably one of the most important and tantalizing recent discoveries in the field of genome biology has been the realization that genomes are nonrandomly organized within the cell nucleus of higher eukaryotes misteli schneider et al analysis of the location of genes and chromosomes in a number of cell types and tissues has revealed that genomic elements occupy preferential positions within the nucleus the positions vary among tissues and cell types and repositioning occurs during physiological processes such as differentiation and in pathological situations positioning patterns are also evolutionarily conserved pointing to a functional role for positioning in genome activity and homeostasis the preferential location of chromosomes and genes to particular nuclear locales has implications for all aspects of genome function an emerging idea is that clustering of genes in transcription hot spots contributes to their efficient regulation and expression fraser et al lanctot et al and that the relative positioning of chromosomes is important in the formation of translocations misteli furthermore gene positioning has been linked to replication timing gilbert and physical interactions between x chromosomes may play a role in x inactivation erwin et al the idea of nonrandom positioning of chromosomes and genes has a long and often anecdotal history much of which is based on correlative observations cremer et al hochstrasser et al a key obstacle in these studies was the difficulty in measuring the position of a chromosome or a gene in a quantitative manner a breakthrough occurred when bickmore and colleagues introduced the concept of radial position that is the position of a chromosome or a gene along the axis between the center of the nucleus and the periphery croft et al in a landmark study they demonstrated using quantitative analysis that in human lymphocytes chromosome was preferentially located toward the periphery whereas chromosome was generally located toward the interior of the nucleus at the time there was little biological reason for analyzing the radial positions of chromosomes it was simply a convenient parameter to measure however having established the concept of nonrandom radial positioning the question arose as to whether the radial location of a locus was related to its function correlating gene activity and radial gene position the pros the possibility that radial positioning is functionally relevant was hinted at in the original study by bickmore and colleagues by the fact that internally localized chromosome is the most gene dense human chromosome whereas peripheral chromosome is one of the least gene dense chromosomes the case for a functional role of position was further strengthened by the finding that the gc rich portion of the genome which is also enriched in genes tends to be more internally positioned than gc poor gene poor dna ferreira et al moreover late replicating regions of the genome containing predominantly nongenic regions are generally found at the nuclear periphery whereas early replicating regions which are rich in active genes are located closer to the center of the nucleus gilbert the strongest support for a functional link between radial position and gene activity thus far comes from the observation of movement of several genes from a peripheral position into the interior upon their activation prominent examples include globin during differentiation of mouse erythroid cells igh and igk in murine b cell differentiation gata and c maf during murine t cell differentiation and during differentiation of mouse neurons hewitt et al kosak et al ragoczy et al williams et al although these genes tend to localize closely with the very edge of the nucleus when inactive others such as gfap in murine astrocytes or and in mouse embryos also undergo a shift toward a more internal location upon activation but even in their inactive state they do not localize at the nuclear envelope chambeyron et al takizawa et al support for a link between radial positioning and gene activity also comes from analysis of the two alleles of the monoallelically expressed gfap gene in single nuclei revealing that the active allele is generally found more internally compared to its inactive copy within the same nucleus takizawa et al changes in radial positions of genes coincidental with changes in their expression are not unique to mammalian cells in yeast silent genes are often associated with the periphery and reporter genes placed near telomeres which cluster at the nuclear periphery are efficiently silenced akhtar et al on the other hand a number of yeast genes move toward the yeast periphery upon activation brown et al lack of correlation between gene activity and radial position the cons despite this list of correlations we now know that the notion of localization of inactive genes at the periphery and active ones in the nuclear interior is an oversimplification and is not a universal hallmark of gene activation for most biallelically expressed genes the two alleles are often in vastly different radial positions in the same nucleus yet their activity status appears similar based on the strength of fluorescence in situ hybridization signals figure additionally a recent study of the monoallelically expressed gfap gene demonstrated that although the inactive locus is generally more peripheral than the active one in a fraction of nuclei the inactive allele was more internally localized than the active allele takizawa et al another general observation argues against a strong link between radial location and gene activity if radial positioning were directly linked to expression it would follow that transcription should occur predominantly in the interior of the nucleus yet active sites of rna polymerase ii transcription are distributed uniformly throughout the nucleus except for the nucleoli with no apparent radial preference wansink et al although preferential internal transcription zones might exist in specialized cells kosak et al similarly heterochromatin which is largely transcriptionally silent is not restricted to a specific radial position and large blocks of heterochromatin can be found throughout the nucleus figure figure radial positioning of genes a active genes can be anywhere in the nucleus the radial positions of biallelically expressed genes often vary between the two homologous alleles in the same nucleus shown are the locations of the two alleles of the igh green and myc red genes in human lymphocytes b functional significance of radial positioning top active genes green exhibit a large range of radial positions the precise radial position of a locus does not correlate with its activity level middle inactive genes red may associate with heterochromatin blocks at various radial positions bottom in contrast to radial positioning physical association with the nuclear periphery is often linked to silencing genes that are in close proximity to the nuclear envelope but do not physically interact with it may be active view larger version in this window in new window a general link between gene activity and radial position is even more strongly challenged by observations on single genes many gene loci remain in the same radial positions when their expression changes hewitt et al meaburn et al zink et al a lack of direct causality between gene expression and radial position is also highlighted by the fact that genes can become repositioned radially in the absence of detectable changes to their transcriptional output for example the pah gene becomes more internally localized during differentiation of mouse neurons and vegf becomes more peripherally localized during the induction of tumor formation in breast epithelia despite no change in expression meaburn et al williams et al in a recent study of randomly selected genes analyzed under various growth and differentiation conditions no general correlation between activity and radial position was found meaburn et al finally even observations on a peripherally silenced gene undermine the notion of a close link between repression and radial positioning the globin gene which is peripheral in its inactive form remains at the periphery during early stages of activation and only then undergoes internalization ragoczy et al this latter observation suggests that internal positioning is not a requirement for activity and that transcription alone does not drive the position of a gene taken together the fact that genes can alter radial position without changes in expression and that many genes do not undergo positional changes when their expression levels are modulated indicates that radial positioning is functionally not tightly linked to gene activity a key experiment the pros and cons in the long standing debate on the role of radial positioning in gene activity are entirely based on correlative observations often in the absence of precise measurements of gene activity a much needed key experiment was to artificially change the position of a gene and test the transcriptional consequences this has recently been done in three laboratories by artificially tethering reporter genes to the nuclear periphery of mammalian cells using various nuclear envelope and lamina proteins the results were more ambiguous than hoped for in one system transcription of a reporter gene was significantly repressed upon association with the nuclear periphery via tethering to the inner nuclear membrane protein emerin reddy et al a second system looked at the expression of multiple endogenous genes in domains tethered to the periphery by the lamin associated protein although expression of some genes was negatively affected that of others was not finlan et al finally in a third approach an inducible reporter was placed at the nuclear periphery by interaction with lamin b location of the reporter at the nuclear periphery did not prevent its activation upon stimulation and the locus retained its full transcriptional competence kumaran et al the apparent discrepancies in these results likely reflect experimental differences between the approaches for example it is not clear whether the induction of transcription after tethering to the periphery involves the same regulatory mechanisms as ongoing transcription additionally although the reporter gene in the study by reddy et al was repressed upon relocation to the periphery the reduction in expression was fold but was not complete unlike the case for endogenous genes in the study by finlan et al this suggests that despite the repressive effect of the nuclear periphery association with the periphery alone does not totally silence the locus but merely reduces its transcription finally the discrepancies might reflect the presence of the various reporter genes in different microenvironments either as part of the endogenous heterogeneity of peripheral chromatin or created by the various experimental approaches although these important experiments do not unequivocally resolve the role of radial positioning in gene expression they do mirror the findings from correlation based experiments so why then is it that radial positioning seems to have different effects on different genes peripheral location is not equal to peripheral association a likely reason for the difficulties in interpreting correlative experiments between positioning and gene activity is that it is important to make a distinction between radial positioning toward the periphery and direct physical association with the nuclear envelope figure although the tethering experiments suggest that physical association with the nuclear envelope can contribute to transcriptional repression there is no reason to think that being near the periphery without physically associating with the nuclear lamina leads to repression a good example is the human cftr geneit is located at the nuclear periphery when inactive and although it becomes more internally localized when active it remains in the very peripheral region of the nucleus zink et al in addition there is very little evidence to suggest that the precise location of a locus along the radial axis matters figure a locus halfway between the nuclear center and the periphery does not seem to have a lower probability of being active than a locus at the very center so we should not think about a correlation between the radial position of a gene and its activity but about the functional role of physical association with the nuclear periphery this seems a particularly critical point given that the light microscopy methods used in many positioning studies have resolution limits of nm and cannot distinguish between physical association and mere spatial proximity how the physical association of genes with the nuclear periphery affects their function is still largely unclear akhtar et al brown et al misteli the nuclear edge has traditionally been thought to provide a repressive milieu this assumption came from microscopy images where the nuclear envelope abuts regions of dense chromatin or heterochromatin this is supported at the molecular level by the fact that the major structural component of the nuclear periphery the lamina interacts with heterochromatin proteins such as heterochromatin protein which is essential for the organization of heterochromatin in support of a repressive role of the nuclear envelope analysis of lamina associated domains in human and fly cells revealed an enrichment of gene poor and transcriptionally inactive regions in the lamin associated domains guelen et al pickersgill et al in addition chip microarray analysis of human cells has detected extensive interactions between the nuclear pore complex npc and gene poor regions brown et al these findings in mammalian cells are closely mirrored by those in the budding yeast saccharomyces cerevisiae where extensive evidence exists for silencing particularly of telomeres and mating type loci at the periphery akhtar et al brown et al misteli however things are more complicated it increasingly appears that different parts of the nuclear periphery have distinct roles in transcriptional regulation chip studies in yeast and mammalian cells suggest a strong correlation between association with the npc and gene activation brown et al brown et al this idea is exemplified in flies where the dosage compensation complex mediating global upregulation of gene expression on the male x chromosome which is localized at the nuclear periphery directly interacts with npcs mendjan et al in contrast genome wide mapping studies in flies and mammalian cells indicate that genome regions associated with stretches of the lamina between the npcs are predominantly transcriptionally repressed guelen et al pickersgill et al thus different regions of the nuclear periphery may exert different regulatory effects on a gene further confounding the analysis of radial gene position and gene activity different genes different behavior another reason why some genes undergo repositioning whereas others do not when gene activity changes is revealed by closer analysis of the types of genes that become repositioned and the circumstances of their repositioning strikingly most genes for which a correlation between positioning and activity has been reported are those whose activity is tightly linked to differentiation eventsexamples include igh and globin during b cell and erythroid cell differentiation respectively kosak et al ragoczy et al and genes of the hoxb cluster during development chambeyron et al these genes transition from a silenced state to an active one as part of the differentiation process on the other hand many of the genes for which movement does not correlate with expression are genes whose activity changes but are never induced or completely silenced during differentiation such as and during mammary epithelial cell differentiation meaburn et al thus a key difference between these groups of genes might be the changes in chromatin status they undergo as part of their activation although differentiation induced genes are generally present in permanently repressed chromatin regions when they are inactive many of the genes that do not change positions are already in an active or possibly poised chromatin state interestingly most genes including igh globin and hoxb that change position during their activation are associated with heterochromatin blocks in their inactive states figure but genes that do not undergo radial repositioning generally are not francastel et al it is then possible that radial gene repositioning reflects to a large extent the dissociation of a locus from often peripheral heterochromatin blocks neighborhood matters a further factor in determining whether a locus changes its position or not is its chromosomal neighborhood although a gene itself may not have an altered expression pattern changes in expression at a nearby locus might drive its repositioning making the gene simply a passenger in the spatial movements of adjacent regions for example in murine embryonic stem cells pah and are adjacent to the locus and remain transcriptionally silent during neuronal differentiation yet become more internally positioned along with differentiation induced williams et al a neighborhood effect is also suggested by the observation that radial positioning correlates with local gene density with locally gene dense regions preferentially having an internal position murmann et al the influence of the chromosomal context of a gene regarding its propensity to become repositioned has also been suggested when comparing gene behavior between species brown et al brown et al in human erythroblasts the globin gene becomes repositioned away from its chromosome territory and upon activation is often juxtaposed with globin during this differentiation process in mice however these genes do not become juxtaposed and globin remains within its chromosome territory interestingly the chromosomal contexts of these genes in the human and mouse genomes are quite different in human cells globin is close to a telomere in a gene dense region enriched in housekeeping genes whereas in the mouse it is proximal to a centromere and in a region of lower gene density brown et al having said that neighborhood effects are not universal cftr and its neighboring genes become repositioned away from the nuclear periphery independently of each other when activated zink et al one difference between the neighborhoods of the and cftr loci is the number and type of genes activated the genes surrounding cftr are not coordinately regulated whereas those flanking one side of are thus movements of genomic neighborhoods may occur preferentially if multiple genes are activated an extreme case of this is the dramatic expulsion of large loops of several micrometers containing active multigene clusters from the body of the chromosome misteli it is highly likely that neighborhood effects also apply to gene repression although no examples have been reported to date beyond the tip of the iceberg radial positioning is routinely used as a surrogate to determine whether a gene undergoes a change in its nuclear location but radial positioning is merely the proverbial tip of the iceberg it is important to point out that a lack of apparent change in radial position does not mean that a locus does not change its position within the nucleus a gene may alter its location relative to other gene loci to intranuclear compartments or to heterochromatin domains such relative movements not necessarily accompanied by radial movements are increasingly being recognized as functionally relevant the importance of relative positioning lies in its ability to bring distantly located genome regions into close spatial proximity allowing their physical interaction a striking example is the recently reported association of the active allele of the monoallelically expressed interferon gene with three regulatory sequences on distinct chromosomes apostolou et al the authors proposed that these sequences facilitate the assembly of the transcriptional machinery via their interaction with the interferon locus how sequences on different chromosomes find each other in the nucleus and whether and how they move within the nucleus are key questions in the field an intriguing possibility comes from the observation in living cells of linear actin myosin mediated motion of genome regions over long distances chuang et al dundr et al perhaps pointing to the existence of directed gene transport mechanisms within the nucleus another manifestation of the potentially important role of relative positioning is the clustering of coregulated genes in nuclear space such clustering is well established for ribosomal genes that aggregate to form the nucleolus similar clustering has been suggested for genes transcribed by rna polymerase ii exemplified by the association of coregulated genes during erythrocyte differentiation in transcription centers enriched in rna polymerase ii fraser et al how generally applicable clustering of coregulated genes is and how precisely clustered genes associate with each other is not known one model suggests that gene clusters represent transcription factories which consist of preassembled transcription complexes that serve multiple genes in a centralized fashion fraser et al alternatively other observations favor the interpretation that clustered genes associate with intranuclear structures termed interchromatin granules which are enriched in pre mrna splicing factors and roughly correspond to nuclear speckles brown et al brown et al lawrence et al in addition to gene activation clustering of genes and chromosomes has also been implicated in repression imprinting and x chromosome inactivation fraser et al misteli schneider et al pointing to a ubiquitous role of relative positioning in genome function it appears from these observations that the relative positioning of genes and genomic regions to each other undergoes more dramatic changes during various events and might therefore be functionally more important than radial positioning conclusions the discovery of distinct radial positions of chromosomes and genes has changed the way we think about genome organization it has highlighted the nonrandomness of higher order genome organization and it has inspired the pursuit of how spatial genome organization contributes to function ironically despite its importance in uncovering this fundamental principle of nuclear organization the functional relevance of radial gene positioning has remained elusive clearly radial gene positioning and probably relative gene positioning too are affected by multiple components and positional changes of a given gene locus are not determined by a single mechanism furthermore it appears that different genes behave very differently and it is not easy to deduce universal rules a complicating factor in unraveling the positioning function relationship is that many studies to date have focused on probing the effect of single parameters on the positioning of single genes more complex systematic and unbiased methods of analysis are required to begin to understand the rules and consequences of genome positioning events fortunately such methods are now available there is no reason why the combined use of genome wide expression analysis genome wide interaction maps based on chromosome conformation capture analysis and high throughput imaging to analyze large numbers of genes should not eventually reveal the true meaning of gene positioning it will likely be one of the most important revelations in our understanding of how function
elucidation of regulatory roles played by micrornas mirs in various biological networks is one of the greatest challenges of present molecular and computational biology the integrated analysis of gene expression data and utr sequences holds great promise for being an effective means to systematically delineate active mirs in different biological processes applying such an integrated analysis we uncovered a striking relationship between utr au content and gene response in numerous microarray datasets we show that this relationship is secondary to a general bias that links gene response and probe au content and reflects the fact that in the majority of current arrays probes are selected from target transcript utrs therefore removal of this bias which is in order in any analysis of microarray datasets is of crucial importance when integrating expression data and utr sequences to identify regulatory elements embedded in this region we developed visualization and normalization schemes for the detection and removal of such au biases and demonstrate that their application to microarray data significantly enhances the computational identification of active mirs our results substantiate that after removal of au biases mrna expression profiles contain ample information which allows in silico detection of mirs that are active in conditions
background mixture model on graphs mmg is a probabilistic model that integrates network topology with gene protein expression data to predict the regulation state of genes and proteins it is remarkably robust to missing data a feature particularly important for its use in quantitative proteomics a new implementation in c and interfaced with r makes mmg extremely fast and easy to use and to extend availability the original implementation matlab is still available from http www dcs shef ac uk guido the new implementation is available from http wrightlab group shef ac uk htm from cran and has been submitted to bioconductor http www org
these third year lecture notes are designed for a semester course in topological quantum field theory tqft assumed background in mathematics and physics are only standard second year subjects multivariable calculus introduction to quantum mechanics and basic electromagnetism keywords quantum mechanics field theory path integral hodge decomposition chern simons and yang mills gauge theories conformal theory
in this article we report a method for coarse grained normal mode analysis called the minimalist network model the main features of the method are that it can deliver accurate low frequency modes on structures without undergoing initial energy minimization and that it also retains the details of molecular interactions the method does not require any additional adjustable parameters after coarse graining and is computationally very fast tests on modeling the experimentally measured anisotropic displacement parameters in biomolecular x ray crystallography demonstrate that the method can consistently perform better than other commonly used methods including our own one we expect this method to be effective for applications such as structural refinement and sampling
the physics of quantum degenerate atomic fermi gases in uniform as well as in harmonically trapped configurations is reviewed from a theoretical perspective emphasis is given to the effect of interactions that play a crucial role bringing the gas into a superfluid phase at low temperature in these dilute systems interactions are characterized by a single parameter the s wave scattering length whose value can be tuned using an external magnetic field near a broad feshbach resonance the bcs limit of ordinary fermi superfluidity the bose einstein condensation bec of dimers and the unitary limit of large scattering length are important regimes exhibited by interacting fermi gases in particular the bec and the unitary regimes are characterized by a high value of the superfluid critical temperature on the order of the fermi temperature different physical properties are discussed including the density profiles and the energy of the ground state configurations the momentum distribution the fraction of condensed pairs collective oscillations and pair breaking effects the expansion of the gas the main thermodynamic properties the behavior in the presence of optical lattices and the signatures of superfluidity such as the existence of quantized vortices the quenching of the moment of inertia and the consequences of spin polarization various theoretical approaches are considered ranging from the mean field description of the bcs bec crossover to nonperturbative methods based on quantum monte carlo techniques a major goal of the review is to compare theoretical predictions with available results
structural genetic variation including copy number variation cnv constitutes a substantial fraction of total genetic variability and the importance of structural genetic variants in modulating human disease is increasingly being recognized early successes in identifying disease associated cnvs via a candidate gene approach mandate that future disease association studies need to include structural genetic variation such analyses should not rely on previously developed methodologies that were designed to evaluate single nucleotide polymorphisms snps instead development of novel technical statistical and epidemiologic methods will be necessary to optimally capture this newly appreciated form of genetic variation in a manner
the binding of a transcription factor tf to a dna operator site can initiate or repress the expression of a gene computational prediction of sites recognized by a tf has traditionally relied upon knowledge of several cognate sites rather than an ab initio approach here we examine the possibility of using structure based energy calculations that require no knowledge of bound sites but rather start with the structure of a protein dna complex we study the purr escherichia coli tf and explore to which extent atomistic models of protein dna complexes can be used to distinguish between cognate and noncognate dna sites particular emphasis is placed on systematic evaluation of this approach by comparing its performance with bioinformatic methods by testing it against random decoys and sites of homologous tfs we also examine a set of experimental mutations in both dna and the protein using our explicit estimates of energy we show that the specificity for purr is dominated by direct protein dna interactions and weakly influenced by bending dna
the pathway interaction database pid http pid nci nih gov is a freely available collection of curated and peer reviewed pathways composed of human molecular signaling and regulatory events and key cellular processes created in a collaboration between the us national cancer institute and nature publishing group the database serves as a research tool for the cancer research community and others interested in cellular pathways such as neuroscientists developmental biologists and immunologists pid offers a range of search features to facilitate pathway exploration users can browse the predefined set of pathways or create interaction network maps centered on a single molecule or cellular process of interest in addition the batch query tool allows users to upload long list s of molecules such as those derived from microarray experiments and either overlay these molecules onto predefined pathways or visualize the complete molecular connectivity map users can also download molecule lists citation lists and complete database content in extensible markup language xml and biological pathways exchange biopax level format the database is updated with new pathway content every month and supplemented by specially commissioned articles on the practical uses of other relevant tools
in bilaterian animals such as humans flies and worms hundreds of micrornas mirnas some conserved throughout bilaterian evolution collectively regulate a substantial fraction of the transcriptome in addition to mirnas other bilaterian small rnas known as piwi interacting rnas pirnas protect the genome from transposons here we identify small rnas from animal phyla that diverged before the emergence of the bilateria the cnidarian nematostella vectensis starlet sea anemone a close relative to the bilateria possesses an extensive repertoire of mirna genes two classes of pirnas and a complement of proteins specific to small rna biology comparable to that of humans the poriferan amphimedon queenslandica sponge one of the simplest animals and a distant relative of the bilateria also possesses mirnas both classes of pirnas and a full complement of the small rna machinery animal mirna evolution seems to have been relatively dynamic with precursor sizes and mature mirna sequences differing greatly between poriferans cnidarians and bilaterians nonetheless mirnas and pirnas have been available as classes of riboregulators to shape gene expression throughout the evolution and radiation of phyla
substitutions of individual amino acids in proteins may be under very different evolutionary restraints depending on their structural and functional roles the environment specific substitution table esst describes the pattern of substitutions in terms of amino acid location within elements of secondary structure solvent accessibility and the existence of hydrogen bonds between side chains and neighbouring amino acid residues clearly amino acids that have very different local environments in their functional state compared to those in the protein analysed will give rise to inconsistencies in the calculation of amino acid substitution tables here we describe how the calculation of essts can be improved by discarding the functional residues from the calculation of substitution tables four categories of functions are examined in this study protein protein interactions protein nucleic acid interactions protein ligand interactions and catalytic activity of enzymes their contributions to residue conservation are measured and investigated we test our new essts using the program crescendo designed to predict functional residues by exploiting knowledge of amino acid substitutions and compare the benchmark results with proteins whose functions have been defined experimentally the new methodology increases the z score by at the active site residues and finds more active sites compared with the old esst we also find that discarding amino acids responsible for protein protein interactions helps in the prediction of those residues although they are not as conserved as the residues of active sites our methodology can make the substitution tables better reflect and describe the substitution patterns of amino acids that are under structural only
social annotation via so called collaborative tagging describes the process by which many users add metadata in the form of unstructured keywords to shared content in this paper we explore and study social annotations and tagging with regard to their usefulness for web document classification by an analysis of large sets of real world data we are interested in finding out which kinds of documents are annotated more by end users than others how users tend to annotate these documents and in particular how this user generated folk sonomy compares with a top down taxonomy maintained by classification experts for the same set of documents we describe what can be deduced from the results for further research and development in the areas of document classification and retrieval
scientific workflow systems have become a necessary tool for many applications enabling the composition and execution of complex analysis on distributed resources today there are many workflow systems often with overlapping functionality a key issue for potential users of workflow systems is the need to be able to compare the capabilities of the various available tools there can be confusion about system functionality and the tools are often selected without a proper functional analysis in this paper we extract a taxonomy of features from the way scientists make use of existing workflow systems and we illustrate this feature set by providing some examples taken from existing workflow systems the taxonomy provides end users with a mechanism by which they can assess the suitability of workflow in general and how they might use these features to make an informed choice about which workflow system would be a good choice for their particular application copyright
background advances in automated dna sequencing technology have accelerated the generation of metagenomic dna sequences especially environmental ribosomal rna gene rdna sequences as the scale of rdna based studies of microbial ecology has expanded need has arisen for software that is capable of managing annotating and analyzing the plethora of diverse data accumulated in these projects results xplorseq is a software package that facilitates the compilation management and phylogenetic analysis of dna sequences xplorseq was developed for but is not limited to high throughput analysis of environmental rrna gene sequences xplorseq integrates and extends several commonly used unix based analysis tools by use of a macintosh os x based graphical user interface gui through this gui users may perform basic sequence import and assembly steps base calling vector primer trimming contig assembly perform blast basic local alignment and search tool a href a searches of ncbi and local databases create multiple sequence alignments build phylogenetic trees assemble operational taxonomic units estimate biodiversity indices and summarize data in a variety of formats furthermore sequences may be annotated with user specified meta data which then can be used to sort data and organize analyses and reports a document based architecture permits parallel analysis of sequence data from multiple clones or amplicons with sequences and other data stored in a single file conclusion xplorseq should benefit researchers who are engaged in analyses of environmental sequence data especially those with little experience using bioinformatics software although xplorseq was developed for management of rdna sequence data it can be applied to most any sequencing project the application is available free of charge for non commercial use at http vent colorado phyloware
abstract background micrornas are small highly conserved non coding rnas which play an important role in regulating gene expression by binding the utr of target mrnas the majority of micrornas are localized within other transcriptional units host genes and are co expressed with them which strongly suggests that micrornas and corresponding host genes use the same promoter and other expression control elements the remaining fraction of micrornas is intergenic and is endowed with an independent regulatory region a number of databases have already been developed to collect information about micrornas but none of them allow an easy exploration of microrna genomic organization across evolution results cogemir is a publicly available microrna centered database whose aim is to offer an overview of the genomic organization of micrornas and of its extent of conservation during evolution in different metazoan species the database collects information on genomic location conservation and expression data of both known and newly predicted micrornas and displays the data by privileging a comparative point of view the database also includes a microrna prediction pipeline to annotate micrornas in recently sequenced genomes this information is easily accessible via web through a user friendly query page the cogemir database is available at http cogemir tigem it conclusion the knowledge of the genomic organization of micrornas can provide useful information to understand their biology in order to have a comparative genomics overview of micrornas genomic organization we developed cogemir to achieve this goal we both collected and integrated data from pre existing databases and generated new ones such as the identification in several species of a number of previously unannotated micrornas for a more effective use of this data we developed a user friendly web interface that simply shows how a microrna genomic context is related in species
dna replication in mammals is regulated via the coordinate firing of clusters of replicons that duplicate megabase sized chromosome segments at specific times during s phase cytogenetic studies show that these replicon clusters coalesce as subchromosomal units that persist through multiple cell generations but the molecular boundaries of such units have remained elusive moreover the extent to which changes in replication timing occur during differentiation and their relationship to transcription changes has not been rigorously investigated we have constructed high resolution replication timing profiles in mouse embryonic stem cells mescs before and after differentiation to neural precursor cells we demonstrate that chromosomes can be segmented into multimegabase domains of coordinate replication which we call replication domains separated by transition regions whose replication kinetics are consistent with large originless segments the molecular boundaries of replication domains are remarkably well conserved between distantly related esc lines and induced pluripotent stem cells unexpectedly esc differentiation was accompanied by the consolidation of smaller differentially replicating domains into larger coordinately replicated units whose replication time was more aligned to isochore gc content and the density of line transposable elements but not gene density replication timing changes were coordinated with transcription changes for weak promoters more than strong promoters and were accompanied by rearrangements in subnuclear position we conclude that replication profiles are cell type specific and changes in these profiles reveal chromosome segments that undergo large changes in organization during differentiation moreover smaller replication domains and a higher density of timing transition regions that interrupt isochore replication timing define a novel characteristic of the state
nima arkani hamed douglas p finkbeiner tracy r slatyer and neal of natural sciences institute for advanced study princeton new jersey usa smithsonian center for astrophysics garden street cambridge massachusetts usa department harvard university cambridge massachusetts usa for cosmology and particle physics department of physics new york university new york new york usa received october published january we propose a comprehensive theory of dark matter that explains the recent proliferation of unexpected observations in high energy astrophysics cosmic ray spectra from atic and pamela require a wimp weakly interacting massive particle with mass mchi gev that annihilates into leptons at a level well above that expected from a thermal relic signals from wmap and egret reinforce this interpretation limits on overline p and gamma s constrain the hadronic channels allowed for dark matter taken together we argue these facts imply the presence of a new force in the dark sector with a compton wavelength mphi gev the long range allows a sommerfeld enhancement to boost the annihilation cross section as required without altering the weak scale annihilation cross section during dark matter freeze out in the early universe if the dark matter annihilates into the new force carrier phi its low mass can make hadronic modes kinematically inaccessible forcing decays dominantly into leptons if the force carrier is a non abelian gauge boson the dark matter is part of a multiplet of states and splittings between these states are naturally generated with size alphamphi mev leading to the exciting dark matter xdm scenario previously proposed to explain the positron annihilation in the galactic center observed by the integral satellite the light boson invoked by xdm to mediate a large inelastic scattering cross section is identified with the phi here somewhat smaller splittings would also be expected providing a natural source for the parameters of the inelastic dark matter idm explanation for the dama annual modulation signal since the sommerfeld enhancement is most significant at low velocities early dark matter halos at redshift potentially produce observable effects on the ionization history of the universe because of the enhanced cross section detection of substructure is more probable than with a conventional wimp moreover the low velocity dispersion of dwarf galaxies and milky way subhalos can increase the substructure annihilation signal by an additional order of magnitude or more the american physical society url http link aps org doi physrevd doi physrevd d
enabling deft data integration from numerous voluminous and heterogeneous data sources is a major bioinformatic challenge several approaches have been proposed to address this challenge including data warehousing and federated databasing yet despite the rise of these approaches integration of data from multiple sources remains problematic and toilsome these two approaches follow a user to computer communication model for data exchange and do not facilitate a broader concept of data sharing or collaboration among users in this report we discuss the potential of web technologies to transcend this model and enhance bioinformatics research we propose a web based scientific social community ssc model for the implementation of these technologies by establishing a social collective and collaborative platform for data creation sharing and integration we promote a web services based pipeline featuring web services for computer to computer data exchange as users add value this pipeline aims to simplify data integration and creation to realize automatic analysis and to facilitate reuse and sharing of data ssc can foster collaboration and harness collective intelligence to create and discover new knowledge in addition to its research potential we also describe its potential role as an e learning platform in education we discuss lessons from information technology predict the next generation of web web and describe its potential impact on the future of bioinformatics bib
during the past decade there has been an explosion in computation and information technology with it have come vast amounts of data in a variety of fields such as medicine biology finance and marketing the challenge of understanding these data has led to the development of new tools in the field of statistics and spawned new areas such as data mining machine learning and bioinformatics many of these tools have common underpinnings but are often expressed with different terminology this book describes the important ideas in these areas in a common conceptual framework while the approach is statistical the emphasis is on concepts rather than mathematics many examples are given with a liberal use of color graphics it should be a valuable resource for statisticians and anyone interested in data mining in science or industry the book s coverage is broad from supervised learning prediction to unsupervised learning the many topics include neural networks support vector machines classification trees and boosting the first comprehensive treatment of this topic in any book this major new edition features many topics not covered in the original including graphical models random forests ensemble methods least angle regression path algorithms for the lasso non negative matrix factorization and spectral clustering there is also a chapter on methods for wide data p bigger than n including multiple testing and false discovery rates trevor hastie robert tibshirani and jerome friedman are professors of statistics at stanford university they are prominent researchers in this area hastie and tibshirani developed generalized additive models and wrote a popular book of that title hastie co developed much of the statistical modeling software and environment in r s plus and invented principal curves and surfaces tibshirani proposed the lasso and is co author of the very successful an introduction to the bootstrap friedman is the co inventor of many data mining tools including cart mars projection pursuit and boosting
background the new research field of metagenomics is providing exciting insights into various previously unclassified ecological systems next generation sequencing technologies are producing a rapid increase of environmental data in public databases there is great need for specialized software solutions and statistical methods for dealing with complex metagenome data sets methodology principal findings to facilitate the development and improvement of metagenomic tools and the planning of metagenomic projects we introduce a sequencing simulator called metasim our software can be used to generate collections of synthetic reads that reflect the diverse taxonomical composition of typical metagenome data sets based on a database of given genomes the program allows the user to design a metagenome by specifying the number of genomes present at different levels of the ncbi taxonomy and then to collect reads from the metagenome using a simulation of a number of different sequencing technologies a population sampler optionally produces evolved sequences based on source genomes and a given evolutionary tree conclusions significance metasim allows the user to simulate individual read datasets that can be used as standardized test scenarios for planning sequencing projects or for benchmarking software
controlled simulations of genome evolution are useful for benchmarking tools however many simulators lack extensibility and cannot measure parameters directly from data these issues are addressed by three new open source programs gsimulator for neutrally evolving dna simgram for generic structured features and simgenome for syntenic genome blocks each offers algorithms for parameter measurement and reconstruction of ancestral sequence all three tools out perform the leading neutral dna simulator dawg in benchmarks the programs are available at http biowiki simulationtools
the sequencer has dramatically increased the volume of sequencing conducted by the scientific community and expanded the range of problems that can be addressed by the direct readouts of dna sequence key breakthroughs in the development of the sequencing platform included higher throughput simplified all in vitro sample preparation and the miniaturization of sequencing chemistries enabling massively parallel sequencing reactions to be carried out at a scale and cost not previously possible together with other recently released next generation technologies the platform has started to democratize sequencing providing individual laboratories with access to capacities that rival those previously found only at a handful of large sequencing centers over the past months sequencing has led to a better understanding of the structure of the human genome allowed the first non sanger sequence of an individual human and opened up new approaches to identify small rnas to make next generation technologies more widely accessible they must become easier to use and less costly in the longer term the principles established by sequencing might reduce cost further potentially enabling genomics
it could be argued that the greatest transformative aspect of the human genome project has been not the sequencing of the genome itself but the resultant development of new technologies a host of new approaches has fundamentally changed the way we approach problems in basic and translational research now a new generation of high throughput sequencing technologies promises to again transform the scientific enterprise potentially supplanting array based technologies and opening up many new possibilities by allowing dna rna to be assayed more rapidly than previously possible these next generation platforms promise a deeper understanding of genome regulation and biology significantly enhancing sequencing throughput will allow us to follow the evolution of viral and bacterial resistance in real time to uncover the huge diversity of novel genes that are currently inaccessible to understand nucleic acid therapeutics to better integrate biological information for a complete picture of health and disease at a personalized level and to move to advances that we cannot imagine
dna sequence represents a single format onto which a broad range of biological phenomena can be projected for high throughput data collection over the past three years massively parallel dna sequencing platforms have become widely available reducing the cost of dna sequencing by over two orders of magnitude and democratizing the field by putting the sequencing capacity of a major genome center in the hands of individual investigators these new technologies are rapidly evolving and near term challenges include the development of robust protocols for generating sequencing libraries building effective new approaches to data analysis and often a rethinking of experimental design next generation dna sequencing has the potential to dramatically accelerate biological and biomedical research by enabling the comprehensive analysis of genomes transcriptomes and interactomes to become inexpensive routine and widespread rather than requiring significant production efforts
a nanopore based device provides single molecule detection and analytical capabilities that are achieved by electrophoretically driving molecules in solution through a nano scale pore the nanopore provides a highly confined space within which single nucleic acid polymers can be analyzed at high throughput by one of a variety of means and the perfect processivity that can be enforced in a narrow pore ensures that the native order of the nucleobases in a polynucleotide is reflected in the sequence of signals that is detected kilobase length polymers single stranded genomic dna or rna or small molecules e g nucleosides can be identified and characterized without amplification or labeling a unique analytical capability that makes inexpensive rapid dna sequencing a possibility further research and development to overcome current challenges to nanopore identification of each successive nucleotide in a dna strand offers the prospect of third generation instruments that will sequence a diploid mammalian genome for approximately in h
genomic data allow the large scale manual or semi automated assembly of metabolic network reconstructions which provide highly curated organism specific knowledge bases although several genome scale network reconstructions describe saccharomyces cerevisiae metabolism they differ in scope and content and use different terminologies to describe the same chemical entities this makes comparisons between them difficult and underscores the desirability of a consolidated metabolic network that collects and formalizes the community knowledge of yeast metabolism we describe how we have produced a consensus metabolic network reconstruction for s cerevisiae in drafting it we placed special emphasis on referencing molecules to persistent databases or using database independent forms such as smiles or inchi strings as this permits their chemical structure to be represented unambiguously and in a manner that permits automated reasoning the reconstruction is readily available via a publicly accessible database and in the systems biology markup language http www comp sys bio org yeastnet it can be maintained as a resource that serves as a common denominator for studying the systems biology of yeast similar strategies should benefit communities studying genome scale metabolic networks of organisms
dna from low biodiversity fracture water collected at kilometer depth in a south african gold mine was sequenced and assembled into a single complete genome this bacterium candidatus desulforudis audaxviator composes of the microorganisms inhabiting the fluid phase of this particular fracture its genome indicates a motile sporulating sulfate reducing chemoautotrophic thermophile that can fix its own nitrogen and carbon by using machinery shared with archaea candidatus desulforudis audaxviator is capable of an independent life style well suited to long term isolation from the photosphere deep within earth s crust and offers an example of a natural ecosystem that appears to have its biological component entirely encoded within a single science
network analysis tools neat is a suite of computer tools that integrate various algorithms for the analysis of biological networks comparison between graphs between clusters or between graphs and clusters network randomization analysis of degree distribution network based clustering and path finding the tools are interconnected to enable a stepwise analysis of the network through a complete analytical workflow in this protocol we present a typical case of utilization where the tasks above are combined to decipher a proteinprotein interaction network retrieved from the string database the results returned by neat are typically subnetworks networks enriched with additional information i e clusters or paths or tables displaying statistics typical networks comprising several thousands of nodes and arcs can be analyzed within a few minutes the complete protocol can be read and executed h
gene set analysis aims to identify differentially expressed gene sets pathways by a phenotype in dna microarray studies we review here important methodological aspects of gene set analysis and illustrate them with varying performance of several methods proposed in the literature we emphasize the importance of distinguishing between self contained versus competitive methods following goeman and buhlmann we also discuss reducing a gene set to its subset consisting of core members that chiefly contribute to the statistical significance of the differential expression of the initial gene set by phenotype significance analysis of microarray for gene set reduction sam gsr can be used for an analytical reduction of gene sets to their core subsets we apply sam gsr on a microarray dataset for identifying biological gene sets pathways whose gene expressions are associated with mutation in cancer cell lines codes to implement sam gsr in the statistical package r can be downloaded from http www ualberta ca yyasui homepage bib
this article collects opinions from leading scientists about how text mining can provide better access to the biological literature how the scientific community can help with this process what the next steps are and what role future biocreative evaluations can play the responses identify several broad themes including the possibility of fusing literature and biological databases through text mining the need for user interfaces tailored to different classes of users and supporting community based annotation the importance of scaling text mining technology and inserting it into larger workflows and suggestions for additional challenge evaluations new applications and additional resources needed to progress
recent studies of the hapmap lymphoblastoid cell lines have identified large numbers of quantitative trait loci for gene expression eqtls reanalyzing these data using a novel bayesian hierarchical model we were able to create a surprisingly high resolution map of the typical locations of sites that affect mrna levels in cis strikingly we found a strong enrichment of eqtls in the bp just upstream of the transcription end site tes in addition to an enrichment around the transcription start site tss most eqtls lie either within genes or close to genes for example we estimate that only of eqtls lie more than kb upstream of the tss after controlling for position effects snps in exons are approximately fold more likely than snps in introns to be eqtls our results suggest an important role for mrna stability in determining steady state mrna levels and highlight the potential of eqtl mapping as a high resolution tool for studying the determinants of regulation
summary here we report the development of socs short oligonucleotide color space a program designed for efficient and flexible mapping of applied biosystems solid sequence data onto a reference genome socs performs its mapping within the context of color space and it maximizes usable data by allowing a user specified number of mismatches sequence census functions facilitate a variety of functional genomics applications including transcriptome mapping and profiling as well as chip seq availability executables source code and sample data are available at http socs biology gatech edu contact nickbergman gatech edu supplementary information supplementary data are available at bioinformatics bioinformatics
this book attempts to marry truth conditional semantics with cognitive linguistics in the church of computational neuroscience to this end it examines the truth conditional meanings of coordinators quantifiers and collective predicates as neurophysiological phenomena that are amenable to a neurocomputational analysis drawing inspiration from work on visual processing and especially the simple complex cell distinction in early vision we claim that a similar two layer architecture is sufficient to learn the truth conditional meanings of the logical coordinators and logical quantifiers as a prerequisite much discussion is given over to what a neurologically plausible representation of the meanings of these items would look like we eventually settle on a representation in terms of correlation so that for instance the semantic input to the universal operators e g and all is represented as maximally correlated while the semantic input to the universal negative operators e g nor no is represented as maximally anticorrelated on the basis this representation the hypothesis can be offered that the function of the logical operators is to extract an invariant feature from natural situations that of degree of correlation between parts of the situation this result sets up an elegant formal analogy to recent models of visual processing which argue that the function of early vision is to reduce the redundancy inherent in natural images computational simulations are designed in which the logical operators are learned by associating their phonological form with some degree of correlation in the inputs so that the overall function of the system is as a simple kind of pattern recognition several learning rules are assayed especially those of the hebbian sort which are the ones with the most neurological support learning vector quantization lvq is shown to be a perspicuous and efficient means of learning the patterns that are of interest we draw a formal parallelism between the initial competitive layer of lvq and the simple cell layer in and between the final linear layer of lvq and the complex cell layer in in that the initial layers are both selective while the final layers both generalize it is also shown how the representations argued for can be used to draw the traditionally recognized inferences arising from coordination and quantification and why the inference of subalternacy breaks down for collective predicates finally the analogies between early vision and the logical operators allow us to advance the claim of cognitive linguistics that language is not processed by proprietary algorithms but rather by algorithms that are general to the entire brain thus in the debate between objectivist and experiential metaphysics this book falls squarely into the camp of the latter yet it does so by means of a rigorous formal mathematical and neurological exposition in contradiction of the experiential claim that formal analysis has no place in the understanding of cognition to make our own counter claim as explicit as possible we present a sketch of the lvq structure in terms of mereotopology in which the initial layer of the network performs topological operations while the final layer performs mereological operations the book is meant to be self contained in the sense that it does not assume any prior knowledge of any of the many areas that are touched upon it therefore contains mini summaries of biological visual processing especially the retinocortical and ventral what parvocellular pathways computational models of neural signaling and in particular the reduction of the hodgkin huxley equations to the connectionist and integrate and fire neurons hebbian learning rules and the elaboration of learning vector quantization the linguistic pathway in the left hemisphere memory and the hippocampus truth conditional vs image schematic semantics objectivist vs experiential metaphysics and mereotopology all of the simulations are implemented in matlab and the code is available from the books website the discovery of several algorithmic similarities between visison and semantics the support of all of this by means of simulations and the packaging of all of this in a coherent framework
recording of multiple neurons from a single electrode is common practice during extra cellular recordings separation and sorting of spikes originating from the different neurons can be performed either on line or off line using multiple methods for pattern matching however all spike sorting techniques fail either fully or partially in identifying spikes from multiple neurons when they overlap due to occurrence within a short time interval this failure that the authors termed the shadowing effect causes the well known phenomenon of decreased cross correlation at zero offset however the shadowing effect also causes other artifacts in the auto and cross correlation of the recorded neurons these artifacts are significant mainly in brain areas with high firing rate or increased firing synchrony leading to a high probability of spike overlap cross correlation of cells recorded from the same electrodes tends to reflect the autocorrelation functions of the two cells even when there are no functional interactions between the cells therefore the cross correlation function tends to have a short term about the length of the refractory period peak a long term hundreds of milliseconds to a few seconds trough in the cross correlation can be seen in cells with bursting and pausing activities recorded from the same electrode even the autocorrelation functions of the recorded neurons feature firing properties of other neurons recorded from the same electrode examples of these effects are given from our recordings in the globus pallidus of behaving primates and from the literature results of simulations of independent simple model neurons exhibit the same properties as the recorded neurons the effect is analyzed and can be estimated to enable better evaluation of the underlying firing patterns and the actual synchronization of neighboring neurons recorded by a electrode
when recording extracellular neural activity it is often necessary to distinguish action potentials arising from distinct cells near the the electrode tip a process commonly referred to as spike sorting in a number of experiments notably those that involve direct neuroprosthetic control of an effector this cell by cell classification of the incoming signal must be achieved in real time several commercial offerings are available for this task but all of these require some manual supervision per electrode making each scheme cumbersome with large electrode counts we present a new infrastructure that leverages existing unsupervised algorithms to sort and subsequently implement the resulting signal classification rules for each electrode using a commercially available cerebus neural signal processor we demonstrate an implementation of this infrastructure to classify signals from a cortical electrode array using a probabilistic clustering algorithm described elsewhere the data were collected from a rhesus monkey performing a delayed center out reach task we used both sorted and unsorted thresholded action potentials from an array implanted in pre motor cortex to predict the reach target a common decoding operation in neuroprosthetic research the use of sorted spikes led to an improvement in decoding accuracy of and
chronically implanted electrode arrays have enabled a broad range of advances in basic electrophysiology and neural prosthetics those successes motivate new experiments particularly the development of prototype implantable prosthetic processors for continuous use in freely behaving subjects both monkeys and humans however traditional experimental techniques require the subject to be restrained limiting both the types and duration of experiments in this paper we present a dual channel battery powered neural recording system with an integrated three axis accelerometer for use with chronically implanted electrode arrays in freely behaving primates the recording system called hermesb is self contained autonomous programmable and capable of recording broadband neural sampled at ks s and acceleration data to a removable compact flash card for up to h we have collected long duration data sets with hermesb from an adult macaque monkey which provide insight into time scales and free behaviors inaccessible under traditional experiments variations in action potential shape and root mean square rms noise are observed across a range of time scales the peak to peak voltage of action potentials varied by up to over a h period including step changes in waveform amplitude up to coincident with high acceleration movements of the head these initial results suggest that spike sorting algorithms can no longer assume stable neural signals and will need to transition to adaptive signal processing methodologies to maximize performance during physically active periods defined by head mounted accelerometer significantly reduced hz local field potential lfp power and increased firing rate variability were observed using a threshold fit to lfp power of min recording blocks were correctly classified as active or inactive potentially providing an efficient tool for identifying different behavioral contexts in prosthetic applications these results demonstrate the utility of the hermesb system and motivate using this type of system to advance neural prosthetics and electrophysiological ieee
wikipedia is a goldmine of information not just for its many readers but also for the growing community of researchers who recognize it as a resource of exceptional scale and utility it represents a vast investment of manual effort and judgment a huge constantly evolving tapestry of concepts and relations that is being applied to a host of tasks this article provides a comprehensive description of this work it focuses on research that extracts and makes use of the concepts relations facts and descriptions found in wikipedia and organizes the work into four broad categories applying wikipedia to natural language processing using it to facilitate information retrieval and information extraction and as a resource for ontology building the article addresses how wikipedia is being used as is how it is being improved and adapted and how it is being combined with other structures to create entirely new resources we identify the research groups and individuals involved and how their work has developed in the last few years we provide a comprehensive list of the open source software they have produced we also discuss the implications of this work for the long awaited web
gr segmental duplications sds are operationally defined as kb stretches of duplicated dna with high sequence identity they arise from copy number variants cnvs fixed in the population to investigate the formation of sds and cnvs we examine their large scale patterns of co occurrence with different repeats elements a major class of genomic repeats had previously been identified as prime drivers of sd formation we also observe this association however we find that it sharply decreases for younger sds continuing this trend we find only weak associations of cnvs with s similarly we find an association of sds with processed pseudogenes which is decreasing for younger sds and absent entirely for cnvs next we find that sds are significantly co localized with each other resulting in a highly skewed power law distribution and chromosomal hotspots we also observe a significant association of cnvs with sds but find that an sd mediated mechanism only accounts for some cnvs overall our results imply that a shift in predominant formation mechanism occurred in recent history million years ago during the burst in retrotransposition activity non allelic homologous recombination first mediated by s and then the by newly formed cnvs themselves was the main driver of genome rearrangements however its relative importance has decreased markedly since then with proportionally more events now stemming from other repeats and from non homologous end joining in addition to a coarse grained analysis we performed targeted sequencing of cnvs and then analyzed a combined set of cnvs breakpoints to verify conclusions
a long standing goal of biology is to map the behavior of all cells during vertebrate embryogenesis we developed digital scanned laser light sheet fluorescence microscopy and recorded nuclei localization and movement in entire wild type and mutant zebrafish embryos over the first hours of development multiview in vivo imaging at billion voxels per minute provides digital embryos that is comprehensive databases of cell positions divisions and migratory tracks our analysis of global cell division patterns reveals a maternally defined initial morphodynamic symmetry break which identifies the embryonic body axis we further derive a model of germ layer formation and show that the mesendoderm forms from one third of the embryo s cells in a single event our digital embryos with million nucleus entries are provided as a science
it has been observed that the evolutionary distances of interacting proteins often display a higher level of similarity than those of noninteracting proteins this finding indicates that interacting proteins are subject to common evolutionary constraints and constitutes the basis of a method to predict protein interactions known as mirrortree it has been difficult however to identify the direct cause of the observed similarities between evolutionary trees one possible explanation is the existence of compensatory mutations between partners binding sites to maintain proper binding this explanation though has been recently challenged and it has been suggested that the signal of correlated evolution uncovered by the mirrortree method is unrelated to any correlated evolution between binding sites we examine the contribution of binding sites to the correlation between evolutionary trees of interacting domains we show that binding neighborhoods of interacting proteins have on average higher coevolutionary signal compared with the regions outside binding sites however when the binding neighborhood is removed the remaining domain sequence still contains some coevolutionary signal in conclusion the correlation between evolutionary trees of interacting domains cannot exclusively be attributed to the correlated evolution of the binding sites or to common evolutionary pressure exerted on the whole protein domain sequence each of which contributes to the signal measured by the approach
abstract background solexa illumina short read ultra high throughput dna sequencing technology produces millions of short tags up to bases by parallel sequencing by synthesis of dna colonies the processing and statistical analysis of such high throughput data poses new challenges currently a fair proportion of the tags are routinely discarded due to an inability to match them to a reference sequence thereby reducing the effective throughput of the technology results we propose a novel base calling algorithm using model based clustering and probability theory to identify ambiguous bases and code them with iupac symbols we also select optimal sub tags using a score based on information content to remove uncertain bases towards the ends of the reads conclusions we show that the method improves genome coverage and number of usable tags as compared with solexa s data processing pipeline by an average of an r package rolexa is provided which allows fast and accurate base calling of solexa s fluorescence intensity files and the production of informative plots
previous investigations of the neural code for complex object shape have focused on two dimensional pattern representation this may be the primary mode for object vision given its simplicity and direct relation to the retinal image in contrast three dimensional shape representation requires higher dimensional coding derived from extensive computation we found evidence for an explicit neural code for complex three dimensional object shape we used an evolutionary stimulus strategy and linear nonlinear response models to characterize three dimensional shape responses in macaque monkey inferotemporal cortex it we found widespread tuning for three dimensional spatial configurations of surface fragments characterized by their three dimensional orientations and joint principal curvatures configural representation of three dimensional shape could provide specific knowledge of object structure to support guidance of complex physical interactions and evaluation of object functionality utility
we introduce the first meta service for information extraction in molecular biology the biocreative metaserver bcms http bcms bioinfo cnio es this prototype platform is a joint effort of research groups and provides automatically generated annotations for pubmed medline abstracts annotation types cover gene names gene ids species and protein protein interactions the annotations are distributed by the meta server in both human and machine readable formats html xml this service is intended to be used by biomedical researchers and database annotators and in biomedical language processing the platform allows direct comparison unified access and result aggregation of annotations
the popularity of collaborative tagging otherwise known as folksonomies emanate from the flexibility they afford users in navigating large information spaces for resources tags or other users unencumbered by a pre defined navigational or conceptual hierarchy despite its advantages social tagging also increases user overhead in search and navigation users are free to apply any tag they wish to a resource often resulting in a large number of tags that are redundant ambiguous or idiosyncratic data mining techniques such as clustering provide a means to overcome this problem by learning aggregate user models and thus reducing noise in this paper we propose a method to personalize search and navigation based on unsupervised hierarchical agglomerative tag clustering given a user profile represented as a vector of tags the learned tag clusters provide the nexus between the user and those resources that correspond more closely to the users intent we validate this assertion through extensive evaluation of the proposed algorithm using data from a real collaborative tagging site
tagging systems such as del icio us and diigo have become important ways for users to organize information gathered from the web however despite their popularity among early adopters tagging still incurs a relatively high interaction cost for the general users we introduce a new tagging system called spartag us which uses an intuitive technique to provide in situ low cost tagging of web content spartag us also lets users highlight text snippets and automatically collects tagged or highlighted paragraphs into a system created notebook which can be later browsed and searched we report several user studies aimed at evaluating and us
gr recently attention has been turned to the problem of reconstructing complete ancestral sequences from large multiple alignments successful generation of these genome wide reconstructions will facilitate a greater knowledge of the events that have driven evolution we present a new evolutionary alignment modeler called ortheus for inferring the evolutionary history of a multiple alignment in terms of both substitutions and importantly insertions and deletions based on a multiple sequence probabilistic transducer model of the type proposed by holmes ortheus uses efficient stochastic graph based dynamic programming methods unlike other methods ortheus does not rely on a single fixed alignment from which to work ortheus is also more scaleable than previous methods while being fast stable and open source large scale simulations show that ortheus performs close to optimally on a deep mammalian phylogeny simulations also indicate that significant proportions of errors due to insertions and deletions can be avoided by not assuming a fixed alignment we additionally use a challenging hold out cross validation procedure to test the method using the reconstructions to predict extant sequence bases we demonstrate significant improvements over using closest extant neighbor sequences accompanying this paper a new public and genome wide set of ortheus ancestor alignments provide an intriguing new resource for evolutionary studies in mammals as a first piece of analysis we attempt to recover fossilized ancestral pseudogenes we confidently find cases in which the ancestral sequence had a more complete sequence than any of the sequences
following a line of research that i have developed for several years i argue that the best strategy for understanding quantum gravity is to build a picture of the physical world where the notion of time plays no role at all i summarize here this point of view explaining why i think that in a fundamental description of nature we must forget time and how this can be done in the classical and in the quantum theory the idea is to develop a formalism that treats dependent and independent variables on the same footing in short i propose to interpret mechanics as a theory of relations between variables rather than the theory of the evolution of variables time
abstract the explosive growth of genomic data provides an opportunity for increased usage of protein markers for phylogenetic inference in response we have developed an automated pipeline for phylogenomic analysis amphora that overcomes the existing bottlenecks limiting large scale protein phylogenetic inference we demonstrated its high throughput capabilities and high quality results by constructing a genome tree of bacterial species and by assigning phylotypes to protein markers identified in metagenomic data collected from the sea
the power of genome wide snp association studies is limited among others by the large number of false positive test results to provide a remedy we combined snp association analysis with the pathway driven gene set enrichment analysis gsea recently developed to facilitate handling of genome wide gene expression data the resulting gsea snp method rests on the assumption that snps underlying a disease phenotype are enriched in genes constituting a signaling pathway or those with a common regulation besides improving power for association mapping gsea snp may facilitate the identification of disease associated snps and pathways as well as the understanding of the underlying biological mechanisms gsea snp may also help to identify markers with weak effects undetectable in association studies without pathway consideration the program is freely available and can be downloaded from our bioinformatics
orthology is a key evolutionary concept in many areas of genomic research it provides a framework for subjects as diverse as the evolution of genomes gene functions cellular networks and functional genome annotation although orthologous proteins usually perform equivalent functions in different species establishing true orthologous relationships requires a phylogenetic approach which combines both trees and graphs networks using reliable species phylogeny and available genomic data from more than two species and an insight into the processes of molecular evolution here we evaluate the available bioinformatics tools and provide a set of guidelines to aid researchers in choosing the most appropriate tool for situation
this paper demonstrates that teamwork in science increasingly spans university boundaries a dramatic shift in knowledge production that generalizes across virtually all fields of science engineering and social science moreover elite universities play a dominant role in this shift by examining million papers published over three decades we found that multi university collaborations i are the fastest growing type of authorship structure ii produce the highest impact papers when they include a top tier university and iii are increasingly stratified by in group university rank despite the rising frequency of research that crosses university boundaries the intensification of social stratification in multi university collaborations suggests a concentration of the production of scientific knowledge in fewer rather than more centers of high impact science
pnas micrornas mirnas regulate gene expression at the posttranscriptional level in the cytoplasm but recent findings suggest additional roles for mirnas in the nucleus to address whether mirnas might transcriptionally silence gene expression we searched for mirna target sites proximal to known gene transcription start sites in the human genome one conserved mirna mir is encoded within the promoter region of the cell cycle gene in the antisense orientation we provide evidence of a regulatory role for mir in transcriptional silencing of expression mir directs the association of rna interference rnai protein argonaute polycomb group pcg component and tri methyl histone lysine with the promoter our results suggest the existence of an epigenetic mechanism of mirna directed transcriptional gene silencing tgs in cells
human evolution is characterized by a rapid increase in brain size and complexity decades of research have made important strides in identifying anatomical and physiological substrates underlying the unique features of the human brain by contrast it has become possible only very recently to examine the genetic basis of human brain evolution through comparative genomics tantalizing insights regarding human brain evolution have emerged the genetic changes that potentially underlie human brain evolution span a wide range from single nucleotide substitutions to large scale structural alterations of the genome similarly the functional consequences of these genetic changes vary greatly including protein sequence alterations cis regulatory changes and even the emergence of new genes and the extinction of existing ones here we provide a general review of recent findings into the genetic basis of human brain evolution highlight the most notable trends that have emerged and caution against over interpretation of data
research on human and animal behavior has long emphasized its hierarchical structure the divisibility of ongoing behavior into discrete tasks which are comprised of subtask sequences which in turn are built of simple actions the hierarchical structure of behavior has also been of enduring interest within neuroscience where it has been widely considered to reflect prefrontal cortical functions in this paper we reexamine behavioral hierarchy and its neural substrates from the point of view of recent developments in computational reinforcement learning specifically we consider a set of approaches known collectively as hierarchical reinforcement learning which extend the reinforcement learning paradigm by allowing the learning agent to aggregate actions into reusable subroutines or skills a close look at the components of hierarchical reinforcement learning suggests how they might map onto neural structures in particular regions within the dorsolateral and orbital prefrontal cortex it also suggests specific ways in which hierarchical reinforcement learning might provide a complement to existing psychological models of hierarchically structured behavior a particularly important question that hierarchical reinforcement learning brings to the fore is that of how learning identifies new action routines that are likely to provide useful building blocks in solving a wide range of future problems here and at many other points hierarchical reinforcement learning offers an appealing framework for investigating the computational and neural underpinnings of hierarchically behavior
unravelling the pathophysiology of depression is a unique challenge not only are depressive syndromes heterogeneous and their aetiologies diverse but symptoms such as guilt and suicidality are impossible to reproduce in animal models nevertheless other symptoms have been accurately modelled and these together with clinical data are providing insight into the neurobiology of depression recent studies combining behavioural molecular and electrophysiological techniques reveal that certain aspects of depression result from maladaptive stress induced neuroplastic changes in specific neural circuits they also show that understanding the mechanisms of resilience to stress offers a crucial new dimension for the development of fundamentally novel treatments
previous research demonstrated the use of evolutionary computation for the discovery of transcription factor binding sites tfbs in promoter regions upstream of coexpressed genes however it remained unclear whether or not composite tfbs elements commonly found in higher organisms where two or more tfbss form functional complexes could also be identified by using this approach here we present an important refinement of our previous algorithm and test the identification of composite elements using nfat ap as an example we demonstrate that by using appropriate existing parameters such as window size novel scoring methods such as central bonusing and methods of self adaptation to automatically adjust the variation operators during the evolutionary search tfbss of different sizes and complexity can be identified as top solutions some of these solutions have known experimental relationships with nfat ap we also indicate that even after properly tuning the model parameters the choice of the appropriate window size has a significant effect on algorithm performance we believe that this improved algorithm will greatly augment tfbs nar
abstract background in many genomics projects numerous lists containing biological identifiers are produced often it is useful to see the overlap between different lists enabling researchers to quickly observe similarities and differences between the data sets they are analyzing one of the most popular methods to visualize the overlap and differences between data sets is the venn diagram a diagram consisting of two or more circles in which each circle corresponds to a data set and the overlap between the circles corresponds to the overlap between the data sets venn diagrams are especially useful when they are area proportional i e the sizes of the circles and the overlaps correspond to the sizes of the data sets currently there are no programs available that can create area proportional venn diagrams connected to a wide range of biological databases results we designed a web application named biovenn to summarize the overlap between two or three lists of identifiers using area proportional venn diagrams the user only needs to input these lists of identifiers in the textboxes and push the submit button parameters like colors and text size can be adjusted easily through the web interface the position of the text can be adjusted by drag and drop principle the output venn diagram can be shown as an svg or png image embedded in the web application or as a standalone svg or png image the latter option is useful for batch queries besides the venn diagram biovenn outputs lists of identifiers for each of the resulting subsets if an identifier is recognized as belonging to one of the supported biological databases the output is linked to that database finally biovenn can map affymetrix and entrezgene identifiers to ensembl genes conclusions biovenn is an easy to use web application to generate area proportional venn diagrams from lists of biological identifiers it supports a wide range of identifiers from the most used biological databases currently available its implementation on the world wide web makes it available for use on any computer with internet connection independent of operating system and without the need to install programs locally biovenn is freely accessible at http www cmbi ru nl biovenn
summary biocaster is an ontology based text mining system for detecting and tracking the distribution of infectious disease outbreaks from linguistic signals on the web the system continuously analyzes documents reported from over rss feeds classifies them for topical relevance and plots them onto a google map using geocoded information the background knowledge for bridging the gap between layman s terms and formal coding systems is contained in the freely available biocaster ontology which includes information in eight languages focused on the epidemiological role of pathogens as well as geographical locations with their latitudes longitudes the system consists of four main stages topic classification named entity recognition ner disease location detection and event recognition higher order event analysis is used to detect more precisely specified warning signals that can then be notified to registered users via email alerts evaluation of the system for topic recognition and entity identification is conducted on a gold standard corpus of annotated news articles availability the biocaster map and ontology are freely available via a web portal at http www biocaster org contact collier nii jp
abstract background for the last eight years microarray based classification has been a major topic in statistics bioinformatics and biomedicine research traditional methods often yield unsatisfactory results or may even be inapplicable in the so called p n setting where the number of predictors p by far exceeds the number of observations n hence the term ill posed problem careful model selection and evaluation satisfying accepted good practice standards is a very complex task for statisticians without experience in this area or for scientists with limited statistical background the multiplicity of available methods for class prediction based on high dimensional data is an additional practical challenge for inexperienced researchers results in this article we introduce a new bioconductor package called cma standing for classification for microarrays for automatically performing variable selection parameter tuning classifier construction and unbiased evaluation of the constructed classifiers using a large number of usual methods without much time and effort users are provided with an overview of the unbiased accuracy of most top performing classifiers furthermore the standardized evaluation framework underlying cma can also be beneficial in statistical research for comparison purposes for instance if a new classifier has to be compared to existing approaches conclusions cma is a user friendly comprehensive package for classifier construction and evaluation implementing most usual approaches it is freely available from the bioconductor website at http bioconductor org packages bioc html html
this article provides an overview of computer software and instructional strategies intended to engage young people in making computer games to achieve a variety of educational goals it briefly describes the most popular of such programs and compares their key features including the kinds of games that can be created with the software the types of communities and resources that are associated with each program claims made for learning outcomes resulting from use of the software and the results of empirical research if any on the application and outcomes of the software in formal or informal educational settings a key finding is that existing software and educational applications stress the goal of teaching users about computer programming and place little or no emphasis on teaching concepts related to game design it concludes by discussing the potential value of explicit attention to design thinking as goal of game making education
evolutionary ecologists are increasingly combining phylogenetic data with distributional and ecological data to assess how and why communities of species differ from random expectations for evolutionary and ecological relatedness of particular interest have been the roles of environmental filtering and competitive interactions or alternatively neutral effects in dictating community composition our goal is to place current research within a dynamic framework specifically using recent phylogenetic studies from insular environments to provide an explicit spatial and temporal context we compare communities over a range of evolutionary ecological and geographic scales that differ in the extent to which speciation and adaptation contribute to community assembly and structure this perspective allows insights into the processes that can generate community structure as well as the evolutionary dynamics of assembly
in this article we revisit the classic problem of tatonnement in price formation from a microstructure point of view reviewing a recent body of theoretical and empirical work explaining how fluctuations in supply and demand are slowly incorporated into prices because revealed market liquidity is extremely low large orders to buy or sell can only be traded incrementally over periods of time as long as months as a result order flow is a highly persistent long memory process maintaining compatibility with market efficiency has profound consequences on price formation on the dynamics of liquidity and on the nature of impact we review a body of theory that makes detailed quantitative predictions about the volume and time dependence of market impact the bid ask spread order book dynamics and volatility comparisons to data yield some encouraging successes this framework suggests a novel interpretation of financial information in which agents are at best only weakly informed and all have a similar and extremely noisy impact on prices most of the processed information appears to come from supply and demand itself rather than from external news the ideas reviewed here are relevant to market microstructure regulation agent based models cost optimal execution strategies and understanding ecologies
the engineering of biological systems is anticipated to provide effective solutions to challenges that include energy and food production environmental quality and health and medicine our ability to transmit information to and from living systems and to process and act on information inside cells is critical to advancing the scale and complexity at which we can engineer manipulate and probe biological systems we developed a general approach for assembling rna devices that can execute higher order cellular information processing operations from standard components the engineered devices can function as logic gates and nor nand or or gates and signal filters and exhibit cooperativity rna devices process and transmit molecular inputs to targeted protein outputs linking computation to gene expression and thus the potential to control cellular science
by monitoring fluorescently labeled lactose permease with single molecule sensitivity we investigated the molecular mechanism of how an escherichia coli cell with the lac operon switches from one phenotype to another at intermediate inducer concentrations a population of genetically identical cells exhibits two phenotypes induced cells with highly fluorescent membranes and uninduced cells with a small number of membrane bound permeases we found that this basal level expression results from partial dissociation of the tetrameric lactose repressor from one of its operators on looped dna in contrast infrequent events of complete dissociation of the repressor from dna result in large bursts of permease expression that trigger induction of the lac operon hence a stochastic single molecule event determines a cell s science
statistical analyses of protein families reveal networks of coevolving amino acids that functionally link distantly positioned functional surfaces such linkages suggest a concept for engineering allosteric control into proteins the intramolecular networks of two proteins could be joined across their surface sites such that the activity of one protein might control the activity of the other we tested this idea by creating pas dhfr a designed chimeric protein that connects a light sensing signaling domain from a plant member of the per arnt sim pas family of proteins with escherichia coli dihydrofolate reductase dhfr with no optimization pas dhfr exhibited light dependent catalytic activity that depended on the site of connection and on known signaling mechanisms in both proteins pas dhfr serves as a proof of concept for engineering regulatory activities into proteins through interface design at conserved allosteric science
gene expression is a fundamentally stochastic process with randomness in transcription and translation leading to cell to cell variations in mrna and protein levels this variation appears in organisms ranging from microbes to metazoans and its characteristics depend both on the biophysical parameters governing gene expression and on gene network structure stochastic gene expression has important consequences for cellular function being beneficial in some contexts and harmful in others these situations include the stress response metabolism development the cell cycle circadian rhythms aging
a potential treatment for paralysis resulting from spinal cord injury is to route control signals from the brain around the injury by artificial connections such signals could then control electrical stimulation of muscles thereby restoring volitional movement to paralysed in previously separate experiments activity of motor cortex neurons related to actual or imagined movements has been used to control computer cursors and robotic and paralysed muscles have been activated by functional electrical here we show that macaca nemestrina monkeys can directly control stimulation of muscles using the activity of neurons in the motor cortex thereby restoring goal directed movements to a transiently paralysed arm moreover neurons could control functional stimulation equally well regardless of any previous association to movement a finding that considerably expands the source of control signals for brain machine interfaces monkeys learned to use these artificial connections from cortical cells to muscles to generate bidirectional wrist torques and controlled multiple neuronmuscle pairs simultaneously such direct transforms from cortical activity to muscle stimulation could be implemented by autonomous electronic circuitry creating a relatively natural neuroprosthesis these results are the first demonstration that direct artificial connections between cortical cells and muscles can compensate for interrupted physiological pathways and restore volitional control of movement to limbs
signaling networks respond to diverse stimuli but how the state of the signaling network is relayed to downstream cellular responses is unclear we modeled how incremental activation of signaling molecules is transmitted to control apoptosis as a function of signal strength and dynamic range a linear relationship between signal input and response output with the dynamic range of signaling molecules uniformly distributed across activation states most accurately predicted cellular responses when nonlinearized signals with compressed dynamic range relay network activation to apoptosis we observe catastrophic stimulus specific prediction failures we develop a general computational technique model breakpoint analysis to analyze the mechanism of these failures identifying new time and stimulus specific roles for akt erk and kinase activity in apoptosis which were experimentally verified dynamic range is rarely measured in signal transduction studies but our experiments using model breakpoint analysis suggest it may be a greater determinant of cell fate than measured strength
micrornas mirnas negatively regulate the expression of target genes at the post transcriptional level little is known about the crosstalk between mirnas and transcription factors tfs here we provide data suggesting that the interaction patterns between tfs and mirnas can influence the biological functions of mirnas from this global survey we find that a regulated feedback loop in which two tfs regulate each other and one mirna regulates both of the factors is the most significantly overrepresented network motif mathematical modeling shows that the mirna in this motif stabilizes the feedback loop to resist environmental perturbation providing one mechanism to explain the robustness of developmental programs that is contributed by mirnas furthermore on the basis of a network motif profile analysis we demonstrate the existence of two classes of mirnas with distinct network topological properties the first class of mirnas is regulated by a large number of tfs whereas the second is regulated by only a few tfs the differential expression level of the two classes of mirnas in embryonic developmental stages versus adult tissues suggests that the two classes may have fundamentally different biological functions our results demonstrate that the tfs and mirnas extensively interact with each other and the biological functions of mirnas may be wired in the regulatory network nar
computational methods to identify functional genomic elements using genetic information have been very successful in determining gene structure and in identifying a handful of cis regulatory elements but the vast majority of regulatory elements have yet to be discovered and it has become increasingly apparent that their discovery will not come from using genetic information alone recently high throughput technologies have enabled the creation of information rich epigenetic maps most notably for histone modifications however tools that search for functional elements using this epigenetic information have been lacking here we describe an unsupervised learning method called chromasig to find in an unbiased fashion commonly occurring chromatin signatures in both tiling microarray and sequencing data applying this algorithm to nine chromatin marks across a sampling of the human genome in hela cells we recover eight clusters of distinct chromatin signatures five of which correspond to known patterns associated with transcriptional promoters and enhancers interestingly we observe that the distinct chromatin signatures found at enhancers mark distinct functional classes of enhancers in terms of transcription factor and coactivator binding in addition we identify three clusters of novel chromatin signatures that contain evolutionarily conserved sequences and potential cis regulatory elements applying chromasig to a panel of chromatin marks mapped genomewide by chip seq reveals classes of genomic elements marked by distinct chromatin signatures interestingly four classes containing enrichment for repressive histone modifications appear to be locally heterochromatic sites and are enriched in quickly evolving regions of the genome the utility of this approach in uncovering novel functionally significant genomic elements will aid future efforts of genome annotation via modifications
a major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution however in many real world applications this assumption may not hold for example we sometimes have a classification task in one domain of interest but we only have sufficient training data in another domain of interest where the latter data may be in a different feature space or follow a different data distribution in such cases knowledge transfer if done successfully would greatly improve the performance of learning by avoiding much expensive data labeling efforts in recent years transfer learning has emerged as a new learning framework to address this problem this survey focuses on categorizing and reviewing the current progress on transfer learning for classification regression and clustering problems in this survey we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation multitask learning and sample selection bias as well as covariate shift we also explore some potential future issues in transfer research
in this report we investigate the statistical power of several tests of selective neutrality based on patterns of genetic diversity within and between species the goal is to compare tests based solely on population genetic data with tests using comparative data or a combination of comparative and population genetic data we show that in the presence of repeated selective sweeps on relatively neutral background tests based on the dn ds ratios in comparative data almost always have more power to detect selection than tests based on population genetic data even if the overall level of divergence is low tests based solely on the distribution of allele frequencies or the site frequency spectrum such as the ewens watterson test or tajima s d have less power in detecting both positive and negative selection because of the transient nature of positive selection and the weak signal left by negative selection the hudson kreitman aguade test is the most powerful test for detecting positive selection among the population genetic tests investigated whereas mcdonald kreitman test typically has more power to detect negative selection we discuss our findings in the light of the discordant results obtained in several recently published genomic molbev
epistasis or interactions between genes has long been recognized as fundamentally important to understanding the structure and function of genetic pathways and the evolutionary dynamics of complex genetic systems with the advent of high throughput functional genomics and the emergence of systems approaches to biology as well as a new found ability to pursue the genetic basis of evolution down to specific molecular changes there is a renewed appreciation both for the importance of studying gene interactions and for addressing these questions in a unified manner
a challenge facing biology is to develop quantitative predictive models of gene regulation eukaryotic promoters contain transcription factor binding sites of differing affinity and accessibility but we understand little about how these variables combine to generate a fine tuned quantitative transcriptional response here we used the promoter in budding yeast to quantify the relationship between transcription factor input and gene expression output termed the gene regulation function grf a model that captures variable interactions between transcription factors nucleosomes and the promoter faithfully reproduced the observed quantitative changes in the grf that occur upon altering the affinity of transcription factor binding sites and implicates nucleosome modulated accessibility of transcription factor binding sites in increasing the diversity of gene expression profiles this work establishes a quantitative framework that can be applied to predict grfs of other genes
there is an explosion of community generated multimedia content available online in particular flickr constitutes a million photo sharing system where users participate following a variety of social motivations and themes flickr groups are increasingly used to facilitate the explicit definition of communities sharing common interests which translates into large amounts of content e g pictures and associated tags about specific subjects however to our knowledge an in depth analysis of user behavior in flickr groups remains open as does the existence of effective tools to find relevant groups using a sample of about million user photos and about flickr groups we present a novel statistical group analysis that highlights relevant patterns of photo to group sharing practices furthermore we propose a novel topic based representation model for groups computed from aggregated group tags groups are represented as multinomial distributions over semantically meaningful latent topics learned via unsupervised probabilistic topic modeling we show this representation to be useful for automatically discovering groups of groups and topic expert groups for designing new group search strategies and for obtaining new insights of the semantic structure of groups
micrornas mirnas are short non protein coding rnas that direct the widespread phenomenon of post transcriptional regulation of metazoan genes the mature nt long rna molecules are processed from genome encoded stem loop structured precursor genes hundreds of such genes have been experimentally validated in vertebrate genomes yet their discovery remains challenging and substantially higher numbers have been estimated the mirortho database http cegg unige ch mirortho presents the results of a comprehensive computational survey of mirna gene candidates across the majority of sequenced metazoan genomes we designed and applied a three tier analysis pipeline i an svm based ab initio screen for potent hairpins plus homologs of known mirnas ii an orthology delineation procedure and iii an svm based classifier of the ortholog multiple sequence alignments the web interface provides direct access to putative mirna annotations ortholog multiple alignments rna secondary structure conservation and sequence data the mirortho data are conceptually complementary to the mirbase catalog of experimentally verified mirna sequences providing a consistent comparative genomics perspective as well as identifying many novel mirna genes with strong evolutionary nar
a manually curated database aims at providing a comprehensive resource of microrna deregulation in various human diseases the current version of documents curated relationships between human micrornas and human diseases by reviewing more than published papers around one seventh of the microrna disease relationships represent the pathogenic roles of deregulated microrna in human disease each entry in the contains detailed information on a microrna disease relationship including a microrna id the disease name a brief description of the microrna disease relationship an expression pattern of the microrna the detection method for microrna expression experimentally verified target gene s of the microrna and a literature reference provides a user friendly interface for a convenient retrieval of each entry by microrna id disease name or target gene in addition offers a submission page that allows researchers to submit established microrna disease relationships that are not documented once approved by the submission review committee the submitted records will be included in the database is freely available at http www nar
the hippocampal expression profiles of wild type mice and mice transgenic for delta c doublecortin like kinase were compared with solexa illumina deep sequencing technology and five different microarray platforms with illumina s digital gene expression assay we obtained million sequence tags per sample their abundance spanning four orders of magnitude results were highly reproducible even across laboratories with a dedicated bayesian model we found differential expression of transcripts with an estimated false discovery rate of this is a much higher figure than found for microarrays the overlap in differentially expressed transcripts found with deep sequencing and microarrays was most significant for affymetrix the changes in expression observed by deep sequencing were larger than observed by microarrays or quantitative pcr relevant processes such as calmodulin dependent protein kinase activity and vesicle transport along microtubules were found affected by deep sequencing but not by microarrays while undetectable by microarrays antisense transcription was found for of all genes and alternative polyadenylation for we conclude that deep sequencing provides a major advance in robustness comparability and richness of expression profiling data and is expected to boost collaborative comparative and integrative genomics nar
motivation artemis and artemis comparison tool act have become mainstream tools for viewing and annotating sequence data particularly for microbial genomes since its first release artemis has been continuously developed and supported with additional functionality for editing and analysing sequences based on feedback from an active user community of laboratory biologists and professional annotators nevertheless its utility has been somewhat restricted by its limitation to reading and writing from flat files therefore a new version of artemis has been developed which reads from and writes to a relational database schema and allows users to annotate more complex often large and fragmented genome sequences results artemis and act have now been extended to read and write directly to the generic model organism database gmod http www gmod org chado relational database schema in addition a gene builder tool has been developed to provide structured forms and tables to edit coordinates of gene models and edit functional annotation based on standard ontologies controlled vocabularies and free text availability artemis and act are freely available under a gpl licence for download for macosx unix and windows at the wellcome trust sanger institute web sites http www sanger ac uk software artemis http www sanger ac uk software act contact artemis sanger ac uk supplementary information supplementary data are available at online
multi car elevator mce that has several elevator cars in a single shaft is the novel system and attracts attention for improvement of transportation in high rise buildings design of controller for mce is very difficult engineering problem and one of the promising approaches is from scratch evolutionary optimization of the controller through discrete event simulation of the mce system in the present paper the authors at first show application of evolutionary multi objective optimization to design traffic sensitive mce controller the controller for mce is optimized for different traffic conditions by combining the multi objective optimization with the exemplar based policy ebp representation that has adequate flexibility and generalization ability as a controller and a policy visualization method is proposed for analyzing how the traffic sensitive works
this is the first of a series of papers in which we present a brief introduction to the relevant mathematical and physical ideas that form the foundation of particle physics including group theory relativistic quantum mechanics quantum field theory and interactions abelian and non abelian gauge theory and the su xsu xu gauge theory that describes our universe apart from gravity these notes are not intended to be a comprehensive introduction to any of the ideas contained in them among the glaring omissions are cpt theorems evaluations of feynman diagrams renormalization and anomalies the topics were chosen according to the authors preferences and agenda these notes are intended for a student who has completed the standard undergraduate physics and mathematics courses furthermore these notes should not and will not in any way take the place of the related courses but rather provide a primer for detailed courses in qft gauge theory string theory etc which will fill in the many gaps left by paper
although transposable elements tes are known to be potent sources of mutation their contribution to the generation of recent adaptive changes has never been systematically assessed in this work we conduct a genome wide screen for adaptive te insertions in drosophila melanogaster that have taken place during or after the spread of this species out of africa we determine population frequencies of of the tes in release of the d melanogaster genome and identify a set of putatively adaptive tes these tes increased in population frequency sharply after the spread out of africa we argue that many of these tes are in fact adaptive by demonstrating that the regions flanking five of these tes display signatures of partial selective sweeps furthermore we show that eight out of the putatively adaptive elements show population frequency heterogeneity consistent with these elements playing a role in adaptation to temperate climates we conclude that tes have contributed considerably to recent adaptive evolution one te induced adaptation every y the majority of these adaptive insertions are likely to be involved in regulatory changes our results also suggest that te induced adaptations arise more often from standing variants than from new mutations such a high rate of te induced adaptation is inconsistent with the number of fixed tes in the d melanogaster genome and we discuss possible explanations for discrepancy
an approach using many intermediate semantic concepts is proposed with the potential to bridge the semantic gap between what a color shape and texture based ldquolow levelrdquo image analysis can extract from video and what users really want to find most likely using text descriptions of their information needs semantic concepts such as cars planes roads people animals and different types of scenes outdoor night time etc can be automatically detected in the video with reasonable accuracy this leads us to ask how can they be used automatically and how does a user or a retrieval system translate the user s information need into a selection of related concepts that would help find the relevant video clips from the large list of available concepts we illustrate how semantic concept retrieval can be automatically exploited by mapping queries into query classes and through pseudo relevance feedback we also provide evidence how a semantic concept can be utilized by users in interactive retrieval through interfaces that provide affordances of explicit concept selection and search concept filtering and relevance feedback how many concepts we actually need and how accurately they need to be detected and linked through various relationships is specified in the structure
background aims nitrate ingestion leads to high luminal concentrations of nitric oxide being generated where saliva meets gastric acid nitric oxide generates n nitrosative stress on reacting with oxygen at neutral ph we aimed to ascertain if luminal nitric oxide exerts nitrosative stress in the human upper gastrointestinal tract and to assess the influence of acid reflux on this phenomenon methods a silicone tube segmented every mm and containing phosphate buffer ph and the secondary amine morpholine was inserted into the upper gastrointestinal tract of healthy volunteers and barrett s esophagus patients the tube wall has the same permeability properties as an epithelial lipid membrane allowing passage of gases such as nitric oxide but not hydrogen ions after hours the tube was removed and the concentrations of nitrite and n nitrosomorpholine in each segment were measured healthy volunteers were studied with and without ingestion of n enriched nitrate and barrett s esophagus patients were studied with and without stimulation of acid reflux results in healthy volunteers n nitrosomorpholine was generated in the tube sections exposed to gastric acid and increased fold after nitrate the n nitrosomorpholine was enriched with n confirming its source was the ingested nitrate in the barrett s patients generation of n nitrosomorpholine was shifted proximally with of nitrosative stress occurring within the esophagus during periods of acid reflux conclusions this study demonstrates the in situ formation of n nitrosamine from dietary nitrate via nitric oxide during acid reflux nitrosative stress occurs almost entirely within the esophagus and may contribute toward carcinogenesis at site
abstract background the bluejay genome browser has been developed over several years to address the challenges posed by the ever increasing number of data types as well as the increasing volume of data in genome research beginning with a browser capable of rendering views of xml based genomic information and providing scalable vector graphics output we have now completed version of the system with many additional features our development efforts were guided by our observation that biologists who use both gene expression profiling and comparative genomics gain functional insights above and beyond those provided by traditional per gene analyses results bluejay is a genome viewer integrating genome annotation with i gene expression information and ii comparative analysis with an unlimited number of other genomes in the same view this allows the biologist to see a gene not just in the context of its genome but also its regulation and its evolution bluejay now has rich provision for personalization by users i numerous display customization features ii the availability of gps style waypoints for marking multiple points of interest on a genome and subsequently utilizing them and iii the ability to take user relevance feedback of annotated genes or textual items to offer personalized recommendations bluejay also embeds the seahawk browser for the moby protocol enabling users to seamlessly invoke hundreds of web services on genomic data of interest without any hard coding conclusions bluejay offers a unique set of customizable genome browsing features with the goal of allowing biologists to quickly focus on analyze compare and retrieve related information on the parts of the genomic data they are most interested in we expect these capabilities of bluejay to benefit the many biologists who want to answer complex questions using the information available from completely genomes
after recovery from acute muscle pain even minor subsequent muscle use can initiate recurrence of the same mechanical hyperalgesia months or years after the initial injury we have recently developed a model of this chronic latent hyperalgesia in the rat in this study we have examined the possibility that interleukin il an inflammatory mediator produced during acute muscle inflammation can mediate the production of this chronic latent hyperalgesic state in which subsequent exposure to inflammatory mediators produces a markedly prolonged mechanical hyperalgesia we now report that i m injection of il produced mechanical hyperalgesia lasting several hours that was prevented by intrathecal injection of antisense to glycoprotein an il receptor subunit furthermore following complete recovery from i m il induced hyperalgesia i m prostaglandin e produced a mechanical hyperalgesia that was remarkably prolonged compared with nave controls indicating the presence of chronic latent hyperalgesia this ability of il to produce chronic latent hyperalgesia was prevented by intrathecal administration of antisense for furthermore antisense also prevented chronic latent hyperalgesia produced by i m injection of the inflammogen carrageenan these results identify a role for il in acute inflammatory muscle pain and as a potential target against which therapies might be directed to treat chronic pain
this keynote paper presents a century vision of computing identifies various computing paradigms promising to deliver the vision of computing utilities defines cloud computing and provides the architecture for creating market oriented clouds by leveraging technologies such as vms provides thoughts on market based resource management strategies that encompass both customer driven service management and computational risk management to sustain sla oriented resource allocation presents some representative cloud platforms especially those developed in industries along with our current work towards realising market oriented resource allocation of clouds by leveraging the generation aneka enterprise grid technology reveals our early thoughts on interconnecting clouds for dynamically creating an atmospheric computing environment along with pointers to future community research and concludes with the need for convergence of competing it paradigms for delivering our vision
background publicly available data repositories facilitate the sharing of an ever increasing amount of microarray data however these datasets remain highly underutilized reutilizing the data could offer insights into questions and diseases entirely distinct from those considered in the original experimental design methods we first analyzed microarray datasets derived from known perturbations of specific pathways using the samr package in r to identify specific patterns of change in gene expression we refer to these pattern of gene expression alteration as a pathway signatures we then used spearman s rank correlation coefficient a non parametric measure of correlation to determine similarities between pathway signatures and disease profiles and permutation analysis to evaluate false discovery rate this enabled detection of statistically significant similarity between these pathway signatures and corresponding changes observed in human disease finally we evaluated pathway activation as indicated by correlation with the pathway signature as a risk factor for poor prognosis using multiple unrelated publicly available datasets results we have developed a novel method expression based pathway signature analysis epsa we demonstrate that espa is a rigorous computational approach for statistically evaluating the degree of similarity between highly disparate sources of microarray expression data we also show how epsa can be used in a number of cases to stratify patients with differential disease prognosis epsa can be applied to many different types of datasets in spite of different platforms different experimental designs and different species applying this method can yield new insights into human disease progression conclusion epsa enables the use of publicly available data for an entirely new translational purpose to enable the identification of potential pathways of dysregulation in human disease as well as potential leads for therapeutic targets
in this article we review the key modeling tools available for simulating biomolecular systems we consider recent developments and representative applications of mixed quantum mechanics molecular mechanics qm mm elastic network models enms coarse grained molecular dynamics and grid based tools for calculating interactions between essentially rigid protein assemblies we consider how the different length scales can be coupled both in a sequential fashion e g a coarse grained or grid model using parameterization from md simulations and via concurrent approaches where the calculations are performed together and together control the progression of the simulation we suggest how the concurrent coupling approach familiar in the context of qm mm calculations can be generalized and describe how this has been done in the charmm macromolecular package
glucose tolerance progressively declines with age in humans and is often accompanied by insulin resistance and a high prevalence of type diabetes little is known about the mechanism underlying the age related changes in glucose metabolism here we reported that acid sensing ion channel is functionally expressed in adipose cells mice were protected against age dependent glucose intolerance with enhanced insulin sensitivity acute administration of selective blocker improved the glucose control and increased the insulin sensitivity in older weeks mice moreover the enhanced glucose control in aging mice was associated with high baseline levels of akt phosphorylation and high copy number of mitochondrial dna in adipose tissues taken together our data suggest that signaling might be involved in the control of age dependent glucose intolerance and resistance
clinical decision support is a powerful tool for improving healthcare quality and patient safety however developing a comprehensive package of decision support interventions is costly and difficult if used well web methods may make it easier and less costly to develop decision support web is characterized by online communities open sharing interactivity and collaboration although most previous attempts at sharing clinical decision support content have worked outside of the web framework several initiatives are beginning to use web to share and collaborate on decision support content we present case studies of three efforts the clinfowiki a world accessible wiki for developing decision support content partners healthcare erooms web based tools for developing decision support within a single organization and epic systems corporation s community library a repository for sharing decision support content for customers of a single clinical system vendor we evaluate the potential of web technologies to enable collaborative development and sharing of clinical decision support systems through the lens of three case studies analyzing technical legal and organizational issues for developers consumers and organizers of clinical decision support content in web we believe the case for web as a tool for collaborating on clinical decision support content appears strong particularly for collaborative content development within organization
graph vertices are often organized into groups that seem to live fairly independently of the rest of the graph with which they share but a few edges whereas the relationships between group members are stronger as shown by the large number of mutual connections such groups of vertices or communities can be considered as independent compartments of a graph detecting communities is of great importance in sociology biology and computer science disciplines where systems are often represented as graphs the task is very hard though both conceptually due to the ambiguity in the definition of community and in the discrimination of different partitions and practically because algorithms must find good partitions among an exponentially large number of them other complications are represented by the possible occurrence of hierarchies i e communities which are nested inside larger communities and by the existence of overlaps between communities due to the presence of nodes belonging to more groups all these aspects are dealt with in some detail and many methods are described from traditional approaches used in computer science and sociology to recent techniques developed mostly within physics
motivation the advent of sequencing and structural genomics projects has provided a dramatic boost in the number of uncharacterized protein structures and sequences consequently many computational tools have been developed to help elucidate protein function however such services are spread throughout the world often with standalone web pages integration of these methods is needed and so far this has not been possible as there was no common vocabulary available that could be used as a standard language results the protein feature ontology has been developed to provide a structured controlled vocabulary for features on a protein sequence or structure and comprises approximately positional terms now integrated into the sequence ontology so and non positional terms which describe features relating to the whole protein sequence in addition post translational modifications are described by using a pre existing ontology the protein modification ontology mod this ontology is being used to integrate over distinct annotations provided by the biosapiens network of excellence a consortium comprising partner sites in europe availability the protein feature ontology can be browsed by accessing the ontology lookup service at the european bioinformatics institute http www ebi ac uk ontology lookup browse do ontname bs contact gabby ebi uk
the interpro database http www ebi ac uk interpro integrates together predictive models or signatures representing protein domains families and functional sites from multiple diverse source databases panther pfam pirsf prints prodom prosite smart superfamily and tigrfams integration is performed manually and approximately half of the total signatures available in the source databases belong to an interpro entry recently we have started to also display the remaining un integrated signatures via our web interface other developments include the provision of non signature data such as structural data in new xml files on our ftp site as well as the inclusion of matchless uniprotkb proteins in the existing match xml files the web interface has been extended and now links out to the adan predicted proteinprotein interaction database and the spice and dasty viewers the latest public release covers of uniprotkb and consists of entries interpro data may be accessed either via the web address above via web services by downloading files by anonymous ftp or by using the interproscan search software http www ebi ac uk interproscan
pnas the expression dynamics of interacting genes depends in part on the structure of regulatory networks genetic regulatory networks include an overrepresentation of subgraphs commonly known as network motifs in this article we demonstrate that gene copy number is an omnipresent parameter that can dramatically modify the dynamical function of network motifs we consider positive feedback bistable feedback and toggle switch motifs and show that variation in gene copy number on the order of a single or few copies can lead to multiple orders of magnitude change in gene expression and in some cases switches in deterministic control further small changes in gene copy number for a gene motif with successive inhibition the repressilator can lead to a qualitative switch in system behavior among oscillatory and equilibrium dynamics in all cases the qualitative change in expression is due to the nonlinear nature of transcriptional feedback in which duplicated motifs interact via common pools of transcription factors we are able to implicitly determine the critical values of copy number which lead to qualitative shifts in system behavior in some cases we are able to solve for the sufficient condition for the existence of a bifurcation in terms of kinetic rates of transcription translation binding and degradation we discuss the relevance of our findings to ongoing efforts to link copy number variation with cell fate determination by viruses dynamics of synthetic gene circuits and constraints on adaptation
this paper focuses on using a wiki for student created content in a collaborative environment it describes the uses a wiki can have as a collaborative concept there is promoted debate over the tool however this paper gives insight to how the wiki should be utilized and maintained this is a good source for those that have never used a wiki it will give insight of the do and donts of wiki collaboration in education steve wheeler is a convenor of e learning research network faculty of education and a faculty coordinator for technology mediated learning and development
the second edition of this classic work first published in and an international bestseller explores the differences in thinking and social action that exist among members of more than modern nations geert hofstede argues that people carry mental programs which are developed in the family in early childhood and reinforced in schools and organizations and that these programs contain components of national culture they are expressed most clearly in the different values that predominate among people from different countries geert hofstede has completely rewritten revised and updated cultures consequences for the twenty first century he has broadened the book s cross disciplinary appeal expanded the coverage of countries examined from to more than reformulated his arguments and a large amount of new literature has been included the book is structured around five major dimensions power distance uncertainty avoidance individualism versus collectivism masculinity versus femininity and long term versus short orientation
citeulike is a collaborative tagging web site which lets users enter academic references into a database and describe these references using tags categorizations of their own choosing we looked at the tagging behavior of people who were describing four frequently entered references we found that while people tend to agree on a few select tags people also tend to use many variants of these tags this lack of consensus means that the collaborative aspect of tagging is not as strong as may have been suggested in past
bone cancer pain is one of the most common complications by patients with breast cancer however precise molecular mechanism of bone pain is still elusive recent studies have shown that the acid sensing nociceptors asns such as the transient receptor potential vanilloid and acid sensing ion channels asics expressed in calcitonin gene related protein cgrp positive nociceptive fibers play a role in transducing peripheral pain signals to cns of note igf is found to regulate activation since igf is released from bone due to increased bone resorption during the progression of bone metastases we reasoned bone derived igf modulates bone pain to approach this we established a mouse model of cancer induced bone pain by inoculating the mouse breast cancer cells into the marrow cavity of tibiae tumor inoculated tibiae showed significantly increased pain behaviors compared with the non tumor bearing tibiae indicating cells induced bone pain immunohistochemical examination revealed substantial expression of cgrp positive fibers and increased proportion of these fibers in the tumor bearing tibiae rt pcr showed elevated mrna expression of and cgrp in the ipsi lateral dorsal root ganglions drgs which were the the cell body of primary sensory neurons innervating bone since c fos perk and were widely accepted markers of neural activation we examined these expression in the drgs rt pcr showed elevated mrna expression of c fos and western blotting showed elevated perk and expression in the ipsi lateral tumor bearing drgs compared with the contra lateral drgs respectively to study the role of bone derived igf in vitro we established organ cultures of mouse drgs the culture supernatants harvested from the mouse neonatal calvariae up regulated cgrp and c fos expression in mouse drgs a specific inhibitor of igf receptor tyrosine kinase decreased the expression of these molecules igf up regulated and cgrp mrna expression in drgs while tgfb another abundant growth factor in bone showed no effects igf also elevated mrna expression of c fos in the f rat drg like cells these results collectively suggest that bone derived igf influences bone pain to verify this we tested in vivo significantly reduced pain behaviors in tumor bearing tibiae in addition mrna expression of and cgrp and c fos was decreased by in the ipsi lateral drgs moreover the expression of perk and were also decreased by in the ipsi lateral drgs these results demonstrated igf up regulates mrna expression of and and also activated and in conclusion our results suggest that bone derived igf enhances breast cancer induced bone pain through stimulating asn expression and activation and that inhibition of bone resorption may reduce bone pain by limiting igf release bone
studies of the evolution of development characterize the way in which gene regulatory dynamics during ontogeny constructs and channels phenotypic variation these studies have identified a number of evolutionary regularities phenotypes occupy only a small subspace of possible phenotypes the influence of mutation is not uniform and is often canalized and a great deal of morphological variation evolved early in the history of multicellular life an important implication of these studies is that diversity is largely the outcome of the evolution of gene regulation rather than the emergence of new structural genes using a simple model that considers a generic property of developmental mapsthe interaction between multiple genetic elements and the nonlinearity of gene interaction in shaping phenotypic traitswe are able to recover many of these empirical regularities we show that visible phenotypes represent only a small fraction of possibilities epistasis ensures that phenotypes are highly clustered in morphospace and that the most frequent phenotypes are the most similar we perform phylogenetic analyses on an evolving developmental model and find that species become more alike through time whereas higher level grades have a tendency to diverge ancestral phenotypes produced by early developmental programs with a low level of gene interaction are found to span a significantly greater volume of the total phenotypic space than derived taxa we suggest that early and late evolution have a different character that we classify into micro and macroevolutionary configurations these findings complement the view of development as a key component in the production of endless forms and highlight the crucial role of development in constraining biotic diversity and trajectories
gene fusion and fission events are key mechanisms in the evolution of gene architecture whose effects are visible in protein architecture when they occur in coding sequences until now the detection of fusion and fission events has been performed at the level of protein sequences with a post facto removal of supernumerary links due to paralogy and often did not include looking for events defined only in single genomes we propose a method for the detection of these events defined on groups of paralogs to compensate for the gene redundancy of eukaryotic genomes and apply it to the proteomes of fungal species we collected an inventory of elementary fusion and fission events in half the cases both composite and element genes are found in the same species per species counts of events correlate with the species genome size suggesting a random mechanism of occurrence some biological functions of the genes involved in fusion and fission events are slightly over or under represented as already noted in previous studies the genes involved in an event tend to belong to the same functional category we inferred the position of each event in the evolution tree of the fungal species the event localization counts for all the segments of the tree provide a metric that depicts the recombinational phylogeny among fungi a possible interpretation of this metric as distance in adaptation space proposed
motivation dna sequence reads from sanger and pyrosequencing platforms differ in cost accuracy typical coverage average read length and the variety of available paired end protocols both read types can complement one another in a hybrid approach to whole genome shotgun sequencing projects but assembly software must be modified to accommodate their different characteristics this is true even of pyrosequencing mated and unmated read combinations without special modifications assemblers tuned for homogeneous sequence data may perform poorly on hybrid data results celera assembler was modified for combinations of abi and flx reads the revised pipeline called cabog celera assembler with the best overlap graph is robust to homopolymer run length uncertainty high read coverage and heterogeneous read lengths in tests on four genomes it generated the longest contigs among all assemblers tested it exploited the mate constraints provided by paired end reads from either platform to build larger contigs and scaffolds which were validated by comparison to a finished reference sequence a low rate of contig mis assembly was detected in some cabog assemblies but this was reduced in the presence of sufficient mate pair data availability the software is freely available as open source from http wgs assembler sf net under the gnu public license contact jmiller jcvi org supplementary information supplementary data are available at bioinformatics bioinformatics
active centres and hot spots of proteins have a paramount importance in enzyme action protein complex formation and drug design recently several publications successfully applied the analysis of residue networks to predict active centres in proteins most real world networks show several properties such as small worldness or scale free degree distribution which are rather general features of networks from molecules to society at large using analogy i propose that existing findings and methodology already enable us to detect active centres in cells and can be expanded to social networks and ecosystems members of these active centres are termed here as creative elements of their respective networks which can help them to survive unprecedented novel challenges and play a key part in the development survival and evolvability of systems
gr pairwise whole genome alignment involves the creation of a homology map capable of performing a near complete transformation of one genome into another for multiple genomes this problem is generalized to finding a set of consistent homology maps for converting each genome in the set of aligned genomes into any of the others the problem can be divided into two principal stages first the partitioning of the input genomes into a set of colinear segments a process which essentially deals with the complex processes of rearrangement second the generation of a base pair level alignment map for each colinear segment we have developed a new genome wide segmentation program enredo which produces colinear segments from extant genomes handling rearrangements including duplications we have then applied the new alignment program pecan which makes the consistency alignment methodology practical at a large scale to create a new set of genome wide mammalian alignments we test both enredo and pecan using novel and existing assessment analyses that incorporate both real biological data and simulations and show that both independently and in combination they outperform existing programs alignments from our pipeline are publicly available within the ensembl browser
pnas micrornas mirnas play an important role in posttranscriptional regulation of genes we developed a method to predict human mirnas without requiring cross species conservation we first identified lowly moderately expressed tissue selective genes using est data and then identified overrepresented motifs of seven nucleotides in the utrs of these genes using these motifs as potential target sites of mirnas we recovered more than two thirds of the known human mirnas we then used those motifs that did not match any known human mirna seed region to infer novel mirnas we predicted new human mirna genes with mature forms and novel alternative mature forms of known mirna genes when a stringent criterion was used and many more novel mirnas when a less stringent criterion was used we tested the expression of predicted mirnas in three human cell lines and found of them expressed in all three cell lines and expressed in one cell line we selected of them p and p to do functional validation using their mimics and inhibitors and using both luciferase assay and western blotting these experiments provided strong evidence that both p and p are novel mirnas and that which encodes camp responsive element binding protein like is a target gene of p whereas which encodes laminin is a target gene p
the massive visual input from the eye to the brain requires selective processing of some visual information at the expense of other information a process referred to as visual attention increases in the responses of visual neurons with attention have been extensively studied along the visual processing streams in monkey cerebral cortex from primary visual areas to parietal and frontal cortex here we show by recording neurons in attending macaque monkeys macaca mulatta that attention modulates visual signals before they even reach cortex by increasing responses of both magnocellular and parvocellular neurons in the first relay between retina and cortex the lateral geniculate nucleus lgn at the same time attention decreases neuronal responses in the adjacent thalamic reticular nucleus trn crick argued for such modulation of the lgn by observing that it is inhibited by the trn and suggested that if the thalamus is the gateway to the cortex the reticular complex might be described as the guardian of the gateway a reciprocal relationship we now show to be more than just hypothesis the reciprocal modulation in lgn and trn appears only during the initial visual response but the modulation of lgn reappears later in the response suggesting separate early and late sources of attentional modulation lgn
the first bacterial genome was sequenced in and the first archaeal genome in soon after these breakthroughs an exponential rate of genome sequencing was established with a doubling time of approximately months for bacteria and approximately months for archaea comparative analysis of the hundreds of sequenced bacterial and dozens of archaeal genomes leads to several generalizations on the principles of genome organization and evolution a crucial finding that enables functional characterization of the sequenced genomes and evolutionary reconstruction is that the majority of archaeal and bacterial genes have conserved orthologs in other often distant organisms however comparative genomics also shows that horizontal gene transfer hgt is a dominant force of prokaryotic evolution along with the loss of genetic material resulting in genome contraction a crucial component of the prokaryotic world is the mobilome the enormous collection of viruses plasmids and other selfish elements which are in constant exchange with more stable chromosomes and serve as hgt vehicles thus the prokaryotic genome space is a tightly connected although compartmentalized network a novel notion that undermines the tree of life model of evolution and requires a new conceptual framework and tools for the study of evolution
the function of a protein is intimately tied to its subcellular localization although localizations have been measured for many yeast proteins through systematic gfp fusions similar studies in other branches of life are still forthcoming in the interim various machine learning methods have been proposed to predict localization using physical characteristics of a protein such as amino acid content hydrophobicity side chain mass and domain composition however there has been comparatively little work on predicting localization using protein networks here we predict protein localizations by integrating an extensive set of protein physical characteristics over a protein s extended protein protein interaction neighborhood using a classification framework called divide and conquer k nearest neighbors dc knn these predictions achieve significantly higher accuracy than two well known methods for predicting protein localization in yeast using new gfp imaging experiments we show that the network based approach can extend and revise previous annotations made from high throughput studies finally we show that our approach remains highly predictive in higher eukaryotes such as fly and human in which most localizations are unknown and the protein network coverage is substantial
motivation a genome wide chip chip tiling array study requires millions of simultaneous comparisons of hybridization for significance controlling the false positive rate in genome wide tiling array studies is very important because the number of computationally identified regions can easily go beyond the capability of experimental verification no accurate and efficient method exists for evaluating statistical significance in tiling arrays the bonferroni method is overly conservative and the permutation test is time consuming for genome wide studies result motivated by the poisson clumping heuristic we propose an accurate and efficient method for evaluating statistical significance in genome wide chip chip tiling arrays the method works accurately for any large number of multiple comparisons and the computational cost for evaluating p values does not increase with the total number of tests based on a moving window approach we demonstrate how to combine results using various window sizes to increase the detection power while maintaining a specified type i error rate we further introduce a new false discovery rate control that is more appropriate in measuring the false proportion of binding intervals in tiling array analysis our method is general and can be applied to many large scale genomic and genetic studies availability http www stat psu edu yuzhang pass tarcontact yuzhang stat edu
online social networking sites like myspace and flickr have become a popular way to share and disseminate content their massive popularity has led to the viral marketing of content products and political campaigns on the sites themselves despite the excitement the precise mechanisms by which information is exchanged over these networks are not well understood in this paper we investigate social cascades or how information disseminates through social links in online social networks using real traces of popular photos and a social network collected from flickr and a theoretical framework borrowed from epidemiology we show that social cascades are an important factor in the dissemination of content our work provides an important first step in understanding how information disseminates in networks
functional partnerships between proteins are at the core of complex cellular phenotypes and the networks formed by interacting proteins provide researchers with crucial scaffolds for modeling data reduction and annotation string is a database and web resource dedicated to protein protein interactions including both physical and functional interactions it weights and integrates information from numerous sources including experimental repositories computational prediction methods and public text collections thus acting as a meta database that maps all interaction evidence onto a common set of genomes and proteins the most important new developments in string over previous releases include a url based programming interface which can be used to query string from other resources improved interaction prediction via genomic neighborhood in prokaryotes and the inclusion of protein structures version of string covers about million proteins from organisms providing the most comprehensive view on protein protein interactions currently available string can be reached at http string org
the topic of quantum noise has become extremely timely due to the rise of quantum information physics and the resulting interchange of ideas between the condensed matter and atomic molecular opticalquantum optics communities this review gives a pedagogical introduction to the physics of quantum noise and its connections to quantum measurement and quantum amplification after introducing quantum noise spectra and methods for their detection the basics of weak continuous measurements are described particular attention is given to the treatment of the standard quantum limit on linear amplifiers and position detectors within a general linear response framework this approach is shown how it relates to the standard haus caves quantum limit for a bosonic amplifier known in quantum optics and its application to the case of electrical circuits is illustrated including mesoscopic detectors and resonant detectors
pnas networks describe a variety of interacting complex systems in social science biology and information technology usually the nodes of real networks are identified not only by their connections but also by some other characteristics examples of characteristics of nodes can be age gender or nationality of a person in a social network the abundance of proteins in the cell taking part in protein interaction networks or the geographical position of airports that are connected by directed flights integrating the information on the connections of each node with the information about its characteristics is crucial to discriminating between the essential and negligible characteristics of nodes for the structure of the network in this paper we propose a general indicator based on entropy measures to quantify the dependence of a network s structure on a given set of features we apply this method to social networks of friendships in u s schools to the protein interaction network of and to the u s airport network showing that the proposed measure provides information that complements other measures
abstract background the analysis of large scale data sets via clustering techniques is utilized in a number of applications biclustering in particular has emerged as an important problem in the analysis of gene expression data since genes may only jointly respond over a subset of conditions biclustering algorithms also have important applications in sample classification where for instance tissue samples can be classified as cancerous or normal many of the methods for biclustering and clustering algorithms in general utilize simplified models or heuristic strategies for identifying the best grouping of elements according to some metric and cluster definition and thus result in suboptimal clusters results in this article we present a rigorous approach to biclustering oreo which is based on the optimal re ordering of the rows and columns of a data matrix so as to globally minimize the dissimilarity metric the physical permutations of the rows and columns of the data matrix can be modeled as either a network flow problem or a traveling salesman problem cluster boundaries in one dimension are used to partition and re order the other dimensions of the corresponding submatrices to generate biclusters the performance of oreo is tested on a metabolite concentration data b an image reconstruction matrix c synthetic data with implanted biclusters and gene expression data for d colon cancer data e breast cancer data as well as f yeast segregant data to validate the ability of the proposed method and compare it to existing biclustering and clustering methods conclusions we demonstrate that this rigorous global optimization method for biclustering produces clusters with more insightful groupings of similar entities such as genes or metabolites sharing common functions than other clustering and biclustering algorithms and can reconstruct underlying fundamental patterns in the data for several distinct sets of data matrices arising in important applications
the enormous complexity of the human brain ultimately derives from a finite set of molecular instructions encoded in the human genome these instructions can be directly studied by exploring the organization of the brain s transcriptome through systematic analysis of gene coexpression relationships we analyzed gene coexpression relationships in microarray data generated from specific human brain regions and identified modules of coexpressed genes that correspond to neurons oligodendrocytes astrocytes and microglia these modules provide an initial description of the transcriptional programs that distinguish the major cell classes of the human brain and indicate that cell type specific information can be obtained from whole brain tissue without isolating homogeneous populations of cells other modules corresponded to additional cell types organelles synaptic function gender differences and the subventricular neurogenic niche we found that subventricular zone astrocytes which are thought to function as neural stem cells in adults have a distinct gene expression pattern relative to protoplasmic astrocytes our findings provide a new foundation for neurogenetic inquiries by revealing a robust and previously unrecognized organization to the human transcriptome
genomics profoundly affects most areas of biology including ecology and evolutionary biology by examining genome sequences from multiple species comparative genomics offers new insight into genome evolution and the way natural selection moulds dna sequence evolution functional divergence as manifested in the accumulation of nonsynonymous substitutions in protein coding genes differs among lineages in a manner seemingly related to population size for example the ratio of nonsynonymous to synonymous substitution d n d s is higher in apes than in rodents compatible with ohta s nearly neutral theory of molecular evolution which suggests that the fixation of slightly deleterious mutations contributes to protein evolution at an extent negatively correlated with effective population size while this supports the idea that functional evolution is not necessarily adaptive comparative genomics is uncovering a role for positive darwinian selection in of all genes in different lineages estimates that are likely to increase when the addition of more genomes gives increased power again population size seems to matter also in this context with a higher proportion of fixed amino acid changes representing advantageous mutations in large populations genes that are particularly prone to be driven by positive selection include those involved with reproduction immune response sensory perception and apoptosis genetic innovations are also frequently obtained by the gain or loss of complete gene sequences moreover it is increasingly realized from comparative genomics that purifying selection conserves much more than just the protein coding part of the genome and this points at an important role for regulatory elements in trait evolution finally genome sequencing using outbred or multiple individuals has provided a wealth of polymorphism data that gives information on population history demography and evolution
rna binding proteins rbps have roles in the regulation of many post transcriptional steps in gene expression but relatively few rbps have been systematically studied we searched for the rna targets of proteins in the yeast saccharomyces cerevisiae a selective sample of the approximately annotated and predicted rbps as well as several proteins not annotated as rbps at least of these proteins including three of the four proteins that were not previously known or predicted to be rbps were reproducibly associated with specific sets of a few to several hundred rnas remarkably many of the rbps we studied bound mrnas whose protein products share identifiable functional or cytotopic features we identified specific sequences or predicted structures significantly enriched in target mrnas of rbps these potential rna recognition elements were diverse in sequence structure and location some were found predominantly in untranslated regions others in untranslated regions some in coding sequences and many in two or more of these features although this study only examined a small fraction of the universe of yeast rbps of the mrna transcriptome had significant associations with at least one of these rbps and on average each distinct yeast mrna interacted with three of the rbps suggesting the potential for a rich multidimensional network of regulation these results strongly suggest that combinatorial binding of rbps to specific recognition elements in mrnas is a pervasive mechanism for multi dimensional regulation of their post fate
abstract background modern sequencing technologies allow rapid sequencing and bioinformatic analysis of genomes and metagenomes with every new sequencing project a vast number of new proteins become available with many genes remaining functionally unclassified based on evidences from sequence similarities alone extending similarity searches with gene pattern approaches defined as genes sharing a distinct genomic neighbourhood have shown to significantly improve the number of functional assignments further functional evidences can be gained by correlating these gene patterns with prevailing environmental parameters metamine was developed to approach the large pool of unclassified proteins by searching for recurrent gene patterns across habitats based on key genes results metamine is an interactive data mining tool which enables the detection of gene patterns in an environmental context the gene pattern search starts with a user defined environmentally interesting key gene with this gene a blast search is carried out against the microbial ecological genomics database megdb containing marine genomic and metagenomic sequences this is followed by the determination of all neighbouring genes within a given distance and a search for functionally equivalent genes in the final step a set of common genes present in a defined number of distinct genomes is determined the gene patterns found are associated with their individual pattern instances describing gene order and directions they are presented together with information about the sample and the habitat metamine is implemented in java and provided as a client server application with a user friendly graphical user interface the system was evaluated with environmentally relevant genes related to the methane cycle and carbon monoxide oxidation conclusions metamine offers a targeted semi automatic search for gene patterns based on expert input the graphical user interface of metamine provides a user friendly overview of the computed gene patterns for further inspection in an ecological context prevailing biological processes associated with a key gene can be used to infer new annotations and shape hypotheses to guide further analyses the use cases demonstrate that meaningful gene patterns can be quickly detected using metamine metamine is freely available for academic use from http www megx metamine
the gene ontology annotation goa project at the ebi http www ebi ac uk goa provides high quality electronic and manual associations annotations of gene ontology go terms to uniprot knowledgebase uniprotkb entries annotations created by the project are collated with annotations from external databases to provide an extensive publicly available go annotation resource currently covering over taxa with greater than million annotations goa remains the largest and most comprehensive open source contributor to the go consortium goc project over the last five years the group has augmented the number and coverage of their electronic pipelines and a number of new manual annotation projects and collaborations now further enhance this resource a range of files facilitate the download of annotations for particular species and go term information and associated annotations can also be viewed and downloaded from the newly developed goa quickgo tool http www ebi ac uk quickgo which allows users to precisely tailor their set
genome wide association studies gwas for quantitative traits and disease in humans and other species have shown that there are many loci that contribute to the observed resemblance between relatives gwas to date have mostly focussed on discovery of genes or regulatory regions habouring causative polymorphisms using single snp analyses and setting stringent type i error rates genome wide marker data can also be used to predict genetic values and therefore predict phenotypes here we propose a bayesian method that utilises all marker data simultaneously to predict phenotypes we apply the method to three traits coat colour cells and mean cell haemoglobin measured in a heterogeneous stock mouse population we find that a model that contains both additive and dominance effects estimated from genome wide marker data is successful in predicting unobserved phenotypes and is significantly better than a prediction based upon the phenotypes of close relatives correlations between predicted and actual phenotypes were in the range of to when half of the number of families was used to estimate effects and the other half for prediction posterior probabilities of snps being associated with coat colour were high for regions that are known to contain loci for this trait the prediction of phenotypes using large samples high density snp data and appropriate statistical methodology is feasible and can be applied in human medicine forensics or artificial programs
results this paper presents the r bioconductor package minet version which provides a set of functions to infer mutual information networks from a dataset once fed with a microarray dataset the package returns a network where nodes denote genes edges model statistical dependencies between genes and the weight of an edge quantifies the statistical evidence of a specific e g transcriptional gene to gene interaction four different entropy estimators are made available in the package minet empirical miller madow schurmann grassberger and shrink as well as four different inference methods namely relevance networks aracne clr and mrnet also the package integrates accuracy assessment tools like f scores pr curves and roc curves in order to compare the inferred network with a reference one conclusion the package minet provides a series of tools for inferring transcriptional networks from microarray data it is freely available from the comprehensive r archive network cran as well as from the website
abstract background inferring cluster structure in microarray datasets is a fundamental task for the so called omic sciences it is also a fundamental question in statistics data analysis and classification in particular with regard to the prediction of the number of clusters in a dataset usually established via internal validation measures despite the wealth of internal measures available in the literature new ones have been recently proposed some of them specifically for microarray data results we consider five such measures clest consensus consensus clustering fom figure of merit gap gap statistics and me model explorer in addition to the classic wcss within cluster sum of squares and kl krzanowski and lai index we perform extensive experiments on six benchmark microarray datasets using both hierarchical and k means clustering algorithms and we provide an analysis assessing both the intrinsic ability of a measure to predict the correct number of clusters in a dataset and its merit relative to the other measures we pay particular attention both to precision and speed moreover we also provide various fast approximation algorithms for the computation of gap fom and wcss the main result is a hierarchy of those measures in terms of precision and speed highlighting some of their merits and limitations not reported before in the literature conclusions based on our analysis we draw several conclusions for the use of those internal measures on microarray data we report the main ones consensus is by far the best performer in terms of predictive power and remarkably algorithm independent unfortunately on large datasets it may be of no use because of its non trivial computer time demand weeks on a state of the art pc fom is the second best performer although quite surprisingly it may not be competitive in this scenario it has essentially the same predictive power of wcss but it is from to times slower in time depending on the dataset the approximation algorithms for the computation of fom gap and wcss perform very well i e they are faster while still granting a very close approximation of fom and wcss the approximation algorithm for the computation of gap deserves to be singled out since it has a predictive power far better than gap it is competitive with the other measures but it is at least two order of magnitude faster in time with respect to gap another important novel conclusion that can be drawn from our analysis is that all the measures we have considered show severe limitations on large datasets either due to computational demand consensus as already mentioned clest and gap or to lack of precision all of the other measures including their approximations the software and datasets are available under the gnu gpl on the supplementary material page
generating adequate recommendations for newcomers is a hard problem for a recommender system rs due to lack of detailed user profiles and social preference data empirical evidence suggests that the incorporation of a trust network among the users of the rs can leverage such cold start cs recommendations hence new users should be encouraged to connect to the network as soon as possible but whom should new users connect to given the impact this choice has on the delivered recommendations it is critical to guide newcomers through this early stage connection process in this paper we identify key figures in the trust network in particular mavens connectors and frequent raters and investigate their influence on the coverage and accuracy of a collaborative filtering rs using a dataset from epinions com we demonstrate that the generated recommendations for new user are more beneficial if they connect to an identified key figure compared to a user
probabilistic models of sequence evolution are in widespread use in phylogenetics and molecular sequence evolution these models have become increasingly sophisticated and combined with statistical model comparison techniques have helped to shed light on how genes and proteins evolve models of codon evolution have been particularly useful because in addition to providing a significant improvement in model realism for protein coding sequences codon models can also be designed to test hypotheses about the selective pressures that shape the evolution of the sequences such models typically assume a phylogeny and can be used to identify sites or lineages that have evolved adaptively recently some of the key assumptions that underlie phylogenetic tests of selection have been questioned such as the assumption that the rate of synonymous changes is constant across sites or that a single phylogenetic tree can be assumed at all sites for recombining sequences while some of these issues have been addressed through the development of novel methods others remain as caveats that need to be considered on a case by case basis here we outline the theory of codon models and their application to the detection of positive selection we review some of the more recent developments that have improved their power and utility laying a foundation for further advances in the modeling of coding sequence bib
we review our recent work on applying the google pagerank algorithm to find scientific gems among all physical review publications and its extension to citerank to find currently popular research directions these metrics provide a meaningful extension to traditionally used importance measures such as the number of citations and journal impact factor we also point out some pitfalls of over relying on quantitative metrics to evaluate quality
individuals communicate and form relationships through internet social networking websites such as facebook and myspace we study risk taking trust and privacy concerns with regard to social networking websites among college students using both reliable scales and behavior individuals with profiles on social networking websites have greater risk taking attitudes than those who do not greater risk taking attitudes exist among men than women facebook has a greater sense of trust than myspace general privacy concerns and identity information disclosure concerns are of greater concern to women than men greater percentages of men than women display their phone numbers and home addresses on social networking websites social networking websites should inform potential users that risk taking and privacy concerns are potentially relevant and important concerns before individuals sign up and create social networking websites psycinfo database record c apa all rights reserved abstract
we developed a method for estimating the positional distribution of transcription factor tf binding sites using chip chip data and applied it to recently published experiments on binding sites of nine tfs nanog and the data were obtained from a genome wide coverage of promoter regions from kb upstream of the transcription start site tss to kb downstream the number of target genes of each tf ranges from few hundred to several thousand we found that for each of the nine tfs the estimated binding site distribution is closely approximated by a mixture of two components a narrow peak localized within bp upstream of the tss and a distribution of almost uniform density within the tested region using gene ontology go and enrichment analysis we were able to associate for each of the tfs studied the target genes of both types of binding with known biological processes most go terms were enriched either among the proximal targets or among those with a uniform distribution of binding sites for example the three stemness related tfs have several hundred target genes that belong to development and morphogenesis whose binding sites belong to the distribution
in the lab the cis regulatory network seems to exhibit great functional redundancy many experiments testing enhancer activity of neighboring cis regulatory elements show largely overlapping expression domains of recent interest mice in which cis regulatory ultraconserved elements were knocked out showed no obvious phenotype further suggesting functional redundancy here we present a global evolutionary analysis of mammalian conserved nonexonic elements cnes and find strong evidence to the contrary given a set of cnes conserved between several mammals we characterize functional dispensability as the propensity for the ancestral element to be lost in mammalian species internal to the spanned species tree we show that ultraconserved like elements are over fold less likely than neutral dna to have been lost during rodent evolution in fact many thousands of noncoding loci under purifying selection display near uniform indispensability during mammalian evolution largely irrespective of nucleotide conservation level these findings suggest that many genomic noncoding elements possess functions that contribute noticeably to organism fitness in naturally populations
many scientists now manage the bulk of their bibliographic information electronically thereby organizing their publications and citation material from digital libraries however a library has been described as textquotedblleft thought in cold storage textquotedblright and unfortunately many digital libraries can be cold impersonal isolated and inaccessible places in this review we discuss the current chilly state of digital libraries for the computational biologist including pubmed ieee xplore the acm digital library isi web of knowledge scopus citeseer arxiv dblp and google scholar we illustrate the current process of using these libraries with a typical workflow and highlight problems with managing data and metadata using uris we then examine a range of new applications such as zotero mendeley mekentosj papers myncbi citeulike connotea and hubmed that exploit the web to make these digital libraries more personal sociable integrated and accessible places we conclude with how these applications may begin to help achieve a digital defrost and discuss some of the issues that will help or hinder this in terms of making libraries on the web warmer places in the future becoming resources that are considerably more useful to both humans machines
one defining goal of synthetic biology is the development of engineering based approaches that enable the construction of gene regulatory networks according to design specifications generated from computational this approach provides a systematic framework for exploring how a given regulatory network generates a particular phenotypic behaviour several fundamental gene circuits have been developed using this approach including toggle and and these have been applied in new contexts such as triggered biofilm and cellular population here we describe an engineered genetic oscillator in escherichia coli that is fast robust and persistent with tunable oscillatory periods as fast as the oscillator was designed using a previously modelled network architecture comprising linked positive and negative feedback using a microfluidic platform tailored for single cell microscopy we precisely control environmental conditions and monitor oscillations in individual cells through multiple cycles experiments reveal remarkable robustness and persistence of oscillations in the designed circuit almost every cell exhibited large amplitude fluorescence oscillations throughout observation runs the oscillatory period can be tuned by altering inducer levels temperature and the media source computational modelling demonstrates that the key design principle for constructing a robust oscillator is a time delay in the negative feedback loop which can mechanistically arise from the cascade of cellular processes involved in forming a functional transcription factor the positive feedback loop increases the robustness of the oscillations and allows for greater tunability examination of our refined model suggested the existence of a simplified oscillator design without positive feedback and we construct an oscillator strain confirming this prediction
the dominant paradigm in drug discovery is the concept of designing maximally selective ligands to act on individual drug targets however many effective drugs act via modulation of multiple proteins rather than single targets advances in systems biology are revealing a phenotypic robustness and a network structure that strongly suggests that exquisitely selective compounds compared with multitarget drugs may exhibit lower than desired clinical efficacy this new appreciation of the role of polypharmacology has significant implications for tackling the two major sources of attrition in drug developmentefficacy and toxicity integrating network biology and polypharmacology holds the promise of expanding the current opportunity space for druggable targets however the rational design of polypharmacology faces considerable challenges in the need for new methods to validate target combinations and optimize multiple structure activity relationships while maintaining drug like properties advances in these areas are creating the foundation of the next paradigm in drug discovery pharmacology
we describe a method for extracting boolean implications if then relationships in very large amounts of gene expression microarray data a meta analysis of data from thousands of microarrays for humans mice and fruit flies finds millions of implication relationships between genes that would be missed by other methods these relationships capture gender differences tissue differences development and differentiation new relationships are discovered that are preserved across all species
human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time consuming we ex plore the use of amazons mechanical turk system a significantly cheaper and faster method for collecting annotations from a broad base of paid non expert contributors over the web we investigate five tasks af fect recognition word similarity recognizing textual entailment event temporal ordering and word sense disambiguation for all five we show high agreement between mechani cal turk non expert annotations and existing gold standard labels provided by expert label ers for the task of affect recognition we also show that using non expert labels for training machine learning algorithms can be as effec tive as using gold standard annotations from experts we propose a technique for bias correction that significantly improves annota tion quality on two tasks we conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the expense
abstract in dalenius articulated a desideratum for statistical databases nothing about an individual should be learnable from the database that cannot be learned without access to the database we give a general impossibility result showing that a formalization of dalenius goal along the lines of semantic security cannot be achieved contrary to intuition a variant of the result threatens the privacy even of someone not in the database this state of affairs suggests a new measure differential privacy which intuitively captures the increased risk to ones privacy incurred by participating in a database the techniques developed in a sequence of papers culminating in those described in can achieve any desired level of privacy under this measure in many cases extremely accurate information about the database can be provided while simultaneously ensuring very high levels privacy
abstract background the challenges of accurate gene prediction and enumeration are further aggravated in large genomes that contain highly repetitive transposable elements tes yet tes play a substantial role in genome evolution and are themselves an important subject of study repeat annotation based on counting occurrences of k mers has been previously used to distinguish tes from low copy genic regions but currently available software solutions are impractical due to high memory requirements or specialization for specific user tasks results here we introduce the tallymer software a flexible and memory efficient collection of programs for k mer counting and indexing of large sequence sets unlike previous methods tallymer is based on enhanced suffix arrays this gives a much larger flexibility concerning the choice of the k mer size tallymer can process large data sizes of several billion bases we used it in a variety of applications to study the genomes of maize and other plant species in particular tallymer was used to index a set of whole genome shotgun sequences from maize total size bp we analyzed k mer frequencies for a wide range of k at this low genome coverage almost equal to highly repetitive mers constituted of the genome but represented only of possible k mers similar low complexity was seen in the repeat fractions of sorghum and rice when applying our method to other maize data sets high derived sequences showed the greatest enrichment for low copy sequences among annotated tes the most highly repetitive were of the gypsy class of retrotransposons followed by the copia class and dna transposons among expressed sequence tags est a notable fraction contained high copy k mers suggesting that transposons are still active in maize retrotransposons in and mcc cultivars were readily detected using the mer frequency index indicating their conservation despite extensive rearrangement across cultivars among one hundred annotated bacterial articial chromosomes bacs k mer frequency could be used to detect transposon encoded genes with sensitivity compared to using alignment based repeat masking while both methods showed specicity conclusion the tallymer software was effective in a variety of applications to aid genome annotation in maize despite limitations imposed by the relatively low coverage of sequence available for more information on the software see http www zbh uni hamburg tallymer
background barley has one of the largest and most complex genomes of all economically important food crops the rise of new short read sequencing technologies such as illumina solexa permits such large genomes to be effectively sampled at relatively low cost based on the corresponding sequence reads a mathematically defined repeat mdr index can be generated to map repetitive regions in genomic sequences results we have generated mbp of illumina solexa sequences from barley total genomic dna representing about of a genome equivalent from these sequences we generated an mdr index which was then used to identify and mark repetitive regions in the barley genome comparison of the mdr plots with expert repeat annotation drawing on the information already available for known repetitive elements revealed a significant correspondence between the two methods mdr based annotation allowed for the identification of dozens of novel repeat sequences though which were not recognised by hand annotation the mdr data was also used to identify gene containing regions by masking of repetitive sequences in eight de novo sequenced bacterial artificial chromosome bac clones for half of the identified candidate gene islands indeed gene sequences could be identified mdr data were only of limited use when mapped on genomic sequences from the closely related species triticum monococcum as only a fraction of the repetitive sequences was recognised conclusion an mdr index for barley which was obtained by whole genome illumina solexa sequencing proved as efficient in repeat identification as manual expert annotation circumventing the labour intensive step of producing a specific repeat library for expert annotation an mdr index provides an elegant and efficient resource for the identification of repetitive and low copy i e potentially gene containing sequences regions in uncharacterised genomic sequences the restriction that a particular mdr index can not be used across species is outweighed by the low costs of illumina solexa sequencing which makes any chosen genome accessible for whole genome sampling
in embryonic stem es cells bivalent chromatin domains with overlapping repressive lysine tri methylation and activating lysine tri methylation histone modifications mark the promoters of more than genes to gain insight into the structure and function of bivalent domains we mapped key histone modifications and subunits of polycomb repressive complexes and and genomewide in human and mouse es cells by chromatin immunoprecipitation followed by ultra high throughput sequencing we find that bivalent domains can be segregated into two classes the first occupied by both and positive and the second specifically bound by only positive bivalent domains appear functionally distinct as they more efficiently retain lysine tri methylation upon differentiation show stringent conservation of chromatin state and associate with an overwhelming number of developmental regulator gene promoters we also used computational genomics to search for sequence determinants of polycomb binding this analysis revealed that the genomewide locations of and can be largely predicted from the locations sizes and underlying motif contents of cpg islands we propose that large cpg islands depleted of activating motifs confer epigenetic memory by recruiting the full repertoire of polycomb complexes in cells
sensitization of the pain transducing ion channel underlies thermal hyperalgesia by proalgesic agents such as nerve growth factor ngf the currently accepted model is that the ngf mediated increase in function during hyperalgesia utilizes activation of phospholipase c plc to cleave proposed to tonically inhibit in this study we tested the plc model and found two lines of evidence that directly challenge its validity polylysine a cationic phosphoinositide sequestering agent inhibited instead of potentiating it and direct application of to inside out excised patches dramatically potentiated furthermore we show four types of experiments indicating that is physically and functionally coupled to the subunit of interacted with the n terminal region of in yeast hybrid experiments coimmunoprecipitated with from both cells and dorsal root ganglia drg neurons interacted with recombinant in vitro and wortmannin a specific inhibitor of completely abolished ngf mediated sensitization in acutely dissociated drg neurons finally simultaneous electrophysiological and total internal reflection fluorescence tirf microscopy recordings demonstrate that ngf increased the number of channels in the plasma membrane we propose a new model for ngf mediated hyperalgesia in which physical coupling of and in a signal transduction complex facilitates trafficking of to the membrane
pnas we study the distributions of citations received by a single publication within several disciplines spanning broad areas of science we show that the probability that an article is cited times has large variations between different disciplines but all distributions are rescaled on a universal curve when the relative indicator is considered where is the average number of citations per article for the discipline in addition we show that the same universal behavior occurs when citation distributions of articles published in the same field but in different years are compared these findings provide a strong validation of as an unbiased indicator for citation performance across disciplines and years based on this indicator we introduce a generalization of the h index suitable for comparing scientists working in fields
through alternative processing of pre messenger rnas individual mammalian genes often produce multiple mrna and protein isoforms that may have related distinct or even opposing functions here we report an in depth analysis of diverse human tissue and cell line transcriptomes on the basis of deep sequencing of complementary dna fragments yielding a digital inventory of gene and mrna isoform expression analyses in which sequence reads are mapped to exonexon junctions indicated that of human genes undergo alternative splicing with a minor isoform frequency of or more differences in isoform specific read densities indicated that most alternative splicing and alternative cleavage and polyadenylation events vary between tissues whereas variation between individuals was approximately twofold to threefold less common extreme or switch like regulation of splicing between tissues was associated with increased sequence conservation in regulatory regions and with generation of full length open reading frames patterns of alternative splicing and alternative cleavage and polyadenylation were strongly correlated across tissues suggesting coordinated regulation of these processes and sequence conservation of a subset of known regulatory motifs in both alternative introns and untranslated regions suggested common involvement of specific factors in tissue level regulation of both splicing polyadenylation
in this look at how concurrency affects practitioners in the real world cantrill and bonwick argue that much of the anxiety over concurrency is unwarranted most developers who build typical mvc systems can leverage parallelism by combining pieces of already concurrent software such as database and operating systems i e concurrency through architecture rather than by writing multithreaded code themselves and for those who actually must deal with threads and locks the authors include a helpful list of best practices to help minimize pain
gr micrornas mirnas are small endogenous rnas that pair to sites in mrnas to direct post transcriptional repression many sites that match the mirna seed nucleotides particularly those in untranslated regions are preferentially conserved here we overhauled our tool for finding preferential conservation of sequence motifs and applied it to the analysis of human increasing by nearly threefold the detected number of preferentially conserved mirna target sites the new tool more efficiently incorporates new genomes and more completely controls for background conservation by accounting for mutational biases dinucleotide conservation rates and the conservation rates of individual utrs the improved background model enabled preferential conservation of a new site type the offset to be detected in total mirna target sites within human are conserved above background levels and of human protein coding genes have been under selective pressure to maintain pairing to mirnas mammalian specific mirnas have far fewer conserved targets than do the more broadly conserved mirnas even when considering only more recently emerged targets although pairing to the end of mirnas can compensate for seed mismatches this class of sites constitutes less than of all preferentially conserved sites detected the new tool enables statistically powerful analysis of individual mirna target sites with the probability of preferentially conserved targeting correlating with experimental measurements of repression our expanded set of target predictions including conserved compensatory sites are available at the targetscan website which displays the for each site and each target
background probability based statistical learning methods such as mutual information and bayesian networks have emerged as a major category of tools for reverse engineering mechanistic relationships from quantitative biological data in this work we introduce a new statistical learning strategy that addresses three common issues in previous methods simultaneously handling of continuous variables detection of more complex three way relationships and better differentiation of causal versus confounding relationships with these improvements we provide a more realistic representation of the underlying biological system results we test the algorithm using both synthetic and experimental data in the synthetic data experiment achieved an absolute sensitivity precision of and a relative sensitivity precision both of in addition significantly outperformed the control methods including bayesian networks classical two way mutual information and a discrete version of we then used and control methods to infer a regulatory network centered at the myc transcription factor from a published microarray dataset models selected by were numerically and biologically distinct from those selected by control methods unlike control methods effectively differentiated true causal models from confounding models recovered major myc cofactors and revealed major mechanisms involved in myc dependent transcriptional regulation which are strongly supported by literature the network showed that limited sets of regulatory mechanisms are employed repeatedly to control the expression of large number of genes conclusion overall our work demonstrates that outperforms the frequently used control methods and provides a powerful method for inferring mechanistic relationships underlying biological and other complex systems the method is implemented in r in the package available under the gnu gpl from http sysbio engin umich edu luow downloads php and from the r package cran
species extinction is both a key process throughout the history of life and a pressing concern in the conservation of present day biodiversity these two facets have largely been studied by separate communities using different approaches this article illustrates with examples some of the ways that considering the evolutionary relationships among species phylogenies has helped the study of both past and present species extinction the focus is on three topics extinction rates and severities phylogenetic nonrandomness of extinction and the testing of hypotheses relating extinction proneness to attributes of organisms or species phylogenetic and taxic approaches to extinction have not fully fused largely because of the difficulties of relating discrete taxa to the underlying continuity of phylogeny phylogeny must be considered in comparative tests of hypotheses about extinction but care must be taken to avoid overcorrecting for phylogenetic nonindependence taxa
this article is concerned with statistical modeling of shotgun resequencing data and the use of such data for population genetic inference we model data produced by sequencing by synthesis technologies such as the solexa and polymerase colony polony systems whose use is becoming increasingly widespread we show how such data can be used to estimate evolutionary parameters mutation and recombination rates despite the fact that the data do not necessarily provide complete or aligned sequence information we also present two refinements of our methods one that is more robust to sequencing errors and another that can be used when no reference genome is genetics
in the years since the publication of the first edition of bodyspace the knowledge base upon which ergonomics rests has increased significantly the need for an authoritative contemporary and above all usable reference is therefore great this third edition maintains the same content and structure as previous editions but updates the material and references to reflect recent developments in the field the book has been substantially revised to include new research and anthropometric surveys the latest techniques and changes in legislation that have taken place in recent years new coverage in the third edition guidance on design strategies and practical advice on conducting trials overview of recent advances in simulation and digital human modes dynamic seating recent work on hand handle interface computer input devices laptop computer use and childrens use of computers design for an aging population and accessibility for people with disabilities new approaches to risk management and new assessment tools legislation and standards as the previous two editions have shown bodyspace is an example of the unusual a text that is a favorite among academics and practitioners losing none of the features that made previous editions so popular the author skillfully integrates new knowledge into the existing text without sacrificing the easily accessible style that makes this book unique more than just a reference text this authoritative book clearly delineates the field ergonomics
sylamer is a method for detecting microrna target and small interfering rna off target signals in untranslated regions from a ranked gene list sorted from upregulated to downregulated after a microrna perturbation or rna interference experiment the output is a landscape plot that tracks occurrence biases using hypergeometric p values for all words across the gene ranking we demonstrated the utility speed and accuracy of this approach on datasets
neutralism and selectionism are extremes of an explanatory spectrum for understanding patterns of molecular evolution and the emergence of evolutionary innovation although recent genome scale data from protein coding genes argue against neutralism molecular engineering and protein evolution data argue that neutral mutations and mutational robustness are important for evolutionary innovation here i propose a reconciliation in which neutral mutations prepare the ground for later evolutionary adaptation key to this perspective is an explicit understanding of molecular phenotypes that has only become accessible in years
regulatory and developmental systems produce phenotypes that are robust to environmental and genetic variation a gene product that normally contributes to this robustness is termed a phenotypic capacitor when a phenotypic capacitor fails for example when challenged by a harsh environment or mutation the system becomes less robust and thus produces greater phenotypic variation a functional phenotypic capacitor provides a mechanism by which hidden polymorphism can accumulate whereas its failure provides a mechanism by which evolutionary change might be promoted the primary example to date of a phenotypic capacitor is a molecular chaperone that targets a large set of signal transduction proteins in both drosophila and arabidopsis compromised function results in pleiotropic phenotypic effects dependent on the underlying genotype for some traits also appears to buffer stochastic variation yet the relationship between environmental and genetic buffering remains an important unresolved question we previously used simulations of knockout mutations in transcriptional networks to predict that many gene products would act as phenotypic capacitors to test this prediction we use high throughput morphological phenotyping of individual yeast cells from single gene deletion strains to identify gene products that buffer environmental variation in saccharomyces cerevisiae we find more than gene products that when absent increase morphological variation overrepresented among these capacitors are gene products that control chromosome organization and dna integrity rna elongation protein modification cell cycle and response to stimuli such as stress capacitors have a high number of synthetic lethal interactions but knockouts of these genes do not tend to cause severe decreases in growth rate each capacitor can be classified based on whether or not it is encoded by a gene with a paralog in the genome capacitors with a duplicate are highly connected in the proteinprotein interaction network and show considerable divergence in expression from their paralogs in contrast capacitors encoded by singleton genes are part of highly interconnected protein clusters whose other members also tend to affect phenotypic variability or fitness these results suggest that buffering and release of variation is a widespread phenomenon that is caused by incomplete functional redundancy at multiple levels in the architecture
within the past few years studies on microrna mirna and cancer have burst onto the scene profiling of the mirnome global mirna expression levels has become prevalent and abundant mirnome data are currently available for various cancers the pattern of mirna expression can be correlated with cancer type stage and other clinical variables so mirna profiling can be used as a tool for cancer diagnosis and prognosis mirna expression analyses also suggest oncogenic or tumor suppressive roles of mirnas mirnas play roles in almost all aspects of cancer biology such as proliferation apoptosis invasion metastasis and angiogenesis given that many mirnas are deregulated in cancers but have not yet been further studied it is expected that more mirnas will emerge as players in the etiology and progression of cancer here we also discuss mirnas as a tool for cancer therapy expected final online publication date for the annual review of pathology mechanisms of disease volume is january please see http www annualreviews org catalog pubdates aspx for estimates
we present a method for accurately predicting the long time popularity of online content from early measurements of user s access using two content sharing portals youtube and digg we show that by modeling the accrual of views and votes on content offered by these services we can predict the long term dynamics of individual submissions from initial data in the case of digg measuring access to given stories during the first two hours allows us to forecast their popularity days ahead with remarkable accuracy while downloads of youtube videos need to be followed for days to attain the same performance the differing time scales of the predictions are shown to be due to differences in how content is consumed on the two portals digg stories quickly become outdated while youtube videos are still found long after they are initially submitted to the portal we show that predictions are more accurate for submissions for which attention decays quickly whereas predictions for evergreen content will be prone to errors
abstract background various measures of semantic similarity of terms in bio ontologies such as the gene ontology go have been used to compare gene products such measures of similarity have been used to annotate uncharacterized gene products and group gene products into functional groups there are various ways to measure semantic similarity either using the topological structure of the ontology the instances gene products associated with terms or a mixture of both we focus on an instance level definition of semantic similarity while using the information contained in the ontology both in the graphical structure of the ontology and the semantics of relations between terms to provide constraints on our instance level description semantic similarity of terms is extended to annotations by various approaches either though aggregation operations such as min max and average or through an extrapolative method these approaches introduce assumptions about how semantic similarity of terms relates to the semantic similarity of annotations that do not necessarily reflect how terms relate to each other results we exploit the semantics of relations in the go to construct an algorithm called ssa that provides the basis of a framework that naturally extends instance based methods of semantic similarity of terms such as resnik s measure to describing annotations and not just terms our measure attempts to correctly interpret how terms combine via their relationships in the ontological hierarchy ssa uses these relationships to identify the most specific common ancestors between terms we outline the set of cases in which terms can combine and associate partial order constraints with each case that order the specificity of terms these cases form the basis for the ssa algorithm the set of associated constraints also provide a set of principles that any improvement on our method should seek to satisfy conclusions we derive a measure of semantic similarity between annotations that exploits all available information without introducing assumptions about the nature of the ontology or data we preserve the principles underlying instance based methods of semantic similarity of terms at the annotation level as a result our measure better describes the information contained in annotations associated with gene products and as a result is better suited to characterizing and classifying gene products through annotations
this lecture treats some enduring misconceptions about modeling one of these is that the goal is always prediction the lecture distinguishes between explanation and prediction as modeling goals and offers sixteen reasons other than prediction to build a model it also challenges the common assumption that scientific theories arise from and summarize data when often theories precede and guide data collection without theory in other words it is not clear what data to collect among other things it also argues that the modeling enterprise enforces habits of mind essential to freedom it is based on the author s bastille day keynote address to the second world congress on social simulation george mason university and earlier addresses at the institute of medicine the university of michigan and the santa institute
intro i tag permettono una navigazione non legata ad una gerarchia concettuale sono intuitivi per l utente danno senso di comunit permettono di connettere persone in base agli interessi assorbono velocemente i trend e flessibili ai cambiamenti di vocabolario il sistema quindi pu monitorare gli interessi degli utenti nelle risorse e il vocabolario che usano per il tagging di queste risorse la libert di vocabolario porta a ambiguit singolo tag ha diversi significati ridondanza diversi tag hanno lo stesso significato quindi recommendation personalizzata deve aiutare l utente a interagire con il sistema abbiamo dimensioni da gestire con il recommender system utenti items risorse gruppi film bookmarks etc tags navigazione nel sistema attraversando tag utente risorsa scelgo un tag vedo recommendations e utenti correlati al tag vedo un utente e scelgo di vedere il suo profilo nel profilo vedo delle risorse dei chicago bulls e le vado a vedere devo essere libero di navigare attraverso queste tre dimensioni della folksonomia devo presentare all utente una serie di avenues strade correlate ai suoi interessi che possono essere viste come una serie di recommendations con il clustering si pu superare l ambiguit dei tags l algoritmo di personalizzazione clustering based funziona cos i input profilo utente insieme di clusters tag selezionato output risorse suggerite l utente seleziona un tag e vuole un elenco di recommendations le folksonomies sono un aiuto per capire i bisogni informativi degli utenti l importanza delle risorse deriva dagli utenti o viceversa l importanza degli utenti deriva dalle risorse dataset obiettivi tecniche algoritmo di personalizzazione per recommendation in folksonomies che si basa su cluster di tags gerarchici folksonomy u utenti r risorse t tags a annotazioni d u r t a le annotazioni sono triple con user tag resource a u r t una folksonomy pu essere vista come ipegrafo tripartito nodi users tags e resources iperarchi le annotazioni che collegano un utente con un tag con una risorsa possibili metodi per recommendation recency authority linkage popularity vector space models ogni utente un vettore sull insieme di tutti i tag dove ogni peso rappresenta l importanza del tag u w w w t anche le risorse possono essere modellate come vettore sull insieme di tags come si calcolano i pesi dei tags nei vettori tag frequency numero di volte che una risorsa stata annotata con quel tag tf t r cardinalit delle annotazioni che hanno u con t e r poi possiamo modificare il tf idf per le folksonomies tf idf t r tf t r log n nt n numero totale di risorse nt numero di tutte le risorse che hanno il tag t le query e le risorse possono essere rappresentate come dei vettori sui tags ipotizziamo che un utente cominci la navigazione con una query che seleziona un solo tag l utente seleziona un tag e vuole un elenco di recommendations somiglianza tra vettori coseno somiglianza jaccard similarity coefficient siccome la query solo un tag si semplifica l equazione misuriamo la somiglianza tra tutte le risorse e la query del tag e selezioniamo le top n c bisogno critico di personalizzazione altrimenti avremmo lo stesso elenco di recommendations per tutti gli utenti tag ambiguity e redundancy due utenti uno dei white sox uno dei red sox dietro il tag sox si otterrebbe la stessa lista di risultati due fasi dell algoritmo di personalizzazione si ottiene la lista di risorse raccomandate si personalizzano tenedo conto del profilo utente e dei cluster di tags il clustering si effettua prima in modalit offline processo di recommendation in dettaglio coseno similarit calcolato tra la query e tutte le risorse r si ottiene un sottoinsieme di risorse r che hanno una certa somiglianza con la query calcolare rilevanza delle risorse r di r per l utente u i cluster sono necessari in quanto punto di collegamento che mette in relazione gli utenti con le risorse permettendo di mostrare risorse che rispecchino gli interessi dell utente input profilo utente risorse in r output rilevanza di ogni risorsa di r per l utente u calcola l interesse dell utente in ogni cluster il rapporto tra numero di risorse che ha annotato con tags di quel cluster e numero totale di annotazioni dell utente calcola i cluster pi vicini ad ogni risorsa la relazione tra risorsa e un cluster si calcola rapporto tra volte che la risorsa stata annotata con un tag del cluster e totale numero di volte che la risorsa stata annotata inferisci gli interessi dell utente nei confronti di ogni risorsa misura di rilevanza utente risorsa si fa sa somma dei prodotti dell interesse utente cluster con relazione risorsa cluster ogni cluster topic calcolare il punteggio di rank personalizzato somiglianza personalizzata utente query risorsa coseno similarit misura di rilevanza abbiamo questo valore per tutte le risorse ed erano state ordinate e sono restituite le prime n risorse nb i pesi cluster risorsa saranno indipendenti dagli utenti mentre i pesi che connettono gli utenti ai cluster dipendono dal profilo utente modellazione utenti e risorse vettori su un insieme di tags gli interessi di un utente si capiscono misurando la rilevanza di un cluster per un utente associo anche le risorse con i cluster di tag per vedere la rilevanza di una risorsa per un topic descritto da un cluster risultato utente tag cluster risorse risultati le folksonomies con un solo argomento sono obiettivi pi facili per la recommendation rispetto a folksonomies con diversi argomenti scorrelati se usiamo una strategia di recommendation progettata per ridurre lo spazio del topic riusciamo a trattare abbastanza bene sia i casi single topic che multi topic ma meglio nel multi topic la chiave ridurre lo spazio dell informazione per focalizzarci sugli interessi dell utente usando il context dependent hierarchical clustering approach filtro i tag ambigui e ridondanti discussione la selezione del topic una strategia importante per la recommendation nelle folksonomies multi topic futuro idee context dependent cluster selection riduco lo spazio dei topic e uso la recommendation strategy basandomi sul profilo utente che rappresenta la memoria a breve termine i pesi cluster risorsa saranno indipendenti dagli utenti mentre i pesi che connettono gli utenti ai cluster dipendono dal profilo utente ergo per il mio scopo dovrei rendere i cluster dipendenti dal profilo utente ed avere cos i pesi risorsa cluster diversi per ogni profilo altrimenti come se usassi tutti gli utenti per formare una folksonomia da scomporre in cluster invece voglio che ogni utente abbia la sua folksonomia di riferimento e suddividere quella cluster
we characterized the relationship of and at distal and proximal regulatory elements by comparing chip seq profiles for these histone modifications and for two functionally different transcription factors in the immortalized hela cell line with and without interferon gamma ifng stimulation and in mouse adult liver tissue in unstimulated and stimulated hela cells respectively we determined approximately and approximately enriched regions and approximately and approximately enriched regions in mouse adult liver we determined approximately and approximately and regions seventy five percent of the approximately binding sites in stimulated hela cells and of the approximately sites in mouse liver were distal to known gene tss in both cell types approximately of these distal sites were associated with at least one of the two histone modifications and was associated with over of marked distal sites after filtering against predicted transcription start sites of approximately marked distal ifng stimulated binding sites but of approximately marked distal sites were associated with only results for hela cells generated additional insights into transcriptional regulation involving binding was associated with of all regions in stimulated hela cells suggesting that a single transcription factor can interact with an unexpectedly large fraction of regulatory regions strikingly for a large majority of the locations of stimulated binding the dominant combinations were established before activation suggesting mechanisms independent of ifng stimulation and high binding
this easy to use fast moving tutorial introduces you to functional programming with haskell you ll learn how to use haskell in a variety of practical ways from short scripts to large and demanding applications world takes you through the basics of functional programming at a brisk pace and then helps you increase your understanding of haskell in real world issues like i o performance dealing with data concurrency and more as you move through each chapter with this book you will understand the differences between procedural and functional programming learn the features of haskell and how to use it to develop useful programs interact with filesystems databases and network services write solid code with automated tests code coverage and error handling harness the power of multicore systems via concurrent and parallel programming you ll find plenty of hands on exercises along with examples of real haskell programs that you can modify compile and run whether or not you ve used a functional language before if you want to understand why haskell is coming into its own as a practical language in so many major organizations world is the best place start
dna sequence information underpins genetic research enabling discoveries of important biological or medical benefit sequencing projects have traditionally used long base pair reads but the existence of reference sequences for the human and many other genomes makes it possible to develop new fast approaches to re sequencing whereby shorter reads are compared to a reference to identify intraspecies genetic variation here we report an approach that generates several billion bases of accurate nucleotide sequence per experiment at low cost single molecules of dna are attached to a flat surface amplified in situ and used as templates for synthetic sequencing with fluorescent reversible terminator deoxyribonucleotides images of the surface are analysed to generate high quality sequence we demonstrate application of this approach to human genome sequencing on flow sorted x chromosomes and then scale the approach to determine the genome sequence of a male yoruba from ibadan nigeria we build an accurate consensus sequence from average depth of paired base reads we characterize four million single nucleotide polymorphisms and four hundred thousand structural variants many of which were previously unknown our approach is effective for accurate rapid and economical whole genome re sequencing and many other applications
here we present the first diploid genome sequence of an asian individual the genome was sequenced to fold average coverage using massively parallel sequencing technology we aligned the short reads onto the ncbi human reference genome to coverage and guided by the reference genome we used uniquely mapped reads to assemble a high quality consensus sequence for of the asian individual s genome we identified approximately million single nucleotide polymorphisms snps inside this region of which were not in the dbsnp database genotyping analysis showed that snp identification had high accuracy and consistency indicating the high sequence quality of this assembly we also carried out heterozygote phasing and haplotype prediction against hapmap chb and jpt haplotypes chinese and japanese respectively sequence comparison with the two available individual genomes j d watson and j c venter and structural variation identification these variations were considered for their potential biological impact our sequence data and analyses demonstrate the potential usefulness of next generation sequencing technologies for genomics
acute myeloid leukaemia is a highly malignant haematopoietic tumour that affects about adults in the united states each year the treatment of this disease has changed little in the past two decades because most of the genetic events that initiate the disease remain undiscovered whole genome sequencing is now possible at a reasonable cost and timeframe to use this approach for the unbiased discovery of tumour specific somatic mutations that alter the protein coding genes here we present the results obtained from sequencing a typical acute myeloid leukaemia genome and its matched normal counterpart obtained from the same patient s skin we discovered ten genes with acquired mutations two were previously described mutations that are thought to contribute to tumour progression and eight were new mutations present in virtually all tumour cells at presentation and relapse the function of which is not yet known our study establishes whole genome sequencing as an unbiased method for discovering cancer initiating mutations in previously unidentified genes that may respond to therapies
the brenda braunschweig enzyme database http www brenda enzymes org represents the largest freely available information system containing a huge amount of biochemical and molecular information on all classified enzymes as well as software tools for querying the database and calculating molecular properties the database covers information on classification and nomenclature reaction and specificity functional parameters occurrence enzyme structure and stability mutants and enzyme engineering preparation and isolation the application of enzymes and ligand related data the data in brenda are manually curated from more than primary literature references each entry is clearly linked to a literature reference the origin organism and where available to the protein sequence of the enzyme protein a new search option provides the access to protein specific data frenda full reference enzyme data and amenda automatic mining of enzyme data are additional databases created by continuously improved text mining procedures these databases ought to provide a complete survey on enzyme data of the literature collection of pubmed the web service via a soap simple object access protocol interface for access to the brenda data has been further nar
many bacterial cellular processes interact intimately with the chromosome such interplay is the major driving force of genome structure or organization interactions take place at different scaleslocal for gene expression global for replicationand lead to the differentiation of the chromosome into organizational units such as operons replichores or macrodomains these processes are intermingled in the cell and create complex higher level organizational features that are adaptive because they favor the interplay between the processes the surprising result of selection for genome organization is that gene repertoires change much more quickly than chromosomal structure comparative genomics and experimental genomic manipulations are untangling the different cellular and evolutionary mechanisms causing such resilience to change since organization results from cellular processes a better understanding of chromosome organization will help unravel the underlying cellular processes and diversity
to appreciate the functional diversity of communities of soil eukaryotic micro organisms we evaluated an experimental approach based on the construction and screening of a cdna library using polyadenylated mrna extracted from a forest soil such a library contains genes that are expressed by each of the different organisms forming the community and represents its metatranscriptome the diversity of the organisms that contributed to this library was evaluated by sequencing a portion of the rdna gene amplified from either soil dna or reverse transcribed rna more than of the sequences were from fungi and unicellular eukaryotes protists while the other most represented group was the metazoa calculation of richness estimators suggested that more than species could be present in the soil samples studied sequencing of cdna identified genes with no homologues in databases and genes coding proteins involved in different biochemical and cellular processes surprisingly the taxonomic distribution of the cdna and of the rdna genes did not coincide with a marked under representation of the protists among the cdna specific genes from such an environmental cdna library could be isolated by expression in a heterologous microbial host saccharomyces cerevisiae this is illustrated by the functional complementation of a histidine auxotrophic yeast mutant by two cdna originating possibly from an ascomycete and a basidiomycete fungal species study of the metatranscriptome has the potential to uncover adaptations of whole microbial communities to local environmental conditions it also gives access to an abundant source of genes of interest
background researchers interested in analysing the expression patterns of functionally related genes usually hope to improve the accuracy of their results beyond the boundaries of currently available experimental data gene ontology go data provides a novel way to measure the functional relationship between gene products many approaches have been reported for calculating the similarities between two go terms known as semantic similarities however biologists are more interested in the relationship between gene products than in the scores linking the go terms to highlight the relationships among genes recent studies have focused on functional similarities results in this study we evaluated five functional similarity methods using both protein protein interaction ppi and expression data of s cerevisiae the receiver operating characteristics roc and correlation coefficient analysis of these methods showed that the maximum method outperformed the other methods statistical comparison of multiple and single term annotated proteins in biological process ontology indicated that genes with multiple go terms may be more reliable for separating true positives from noise conclusion this study demonstrated the reliability of current approaches that elevate the similarity of go terms to the similarity of proteins suggestions for further improvements in functional similarity analysis are provided
excerpt translating genomic research into health care improvements will require linking genotypes with medical information that has long been considered private fortuitously as genomics has progressed so too have electronic medical records including personal health records that are now an important part of the electronic medical information accompanying these developments however is an argument advocated in the us congress and elsewhere that biomedical ethics requires subjecting any uses of electronic medical records to patient consent although well intentioned such arguments spell trouble linked data are crucial for research and improving health care quality people might fear that information will be revealed or misused but the impulse to block all access in the absence of consent mistaken
particulate pollutants cause adverse health effects through the generation of oxidative stress a key question is whether these effects are mediated by the particles or their chemical compounds in this article we show that aliphatic aromatic and polar organic compounds fractionated from diesel exhaust particles deps exert differential toxic effects in raw cells cellular analyses showed that the quinone enriched polar fraction was more potent than the polycyclic aromatic hydrocarbon pah enriched aromatic fraction in generation decrease of membrane potential delta psi m loss of mitochondrial membrane mass and induction of apoptosis a major effect of the polar fraction was to promote cyclosporin a csa sensitive permeability transition pore ptp opening in isolated liver mitochondria this opening effect is dependent on a direct effect on the ptp at low doses as well as on an effect on delta psi m at high doses in calcium loaded mitochondria the direct ptp effect was mimicked by redox cycling dep quinones although the aliphatic fraction failed to perturb mitochondrial function the aromatic fraction increased the retention capacity at low doses and induced mitochondrial swelling and a decrease in delta psi m at high doses this swelling effect was mostly csa insensitive and could be reproduced by a mixture of pahs present in deps these chemical effects on isolated mitochondria could be reproduced by intact deps as well as ambient ultrafine particles ufps in contrast commercial polystyrene nanoparticles failed to exert mitochondrial effects these results suggest that dep and ufp effects on the ptp and delta psi m are mediated by adsorbed chemicals rather than the themselves
pnas cell membranes contain a large number of different lipid species such a multicomponent mixture exhibits a complex phase behavior with regions of structural and compositional heterogeneity especially domains formed in ternary mixtures composed of saturated and unsaturated lipids together with cholesterol have received a lot of attention as they may resemble raft formation in real cells here we apply a simulation model to assess the molecular nature of these domains at the nanoscale information that has thus far eluded experimental determination we are able to show the spontaneous separation of a saturated phosphatidylcholine pc unsaturated pc cholesterol mixture into a liquid ordered and a liquid disordered phase with structural and dynamic properties closely matching experimental data the near atomic resolution of the simulations reveals remarkable features of both domains and the boundary domain interface furthermore we predict the existence of a small surface tension between the monolayer leaflets that drives registration of the domains at the level of molecular detail raft like lipid mixtures show a surprising face with possible implications for many cell processes
learning opencv puts you right in the middle of the rapidly expanding field of computer vision written by the creators of opencv the widely used free open source library this book introduces you to computer vision and demonstrates how you can quickly build applications that enable computers to see and make decisions based on the data computer vision is everywhere in security systems manufacturing inspection systems medical image analysis unmanned aerial vehicles and more it helps robot cars drive by themselves stitches google maps and google earth together checks the pixels on your laptop s lcd screen and makes sure the stitches in your shirt are ok opencv provides an easy to use computer vision infrastructure along with a comprehensive library containing more than functions that can run vision code in real time with learning opencv any developer or hobbyist can get up and running with the framework quickly whether it s to build simple or sophisticated vision applications the book includes a thorough introduction to opencv getting input from cameras transforming images shape matching pattern recognition including face detection segmenting images tracking and motion in and dimensions machine learning algorithms hands on exercises at the end of each chapter help you absorb the concepts and an appendix explains how to set up an opencv project in visual studio opencv is written in performance optimized c c code runs on windows linux and mac os x and is free for commercial and research use under a bsd license getting machines to see is a challenging but entertaining goal if you re intrigued by the possibilities learning opencv gets you started onbuilding computer vision applications of own
we carried out the first analysis of alternative splicing complexity in human tissues using mrna seq data new splice junctions were detected in approximately of multiexon genes many of which are tissue specific by combining mrna seq and est cdna sequence data we estimate that transcripts from approximately of multiexon genes undergo alternative splicing and that there are approximately intermediate to high abundance alternative splicing events in major human tissues from a comparison with quantitative alternative splicing microarray profiling data we also show that mrna seq data provide reliable measurements for exon levels
motivation a plethora of alignment tools have been created that are designed to best fit different types of alignment conditions while some of these are made for aligning illumina sequence analyzer reads none of these are fully utilizing its probability prb output in this article we will introduce a new alignment approach slider that reduces the alignment problem space by utilizing each read base s probabilities given in the prb files results compared with other aligners slider has higher alignment accuracy and efficiency in addition given that slider matches bases with probabilities other than the most probable it significantly reduces the percentage of base mismatches the result is that its snp predictions are more accurate than other snp prediction approaches used today that start from the most probable sequence including those using base quality contact nmalhis bcgsc ca supplementary information and availability http www bcgsc ca platform bioinfo software bioinformatics
in silico prediction of transcription factor binding sites tfbss is central to the task of gene regulatory network elucidation genomic dna sequence information provides a basis for these predictions due to the sequence specificity of tf binding events however dna sequence alone is an impoverished source of information for the task of tfbs prediction in eukaryotes as additional factors such as chromatin structure regulate binding events we show that incorporating high throughput chromatin modification estimates can greatly improve the accuracy of in silico prediction of in vivo binding for a wide range of tfs in human and mouse this improvement is superior to the improvement gained by equivalent use of either transcription start site proximity or phylogenetic conservation information importantly predictions made with the use of chromatin structure information are tissue specific this result supports the biological hypothesis that chromatin modulates tf binding to produce tissue specific binding profiles in higher eukaryotes and suggests that the use of chromatin modification information can lead to accurate tissue specific transcriptional regulatory network nar
genetic mapping provides a powerful approach to identify genes and biological processes underlying any trait influenced by inheritance including human diseases we discuss the intellectual foundations of genetic mapping of mendelian and complex traits in humans examine lessons emerging from linkage analysis of mendelian diseases and genome wide association studies of common diseases and discuss questions and challenges that ahead
learning the medial prefrontal cortex mpfc gradually comes to modulate the expression of memories that initially depended on the hippocampus we show that during this consolidation period neural firing in the mpfc becomes selective for the acquired memories after acquisition of memory associations neuron populations in the mpfc of rats developed sustained activity during the interval between two paired stimuli but reduced activity during the corresponding interval between two unpaired stimuli these new patterns developed over a period of several weeks after learning with and without continued conditioning trials thus in agreement with a central tenet of consolidation theory acquired associations initiate subsequent gradual processes that result in lasting changes of the mpfc s code without training
since achieving recommendation status in the web ontology language owl has been successfully applied to many problems in computer science practical experience with owl has been quite positive in general however it has also revealed room for improvement in several areas we systematically analyze the identified shortcomings of owl such as expressivity issues problems with its syntaxes and deficiencies in the definition of owl species furthermore we present an overview of owl extension to and revision of owl that is currently being developed within the owl working group many aspects of owl have been thoroughly reengineered in owl thus producing a robust platform for future development of language
motivation gene expression class comparison studies may identify hundreds or thousands of genes as differentially expressed de between sample groups gaining biological insight from the result of such experiments can be approached for instance by identifying the signaling pathways impacted by the observed changes most of the existing pathway analysis methods focus on either the number of de genes observed in a given pathway enrichment analysis methods or on the correlation between the pathway genes and the class of the samples functional class scoring methods both approaches treat the pathways as simple sets of genes disregarding the complex gene interactions that these pathways are built to describe results we describe a novel signaling pathway impact analysis spia that combines the evidence obtained from the classical enrichment analysis with a novel type of evidence which measures the actual perturbation on a given pathway under a given condition a bootstrap procedure is used to assess the significance of the observed total pathway perturbation using simulations we show that the evidence derived from perturbations is independent of the pathway enrichment evidence this allows us to calculate a global pathway significance p value which combines the enrichment and perturbation p values we illustrate the capabilities of the novel method on four real datasets the results obtained on these data show that spia has better specificity and more sensitivity than several widely used pathway analysis methods availability spia was implemented as an r package available at http vortex cs wayne edu ontoexpress contact sorin wayne edu supplementary information supplementary data are available at online
olfactory receptors ors which are involved in odorant recognition form the largest mammalian protein superfamily the genomic content of or genes is considerably reduced in humans as reflected by the relatively small repertoire size and the high fraction approximately of human pseudogenes since several recent low resolution surveys suggested that or genomic loci are frequently affected by copy number variants cnvs we hypothesized that cnvs may play an important role in the evolution of the human olfactory repertoire we used high resolution oligonucleotide tiling microarrays to detect cnvs across or gene and pseudogene loci examining genomic dna from individuals with ancestry from three populations we identified or gene loci and pseudogene loci affected by cnvs generating a mosaic of or dosages across persons our data suggest that approximately of the cnvs involve more than one or with the largest cnv spanning loci in contrast to earlier reports we observe that cnvs are more frequent among or pseudogenes than among intact genes presumably due to both selective constraints and cnv formation biases furthermore our results show an enrichment of cnvs among ors with a close human paralog or lacking a one to one ortholog in chimpanzee interestingly among the latter we observed an enrichment in cnv losses over gains a finding potentially related to the known diminution of the human or repertoire quantitative pcr experiments performed for sampled ors agreed well with the microarray results and uncovered additional cnvs importantly these experiments allowed us to uncover nine common deletion alleles that affect or genes and five pseudogenes comparison to the chimpanzee reference genome revealed that all of the deletion alleles are human derived therefore indicating a profound effect of human specific deletions on the individual or gene content furthermore these deletion alleles may be used in future genetic association studies of olfactory inter differences
the detailed positions of nucleosomes profoundly impact gene regulation and are partly encoded by the genomic dna sequence however less is known about the functional consequences of this encoding here we address this question using a genome wide map of yeast nucleosomes that we sequenced in their entirety utilizing the high resolution of our map we refine our understanding of how nucleosome organizations are encoded by the dna sequence and demonstrate that the genomic sequence is highly predictive of the in vivo nucleosome organization even across new nucleosome bound sequences that we isolated from fly and human we find that poly da dt tracts are an important component of these nucleosome positioning signals and that their nucleosome disfavoring action results in large nucleosome depletion over them and over their flanking regions and enhances the accessibility of transcription factors to their cognate sites our results suggest that the yeast genome may utilize these nucleosome positioning signals to regulate gene expression with different transcriptional noise and activation kinetics and dna replication with different origin efficiency these distinct functions may be achieved by encoding both relatively closed nucleosome covered chromatin organizations over some factor binding sites where factors must compete with nucleosomes for dna access and relatively open nucleosome depleted organizations over other factor sites where factors bind competition
one of the striking features of evolution is the appearance of novel structures in organisms recently kirschner and gerhart have integrated discoveries in evolution genetics and developmental biology to form a theory of facilitated variation fv the key observation is that organisms are designed such that random genetic changes are channeled in phenotypic directions that are potentially useful an open question is how fv spontaneously emerges during evolution here we address this by means of computer simulations of two well studied model systems logic circuits and rna secondary structure we find that evolution of fv is enhanced in environments that change from time to time in a systematic way the varying environments are made of the same set of subgoals but in different combinations we find that organisms that evolve under such varying goals not only remember their history but also generalize to future environments exhibiting high adaptability to novel goals rapid adaptation is seen to goals composed of the same subgoals in novel combinations and to goals where one of the subgoals was never seen in the history of the organism the mechanisms for such enhanced generation of novelty generalization are analyzed as is the way that organisms store information in their genomes about their past environments elements of facilitated variation theory such as weak regulatory linkage modularity and reduced pleiotropy of mutations evolve spontaneously under these conditions thus environments that change in a systematic modular fashion seem to promote facilitated variation and allow evolution to generalize to conditions
the advent of microarray technology has made it possible to classify disease states based on gene expression profiles of patients typically marker genes are selected by measuring the power of their expression profiles to discriminate among patients of different disease states however expression based classification can be challenging in complex diseases due to factors such as cellular heterogeneity within a tissue sample and genetic heterogeneity across patients a promising technique for coping with these challenges is to incorporate pathway information into the disease classification procedure in order to classify disease based on the activity of entire signaling pathways or protein complexes rather than on the expression levels of individual genes or proteins we propose a new classification method based on pathway activities inferred for each patient for each pathway an activity level is summarized from the gene expression levels of its condition responsive genes corgs defined as the subset of genes in the pathway whose combined expression delivers optimal discriminative power for the disease phenotype we show that classifiers using pathway activity achieve better performance than classifiers based on individual gene expression for both simple and complex case control studies including differentiation of perturbed from non perturbed cells and subtyping of several different kinds of cancer moreover the new method outperforms several previous approaches that use a static i e non conditional definition of pathways within a pathway the identified corgs may facilitate the development of better diagnostic markers and the discovery of core alterations in disease
we present cisgenome a software system for analyzing genome wide chromatin immunoprecipitation chip data cisgenome is designed to meet all basic needs of chip data analyses including visualization data normalization peak detection false discovery rate computation gene peak association and sequence and motif analysis in addition to implementing previously published chipmicroarray chip chip analysis methods the software contains statistical methods designed specifically for chlp sequencing chip seq data obtained by coupling chip with massively parallel sequencing the modular design of cisgenome enables it to support interactive analyses through a graphic user interface as well as customized batch mode computation for advanced data mining a built in browser allows visualization of array images signals gene structure conservation and dna sequence and motif information we demonstrate the use of these tools by a comparative analysis of chip chip and chip seq data for the transcription factor nrsf rest a study of chip seq analysis with or without a negative control sample and an analysis of a new motif in nanog and regions
significant insight about biological networks arises from the study of network motifsoverly abundant network such wiring patterns do not specify when and how potential routes within a cellular network are used to address this limitation we introduce activity motifs which capture patterns in the dynamic use of a network using this framework to analyze transcription in saccharomyces cerevisiae metabolism we find that cells use different timing activity motifs to optimize transcription timing in response to changing conditions forward activation to produce metabolic compounds efficiently backward shutoff to rapidly stop production of a detrimental product and synchronized activation for co production of metabolites required for the same reaction measuring protein abundance over a time course reveals that mrna timing motifs also occur at the protein level timing motifs significantly overlap with binding activity motifs where genes in a linear chain have ordered binding affinity to a transcription factor suggesting a mechanism for ordered transcription finely timed transcriptional regulation is therefore abundant in yeast metabolism optimizing the organism s adaptation to new conditions
learning regulatory networks from genomics data is an important problem with applications spanning all of biology and biomedicine functional genomics projects offer a cost effective means of greatly expanding the completeness of our regulatory models and for some prokaryotic organisms they offer a means of learning accurate models that incorporate the majority of the genome there are however several reasons to believe that regulatory network inference is beyond our current reach such as i the combinatorics of the problem ii factors we can t or don t often collect genome wide measurements for and iii dynamics that elude cost effective experimental designs recent works have demonstrated the ability to reconstruct large fractions of prokaryotic regulatory networks from compendiums of genomics data they have also demonstrated that these global regulatory models can be used to predict the dynamics of the transcriptome we review an overall strategy for the reconstruction of global networks based on these results in systems
the millions of mutations and polymorphisms that occur in human populations are potential predictors of disease of our reactions to drugs of predisposition to microbial infections and of age related conditions such as impaired brain and cardiovascular functions however predicting the phenotypic consequences and eventual clinical significance of a sequence variant is not an easy task computational approaches have found perturbation of conserved amino acids to be a useful criterion for identifying variants likely to have phenotypic consequences to our knowledge however no study to date has explored the potential of variants that occur at homologous positions within paralogous human proteins as a means of identifying polymorphisms with likely phenotypic consequences in order to investigate the potential of this approach we have assembled a unique collection of known disease causing variants from omim and the human genome mutation database hgmd and used them to identify and characterize pairs of sequence variants that occur at homologous positions within paralogous human proteins our analyses demonstrate that the locations of variants are correlated in paralogous proteins moreover if one member of a variant pair is disease causing its partner is likely to be disease causing as well thus information about variant pairs can be used to identify potentially disease causing variants extend existing procedures for polymorphism prioritization and provide a suite of candidates for further diagnostic and purposes
reactome http www reactome org is an expert authored peer reviewed knowledgebase of human reactions and pathways that functions as a data mining resource and electronic textbook its current release includes human proteins reactions and literature citations a new entity level pathway viewer and improved search and data mining tools facilitate searching and visualizing pathway data and the analysis of user supplied high throughput data sets reactome has increased its utility to the model organism communities with improved orthology prediction methods allowing pathway inference for species and through collaborations to create manually curated reactome pathway datasets for species including arabidopsis oryza sativa rice drosophila and gallus gallus chicken reactome s data content and software can all be freely used and redistributed under open terms
the pips database http www compbio dundee ac uk www pips is a resource for studying protein protein interactions in human it contains predictions of high probability interactions of which are not reported in the interaction databases hprd bind dip or ophid the interactions in pips were calculated by a bayesian method that combines information from expression orthology domain co occurrence post translational modifications and sub cellular location the predictions also take account of the topology of the predicted interaction network the web interface to pips ranks predictions according to their likelihood of interaction broken down by the contribution from each information source and with easy access to the evidence that supports each prediction where data exists in ophid hprd dip or bind for a protein pair this is also reported in the output tables returned by a search a network browser is included to allow convenient browsing of the interaction network for any protein in the database the pips database provides a new resource on protein protein interactions in human that is straightforward to browse or can be exploited completely for interaction modelling
alternative pre messenger rna splicing influences development physiology and disease but its regulation in humans is not well understood partially because of the limited scale at which the expression of specific splicing events has been measured we generated the first genome scale expression compendium of human alternative splicing events using custom whole transcript microarrays monitoring expression of alternative splicing events in diverse human samples over genes and splicing events were differentially expressed providing a rich resource for studying splicing regulation an unbiased systematic screen of mer to mer words for cis regulatory motifs identified rna words enriched near regulated cassette exons including six clusters of motifs represented by ucucu ugcaug ugcu ugugu uuuu and aggg which map to trans acting regulators ptb fox muscleblind celf cug bp tia and hnrnp f h respectively each cluster showed a distinct pattern of genomic location and tissue specificity for example ucucu occurs to nucleotides preceding cassette exons upregulated in brain and striated muscle but depleted in other tissues ucucu and ugcaug seem to have similar function but independent action occurring and respectively of of the cassette exons upregulated in skeletal muscle but co occurring only
personalization has been deemed one of the major challenges in information retrieval with a significant potential for providing better search experience to individual users especially the need for enhanced user models better capturing elements such as users goals tasks and contexts has been identified in this paper we introduce a statistical language model for user tasks representing different granularity levels of a user profile ranging from very specific search goals to broad topics we propose a personalization framework that selectively matches the actual user information need with relevant past user tasks and allows to dynamically switch the course of personalization from re finding very precise information to biasing results to general user interests in the extreme our model is able to detect when the user s search and browse history is not appropriate for aiding the user in satisfying her current information quest instead of blindly applying personalization to all user queries our approach refrains from undue actions in these cases accounting for the user s desire of discovering new topics and changing interests over time the effectiveness of our method is demonstrated by an empirical study
automatically judging the quality of retrieval functions based on observable user behavior holds promise for making retrieval evaluation faster cheaper and more user centered however the relationship between observable user behavior and retrieval quality is not yet fully understood we present a sequence of studies investigating this relationship for an operational search engine on the arxiv org e print archive we find that none of the eight absolute usage metrics we explore e g number of clicks frequency of query reformulations abandonment reliably reflect retrieval quality for the sample sizes we consider however we find that paired experiment designs adapted from sensory analysis produce accurate and reliable statements about the relative quality of two retrieval functions in particular we investigate two paired comparison tests that analyze clickthrough data from an interleaved presentation of ranking pairs and we find that both give accurate and consistent results we conclude that both paired comparison tests give substantially more accurate and sensitive evaluation results than absolute usage metrics in domain
we describe results from web search log studies aimed at elucidating user behaviors associated with queries and destination urls that appear with different frequencies we note the diversity of information goals that searchers have and the differing ways that goals are specified we examine rare and common information goals that are specified using rare or common queries we identify several significant differences in user behavior depending on the rarity of the query and the destination url we find that searchers are more likely to be successful when the frequencies of the query and destination url are similar we also establish that the behavioral differences observed for queries and goals of varying rarity persist even after accounting for potential confounding variables including query length search engine ranking session duration and task difficulty finally using an information theoretic measure of search difficulty we show that the benefits obtained by search and navigation actions depend on the frequency of the goal
search engine logs are an emerging new type of data that offers interesting opportunities for data mining existing work on mining such data has mostly attempted to discover knowledge at the level of queries e g query clusters in this paper we propose to mine search engine logs for patterns at the level of terms through analyzing the relations of terms inside a query we define two novel term association patterns i e context sensitive term substitutions and term additions and propose new methods for mining such patterns from search engine logs these two patterns can be used to address the mis specification and under specification problems of ineffective queries experiment results on real search engine logs show that the mined context sensitive term substitutions can be used to effectively reword queries and improve their accuracy while the mined context sensitive term addition patterns can be used to support query refinement in a more way
folksonomy systems have shown to contribute to the quality of web search ranking strategies in this paper we analyze and compare different graph based ranking algorithms namely folkrank socialpagerank and socialsimrank we enhance these algorithms by exploiting the context of tag assignmets and evaluate the results on the groupme dataset in groupme users can organize and maintain arbitrary web resources in self defined groups when users annotate resources in groupme this can be interpreted in context of a certain group the grouping activity delivers valuable semantic information about resources and their context we show how to use this information to improve the detection of relevant search results and compare different strategies for ranking result lists in systems
this protocol shows how to detect putative cis regulatory elements and regions enriched in such elements with the regulatory sequence analysis tools rsat web server http rsat ulb ac be rsat the approach applies to known transcription factors whose binding specificity is represented by position specific scoring matrices using the program matrix scan the detection of individual binding sites is known to return many false predictions however results can be strongly improved by estimating p value and by searching for combinations of sites homotypic and heterotypic models we illustrate the detection of sites and enriched regions with a study case the upstream sequence of the drosophila melanogaster gene even skipped this protocol is also tested on random control sequences to evaluate the reliability of the predictions each task requires a few minutes of computation time on the server the complete protocol can be executed in about hour
arrayexpress http www ebi ac uk arrayexpress consists of three components the arrayexpress repository a public archive of functional genomics experiments and supporting data the arrayexpress warehouse a database of gene expression profiles and other bio measurements and the arrayexpress atlas a new summary database and meta analytical tool of ranked gene expression across multiple experiments and different biological conditions the repository contains data from over experiments comprising approximately assays and the database doubles in size every months the majority of the data are array based but other data types are included most recently ultra high throughput sequencing transcriptomics and epigenetic data the warehouse and atlas allow users to query for differentially expressed genes by gene names and properties experimental conditions and sample properties or a combination of both in this update we describe the arrayexpress developments over the last years
granular materials are ubiquitous in our daily lives while they have been the subject of intensive engineering research for centuries in the last two decades granular matter has attracted significant attention from physicists yet despite major efforts by many groups the theoretical description of granular systems remains largely a plethora of different often contradictory concepts and approaches various theoretical models have emerged for describing the onset of collective behavior and pattern formation in granular matter this review surveys a number of situations in which nontrivial patterns emerge in granular systems elucidates important distinctions between these phenomena and similar ones occurring in continuum fluids and describes general principles and models of pattern formation in complex systems that have been successfully applied to systems
social or folksonomic tagging has become a very popular way to describe categorise search discover and navigate content within web websites unlike taxonomies which overimpose a hierarchical categorisation of content folksonomies empower end users by enabling them to freely create and choose the categories in this case tags that best describe some content however as tags are informally defined continually changing and ungoverned social tagging has often been criticised for lowering rather than increasing the efficiency of searching due to the number of synonyms homonyms polysemy as well as the heterogeneity of users and the noise they introduce in this paper we propose social ranking a method that exploits recommender system techniques to increase the efficiency of searches within web we measure users similarity based on their past tag activity we infer tags relationships based on their association to content we then propose a mechanism to answer a user s query that ranks recommends content based on the inferred semantic distance of the query to the tags associated to such content weighted by the similarity of the querying user to the users who created those tags a thorough evaluation conducted on the citeulike dataset demonstrates that social ranking neatly improves coverage while not compromising accuracy
collaborative tagging has become an increasingly popular means for sharing and organizing web resources leading to a huge amount of user generated metadata these tags represent quite a few different aspects of the resources they describe and it is not obvious whether and how these tags or subsets of them can be used for search this paper is the first to present an in depth study of tagging behavior for very different kinds of resources and systems web pages del icio us music last fm and images flickr and compares the results with anchor text characteristics we analyze and classify sample tags from these systems to get an insight into what kinds of tags are used for different resources and provide statistics on tag distributions in all three tagging environments since even relevant tags may not add new information to the search procedure we also check overlap of tags with content with metadata assigned by experts and from other sources we discuss the potential of different kinds of tags for improving search comparing them with user queries posted to search engines as well as through a user survey the results are promising and provide more insight into both the use of different kinds of tags for improving search and possible extensions of tagging systems to support the creation of potentially search tags
background increased levels of the inflammatory biomarker high sensitivity c reactive protein predict cardiovascular events since statins lower levels of high sensitivity c reactive protein as well as cholesterol we hypothesized that people with elevated high sensitivity c reactive protein levels but without hyperlipidemia might benefit from statin treatment methods we randomly assigned apparently healthy men and women with low density lipoprotein ldl cholesterol levels of less than mg per deciliter mmol per liter and high sensitivity c reactive protein levels of mg per liter or higher to rosuvastatin mg daily or placebo and followed them for the occurrence of the combined primary end point of myocardial infarction stroke arterial revascularization hospitalization for unstable angina or death from cardiovascular causes results the trial was stopped after a median follow up of years maximum rosuvastatin reduced ldl cholesterol levels by and high sensitivity c reactive protein levels by the rates of the primary end point were and per person years of follow up in the rosuvastatin and placebo groups respectively hazard ratio for rosuvastatin confidence interval ci to p with corresponding rates of and for myocardial infarction hazard ratio ci to p and for stroke hazard ratio ci to p and for revascularization or unstable angina hazard ratio ci to p and for the combined end point of myocardial infarction stroke or death from cardiovascular causes hazard ratio ci to p and and for death from any cause hazard ratio ci to p consistent effects were observed in all subgroups evaluated the rosuvastatin group did not have a significant increase in myopathy or cancer but did have a higher incidence of physician reported diabetes conclusions in this trial of apparently healthy persons without hyperlipidemia but with elevated high sensitivity c reactive protein levels rosuvastatin significantly reduced the incidence of major cardiovascular events clinicaltrials number
agile software development represents a major departure from traditional plan based approaches to software engineering a systematic review of empirical studies of agile software development up to and including was conducted the search strategy identified studies of which were identified as empirical studies the studies were grouped into four themes introduction and adoption human and social factors perceptions on agile methods and comparative studies the review investigates what is currently known about the benefits and limitations of and the strength of evidence for agile methods implications for research and practice are presented the main implication for research is a need for more and better empirical studies of agile software development within a common research agenda for the industrial readership the review provides a map of findings according to topic that can be compared for relevance to their own settings situations
harnessing data from the growing number of protein ligand complexes in the protein data bank is an important task in drug discovery in order to benefit from the abundance of three dimensional structures structural data must be integrated with sequence as well as chemical data and the protein small molecule interactions characterized structurally at the inter atomic level in this study we present credo a new publicly available database of protein ligand interactions which represents contacts as structural interaction fingerprints implements novel features and is completely scriptable through its application programming interface features of credo include implementation of molecular shape descriptors with ultrafast shape recognition fragmentation of ligands in the protein data bank sequence to structure mapping and the identification of approved drugs selected analyses of these key features are presented to highlight a range of potential applications of credo the credo dataset has been released into the public domain together with the application programming interface under a creative commons license at http www cryst bioc cam ac uk credo we believe that the free availability and numerous features of credo database will be useful not only for commercial but also for academia driven drug programmes
genetic and biochemical analyses have uncovered an essential role for nuclear factor erythroid related factor in regulating phase ii xenobiotic metabolism and antioxidant response here we show that protects against the ovarian toxicity of vinylcyclohexene diepoxide vcd in mice female mice exposed to vcd exhibit an age dependent decline in reproduction leading to secondary infertility accompanied by hypergonadotropic hypogonadism after weeks of age vcd is shown to selectively destroy small ovarian follicles resulting in early depletion of functional follicles treatment with vcd induces apoptotic death in cultured cells and in ovarian follicles suggesting apoptosis as a mechanism of follicle loss loss of function blocks the basal and inducible expression of microsomal epoxide hydrolase a key enzyme in the detoxification of vcd and increases the oxidative stress in cells that is further exacerbated by vcd a repressor in the early stages of follicle activation displays reduced expression in ovaries causing accelerated growth of follicles in the absence of exposure to exogenous chemicals furthermore is degraded through the proteasome pathway in untreated cells and is induced by vcd via both dependent transcription and protein stabilization this study demonstrates that serves as an essential sensor and regulator of chemical homeostasis in ovarian cells protecting the cells from toxic chemicals by controlling metabolic detoxification reactive oxygen species defense and expression in addition these findings raise the possibility that exposure to environmental or occupational ovotoxicants plays a role in the premature ovarian failure commonly associated with infertility and premature aging women
micrornas mirnas are an important class of small noncoding rnas capable of regulating other genes expression much progress has been made in computational target prediction of mirnas in recent years more than mirna target prediction programs have been established yet the prediction of animal mirna targets remains a challenging task we have developed mirecords an integrated resource for animal mirna target interactions the validated targets component of this resource hosts a large high quality manually curated database of experimentally validated mirna target interactions with systematic documentation of experimental support for each interaction the current release of this database includes records of validated mirna target interactions between mirnas and target genes in seven animal species the predicted targets component of mirecords stores predicted mirna targets produced by established mirna target prediction programs mirecords is expected to serve as a useful resource not only for experimental mirna researchers but also for informatics scientists developing the next generation mirna target prediction programs the mirecords is available at http mirecords umn edu nar
gr correctly locating the gene transcription start site and the core promoter is important for understanding transcriptional regulation mechanism here we have integrated specific genome wide histone modification and dna sequence features together to predict rna polymerase ii core promoters in the human genome our new predictor outperforms existing promoter prediction algorithms by providing significantly higher sensitivity and specificity at high resolution we demonstrated that even though the histone modification data used in this study are from a specific cell type t cell our method can be used to identify both active and repressed promoters we have applied it to search the upstream regions of microrna genes and show that can accurately identify the known promoters of the intergenic micrornas we also identified a few intronic micrornas that may have their own promoters this result suggests that our new method can help to identify and characterize the core promoters of both coding and genes
the well known kalman filter is the optimal filter for a linear gaussian state space model furthermore the kalman filter is one of the few known finite dimensional filters in search of other discrete time finite dimensional filters this paper derives filters for general linear exponential state space models of which the kalman filter is a special case one particularly interesting model for which a finite dimensional filter is found to exist is a doubly stochastic discrete time poisson process whose rate evolves as the square of the state of a linear gaussian dynamical system such a model has wide applications in communications systems and queueing theory another filter also with applications in communications systems is derived for estimating the arrival times of a poisson process based on negative exponentially delayed observations copyright copyright john wiley ltd
background in recent years the maturation of microarray technology has allowed the genome wide analysis of gene expression patterns to identify tissue specific and ubiquitously expressed housekeeping genes we have performed a functional and topological analysis of housekeeping and tissue specific networks to identify universally necessary biological processes and those unique to or characteristic of particular tissues results we measured whole genome expression in human tissues identifying housekeeping genes expressed in all tissues and genes uniquely expressed in each tissue comprehensive functional analysis showed that the housekeeping set is substantially larger than previously thought and is enriched with vital processes such as oxidative phosphorylation ubiquitin dependent proteolysis translation and energy metabolism network topology of the housekeeping network was characterized by higher connectivity and shorter paths between the proteins than the global network ontology enrichment scoring and network topology of tissue specific genes were consistent with each tissue s function and expression patterns clustered together in accordance with tissue origin tissue specific genes were twice as likely as housekeeping genes to be drug targets allowing the identification of tissue signature networks that will facilitate the discovery of new therapeutic targets and biomarkers of tissue targeted diseases conclusion a comprehensive functional analysis of housekeeping and tissue specific genes showed that the biological function of housekeeping and tissue specific genes was consistent with tissue origin network analysis revealed that tissue specific networks have distinct network properties related to each tissue s function tissue signature networks promise to be a rich source of targets and biomarkers for disease treatment diagnosis
background comparative genomics is the analysis and comparison of genomes from different species this area of research is driven by the large number of sequenced genomes and heavily relies on efficient algorithms and software to perform pairwise and multiple genome comparisons results most of the software tools available are tailored for one specific task in contrast we have developed a novel system coconut computational comparative genomics utility toolkit that allows solving several different tasks in a unified framework finding regions of high similarity among multiple genomic sequences and aligning them comparing two draft or multi chromosomal genomes locating large segmental duplications in large genomic sequences and mapping cdna est to genomic sequences conclusion coconut is competitive with other software tools w r t the quality of the results the use of state of the art algorithms and data structures allows coconut to solve comparative genomics tasks more efficiently than previous tools with the improved user interface including an interactive visualization component coconut provides a unified versatile and easy to use software tool for large scale studies in genomics
background in vivo positioning and covalent modifications of nucleosomes play an important role in epigenetic regulation but genome wide studies of positioned nucleosomes and their modifications in human still remain limited results this paper describes a novel computational framework to efficiently identify positioned nucleosomes and their histone modification profiles from nucleosome resolution histone modification chip seq data we applied the algorithm to histone methylation chip seq data in human t cells and identified over positioned nucleosomes which appear predominantly at functionally important regions such as genes promoters dnase i hypersensitive regions and transcription factor binding sites our analysis shows the identified nucleosomes play a key role in epigenetic gene regulation within those functionally important regions via their positioning and histone modifications conclusion our method provides an effective framework for studying nucleosome positioning and epigenetic marks in mammalian genomes the algorithm is open source and available at http liulab dfci harvard nps
the ribosomal database project rdp provides researchers with quality controlled bacterial and archaeal small subunit rrna alignments and analysis tools an improved alignment strategy uses the infernal secondary structure aware aligner to provide a more consistent higher quality alignment and faster processing of user sequences substantial new analysis features include a new pyrosequencing pipeline that provides tools to support analysis of ultra high throughput rrna sequencing data this pipeline offers a collection of tools that automate the data processing and simplify the computationally intensive analysis of large sequencing libraries in addition a new taxomatic visualization tool allows rapid visualization of taxonomic inconsistencies and suggests corrections and a new class assignment generator provides instructors with a lesson plan and individualized teaching materials details about rdp data and analytical functions can be found at http rdp cme edu
summary the amount of genomic sequence data being generated and made available through public databases continues to increase at an ever expanding rate downloading copying sharing and manipulating these large datasets are becoming difficult and time consuming for researchers we need to consider using advanced compression techniques as part of a standard data format for genomic data the inherent structure of genome data allows for more efficient lossless compression than can be obtained through the use of generic compression programs we apply a series of techniques to james watson s genome that in combination reduce it to a mere small enough to be sent as an email attachment availability our algorithms are implemented in c and are freely available from http www ics uci edu xhx project dnazip contact chenli ics uci edu xhx ics uci edu supplementary information supplementary data are available at bioinformatics bioinformatics
abstract arrayplex is a software package that centrally provides a large number of flexible toolsets useful for functional genomics including microarray data storage quality assessments data visualization gene annotation retrieval statistical tests genomic sequence retrieval and motif analysis it uses a client server architecture based on open source components provides graphical command line and programmatic access to all needed resources and is extensible by virtue of a documented application programming interface arrayplex is available at http sourceforge net arrayplex
our modern society is flooded with all sorts of devices tv sets automobiles microwaves mobile phones how are all these things affecting us how can their role in our lives be understood what things do answers these questions by focusing on how technologies mediate our actions and our perceptions of the world peter paul verbeek develops this innovative approach by first distinguishing it from the classical philosophy of technology formulated by jaspers and heidegger who were concerned that technology would alienate us from ourselves and the world around us against this gloomy and overly abstract view verbeek draws on and extends the work of more recent philosophers of technology like don ihde bruno latour and albert borgmann to present a much more empirically rich and nuanced picture of how material artifacts shape our existence and experiences in the final part of the book verbeek shows how his postphenomenological approach applies to the technological practice of industrial designers its systematic and historical review of the philosophy of technology makes what things do suitable for use as an introductory text while its innovative approach will make it appealing to readers in many fields including philosophy sociology engineering and design
enhancer of zeste homolog is a mammalian histone methyltransferase that contributes to the epigenetic silencing of target genes and regulates the survival and metastasis of cancer cells is overexpressed in aggressive solid tumors by mechanisms that remain unclear here we show that the expression and function of in cancer cell lines are inhibited by microrna mir analysis of human prostate tumors revealed that mir expression decreases during cancer progression paralleling an increase in expression one or both of the two genomic loci encoding mir were somatically lost in of clinically localized prostate cancer cells of and of metastatic disease cells of we propose that the genomic loss of mir in cancer leads to overexpression of and concomitant dysregulation of epigenetic pathways resulting in cancer science
this paper explores the intersection of emerging surface technologies capable of sensing multiple contacts and of ten shape information and advanced games physics engines we define a technique for modeling the data sensed from such surfaces as input within a physics simulation this affords the user the ability to interact with digital objects in ways analogous to manipulation of real objects our technique is capable of modeling both multiple contact points and more sophisticated shape information such as the entire hand or other physical objects and of mapping this user input to contact forces due to friction and collisions within the physics simulation this enables a variety of fine grained and casual interactions supporting finger based whole hand and tangible input we demonstrate how our technique can be used to add real world dynamics to interactive surfaces such as a vision based tabletop creating a fluid and natural experience our approach hides from application developers many of the complexities inherent in using physics engines allowing the creation of applications without preprogrammed interaction behavior or recognition
motivation modern transcriptomics and proteomics enable us to survey the expression of rnas and proteins at large scales while these data are usually generated and analyzed separately there is an increasing interest in comparing and co analyzing transcriptome and proteome expression data a major open question is whether transcriptome and proteome expression is linked and how it is coordinated results here we have developed a probabilistic clustering model that permits analysis of the links between transcriptomic and proteomic profiles in a sensible and flexible manner our coupled mixture model defines a prior probability distribution over the component to which a protein profile should be assigned conditioned on which component the associated mrna profile belongs to we apply this approach to a large dataset of quantitative transcriptomic and proteomic expression data obtained from a human breast epithelial cell line hmec the results reveal a complex relationship between transcriptome and proteome with most mrna clusters linked to at least two protein clusters and vice versa a more detailed analysis incorporating information on gene function from the gene ontology database shows that a high correlation of mrna and protein expression is limited to the components of some molecular machines such as the ribosome cell adhesion complexes and the tcp chaperonin involved in protein folding availability matlab code is available from the authors on request contact srogers dcs gla ac uk supplementary information supplementary data are available at bioinformatics bioinformatics
the development of affordable high throughput sequencing technology has led to a flood of publicly available bacterial genome sequence data the availability of multiple genome sequences presents both an opportunity and a challenge for microbiologists and new computational approaches are needed to extract the knowledge that is required to address specific biological problems and to analyse genomic data the field of e science is maturing and grid based technologies can help address challenge
the evolutionary potential of a gene is constrained not only by the amino acid sequence of its product but by its dna sequence as well the topology of the genetic code is such that half of the amino acids exhibit synonymous codons that can reach different subsets of amino acids from each other through single mutation thus synonymous dna sequences should access different regions of the protein sequence space through a limited number of mutations and this may deeply influence the evolution of natural proteins here we demonstrate that this feature can be of value for manipulating protein evolvability we designed an algorithm that starting from an input gene constructs a synonymous sequence that systematically includes the codons with the most different evolutionary perspectives i e codons that maximize accessibility to amino acids previously unreachable from the template by point mutation a synonymous version of a bacterial antibiotic resistance gene was computed and synthesized when concurrently submitted to identical directed evolution protocols both the wild type and the recoded sequence led to the isolation of specific advantageous phenotypic variants simulations based on a mutation isolated only from the synthetic gene libraries were conducted to assess the impact of sub functional selective constraints such as codon usage on natural adaptation our data demonstrate that rational design of synonymous synthetic genes stands as an affordable improvement to any directed evolution protocol we show that using two synonymous dna sequences improves the overall yield of the procedure by increasing the diversity of mutants generated these results provide conclusive evidence that synonymous coding sequences do experience different areas of the corresponding protein adaptive landscape and that a sequence s codon usage effectively constrains the evolution of the protein
alzheimer s disease ad characterized by accumulation of amyloid beta protein abeta in brain parenchyma is closely associated with brain ischemia decreased clearance of abeta from brain is the main cause of abeta accumulation in sporadic ad however the mechanisms underlying ischemia mediated ad pathogenesis remain unclear the receptor for advanced end glycation products rage and low density lipoprotein receptor related protein lrp expressed at blood brain barrier bbb are actively involved in abeta clearance rage is thought to be a primary transporter of abeta across bbb into the brain from the systemic circulation while lrp mediates the transport of abeta out of the brain ginkgo biloba extract a traditional chinese medicine has been widely used in the treatment of ad to investigate the effects of on the expression of rage and lrp in endothelial cells in response to ischemic injury we cultured bend cells an immortalized mouse cerebral microvessel endothelial cell line under a chronic hypoxic and hypoglycemic condition chh to mimic ischemic injury of bbb and then treated with egb we found that egb markedly ameliorated the damage evaluated by mtt assay from chh moreover we demonstrated that chh led to a significant increase in the expression of rage both at the mrna and protein levels at all times and h conversely chh induced a dramatic decrease in lrp mrna and protein expression at both and h the results indicated that chh has differential effects on the expression of rage and lrp furthermore significantly reversed chh induced upregulation of rage expression and downregulation of lrp expression our findings suggest that favor clearance of abeta via regulating the expression of rage and lrp during brain ischemia this may provide a new insight into a possible molecular mechanism underlying brain ischemia mediated ad pathogenesis and potential therapeutic application of egb in treatment ad
gr it has previously been suggested that the phylogeny of microbial species might be better described as a network containing vertical and horizontal gene transfer hgt events yet all phylogenetic reconstructions so far have presented microbial trees rather than networks here we present a first attempt to reconstruct such an evolutionary network which we term the net of life we use available tree reconstruction methods to infer vertical inheritance and use an ancestral state inference algorithm to map hgt events on the tree we also describe a weighting scheme used to estimate the number of genes exchanged between pairs of organisms we demonstrate that vertical inheritance constitutes the bulk of gene transfer on the tree of life we term the bulk of horizontal gene flow between tree nodes as vines and demonstrate that multiple but mostly tiny vines interconnect the tree our results strongly suggest that the hgt network is a scale free graph a finding with important implications for genome evolution we propose that genes might propagate extremely rapidly across microbial species through the hgt network using certain organisms hubs
abstract background image based screens can produce hundreds of measured features for each of hundreds of millions of individual cells in a single experiment results here we describe cellprofiler analyst open source software for the interactive exploration and analysis of multidimensional data particularly data from high throughput image based experiments conclusions the system enables interactive data exploration for image based screens and automated scoring of complex phenotypes that require combinations of multiple measured features cell
the sea slug elysia chlorotica acquires plastids by ingestion of its algal food source vaucheria litorea organelles are sequestered in the mollusc s digestive epithelium where they photosynthesize for months in the absence of algal nucleocytoplasm this is perplexing because plastid metabolism depends on the nuclear genome for of the needed proteins two possible explanations for the persistence of photosynthesis in the sea slug are i the ability of v litorea plastids to retain genetic autonomy and or ii more likely the mollusc provides the essential plastid proteins under the latter scenario genes supporting photosynthesis have been acquired by the animal via horizontal gene transfer and the encoded proteins are retargeted to the plastid we sequenced the plastid genome and confirmed that it lacks the full complement of genes required for photosynthesis in support of the second scenario we demonstrated that a nuclear gene of oxygenic photosynthesis psbo is expressed in the sea slug and has integrated into the germline the source of psbo in the sea slug is v litorea because this sequence is identical from the predator and prey genomes evidence that the transferred gene has integrated into sea slug nuclear dna comes from the finding of a highly diverged psbo flanking sequence in the algal and mollusc nuclear homologues and gene absence from the mitochondrial genome of e chlorotica we demonstrate that foreign organelle retention generates metabolic novelty green animals and is explained by anastomosis of distinct branches of the tree of life driven by predation and horizontal transfer
we present performance results for dense linear algebra using recent nvidia gpus our matrix matrix multiply routine gemm runs up to faster than the vendor s implementation and approaches the peak of hardware capabilities our lu qr and cholesky factorizations achieve up to of the peak gemm rate our parallel lu running on two gpus achieves up to gflop s these results are accomplished by challenging the accepted view of the gpu architecture and programming guidelines we argue that modern gpus should be viewed as multithreaded multicore vector units we exploit blocking similarly to vector computers and heterogeneity of the system by computing both on gpu and cpu this study includes detailed benchmarking of the gpu memory system that reveals sizes and latencies of caches and tlb we present a couple of algorithmic optimizations aimed at increasing parallelism and regularity in the problem that provide us with slightly performance
resveratrol has been shown to protect against oxidative stress through modulating antioxidant capacity in this study we investigated resveratrol mediated induction of glutathione gsh and glutamate cysteine ligase gcl and the combined effect of resveratrol and hydroxynonenal hne on gsh synthesis in cultured human bronchial epithelial cells resveratrol increased gsh and the mrna contents of both the catalytic gclc and modulatory subunit gclm of gcl combined hne and resveratrol treatment increased gsh content and gcl mrnas to a greater extent than either compound did alone compared to individual agent combining exposure to hne and resveratrol also showed more protection against cell death caused by oxidative stress these effects of combined exposure were additive rather than synergistic in addition silencing significantly decreased the combined effect of hne and resveratrol on gcl induction our data suggest that resveratrol increases gsh and gcl gene expression and that there is an additive effect on gsh synthesis between resveratrol and hne the results also reveal that epre signaling was involved in the effects
in this paper we suggest that cortical anatomy recapitulates the temporal hierarchy that is inherent in the dynamics of environmental states many aspects of brain function can be understood in terms of a hierarchy of temporal scales at which representations of the environment evolve the lowest level of this hierarchy corresponds to fast fluctuations associated with sensory processing whereas the highest levels encode slow contextual changes in the environment under which faster representations unfold first we describe a mathematical model that exploits the temporal structure of fast sensory input to track the slower trajectories of their underlying causes this model of sensory encoding or perceptual inference establishes a proof of concept that slowly changing neuronal states can encode the paths or trajectories of faster sensory states we then review empirical evidence that suggests that a temporal hierarchy is recapitulated in the macroscopic organization of the cortex this anatomic temporal hierarchy provides a comprehensive framework for understanding cortical function the specific time scale that engages a cortical area can be inferred by its location along a rostro caudal gradient which reflects the anatomical distance from primary sensory areas this is most evident in the prefrontal cortex where complex functions can be explained as operations on representations of the environment that change slowly the framework provides predictions about and principled constraints on cortical structurefunction relationships which can be tested by manipulating the time scales of input
this paper describes a general model that subsumes many parametric models for continuous data the model comprises hidden layers of state space or dynamic causal models arranged so that the output of one provides input to another the ensuing hierarchy furnishes a model for many types of data of arbitrary complexity special cases range from the general linear model for static data to generalised convolution models with system noise for nonlinear time series analysis crucially all of these models can be inverted using exactly the same scheme namely dynamic expectation maximization this means that a single model and optimisation scheme can be used to invert a wide range of models we present the model and a brief review of its inversion to disclose the relationships among apparently diverse generative models of empirical data we then show that this inversion can be formulated as a simple neural network and may provide a useful metaphor for inference and learning in brain
the world wide web consists of a huge number of unstructured documents but it also contains structured data in the form of html tables we extracted billion html tables from google s general purpose web crawl and used statistical classification techniques to find the estimated that contain high quality relational data because each relational table has its own schema of labeled and typed columns each such table can be considered a small structured database the resulting corpus of databases is larger than any other corpus we are aware of by at least five orders magnitude
one of the most well characterized symptoms of lead poisoning is porphyria the biochemical signs of lead intoxication related to porphyria are aminolevulinic aciduria coproporphyrinuria and accumulation of free and zinc protoporphyrin in erythrocytes from the to the early almost all of the enzymes in the heme pathway had been purified and characterized and it was demonstrated that aminolevulinic aciduria is due to inhibition of aminolevulinate dehydratase by lead lead also inhibits purified ferrochelatase however the magnitude of inhibition was essentially nil even under pathological conditions further study proved the disturbance of iron reducing activity by moderate lead exposure far different from these two enzymes lead failed to inhibit purified coproporphyrinogen oxidase i e the mechanism of coproporphyrinuria has not yet been understood during the to the the effects of environmental hazards including lead were elucidated through stress proteins indicating the induction of some heme pathway enzymes as stress proteins at that time gene environment interaction was another focus of toxicology since gene carriers of porphyrias are considered to be a high risk group to chemical pollutants toxicological studies from the to the focused on the direct effect of hazards on biological molecules such as the heme pathway enzymes and many environmental pollutants were proved to affect cytosolic heme recently we demonstrated the mechanism of the heme controlled transcription system which suggests that the indirect effects of environmental hazards are also important for elucidating toxicity i e the hazards can affect cell functions through such biological mediators as regulatory heme it is therefore probable that toxicology in the future will focus on biological systems such as gene regulation and signal systems
gene duplication provides raw material for functional innovation recent advances have shed light on two fundamental questions regarding gene duplication which genes tend to undergo duplication and how does natural selection subsequently act on them genomic data suggest that different gene classes tend to be retained after single gene and whole genome duplications we also know that functional differences between duplicate genes can originate in several different ways including mutations that directly impart new functions subdivision of ancestral functions and selection for changes in gene dosage interestingly in many cases the new function of one copy is a secondary property that was always present but that has been co opted to a primary role after duplication
inorganic arsenic is a well documented human carcinogen that targets the skin the induction of oxidative stress as shown with arsenic may have a bearing on the carcinogenic mechanism of this metalloid the transcription factor is a key player in the regulation of genes encoding for many antioxidative response enzymes thus the effect of inorganic arsenic as sodium arsenite on expression and localization was studied in hacat cells an immortalized human keratinocyte cell line we found for the first time that arsenic enhanced cellular expression of at the transcriptional and protein levels and activated expression of related genes in these cells in addition arsenic exposure caused nuclear accumulation of in association with downstream activation of mediated oxidative response genes arsenic simultaneously increased the expression of a regulator of activity the coordinated induction of expression and nuclear accumulation induced by arsenic suggests that is important to arsenic induced activation furthermore when cells were pretreated with scavengers of hydrogen peroxide h o such as catalasepolyethylene glycol peg cat or tiron arsenic induced nuclear accumulation was suppressed whereas cudipsh a cell permeable superoxide dismutase sod mimic compound that produces h o from superoxide o enhanced nuclear accumulation these results indicate that h o rather than o is the mediator of nuclear accumulation additional study showed that arsenic causes increased cellular h o production and that h o itself has the ability to increase expression at both the transcription and protein levels in hacat cells taken together these data clearly show that arsenic increases expression and activity at multiple levels and that h o is one of the mediators of process
the human intestinal microbiota is essential to the health of the host and plays a role in nutrition development metabolism pathogen resistance and regulation of immune responses antibiotics may disrupt these coevolved interactions leading to acute or chronic disease in some individuals our understanding of antibiotic associated disturbance of the microbiota has been limited by the poor sensitivity inadequate resolution and significant cost of current research methods the use of pyrosequencing technology to generate large numbers of rdna sequence tags circumvents these limitations and has been shown to reveal previously unexplored aspects of the rare biosphere we investigated the distal gut bacterial communities of three healthy humans before and after treatment with ciprofloxacin obtaining more than full length rrna sequences and over pyrosequencing reads from two hypervariable regions of the rrna gene a companion paper in plos genetics see huse et al doi journal pgen shows that the taxonomic information obtained with these methods is concordant pyrosequencing of the and variable regions identified taxa that collectively accounted for over of the variable region sequence tags that could be obtained from these samples ciprofloxacin treatment influenced the abundance of about a third of the bacterial taxa in the gut decreasing the taxonomic richness diversity and evenness of the community however the magnitude of this effect varied among individuals and some taxa showed interindividual variation in the response to ciprofloxacin while differences of community composition between individuals were the largest source of variability between samples we found that two unrelated individuals shared a surprising degree of community similarity in all three individuals the taxonomic composition of the community closely resembled its pretreatment state by weeks after the end of treatment but several taxa failed to recover within months these pervasive effects of ciprofloxacin on community composition contrast with the reports by participants of normal intestinal function and with prior assumptions of only modest effects of ciprofloxacin on the intestinal microbiota these observations support the hypothesis of functional redundancy in the human gut microbiota the rapid return to the pretreatment community composition is indicative of factors promoting community resilience the nature of which deserves investigation
background an important emerging trend in the analysis of microarray data is to incorporate known pathway information a priori expression level summaries for pathways obtained from the expression data for the genes constituting the pathway permit the inclusion of pathway information reduce the high dimensionality of microarray data and have the power to elucidate gene interaction dependencies which are not already accounted for through known pathway identification results we present a novel method for the analysis of microarray data that identifies joint differential expression in gene pathway pairs this method takes advantage of known gene pathway memberships to compute a summary expression level for each pathway as a whole correlations between the pathway expression summary and the expression levels of genes not already known to be associated with the pathway provide clues to gene interaction dependencies that are not already accounted for through known pathway identification and statistically significant differences between gene pathway correlations in phenotypically different cells e g where the expression level of a single gene and a given pathway summary correlate strongly in normal cells but weakly in tumor cells may indicate biologically relevant gene pathway interactions here we detail the methodology and present the results of this method applied to two gene expression datasets identifying gene pathway pairs which exhibit differential joint expression by phenotype conclusion the method described herein provides a means by which interactions between large numbers of genes may be identified by incorporating known pathway information to reduce the dimensionality of gene interactions the method is efficient and easily applied to data sets of arrays application of this method to two publicly available cancer data sets yields suggestive and promising results this method has the potential to complement gene at a time analysis techniques for microarray analysis by indicating relationships between pathways and genes that have not previously been identified and which may play a role disease
the level of paradigm development technical certainty and consensus characterizing a field of study has numerous consequences for the social organization and operation of that field these consequences ranging from the ability to obtain resources to the ease of working collaboratively on research have an impact on the subsequent development of the field i e through a positive feedback loop although the degree of technical certainty or consensus is clearly affected by the fundamental nature of the subject of study consensus is also produced by social practices that differentiate fields that are more or less paradigmatically developed the study of organizations is arguably paradigmatically not well developed in part because of values that emphasize representativeness inclusiveness and theoretical and methodological diversity although these values are attractive ideals there are consequences for the field s ability to make scientific progress which almost requires some level of consensus as well as for its likely ability to compete successfully with adjacent social sciences such as economics in the contest for resources recognizing the trade offs and processes involved in scientific progress seems to be a necessary first step for thinking about the dilemmas that are implicit in the sociology of literature
summary dnaplotter is an interactive java application for generating circular and linear representations of genomes making use of the artemis libraries to provide a user friendly method of loading in sequence files embl genbank gff as well as data from relational databases it filters features of interest to display on separate user definable tracks it can be used to produce publication quality images for papers or web pages availability dnaplotter is freely available under a gpl licence for download for macosx unix and windows at the wellcome trust sanger institute web sites http www sanger ac uk software artemis circular contact artemis sanger ac bioinformatics
motivation achieving a comprehensive map of all the regulatory elements encoded in the human genome is a fundamental challenge of biomedical research so far only a small fraction of the regulatory elements have been characterized and there is great interest in applying computational techniques to systematically discover these elements such efforts however have been significantly hindered by the overwhelming size of non coding dna regions and the statistical variability and complex spatial organizations of mammalian regulatory elements results here we combine information from multiple mammalian genomes to derive the first fairly comprehensive map of regulatory elements in the human genome we develop a procedure for identifying regulatory sites with high levels of conservation across different species using a new scoring scheme the bayesian branch length score bbls using bbls we predict million regulatory sites corresponding to known regulatory motifs with an estimated false discovery rate fdr of we demonstrate that the method is particularly effective for motifs for which sites can be mapped with an estimated fdr of over snps are located in regions overlapping the million predicted motif sites suggesting potential functional implications for these snps we have deposited these elements in a database and created a user friendly web server for the retrieval analysis and visualization of these elements the initial map provides a systematic view of gene regulation in the genome which will be refined as additional motifs become available availability http motifmap ics uci edu contact xhx ics uci edu pfbaldi ics uci edu supplementary information supplementary data are available at bioinformatics bioinformatics
in two independent groups extracted dna from several pleistocene epoch mammoths and noted differences among individual specimens subsequently dna sequences have been published for a number of extinct species however such ancient dna is often fragmented and damaged and studies to date have typically focused on short mitochondrial sequences never yielding more than a fraction of a per cent of any nuclear genome here we describe billion bases gb of sequence from several mammoth specimens billion of which are from the woolly mammoth mammuthus primigenius genome and thus comprise an extensive set of genome wide sequence from an extinct species our data support earlier reports that elephantid genomes exceed gb the estimated divergence rate between mammoth and african elephant is half of that between human and chimpanzee the observed number of nucleotide differences between two particular mammoths was approximately one eighth of that between one of them and the african elephant corresponding to a separation between the mammoths of myr the estimated probability that orthologous elephant and mammoth amino acids differ is corresponding to about one residue per protein differences were discovered between mammoth and african elephant in amino acid positions that are otherwise invariant over several billion years of combined mammalian evolution this study shows that nuclear genome sequencing of extinct species can reveal population differences not evident from the fossil record and perhaps even discover genetic factors that extinction
motivation an important problem in systems biology is reconstructing complete networks of interactions between biological objects by extrapolating from a few known interactions as examples while there are many computational techniques proposed for this network reconstruction task their accuracy is consistently limited by the small number of high confidence examples and the uneven distribution of these examples across the potential interaction space with some objects having many known interactions and others few results to address this issue we propose two computational methods based on the concept of training set expansion they work particularly effectively in conjunction with kernel approaches which are a popular class of approaches for fusing together many disparate types of features both our methods are based on semi supervised learning and involve augmenting the limited number of gold standard training instances with carefully chosen and highly confident auxiliary examples the first method prediction propagation propagates highly confident predictions of one local model to another as the auxiliary examples thus learning from information rich regions of the training network to help predict the information poor regions the second method kernel initialization takes the most similar and most dissimilar objects of each object in a global kernel as the auxiliary examples using several sets of experimentally verified protein protein interactions from yeast we show that training set expansion gives a measurable performance gain over a number of representative state of the art network reconstruction methods and it can correctly identify some interactions that are ranked low by other methods due to the lack of training examples of the involved proteins contact mark gerstein yale edu availability the datasets and additional materials can be found at http networks gersteinlab org bioinformatics
solving linear systems of equations is a common problem that arises both on its own and as a subroutine in more complex problems given a matrix a and and an vector b find a vector x such that ax b often one does not need to know the solution x itself but rather an approximation of the expectation value of some operator associated with x e g x dag m x for some matrix m in this case when a is sparse and well conditioned with largest dimension n the best classical algorithms can find x and estimate x dag m x in o n time here we exhibit a quantum algorithm for solving linear sets of equations that runs in o log n time an exponential improvement over the best algorithm
social tagging systems have become increasingly popular over the past years users tagging practices have been little studied and understood so far however understanding tagging behaviour can contribute towards a thorough understanding of the tagging phenomenon from multiple perspectives in the present paper results of a comparative analysis of tag characteristics on the tagging platforms connotea org scientific articles del icio us bookmarks flickr com photos and youtube com videos are presented results show that differences in tagging behaviour can be observed for different digital resource types finally a short discussion of the possible implications of the results for the design of future tagging systems presented
summary the protorp server analyses protein protein associations in structures the server calculates a series of physical and chemical parameters of the protein interaction sites that contribute to the binding energy of the association these parameters include size and shape intermolecular bonding residue and atom composition and secondary structure contributions the server is flexible in that it allows users to analyse individual protein associations or large datasets of associations deposited in the pdb or upload and analyse proprietary files the properties calculated can be compared with parameter distributions for non homologous datasets of different classes of protein associations provided on the server website the server provides an efficient way of characterizing protein protein associations of new or existing proteins and a means of putting these values in the context of previously observed associations availability http www bioinformatics sussex ac uk protorp contact s jones sussex ac bioinformatics
rna seq is a recently developed approach to transcriptome profiling that uses deep sequencing technologies studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes rna seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods this article describes the rna seq approach the challenges associated with its application and the advances made so far in characterizing several transcriptomes
we present single molecule real time sequencing data obtained from a dna polymerase performing uninterrupted template directed synthesis using four distinguishable fluorescently labeled deoxyribonucleoside triphosphates dntps we detected the temporal order of their enzymatic incorporation into a growing dna strand with zero mode waveguide nanostructure arrays which provide optical observation volume confinement and enable parallel simultaneous detection of thousands of single molecule sequencing reactions conjugation of fluorophores to the terminal phosphate moiety of the dntps allows continuous observation of dna synthesis over thousands of bases without steric hindrance the data report directly on polymerase dynamics revealing distinct polymerization states and pause sites corresponding to dna secondary structure sequence data were aligned with the known reference sequence to assay biophysical parameters of polymerization for each template position consensus sequences were generated from the single molecule reads at fold coverage showing a median accuracy of with no systematic error beyond fluorophore dependent error science
host genetic factors that regulate innate immunity determine susceptibility to sepsis disruption of nuclear factor erythroid related factor a basic leucine zipper transcription factor that regulates redox balance and stress response dramatically increased the mortality of mice in response to endotoxin and cecal ligation and puncture induced septic shock lps as well as tnf alpha stimulus resulted in greater lung inflammation in deficient mice temporal analysis of pulmonary global gene expression after lps challenge revealed augmented expression of large numbers of proinflammatory genes associated with the innate immune response at as early as minutes in lungs of deficient mice indicating severe immune dysregulation the expression profile indicated that has a global influence on both dependent and independent signaling deficient mouse embryonic fibroblasts showed greater activation of nf kappab and interferon regulatory factor in response to lps and polyinosinic polycytidylic acid poly i c stimulus corroborating the effect of on dependent and independent signaling s regulation of cellular glutathione and other antioxidants is critical for optimal nf kappab activation in response to lps and tnf alpha our study reveals as a novel modifier gene of sepsis that determines survival by mounting an appropriate innate response
twenty seven automatically extractable bug fix patterns are defined using the syntax components and context of the source code involved in bug fix changes bug fix patterns are extracted from the configuration management repositories of seven open source projects all written in java eclipse columba jedit scarab argouml lucene and megamek defined bug fix patterns cover to of the total bug fix hunk pairs in these projects the frequency of occurrence of each bug fix pattern is computed across all projects the most common individual patterns are mc dap method call with different actual parameter values at if cc change in if conditional at and as ce change of assignment expression at a correlation analysis on the extracted pattern instances on the seven projects shows that six have very similar bug fix pattern frequencies analysis of if conditional bug fix sub patterns shows a trend towards increasing conditional complexity in if conditional fixes analysis of five developers in the eclipse projects shows overall consistency with project level bug fix pattern frequencies as well as distinct variations among developers in their rates of producing various bug patterns overall data in the paper suggest that developers have difficulty with specific code situations at surprisingly consistent rates there appear to be broad mechanisms causing the injection of bugs that are largely independent of the type of software produced
abstractdue to the volume conduction multi channel elec troencephalogram eeg recordings give a rather blurred image of brain activity therefore spatial filters are extremely useful in single trial analysis in order to improve the signal to noise ratio there are powerful methods from machine learning and signal processing that permit the optimization of spatio temporal filters for each subject in a data dependent fashion beyond the fixed filters based on the sensor geometry e g laplacians here we elucidate the theoretical background of the common spatial pattern csp algorithm a popular method in brain computer interface bci research apart from reviewing several variants of the basic algorithm we reveal tricks of the trade for achieving a powerful csp performance briefly elaborate on theoretical aspects of csp and demonstrate the application of csp type preprocessing in our studies of the berlin brain computer project
we developed a novel approach for de novo genome assembly using only sequence data from high throughput short read sequencing technologies by combining data generated from life sciences roche and illumina formerly known as solexa sequencing sequencing platforms we reliably assembled genomes into large scaffolds at a fraction of the traditional cost and without use of a reference sequence we applied this method to two isolates of the phytopathogenic bacteria pseudomonas syringae sequencing and reassembly of the well studied tomato and arabidopsis pathogen pto facilitated development and testing of our method sequencing of a distantly related rice pathogen por demonstrated our method s efficacy for de novo assembly of novel genomes our assembly of por yielded an scaffold size of bp with of the predicted genome covered by scaffolds over bp one of the critical phenotypic differences between strains of p syringae is the range of plant hosts they infect this is largely determined by their complement of type iii effector proteins the genome of por is the first sequenced for a p syringae isolate that is a pathogen of monocots and as might be predicted its complement of type iii effectors differs substantially from the previously sequenced isolates of this species the genome of por helps to define an expansion of the p syringae pan genome a corresponding contraction of the core genome and a further diversification of the type iii effector complement for this important plant species
why do seemingly identical cells respond differently to a drug to address this we studied the dynamics and variability of the protein response of human cancer cells to a chemotherapy drug camptothecin we present a dynamic proteomics approach that measures the levels and locations of nearly different endogenously tagged proteins in individual living cells at high temporal resolution all cells show rapid translocation of proteins specific to the drug mechanism including the drug target topoisomerase and slower wide ranging temporal waves of protein degradation and accumulation however the cells differ in the behavior of a subset of proteins we identify proteins whose dynamics differ widely between cells in a way that corresponds to the outcomescell death or survival this opens the way to understanding molecular responses to drugs in cells
regulatory changes have long been hypothesized to play an important role in primate evolution to identify adaptive regulatory changes in humans we performed a genome wide survey for genes in which regulation has likely evolved under natural selection to do so we used a multi species microarray to measure gene expression levels in livers kidneys and hearts from six humans chimpanzees and rhesus macaques this comparative gene expression data allowed us to identify a large number of genes as well as specific pathways whose inter species expression profiles are consistent with the action of stabilizing or directional selection on gene regulation among the latter set we found an enrichment of genes involved in metabolic pathways consistent with the hypothesis that shifts in diet underlie many regulatory adaptations in humans in addition we found evidence for tissue specific selection pressures as well as lower rates of protein evolution for genes in which regulation evolves under natural selection these observations are consistent with the notion that adaptive circumscribed changes in gene regulation have fewer deleterious pleiotropic effects compared with changes at the protein level
in ernst abbe discovered that features closer than approximately nm cannot be resolved by lens based light microscopy in recent years however several new far field super resolution imaging techniques have broken this diffraction limit producing for example video rate movies of synaptic vesicles in living neurons with nm spatial resolution current research is focused on further improving spatial resolution in an effort to reach the goal of video rate imaging of live cells with molecular nm resolution here we describe the contributions of fluorescent probes to far field super resolution imaging focusing on fluorescent proteins and organic small molecule fluorophores we describe the features of existing super resolution fluorophores and highlight areas of importance for future research development
imagine that the neighborhood you are living in is covered with graffiti litter and unreturned shopping carts would this reality cause you to litter more trespass or even steal a thesis known as the broken windows theory suggests that signs of disorderly and petty criminal behavior trigger more disorderly and petty criminal behavior thus causing the behavior to spread this may cause neighborhoods to decay and the quality of life of its inhabitants to deteriorate for a city government this may be a vital policy issue but does disorder really spread in neighborhoods so far there has not been strong empirical support and it is not clear what constitutes disorder and what may make it spread we generated hypotheses about the spread of disorder and tested them in six field experiments we found that when people observe that others violated a certain social norm or legitimate rule they are more likely to violate other norms or rules which causes disorder to science
embryonic development is defined by the hierarchical dynamical process that translates genetic information genotype into a spatial gene expression pattern phenotype providing the positional information for the correct unfolding of the organism the nature and evolutionary implications of genotype phenotype mapping still remain key topics in evolutionary developmental biology evo devo we have explored here issues of neutrality robustness and diversity in evo devo by means of a simple model of gene regulatory networks the small size of the system allowed an exhaustive analysis of the entire fitness landscape and the extent of its neutrality this analysis shows that evolution leads to a class of robust genetic networks with an expression pattern characteristic of lateral inhibition this class is a repertoire of distinct implementations of this key developmental process the diversity of which provides valuable clues about its underlying principles
recent years have seen a huge increase in the generation of genomic and proteomic data this has been due to improvements in current biological methodologies the development of new experimental techniques and the use of computers as support tools all these raw data are useless if they cannot be properly analysed annotated stored and displayed consequently a vast number of resources have been created to present the data to the wider community annotation tools and databases provide the means to disseminate these data and to comprehend their biological importance this review examines the various aspects of annotation type methodology and availability moreover it puts a special interest on novel annotation fields such as that of phenotypes and highlights the recent efforts focused on the annotations
the capacity for voluntary action is seen as essential to human nature yet neuroscience and behaviourist psychology have traditionally dismissed the topic as unscientific perhaps because the mechanisms that cause actions have long been unclear however new research has identified networks of brain areas including the pre supplementary motor area the anterior prefrontal cortex and the parietal cortex that underlie voluntary action these areas generate information for forthcoming actions and also cause the distinctive conscious experience of intending to act and then controlling one s own actions volition consists of a series of decisions regarding whether to act what action to perform and when to perform it neuroscientific accounts of voluntary action may inform debates about the nature of responsibility
cellular rhythms are generated by complex interactions among genes proteins and metabolites they are used to control every aspect of cell physiology from signalling motility and development to growth division and death we consider specific examples of oscillatory processes and discuss four general requirements for biochemical oscillations negative feedback time delay sufficient nonlinearity of the reaction kinetics and proper balancing of the timescales of opposing chemical reactions positive feedback is one mechanism to delay the negative feedback signal biological oscillators can be classified according to the topology of the positive and negative feedback loops in the underlying mechanism
massively parallel pyrosequencing of hypervariable regions from small subunit ribosomal rna ssu rrna genes can sample a microbial community two or three orders of magnitude more deeply per dollar and per hour than capillary sequencing of full length ssu rrna as with full length rrna surveys each sequence read is a tag surrogate for a single microbe however rather than assigning taxonomy by creating gene trees de novo that include all experimental sequences and certain reference taxa we compare the hypervariable region tags to an extensive database of rrna sequences and assign taxonomy based on the best match in a global alignment for sequence taxonomy gast process the resulting taxonomic census provides information on both composition and diversity of the microbial community to determine the effectiveness of using only hypervariable region tags for assessing microbial community membership we compared the taxonomy assigned to the and hypervariable regions with the taxonomy assigned to full length ssu rrna sequences isolated from both the human gut and a deep sea hydrothermal vent the hypervariable region tags and full length rrna sequences provided equivalent taxonomy and measures of relative abundance of microbial communities even for tags up to divergent from their nearest reference match the greater sampling depth per dollar afforded by massively parallel pyrosequencing reveals many more members of the rare biosphere than does capillary sequencing of the full length gene in addition tag sequencing eliminates cloning bias and the sequences are short enough to be completely sequenced in a single read maximizing the number of organisms sampled in a run while minimizing chimera formation this technique allows the cost effective exploration of changes in microbial community structure including the rare biosphere over space and time and can be applied immediately to initiatives such as the human project
despite more than a decade of experimental work in multi robot systems important theoretical aspects of multi robot coordination mechanisms have to date been largely untreated to address this issue we focus on the problem of multi robot task allocation mrta most work on mrta has been ad hoc and empirical with many coordination architectures having been proposed and validated in a proof of concept fashion but infrequently analyzed with the goal of bringing objective grounding to this important area of research we present a formal study of mrta problems a domain independent taxonomy of mrta problems is given and it is shown how many such problems can be viewed as instances of other well studied optimization problems we demonstrate how relevant theory from operations research and combinatorial optimization can be used for analysis and greater understanding of existing approaches to task allocation and to show how the same theory can be used in the synthesis of approaches
abstract in this essay i argue that the seemingly indestructible concept of the community as a local interacting assemblage of species has hindered progress toward understanding species richness at local to regional scales i suggest that the distributions of species within a region reveal more about the processes that generate diversity patterns than does the cooccurrence of species at any given point the local community is an epiphenomenon that has relatively little explanatory power in ecology and evolutionary biology local coexistence cannot provide insight into the ecogeographic distributions of species within a region from which local assemblages of species derive nor can local communities be used to test hypotheses concerning the origin maintenance and regulation of species richness either locally or regionally ecologists are moving toward a community concept based on interactions between populations over a continuum of spatial and temporal scales within entire regions including the population and evolutionary processes that produce species
transcription factor binding sites are being discovered at a rapid pace it is now necessary to turn attention towards understanding how these sites work in combination to influence gene expression quantitative models that accurately predict gene expression from promoter sequence will be a crucial part of solving this problem here we present such a model based on the analysis of synthetic promoter libraries in yeast saccharomyces cerevisiae thermodynamic models based only on the equilibrium binding of transcription factors to dna and to each other captured a large fraction of the variation in expression in every library thermodynamic analysis of these libraries uncovered several phenomena in our system including cooperativity and the effects of weak binding sites when applied to the s cerevisiae genome a model of repression by which was trained on synthetic promoters predicts a number of regulated genes that lack significant binding sites in their promoters the success of the thermodynamic approach suggests that the information encoded by combinations of cis regulatory sites is interpreted primarily through simple protein dna and protein protein interactions with complicated biochemical reactions such as nucleosome modifications being downstream events quantitative analyses of synthetic promoter libraries will be an important tool in unravelling the rules underlying combinatorial regulation
a recent article by james evans in science evans is being widely discussed in the science and publishing communities evans in depth research on citations in over million articles and how online availability affects citing patterns found that the more issues of a journal that are available online the fewer numbers of articles in that journal are cited if the journal is available for free online it is cited even less evans attributes this phenomenon to more searching and less browsing which he feels eliminates marginally relevant articles that may have been found by browsing and the ability to follow links to see what other authors are citing he concludes that electronic journals have resulted in a narrowing of scientific citation patterns this brief article expands on the evidence cited by evans boyce et al tenopir et al based on the authors ongoing surveys of academic readers of scholarly articles reading patterns and citation patterns differ as faculty read many more articles than they ultimately cite and read for many purposes in addition to research and writing the number of articles read has steadily increased over the last three decades so the actual numbers of articles found by browsing has not decreased much even though the percentage of readings found by searching has increased readings from library provided electronic journals has increased substantially while readings of older articles have recently increased somewhat ironically reading patterns have broadened with electronic journals at the same time citing patterns narrowed
this paper argues that a scientific basis for presence as it s usually understood in virtual environments research can not be established on the basis of postexperience presence questionnaires alone to illustrate the point an arbitrary mental attribute called colorfulness of the experience is conjured up and a set of questions administered to respondents with an online questionnaire the results suggested that colorfulness of yesterday s experiences was associated with the extent to which a person accomplished their tasks and also associated with yesterday being a good pleasant but not frustrating day the meaning lessness of this analysis illustrates that the equivalent methodology used by presence researchers may similarly bring into being the idea of presence in the minds of ve participants however it is argued that there can be no evidence on this methodological basis that presence played any role in their actual mental activity or behavior at the time of the experience it is concluded that presence researchers must move away from heavy reliance on questionnaires in order to make any progress in area
this book is a sophisticated and insightful conceptualization of outcomes based learning developed from the concept of constructive alignment the first author has already made a significant contribution to the scholarship and practice of teaching and learning in universities together with the second author there is now added richness through the practical implementation and practices the ideas in this book are all tried and shown to contribute to more successful learning experience and outcome for students denise chalmers carrick institute of education australia teaching for quality learning at university focuses on implementing a constructively aligned outcomes based model at both classroom and institutional level the theory which is now used worldwide as a framework for good teaching and assessment is shown to assist university teachers who wish to improve the quality of their own teaching their students learning and their assessment of learning outcomes aid staff developers in providing support for teachers and provide a framework for administrators interested in quality assurance and enhancement of teaching across the whole university the book s how to approach addresses several important issues designing high level outcomes the learning activities most likely to achieve them in small and large classes and appropriate assessment and grading procedures it is an accessible jargon free guide to all university teachers interested in enhancing their teaching and their students learning and for administrators and teaching developers who are involved in teaching related decisions on an institution wide basis the authors have also included useful web links to material
abstract a web of linked rdf data may be enabled by standards specifying how links should be made in rdf and under what conditions they should be followed as well as powerful generic rdf browsers that can traverse an open web of rdf resources the tabulator is an rdf browser which is designed both for new users to provoke interest in the semantic web and give them a means to access and interact with the entire web of rdf data and for developers of rdf content to provide incentive for them to post their data in rdf to refine and promote rdf linking standards and to let providers see how their data interacts with the rest of the semantic web a challenge for semantic web browsers is to bring the power of domain specific applications to a generic program when new unexpected domains can be encountered in real time the tabulator project is an attempt to demonstrate and utilize the power of linked rdf data with a user friendly semantic web browser that is able to recognize and follow rdf links to other rdf resources based on the users exploration analysis
seasonal influenza epidemics are a major public health concern causing tens of millions of respiratory illnesses and to deaths worldwide each in addition to seasonal influenza a new strain of influenza virus against which no previous immunity exists and that demonstrates human to human transmission could result in a pandemic with millions of early detection of disease activity when followed by a rapid response can reduce the impact of both seasonal and pandemic one way to improve early detection is to monitor health seeking behaviour in the form of queries to online search engines which are submitted by millions of users around the world each day here we present a method of analysing large numbers of google search queries to track influenza like illness in a population because the relative frequency of certain queries is highly correlated with the percentage of physician visits in which a patient presents with influenza like symptoms we can accurately estimate the current level of weekly influenza activity in each region of the united states with a reporting lag of about one day this approach may make it possible to use search queries to detect influenza epidemics in areas with a large population of web users
the ability to directly visualize nanoscopic cellular structures and their spatial relationship in all three dimensions will greatly enhance our understanding of molecular processes in cells here we demonstrated multicolor three dimensional stochastic optical reconstruction microscopy storm as a tool to quantitatively probe cellular structures and their interactions to facilitate storm imaging we generated photoswitchable probes in several distinct colors by covalently linking a photoswitchable cyanine reporter and an activator molecule to assist bioconjugation we performed localization in conjunction with focal plane scanning and correction for refractive index mismatch to obtain whole cell images with a spatial resolution of nm and nm in the lateral and axial dimensions respectively using this approach we imaged the entire mitochondrial network in fixed monkey kidney bs c cells and studied the spatial relationship between mitochondria and microtubules the storm images resolved mitochondrial morphologies as well as mitochondria microtubule contacts that were obscured in conventional images
despite the obvious utility of the concept it has often been argued that time does not exist i take the opposite perspective let s imagine that time does exist and the universe is described by a quantum state obeying ordinary time dependent quantum mechanics reconciling this simple picture with the known facts about our universe turns out to be a non trivial task but by taking it seriously we can infer deep facts about the fundamental nature of reality the arrow of time finds a plausible explanation in a heraclitean universe described by a quantum state eternally evolving in an infinite dimensional space
we develop a general framework for performing large scale significance testing in the presence of arbitrarily strong dependence we derive a low dimensional set of random vectors called a dependence kernel that fully captures the dependence structure in an observed high dimensional dataset this result shows a surprising reversal of the curse of dimensionality in the high dimensional hypothesis testing setting we show theoretically that conditioning on a dependence kernel is sufficient to render statistical tests independent regardless of the level of dependence in the observed data this framework for multiple testing dependence has implications in a variety of common multiple testing problems such as in gene expression studies brain imaging and epidemiology
the paper studies the long tail problem of recommender systems when many items in the long tail have only few ratings thus making it hard to use them in recommender systems the approach presented in the paper splits the whole itemset into the head and the tail parts and clusters only the tail items then recommendations for the tail items are based on the ratings in these clusters and for the head items on the ratings of individual items if such partition and clustering are done properly we show that this reduces the recommendation error rates for the tail items while maintaining reasonable performance
basic content personalization consists in matching up the attributes of a user profile in which preferences and interests are stored with the attributes of a content object the web r evolution and the advent of user generated content have changed the game for personalization since the role of people has evolved from passive consumers of information to that of active contributors one of the forms of user generated content that has drawn more attention from the research community is folksonomy a taxonomy generated by users who collaboratively annotate and categorize resources of interests with freely chosen keywords tags
abstract background microarray experimentation requires the application of complex analysis methods as well as the use of non trivial computer technologies to manage the resultant large data sets this together with the proliferation of tools and techniques for microarray data analysis makes it very challenging for a laboratory scientist to keep up to date with the latest developments in this field our aim was to develop a distributed e support system for microarray data analysis and management results emaas extensible microarray analysis system is a multi user rich internet application ria providing simple robust access to up to date resources for microarray data storage and analysis combined with integrated tools to optimise real time user support and training the system leverages the power of distributed computing to perform microarray analyses and provides seamless access to resources located at various remote facilities the emaas framework allows users to import microarray data from several sources to an underlying database to pre process quality assess and analyse the data to perform functional analyses and to track data analysis steps through a single easy to use web portal this interface offers distance support to users both in the form of video tutorials and via live screen feeds using the web conferencing tool evo a number of analysis packages including r bioconductor and affymetrix power tools have been integrated on the server side and are available programmatically through the postgres plr library or on grid compute clusters integrated distributed resources include the functional annotation tool david genecards and the microarray data repositories geo celsius and mimir emaas currently supports analysis of affymetrix and exon expression arrays and the system is extensible to cater for other microarray and transcriptomic platforms conclusions emaas enables users to track and perform microarray data management and analysis tasks through a single easy to use web application the system architecture is flexible and scalable to allow new array types analysis algorithms and tools to be added with relative ease and to cope with large increases in volume
background the journal impact factor if is generally accepted to be a good measurement of the relevance quality of articles that a journal publishes in spite of an apparently homogenous peer review process for a given journal we hypothesize that the country affiliation of authors from developing latin american la countries affects the if of a journal detrimentally methodology principal findings seven prestigious international journals one multidisciplinary journal and six serving specific branches of science were examined in terms of their if in the web of science two subsets of each journal were then selected to evaluate the influence of author s affiliation on the if they comprised contributions i with authorship from four latin american la countries argentina brazil chile and mexico and ii with authorship from five developed countries england france germany japan and usa both subsets were further subdivided into two groups articles with authorship from one country only and collaborative articles with authorship from other countries articles from the five developed countries had if close to the overall if of the journals and the influence of collaboration on this value was minor in the case of la articles the effect of collaboration virtually all with developed countries was significant the ifs for non collaborative articles averaged of the overall if of the journals whereas the articles in collaboration raised the ifs to values close to the overall if conclusion significance the study shows a significantly lower if in the group of the subsets of non collaborative la articles and thus that country affiliation of authors from non developed la countries does affect the if of a journal detrimentally there are no data to indicate whether the lower ifs of la articles were due to their inherent inferior quality relevance or psycho social trend towards under citation of articles from these countries however further study is required since there are foreseeable consequences of this trend as it may stimulate strategies by editors to turn down articles that tend to be cited
the wellcome trust sanger institute is one of the world s largest genome centers and a substantial amount of our sequencing is performed with next generation massively parallel sequencing technologies in june the quantity of purity filtered sequence data generated by our genome analyzer illumina platforms reached terabase and our average weekly illumina production output is currently gigabases here we describe a set of improvements we have made to the standard illumina protocols to make the library preparation more reliable in a high throughput environment to reduce bias tighten insert size distribution and reliably obtain high yields data
pnas glutathione gsh significantly declines in the aging rat liver because gsh levels are partly a reflection of its synthetic capacity we measured the levels and activity of glutamylcysteine ligase gcl the rate controlling enzyme in gsh synthesis with age both the catalytic gclc and modulatory gclm subunits of gcl decreased by and respectively concomitant with lower subunit levels gcl activity also declined by because nuclear factor related factor governs basal and inducible gclc and gclm expression by means of the antioxidant response element are we hypothesized that aging results in dysregulation of mediated gcl expression we observed an age related loss in total and nuclear levels which suggests attenuation in dependent gene transcription by using gel shift and supershift assays a marked reduction in are binding in old vs young rats was noted to determine whether the constitutive loss of transcriptional activity also affects the inducible nature of nuclear translocation old rats were treated with lipoic acid la mg kg i p up to h a disulfide compound shown to induce activation and improve gsh levels la administration increased nuclear levels in old rats after h la also induced binding to the are and consequently higher gclc levels and gcl activity were observed h after la injection thus the age related loss in gsh synthesis may be caused by dysregulation of are mediated gene expression but chemoprotective agents like la can attenuate loss
background the decrease in cellular immunity with aging is of considerable public health importance recent studies suggest that the redox equilibrium of dendritic cells dcs is a key factor in maintaining protective cellular immunity and that a disturbance of this homeostatic mechanism could contribute to immune senescence objectives we sought to elucidate the role of dc redox equilibrium in the decrease of contact hypersensitivity chs and t h immunity during aging and to determine how restoration of glutathione gsh levels by the mediated antioxidant defense pathway affects this decrease methods we assessed the effect of deficiency and boosting of gsh levels by the agonist sulforaphane or the thiol precursor n acetyl cysteine nac on the chs response to contact antigens in old mice we studied the effect of sfn and nac on restoring t h immunity by treating dcs ex vivo before adoptive transfer and in vivo challenge results aging was associated with a decreased chs response that was accentuated by deficiency systemic sfn treatment reversed this decrease through mediated antioxidant enzyme expression and gsh synthesis adoptive transfer of dcs from old animals induced a weakened chs response in recipient animals treatment of dcs from old animals with sfn or nac ex vivo restored the in vivo challenge response conclusion sfn and nac upregulate t h immunity in aging through a restoration of equilibrium
we propose a novel hybrid recommendation model in which user preferences and item features are described in terms of semantic concepts defined in domain ontologies the exploitation of meta information describing the recommended items and user profiles in a general portable way along with the capability of inferring knowledge from the relations defined in the ontologies are the key aspects of the presented proposal taking advantage of the enhanced semantics representation user profiles are compared at a finer grain size than they are in usual recommender systems more specifically the concept item and user spaces are clustered in a coordinated way and the resulting clusters are used to find similarities among individuals at multiple semantic layers such layers correspond to implicit communities of interest coi and enable collaborative recommendations of enhanced precision our approach is tested in two sets of experiments one including profiles manually defined by real users and another with automatically generated profiles based on data from the imdb and datasets
we have developed a comprehensive gene orientated phylogenetic resource ensemblcompara genetrees based on a computational pipeline to handle clustering multiple alignment and tree generation including the handling of large gene families we developed two novel non sequence based metrics of gene tree correctness and benchmarked a number of tree methods the treebest method from treefam shows the best performance in our hands we also compared this phylogenetic approach to clustering approaches for ortholog prediction showing a large increase in coverage using the phylogenetic approach all data are made available in a number of formats and will be kept up to date with the project
background the identification of transcription factor binding sites is difficult since they are only a small number of nucleotides in size resulting in large numbers of false positives and false negatives in current approaches computational methods to reduce false positives are to look for over representation of transcription factor binding sites in a set of similarly regulated promoters or to look for conservation in orthologous promoter alignments results we have developed a novel tool conserved and over represented transcription factor binding sites that identifies common transcription factor binding sites in promoters of co regulated genes to improve upon existing binding site predictions the tool searches for position weight matrices from the transfac r database that are over represented in an experimental set compared to a random set of promoters and identifies cross species conservation of the predicted transcription factor binding sites the algorithm has been evaluated with expression and chromatin immunoprecipitation on microarray data we also implement and demonstrate the importance of matching the random set of promoters to the experimental promoters by gc content which is a unique feature of our tool conclusion the program is accessible in a user friendly web interface at http www lgtc nl it provides a table of over represented transcription factor binding sites in the users input genes promoters and a graphical view of evolutionary conserved transcription factor binding sites in our test data sets it successfully predicts target transcription factors and their sites
neurons in visual cortex are linked by an extensive network of lateral connections to study the effect of these connections on neural responses we recorded spikes and local field potentials lfps from multi electrode arrays that were implanted in monkey and cat primary visual cortex spikes at each location generated outward traveling lfp waves when the visual stimulus was absent or had low contrast these lfp waves had large amplitudes and traveled over long distances their effect was strong lfp traces at any site could be predicted by the superposition of waves that were evoked by spiking in a approximately mm radius as stimulus contrast increased both the magnitude and the distance traveled by the waves progressively decreased we conclude that the relative weight of feedforward and lateral inputs in visual cortex is not fixed but rather depends on stimulus contrast lateral connections dominate at low contrast when spatial integration of signals is perhaps beneficial
background epidemiologic studies show that exposure to ambient particulate matter leads to asthma exacerbation diesel exhaust particles deps a model pollutant act as an adjuvant for allergic sensitization increasing evidence shows that this effect could be mediated by an effect on dendritic cells dcs objective our aim was to elucidate the mechanism by which pro oxidative dep chemicals change dc function so that these antigen presenting cells strengthen the immune response to an experimental allergen methods we exposed murine bone marrow derived dcs and a homogeneous myeloid dc line to deps and organic extracts made from these particles to determine how the induction of oxidative stress affects cellular maturation cytokine production and activation of antigen specific t cells results dep extracts induced oxidative stress in dcs this change in redox equilibrium interfered in the ability of toll like receptor agonists to induce the expression of maturation receptors eg and i a d and il production this perturbation of dc function was accompanied by decreased ifn gamma and increased il induction in antigen specific t cells the molecular basis for the perturbation of dc function is the activation of a nuclear factor erythroid nf related factor mediated signaling pathway that suppresses il production nf related factor deficiency abrogates the perturbation of dc function by deps conclusion these data provide the first report that pro oxidative dep chemicals can interfere in t h promoting response pathways in a homogeneous dc population and provide a novel explanation for the adjuvant effect of deps on allergic inflammation clinical implications these data clarify the adjuvant effect of particulate air pollutants in allergic disease
amigo is a web application that allows users to query browse and visualize ontologies and related gene product annotation association data amigo can be used online at the gene ontology go website to access the data provided by the go it can also be downloaded and installed to browse local ontologies and annotations amigo is free open source software developed and maintained by the go consortium availability http amigo geneontology org download http sourceforge net projects geneontology contact sjcarbon berkeleybop bioinformatics
background there is accumulating evidence that the milieu of repeat elements and other non genic sequence features at a given chromosomal locus here defined as the genome environment can play an important role in regulating chromosomal processes such as transcription replication and recombination the availability of whole genome sequences has allowed us to annotate the genome environment of any locus in detail the development of genome wide experimental analyses of gene expression chromatin modification and chromatin proteins means that it is now possible to identify potential links between chromosomal processes and the underlying genome environment there is a need for novel bioinformatic tools that facilitate these studies results we developed the genome environment browser geb in order to visualise the integration of experimental data from large scale high throughput analyses with repeat sequence features that define the local genome environment the browser has incorporated dynamic scales adjustable in real time which enables scanning of large regions of the genome as well as detailed investigation of local regions on the same page without the need to load new pages the interface also accommodates a dimensional display of repetitive features which vary substantially in size such as line repeats specific queries for preliminary quantitative analysis of genome features can also be formulated results of which can be exported for further analysis conclusion the genome environment browser is a versatile program which can be easily adapted for displaying all types of genome data with known genomic coordinates it is currently available at http web bioinformatics ic ac geb
background the analysis of gene sets has become a popular topic in recent times with researchers attempting to improve the interpretability and reproducibility of their microarray analyses through the inclusion of supplementary biological information while a number of options for gene set analysis exist no consensus has yet been reached regarding which methodology performs best and under what conditions the goal of this work was to examine the performance characteristics of a collection of existing gene set analysis methods on both simulated and real microarray data sets of particular interest was the potential utility gained through the incorporation of inter gene correlation into the analysis process results each of six gene set analysis methods was applied to both simulated and publicly available microarray data sets overall the various methodologies were all found to be better at detecting gene sets that moved from non active i e genes not expressed to active states or vice versa rather than those that simply changed their level of activity methods which incorporate correlation structures were found to provide increased ability to detect altered gene sets in some settings conclusion based on the results obtained through the analysis of simulated data it is clear that the performance of gene set analysis methods is strongly influenced by the features of the data set in question and that methods which incorporate correlation structures into the analysis process tend to achieve better performance relative to methods which rely on univariate statistics
background micrornas mirnas are important regulators of gene expression and have been implicated in development differentiation and pathogenesis hundreds of mirnas have been discovered in mammalian genomes approximately of mammalian mirnas are expressed from introns of protein coding genes the primary transcript pri mirna is therefore assumed to be the host transcript however very little is known about the structure of pri mirnas expressed from intergenic regions here we annotate transcript boundaries of mirnas in human mouse and rat genomes using various transcription features the end of the pri mirna is predicted from transcription start sites cpg islands and cage tags mapped in the upstream flanking region surrounding the precursor mirna pre mirna the end of the pri mirna is predicted based on the mapping of polya signals and supported by cdna est and ditags data the predicted pri mirnas are also analyzed for promoter and insulator associated regulatory regions results we define sets of conserved and non conserved human mouse and rat pre mirnas using bidirectional blast and synteny analysis transcription features in their flanking regions are used to demarcate the and boundaries of the pri mirnas the lengths and boundaries of primary transcripts are highly conserved between orthologous mirnas a significant fraction of pri mirnas have lengths between and with very few introns we annotate a total of pri mirna structures which include pre mirnas pri mirnas are conserved in all species in total of the confidently annotated transcripts express more than one pre mirna the upstream regions of of the predicted pri mirnas are found to be associated with promoter and insulator regulatory sequences conclusions little is known about the primary transcripts of intergenic mirnas using comparative data we are able to identify the boundaries of a significant proportion of human mouse and rat pri mirnas we confidently predict the transcripts including a total of and human mouse and rat pre mirnas respectively our computational annotations provide a basis for subsequent experimental validation of predicted mirnas
abstract background complexity is a key problem when visualizing biological networks as the number of entities increases most graphical views become incomprehensible our goal is to enable many thousands of entities to be visualized meaningfully and with high performance results we present a new visualization tool which introduces a new concept of staggered layers in space related data such as proteins chemicals or pathways can be grouped onto separate layers and arranged via layout algorithms such as fruchterman reingold distance geometry and a novel hierarchical layout data on a layer can be clustered via k means affinity propagation markov clustering neighbor joining hierarchical clustering or upgma unweighted pair group method with arithmetic mean a simple input format defines the name and url for each node and defines connections or similarity scores between pairs of nodes the use of is illustrated with datasets related to huntington s disease conclusion is a user friendly visualization tool that is able to visualize biological or any other network in space it is free for academic use and runs on any platform it can be downloaded or launched directly from http org library and java need to be pre installed for the software run
functional analysis of large gene lists derived in most cases from emerging high throughput genomic proteomic and bioinformatics scanning approaches is still a challenging and daunting task the gene annotation enrichment analysis is a promising high throughput strategy that increases the likelihood for investigators to identify biological processes most pertinent to their study approximately bioinformatics enrichment tools that are currently available in the community are collected in this survey tools are uniquely categorized into three major classes according to their underlying enrichment algorithms the comprehensive collections unique tool classifications and associated questions issues will provide a more comprehensive and up to date view regarding the advantages pitfalls and recent trends in a simpler tool class level rather than by a tool by tool approach thus the survey will help tool designers developers and experienced end users understand the underlying algorithms and pertinent details of particular tool categories tools enabling them to make the best choices for their particular interests
micrornas mirnas are important regulators of eukaryotic gene expression they have been implicated in a broad range of biological processes and mirna related genetic alterations probably underlie more human diseases than currently appreciated several studies have identified genetic variants in mirna target sites that are claimed to be associated with disorders ranging from parkinson s disease to cancer however careful assessment of these studies indicates that very few provide a combination of rigorous genetic and functional evidence we therefore suggest a set of concrete recommendations to guide future investigations specifically we highlight the importance of unbiased association studies and follow up functional experiments for providing a clearer picture of the extent to which microrna target site variations are relevant in various diseases
gr micrornas mirnas have been shown to play important roles in physiological as well as multiple malignant processes including acute myeloid leukemia aml in an effort to gain further insight into the role of mirnas in aml we have applied the illumina massively parallel sequencing platform to carry out an in depth analysis of the mirna transcriptome in a murine leukemia progression model this model simulates the stepwise conversion of a myeloid progenitor cell by an engineered overexpression of the nucleoporin homeobox fusion gene to aggressive aml inducing cells upon transduction with the oncogenic collaborator from this data set we identified mirna mirna species in the cells and mirna mirna species in cells corresponding to and mirna genes sequence counts varied between two and indicating a remarkable expression range between the detected mirna species the large number of mirnas expressed and the nature of differential expression suggest that leukemic progression as modeled here is dictated by the repertoire of shared but differentially expressed mirnas our finding of extensive sequence variations isomirs for almost all mirna and mirna species adds additional complexity to the mirna transcriptome a stringent target prediction analysis coupled with in vitro target validation revealed the potential for mirna mediated release of oncogenes that facilitates leukemic progression from the preleukemic to leukemia inducing state finally novel mirnas species were identified in our data set adding further complexity to the emerging world of rnas
the assumption that rna can be readily classified into either protein coding or non proteincoding categories has pervaded biology for close to years until recently discrimination between these two categories was relatively straightforward most transcripts were clearly identifiable as protein coding messenger rnas mrnas and readily distinguished from the small number of well characterized non proteincoding rnas ncrnas such as transfer ribosomal and spliceosomal rnas recent genome wide studies have revealed the existence of thousands of noncoding transcripts whose function and significance are unclear the discovery of this hidden transcriptome and the implicit challenge it presents to our understanding of the expression and regulation of genetic information has made the need to distinguish between mrnas and ncrnas both more pressing and more complicated in this review we consider the diverse strategies employed to discriminate between protein coding and noncoding transcripts and the fundamental difficulties that are inherent in what may superficially appear to be a simple problem misannotations can also run in both directions some ncrnas may actually encode peptides and some of those currently thought to do so may not moreover recent studies have shown that some rnas can function both as mrnas and intrinsically as functional ncrnas which may be a relatively widespread phenomenon we conclude that it is difficult to annotate an rna unequivocally as protein coding or noncoding with overlapping protein coding and noncoding transcripts further confounding this distinction in addition the finding that some transcripts can function both intrinsically at the rna level and to encode proteins suggests a false dichotomy between mrnas and ncrnas therefore the functionality of any transcript at the rna level should not discounted
genomic instability and alterations in gene expression are hallmarks of eukaryotic aging the yeast histone deacetylase silences transcription and stabilizes repetitive dna but during aging or in response to a dna break the sir complex relocalizes to sites of genomic instability resulting in the desilencing of genes that cause sterility a characteristic of yeast aging using embryonic stem cells we show that mammalian represses repetitive dna and a functionally diverse set of genes across the mouse genome in response to dna damage dissociates from these loci and relocalizes to dna breaks to promote repair resulting in transcriptional changes that parallel those in the aging mouse brain increased expression promotes survival in a mouse model of genomic instability and suppresses age dependent transcriptional changes thus dna damage induced redistribution of and other chromatin modifying proteins may be a conserved mechanism of aging eukaryotes
altered abundance of several intrinsically unstructured proteins iups has been associated with perturbed cellular signaling that may lead to pathological conditions such as cancer therefore it is important to understand how cells precisely regulate the availability of iups we observed that regulation of transcript clearance proteolytic degradation and translational rate contribute to controlling the abundance of iups some of which are present in low amounts and for short periods of time abundant phosphorylation and low stochasticity in transcription and translation indicate that the availability of iups can be finely tuned fidelity in signaling may require that most iups be available in appropriate amounts and not present longer needed
in plants and mammals small rnas indirectly mediate epigenetic inheritance by specifying cytosine methylation we found that small rnas themselves serve as vectors for epigenetic information crosses between drosophila strains that differ in the presence of a particular transposon can produce sterile progeny a phenomenon called hybrid dysgenesis this phenotype manifests itself only if the transposon is paternally inherited suggesting maternal transmission of a factor that maintains fertility in both p and i element mediated hybrid dysgenesis models daughters show a markedly different content of piwi interacting rnas pirnas targeting each element depending on their parents of origin such differences persist from fertilization through adulthood this indicates that maternally deposited pirnas are important for mounting an effective silencing response and that a lack of maternal pirna inheritance underlies dysgenesis
proteins interact in complex protein protein interaction ppi networks whose topological propertiessuch as scale free topology hierarchical modularity and dissortativityhave suggested models of network evolution currently preferred models invoke preferential attachment or gene duplication and divergence to produce networks whose topology matches that observed for real ppis thus supporting these as likely models for network evolution here we show that the interaction density and homodimeric frequency are highly protein age dependent in real ppi networks in a manner which does not agree with these canonical models in light of these results we propose an alternative stochastic model which adds each protein sequentially to a growing network in a manner analogous to protein crystal growth cg in solution the key ideas are interaction probability increases with availability of unoccupied interaction surface thus following an anti preferential attachment rule as a network grows highly connected sub networks emerge into protein modules or complexes and once a new protein is committed to a module further connections tend to be localized within that module the cg model produces ppi networks consistent in both topology and age distributions with real ppi networks and is well supported by the spatial arrangement of protein complexes of known d structure suggesting a plausible physical mechanism for evolution
the rapidly evolving field of metabolomics aims at a comprehensive measurement of ideally all endogenous metabolites in a cell or body fluid it thereby provides a functional readout of the physiological state of the human body genetic variants that associate with changes in the homeostasis of key lipids carbohydrates or amino acids are not only expected to display much larger effect sizes due to their direct involvement in metabolite conversion modification but should also provide access to the biochemical context of such variations in particular when enzyme coding genes are concerned to test this hypothesis we conducted what is to the best of our knowledge the first gwa study with metabolomics based on the quantitative measurement of metabolites in serum of male participants of the kora study we found associations of frequent single nucleotide polymorphisms snps with considerable differences in the metabolic homeostasis of the human body explaining up to of the observed variance using ratios of certain metabolite concentrations as a proxy for enzymatic activity up to of the variance can be explained p values to we identified four genetic variants in genes coding for enzymes lipc scad mcad where the corresponding metabolic phenotype metabotype clearly matches the biochemical pathways in which these enzymes are active our results suggest that common genetic polymorphisms induce major differentiations in the metabolic make up of the human population this may lead to a novel approach to personalized health care based on a combination of genotyping and metabolic characterization these genetically determined metabotypes may subscribe the risk for a certain medical phenotype the response to a given drug treatment or the reaction to a nutritional intervention or challenge
although it has been tacitly assumed that the hippocampus exerts an influence on neocortical networks the mechanisms of this process are not well understood we examined whether and how hippocampal theta oscillations affect neocortical assembly patterns by recording populations of single cells and transient gamma oscillations in multiple cortical regions including the somatosensory area and prefrontal cortex in behaving rats and mice laminar analysis of neocortical gamma bursts revealed multiple gamma oscillators of varying frequency and location which were spatially confined and synchronized local groups of neurons a significant fraction of putative pyramidal cells and interneurons as well as localized gamma oscillations in all recorded neocortical areas were phase biased by the hippocampal theta rhythm we hypothesize that temporal coordination of neocortical gamma oscillators by hippocampal theta is a mechanism by which information contained in spatially widespread neocortical assemblies can besynchronously transferred to the associative networks of hippocampus
nuclear erythroid related factor a redox sensitive transcription factor is involved in transcriptional regulation of many antioxidant genes including glutamate cysteine ligase gcl cigarette smoke cs is known to cause oxidative stress and deplete glutathione gsh levels in alveolar epithelial cells we hypothesized that resveratrol a polyphenolic phytoalexin has antioxidant signaling properties by inducing gsh biosynthesis via the activation of and protects lung epithelial cells against cs mediated oxidative stress treatment of human primary small airway epithelial and human alveolar epithelial cells with cs extract cse dose dependently decreased gsh levels and gcl activity effects that were associated with enhanced production of reactive oxygen species resveratrol restored cse depleted gsh levels by upregulation of gcl via activation of and also quenched cse induced release of reactive oxygen species interestingly cse failed to induce nuclear translocation of in and small airway epithelial cells on the contrary was localized in the cytosol of alveolar and airway epithelial cells due to cse mediated posttranslational modifications such as aldehyde carbonyl adduct formation and nitration on the other hand resveratrol attenuated cse mediated modifications thereby inducing its nuclear translocation associated with gcl gene transcription as demonstrated by gcl promoter reporter and small interfering rna approaches thus resveratrol attenuates cse mediated gsh depletion by inducing gsh synthesis and protects epithelial cells by reversing cse induced posttranslational modifications of these data may have implications in dietary modulation of antioxidants in treatment of chronic obstructive pulmonary ajplung
the aryl hydrocarbon receptor ahr is a ligand activated transcription factor with important roles in metabolic adaptation normal physiology and dioxin toxicology metabolic adaptation is based on coordinate regulation of a set of xenobiotic metabolizing enzymes xmes termed ahr battery coordination is achieved by ahr arnt binding to xres xenobiotic response elements identified in the upstream region of ahr target genes the ahr battery encodes phase i and ii enzymes interestingly these phase ii genes are linked to the gene battery that encodes enzymes that are essential in protection against oxidative electrophile stress binds to ares antioxidant response elements in the regulatory region of a large and distinct set of target genes functionally characterized response elements such as xres and ares in the regulatory region of target genes may provide a genetic basis to understand ahr and induced genes linkage between ahr and batteries is probably achieved by multiple mechanisms including as a target gene of the ahr indirect activation of via generated reactive oxygen species and direct cross interaction of ahr xre and are signaling linkage appears to be species and cell dependent however mechanisms linking xre and are controlled phase ii genes need further investigation tightened coupling between phases i and ii by ahr and induced xmes may greatly attenuate health risks posed by generated toxic intermediates and reactive oxygen species better recognition of coordinate phase i and ii metabolisms may improve risk assessment of reactive toxic intermediates in the extrapolation to low level endo and exposure
the human distal gut harbours a vast ensemble of microbes the microbiota that provide important metabolic capabilities including the ability to extract energy from otherwise indigestible dietary studies of a few unrelated healthy adults have revealed substantial diversity in their gut communities as measured by sequencing rrna yet how this diversity relates to function and to the rest of the genes in the collective genomes of the microbiota the gut microbiome remains obscure studies of lean and obese mice suggest that the gut microbiota affects energy balance by influencing the efficiency of calorie harvest from the diet and how this harvested energy is used and here we characterize the faecal microbial communities of adult female monozygotic and dizygotic twin pairs concordant for leanness or obesity and their mothers to address how host genotype environmental exposure and host adiposity influence the gut microbiome analysis of individuals yielded near full length and partial bacterial rrna sequences plus gigabases from their microbiomes the results reveal that the human gut microbiome is shared among family members but that each person s gut microbial community varies in the specific bacterial lineages present with a comparable degree of co variation between adult monozygotic and dizygotic twin pairs however there was a wide array of shared microbial genes among sampled individuals comprising an extensive identifiable core microbiome at the gene rather than at the organismal lineage level obesity is associated with phylum level changes in the microbiota reduced bacterial diversity and altered representation of bacterial genes and metabolic pathways these results demonstrate that a diversity of organismal assemblages can nonetheless yield a core microbiome at a functional level and that deviations from this core are associated with different physiological states obese compared lean
we propose a link between logical independence and quantum physics we demonstrate that quantum systems in the eigenstates of pauli group operators are capable of encoding mathematical axioms and show that pauli group quantum measurements are capable of revealing whether or not a given proposition is logically dependent on the axiomatic system whenever a mathematical proposition is logically independent of the axioms encoded in the measured state the measurement associated with the proposition gives random outcomes this allows for an experimental test of logical independence conversely it also allows for an explanation of the probabilities of random outcomes observed in pauli group measurements from logical independence without invoking quantum theory the axiomatic systems we study can be completed and are therefore not subject to goedel s theorem
ldquocloudrdquo computing a relatively recent term builds on decades of research in virtualization distributed computing utility computing and more recently networking web and software services it implies a service oriented architecture reduced information technology overhead for the end user great flexibility reduced total cost of ownership on demand services and many other things this paper discusses the concept of ldquocloudrdquo computing issues it tries to address related research topics and a ldquocloudrdquo implementation today
climate change creates new challenges for biodiversity conservation species ranges and ecological dynamics are already responding to recent climate shifts and current reserves will not continue to support all species they were designed to protect these problems are exacerbated by other global changes scholarly articles recommending measures to adapt conservation to climate change have proliferated over the last years we systematically reviewed this literature to explore what potential solutions it has identified and what consensus and direction it provides to cope with climate change several consistent recommendations emerge for action at diverse spatial scales requiring leadership by diverse actors broadly adaptation requires improved regional institutional coordination expanded spatial and temporal perspective incorporation of climate change scenarios into all planning and action and greater effort to address multiple threats and global change drivers simultaneously in ways that are responsive to and inclusive of human communities however in the case of many recommendations the how by whom and under what conditions they can be implemented is not specified we synthesize recommendations with respect to three likely conservation pathways regional planning site scale management and modification of existing conservation plans we identify major gaps including the need for more specific operational examples of adaptation principles that are consistent with unavoidable uncertainty about the future a practical adaptation planning process to guide selection and integration of recommendations into existing policies and programs and greater integration of social science into an endeavor that although dominated by ecology increasingly recommends extension beyond reserves and into human landscapes
diabetes and obesity are two metabolic diseases characterized by insulin resistance and a low grade inflammation seeking an inflammatory factor causative of the onset of insulin resistance obesity and diabetes we have identified bacterial lipopolysaccharide lps as a triggering factor we found that normal endotoxemia increased or decreased during the fed or fasted state respectively on a nutritional basis and that a week high fat diet chronically increased plasma lps concentration two to three times a threshold that we have defined as metabolic endotoxemia importantly a high fat diet increased the proportion of an lps containing microbiota in the gut when metabolic endotoxemia was induced for weeks in mice through continuous subcutaneous infusion of lps fasted glycemia and insulinemia and whole body liver and adipose tissue weight gain were increased to a similar extent as in high fat fed mice in addition adipose tissue positive cells and markers of inflammation and liver triglyceride content were increased furthermore liver but not whole body insulin resistance was detected in lps infused mice mutant mice resisted most of the lps and high fat diet induced features of metabolic diseases this new finding demonstrates that metabolic endotoxemia dysregulates the inflammatory tone and triggers body weight gain and diabetes we conclude that the lps system sets the tone of insulin sensitivity and the onset of diabetes and obesity lowering plasma lps concentration could be a potent strategy for the control of diseases
abstract the analysis and interpretation of relationships between biological molecules networks and concepts is becoming a major bottleneck in systems biology very often the pure amount of data and their heterogeneity provides a challenge for the visualization of the data a wide variety of graph representations is available which most often map the data on graphs to visualize biological interactions these methods are applicable to a wide range of problems nevertheless many of them reach a limit in terms of user friendliness when thousands of nodes and connections have to be analyzed and visualized in this study we are reviewing visualization tools that are currently available for visualization of biological networks mainly invented over the past five years we comment on the functionality the limitations and the specific strengths of these tools and how these tools could be further developed in the direction of data integration and sharing
rsif the formal description of experiments for efficient analysis annotation and sharing of results is a fundamental part of the practice of science ontologies are required to achieve this objective a few subject specific ontologies of experiments currently exist however despite the unity of scientific experimentation no general ontology of experiments exists we propose the ontology expo to meet this need expo links the sumo the suggested upper merged ontology with subject specific ontologies of experiments by formalizing the generic concepts of experimental design methodology and results representation expo is expressed in the standard ontology language owl dl we demonstrate the utility of expo and its ability to describe different experimental domains by applying it to two experiments one in high energy physics and the other in phylogenetics the use of expo made the goals and structure of these experiments more explicit revealed ambiguities and highlighted an unexpected similarity we conclude that expo is of general value in describing experiments and a step towards the formalization science
the current issue of nucleic acids research includes descriptions of databases of which are new these databases along with several molecular biology databases described in other journals have been included in the nucleic acids research online molecular biology database collection bringing the total number of databases in the collection to in this introductory comment we briefly describe some of these new databases and review the principles guiding the selection of databases for inclusion in the nucleic acids research annual database issue and the nucleic acids research online molecular biology database collection the complete database list and summaries are available online at the nucleic acids research web site http nar oxfordjournals nar
quantum computers can execute algorithms that dramatically outperform classical computation as the best known example shor discovered an efficient quantum algorithm for factoring integers whereas factoring appears to be difficult for classical computers understanding what other computational problems can be solved significantly faster using quantum algorithms is one of the major challenges in the theory of quantum computation and such algorithms motivate the formidable task of building a large scale quantum computer this article reviews the current state of quantum algorithms focusing on algorithms with superpolynomial speedup over classical computation and in particular on problems with an flavor
the concept of an individual swapping his or her body with that of another person has captured the imagination of writers and artists for decades although this topic has not been the subject of investigation in science it exemplifies the fundamental question of why we have an ongoing experience of being located inside our bodies here we report a perceptual illusion of body swapping that addresses directly this issue manipulation of the visual perspective in combination with the receipt of correlated multisensory information from the body was sufficient to trigger the illusion that another person s body or an artificial body was one s own this effect was so strong that people could experience being in another person s body when facing their own body and shaking hands with it our results are of fundamental importance because they identify the perceptual processes that produce the feeling of ownership of one body
there are many thousands of hereditary diseases in humans each of which has a specific combination of phenotypic features but computational analysis of phenotypic data has been hampered by lack of adequate computational data structures therefore we have developed a human phenotype ontology hpo with over terms representing individual phenotypic anomalies and have annotated all clinical entries in online mendelian inheritance in man with the terms of the hpo we show that the hpo is able to capture phenotypic similarities between diseases in a useful and highly fashion
pnas in bacterial cells the peptidoglycan cell wall is the stress bearing structure that dictates cell shape although many molecular details of the composition and assembly of cell wall components are known how the network of peptidoglycan subunits is organized to give the cell shape during normal growth and how it is reorganized in response to damage or environmental forces have been relatively unexplored in this work we introduce a quantitative physical model of the bacterial cell wall that predicts the mechanical response of cell shape to peptidoglycan damage and perturbation in the rod shaped gram negative bacterium to test these predictions we use time lapse imaging experiments to show that damage often manifests as a bulge on the sidewall coupled to large scale bending of the cylindrical cell wall around the bulge our physical model also suggests a surprising robustness of cell shape to peptidoglycan defects helping explain the observed porosity of the cell wall and the ability of cells to grow and maintain their shape even under conditions that limit peptide crosslinking finally we show that many common bacterial cell shapes can be realized within the same model via simple spatial patterning of peptidoglycan defects suggesting that minor patterning changes could underlie the great diversity of shapes observed in the kingdom
summary as random shotgun metagenomic projects proliferate and become the dominant source of publicly available sequence data procedures for the best practices in their execution and analysis become increasingly important based on our experience at the joint genome institute we describe the chain of decisions accompanying a metagenomic project from the viewpoint of the bioinformatic analysis step by step we guide the reader through a standard workflow for a metagenomic project beginning with presequencing considerations such as community composition and sequence data type that will greatly influence downstream analyses we proceed with recommendations for sampling and data generation including sample and metadata collection community profiling construction of shotgun libraries and sequencing strategies we then discuss the application of generic sequence processing steps read preprocessing assembly and gene prediction and annotation to metagenomic data sets in contrast to genome projects different types of data analyses particular to metagenomes are then presented including binning dominant population analysis and gene centric analysis finally data management issues are presented and discussed we hope that this review will assist bioinformaticians and biologists in making better informed decisions on their journey during a metagenomic mmbr
in theoretical ecology simple stochastic models that satisfy two basic conditions about the distribution of niche values and feeding ranges have proved successful in reproducing the overall structural properties of real food webs using species richness and connectance as the only input parameters recently more detailed models have incorporated higher levels of constraint in order to reproduce the actual links observed in real food webs here building on previous stochastic models of consumerresource interactions between species we propose a highly parsimonious model that can reproduce the overall bipartite structure of cooperative partnerpartner interactions as exemplified by plantanimal mutualistic networks our stochastic model of bipartite cooperation uses simple specialization and interaction rules and only requires three empirical input parameters we test the bipartite cooperation model on ten large pollination data sets that have been compiled in the literature and find that it successfully replicates the degree distribution nestedness and modularity of the empirical networks these properties are regarded as key to understanding cooperation in mutualistic networks we also apply our model to an extensive data set of two classes of company engaged in joint production in the garment industry using the same metrics we find that the network of manufacturercontractor interactions exhibits similar structural patterns to plantanimal pollination networks this surprising correspondence between ecological and organizational networks suggests that the simple rules of cooperation that generate bipartite networks may be generic and could prove relevant in many different domains ranging from biological systems to society
background conserved non coding sequences in the human genome are approximately tenfold more abundant than known genes and have been hypothesized to mark the locations of cis regulatory elements however the global contribution of conserved non coding sequences to the transcriptional regulation of human genes is currently unknown deeply conserved elements shared between humans and teleost fish predominantly flank genes active during morphogenesis and are enriched for positive transcriptional regulatory elements however such deeply conserved elements account for of the conserved non coding sequences in the human genome which are predominantly mammalian results we explored the regulatory potential of a large sample of these common conserved non coding sequences using a variety of classic assays including chromatin remodeling and enhancer repressor and promoter activity when tested across diverse human model cell types we find that the fraction of experimentally active conserved non coding sequences within any given cell type is low approximately and that this proportion increases only modestly when considered collectively across cell types conclusions the results suggest that classic assays of cis regulatory potential are unlikely to expose the functional potential of the substantial majority of mammalian conserved non coding sequences in the genome
abstract background oma is a project that aims to identify orthologs within publicly available complete genomes with genomes analyzed to date oma is one of the largest projects of its kind results the algorithm of oma improves upon standard bidirectional best hit approach in several respects it uses evolutionary distances instead of scores considers distance inference uncertainty includes many to many orthologous relations and accounts for differential gene losses herein we describe in detail the algorithm for inference of orthology and provide the rationale for parameter selection through multiple tests conclusions oma contains several novel improvement ideas for orthology inference and provides a unique dataset of large scale assignments
recent progress in massively parallel sequencing platforms has enabled genome wide characterization of dna associated proteins using the combination of chromatin immunoprecipitation and sequencing chip seq although a variety of methods exist for analysis of the established alternative chip microarray chip chip few approaches have been described for processing chip seq data to fill this gap we propose an analysis pipeline specifically designed to detect protein binding positions with high accuracy using previously reported data sets for three transcription factors we illustrate methods for improving tag alignment and correcting for background signals we compare the sensitivity and spatial precision of three peak detection algorithms with published methods demonstrating gains in spatial precision when an asymmetric distribution of tags on positive and negative strands is considered we also analyze the relationship between the depth of sequencing and characteristics of the detected binding positions and provide a method for estimating the sequencing depth necessary for a desired coverage of protein sites
motivation the elucidation of biological pathways enriched with differentially expressed genes has become an integral part of the analysis and interpretation of microarray data several statistical methods are commonly used in this context but the question of the optimal approach has still not been resolved results we present a logistic regression based method lrpath for identifying predefined sets of biologically related genes enriched with or depleted of differentially expressed transcripts in microarray experiments we functionally relate the odds of gene set membership with the significance of differential expression and calculate adjusted p values as a measure of statistical significance the new approach is compared to fisher s exact test and other relevant methods in a simulation study and in the analysis of two breast cancer datasets overall results were concordant between the simulation study and the experimental data analysis and provide useful information to investigators seeking to choose the appropriate method lrpath displayed robust behavior and improved statistical power compared to tested alternatives it is applicable in experiments involving two or more sample types and accepts significance statistics of the investigator s choice as input availability an r function implementing lrpath can be downloaded from http uc edu lrpath contact mario medvedovic uc edu supplementary information supplementary data are available at bioinformatics online and at http uc edu bioinformatics
insulators are dna elements that prevent inappropriate interactions between the neighboring regions of the genome they can be functionally classified as either enhancer blockers or domain barriers ctcf ccctc binding factor is the only known major insulator binding protein in the vertebrates and has been shown to bind many enhancer blocking elements however it is not clear whether it plays a role in chromatin domain barriers between active and repressive domains here we used chip seq to map the genome wide binding sites of ctcf in three cell types and identified significant binding of ctcf to the boundaries of repressive chromatin domains marked by although we find an extensive overlapping of ctcf binding sites across the three cell types its association with the domain boundaries is cell type specific we further show that the nucleosomes flanking ctcf binding sites are well positioned interestingly we found a complementary pattern between the repressive and the active regions which are separated by ctcf our data indicate that ctcf may play important roles in the barrier activity of insulators and this study provides a resource for further investigation of the ctcf function in organizing chromatin in the genome
objectives we examine recent published research on the extraction of information from textual documents in the electronic health record ehr methods literature review of the research published after based on pubmed conference proceedings and the acm digital library as well as on relevant publications referenced in papers already included results publications were selected and are discussed in this review in terms of methods used pre processing of textual documents contextual features detection and analysis extraction of information in general extraction of codes and of information for decision support and enrichment of the ehr information extraction for surveillance research automated terminology management and data mining and de identification of clinical text conclusions performance of information extraction systems with clinical text has improved since the last systematic review in but they are still rarely applied outside of the laboratory they have been developed in competitive challenges for information extraction from clinical text along with the availability of annotated clinical text corpora and further improvements in system performance are important factors to stimulate advances in this field and to increase the acceptance and usage of these systems in concrete clinical and biomedical contexts
objectives to evaluate whether happiness can spread from person to person and whether niches of happiness form within social networks design longitudinal social network analysis setting framingham heart study social network participants individuals followed from to main outcome measures happiness measured with validated four item scale broad array of attributes of social networks and diverse social ties results clusters of happy and unhappy people are visible in the network and the relationship between people s happiness extends up to three degrees of separation for example to the friends of one s friends friends people who are surrounded by many happy people and those who are central in the network are more likely to become happy in the future longitudinal statistical models suggest that clusters of happiness result from the spread of happiness and not just a tendency for people to associate with similar individuals a friend who lives within a mile about km and who becomes happy increases the probability that a person is happy by confidence interval to similar effects are seen in coresident spouses to siblings who live within a mile to and next door neighbours to effects are not seen between coworkers the effect decays with time and with geographical separation conclusions people s happiness depends on the happiness of others with whom they are connected this provides further justification for seeing happiness like health as a phenomenon
transcription in mammalian cells can be assessed at a genome wide level but it has been difficult to reliably determine whether individual transcripts are derived from the plus or minus strands of chromosomes this distinction can be critical for understanding the relationship between known transcripts sense and the complementary antisense transcripts that may regulate them here we describe a technique that can be used to i identify the dna strand of origin for any particular rna transcript and ii quantify the number of sense and antisense transcripts from expressed genes at a global level we examined five different human cell types and in each case found evidence for antisense transcripts in to human genes the distribution of antisense transcripts was distinct from that of sense transcripts was nonrandom across the genome and differed among cell types antisense transcripts thus appear to be a pervasive feature of human cells suggesting that they are a fundamental component of gene science
variation in gene expression is a fundamental aspect of human phenotypic variation several recent studies have analyzed gene expression levels in populations of different continental ancestry and reported population differences at a large number of genes however these differences could largely be due to non genetic e g environmental effects here we analyze gene expression levels in african american cell lines which differ from previously analyzed cell lines in that individuals from this population inherit variable proportions of two continental ancestries we first relate gene expression levels in individual african americans to their genome wide proportion of european ancestry the results provide strong evidence of a genetic contribution to expression differences between european and african populations validating previous findings second we infer local ancestry or european chromosomes at each location in the genome and investigate the effects of ancestry proximal to the expressed gene cis versus ancestry elsewhere in the genome trans both effects are highly significant and we estimate that of all heritable variation in human gene expression is due to variants
present drug screening libraries are constrained by biophysical properties that predict desirable pharmacokinetics and structural descriptors of drug likeness or lead likeness recent surveys however indicate that to enter cells most drugs require solute carriers that normally transport the naturally occurring intermediary metabolites and many drugs are likely to interact similarly the existence of increasingly comprehensive summaries of the human metabolome allows the assessment of the concept of metabolite likeness we compare the similarity of known drugs and library compounds to naturally occurring metabolites endogenites using relevant cheminformatics molecular descriptor spaces in which known drugs are more akin to such endogenites than are most compounds
background candidate single nucleotide polymorphisms snps from genome wide association studies gwass were often selected for validation based on their functional annotation which was inadequate and biased we propose to use the more than microarray studies in the gene expression omnibus to systematically prioritize candidate snps from gwass results we analyzed all human microarray studies from the gene expression omnibus and calculated the observed frequency of differential expression which we called differential expression ratio for every human gene analysis conducted in a comprehensive list of curated disease genes revealed a positive association between differential expression ratio values and the likelihood of harboring disease associated variants by considering highly differentially expressed genes we were able to rediscover disease genes with specificity and sensitivity we successfully distinguished true disease genes from false positives in multiple gwass for multiple diseases we then derived a list of functionally interpolating snps fitsnps to analyze the top seven loci of wellcome trust case control consortium type diabetes mellitus gwass rediscovered all type diabetes mellitus genes and predicted a novel gene for an unexplained locus we suggest that fitsnps would work equally well for both mendelian and complex diseases being more effective for cancer and proposed candidate genes to sequence for their association with syndromes with unknown molecular basis conclusions our study demonstrates that highly differentially expressed genes are more likely to harbor disease associated dna variants fitsnps can serve as an effective tool to systematically prioritize candidate snps gwass
how can you know when someone is bluffing paying attention genuinelyinterested the answer writes sandy pentland in signals is thatsubtle patterns in how we interact with other people reveal our attitudestoward them these unconscious social signals are not just a back channel or acomplement to our conscious language they form a separate communicationnetwork biologically based honest signaling evolved from ancient primatesignaling mechanisms offers an unmatched window into our intentions goals and values if we understand this ancient channel of communication pentlandclaims we can accurately predict the outcomes of situations ranging from jobinterviews to first dates pentland an mit professor has used a specially designed digital sensor wornlike an id badgea sociometer to monitor and analyze the back and forthpatterns of signaling among groups of people he and his researchers foundthat this second channel of communication revolving not around words butaround social relations profoundly influences major decisions in ourliveseven though we are largely unaware of it pentland presents thescientific background necessary for understanding this form of communication applies it to examples of group behavior in real organizations and shows howby reading our social networks we can become more successful at pitching anidea getting a job or closing a deal using this network intelligence theory of social signaling pentland describes how we can harness theintelligence of our social network to become better managers andcommunicators
rflps were used to study genome evolution and phylogeny in brassica and related genera thirtyeight accessions including accessions of b rapa syn campestris cultivated types of b oleracea nine chromosome wild brassicas related to b oleracea and other species in brassica and allied genera were examined with more then random genomic dna probes which identified rflps mapping to nine different linkage groups of the b rapa genome based on the rflp data phylogenetic trees were constructed using the paup microcomputer program within b rapa accessions of pak choi narinosa and chinese cabbage from east asia constituted a group distinct from turnip and wild european populations consistent with the hypothesis that b rapa had two centers of domestication a wild b rapa accession from india was positioned in the tree between european types and east asian types suggesting an evolutionary pathway from europe to india then to south china cultivated b oleracea morphotypes showed monophyletic origin with wild b oleracea or b alboglabra as possible ancestors various kales constitute a highly diverse group and represent the primitive morphotypes of cultivated b oleracea from which cabbage broccoli cauliflower etc probably have evolved cauliflower was found to be closely related to broccoli whereas cabbage was closely related to leafy kales a great diversity existed among the collections of nine chromosome wild brassicas related to b oleracea representing various taxonomic states from subspecies to species results from these studies suggested that two basic evolutionary pathways exist for the diploid species examined one pathway gave rise to b fruticulosa b nigra and sinapis arvensis with b adpressa or a close relative as the initial ancestor another pathway gave rise to b oleracea and b rapa with diplotaxis erucoides or a close relative as the initial ancestor raphanus sativus and eruca sativus represented intermediate types between the two lineages and might have been derived from introgression or hybridization between species belonging to different lineages molecular evidence for an ascending order of chromosome numbers in the evolution of brassica and allied genera was obtained on the basis of rflp data and analysis
studies have shown that the bulk of eukaryotic genomes is transcribed transcriptome maps are frequently updated but low abundant transcripts have probably gone unnoticed to eliminate rna degradation we depleted the exonucleolytic rna exosome from human cells and then subjected the rna to tiling microarray analysis this revealed a class of short polyadenylated and highly unstable rnas these promoter upstream transcripts prompts are produced approximately to kilobases upstream of active transcription start sites prompt transcription occurs in both sense and antisense directions with respect to the downstream gene in addition it requires the presence of the gene promoter and is positively correlated with gene activity we propose that prompt transcription is a common characteristic of rna polymerase ii rnapii transcribed genes with a possible potential
background high throughput signature sequencing holds many promises one of which is the ready identification of in vivo transcription factor binding sites histone modifications changes in chromatin structure and patterns of dna methylation across entire genomes in these experiments chromatin immunoprecipitation is used to enrich for particular dna sequences of interest and signature sequencing is used to map the regions to the genome chip seq elucidation of these sites of dna protein binding modification are proving instrumental in reconstructing networks of gene regulation and chromatin remodelling that direct development response to cellular perturbation and neoplastic transformation results here we present a package of algorithms and software that makes use of control input data to reduce false positives and estimate confidence in chip seq peaks several different methods were compared using two simulated spike in datasets use of control input data and a normalized difference score were found to more than double the recovery of chip seq peaks at a false discovery rate fdr moreover both a binomial p value q value and an empirical fdr were found to predict the true fdr within fold and are more reliable estimators of confidence than a global poisson p value these methods were then used to reanalyze johnson et al s neuron restrictive silencer factor nrsf chip seq data without relying on extensive qpcr validated nrsf sites and the presence of nrsf binding motifs for setting thresholds conclusion the methods developed and tested here show considerable promise for reducing false positives and estimating confidence in chip seq data without any prior knowledge of the chip target they are part of a larger open source package freely available from http useq net
transcription initiation by rna polymerase ii rnapii is thought to occur unidirectionally from most genes here we present evidence of widespread divergent transcription at protein encoding gene promoters transcription start site associated rnas tssa rnas nonrandomly flank active promoters with peaks of antisense and sense short rnas at nucleotides upstream and nucleotides downstream of tsss respectively northern analysis shows that tssa rnas are subsets of an rna population to nucleotides in length promoter associated rnapii and trimethylated histones transcription initiation hallmarks colocalize at sense and antisense tssa rna positions however dimethylated histones characteristic of elongating rnapii are only present downstream of tsss these results suggest that divergent transcription over short distances is common for active promoters and may help promoter regions maintain a state poised for regulation
rna polymerases are highly regulated molecular machines we present a method global run on sequencing gro seq that maps the position amount and orientation of transcriptionally engaged rna polymerases genome wide in this method nuclear run on rna molecules are subjected to large scale parallel sequencing and mapped to the genome we show that peaks of promoter proximal polymerase reside on approximately of human genes transcription extends beyond pre messenger rna cleavage and antisense transcription is prevalent additionally most promoters have an engaged polymerase upstream and in an orientation opposite to the annotated gene this divergent polymerase is associated with active genes but does not elongate effectively beyond the promoter these results imply that the interplay between polymerases and regulators over broad promoter regions dictates the orientation and efficiency of transcription
motivation rapidly advancing genome technology has allowed access to a large number of diverse genomes and annotation data we have defined a systems model that integrates assembly data comparative genomics gene predictions mrna and est align ments and physiological tissue expression using these as predictive parameters we engineered a machine learning ap proach to decipher putative active regions in the genome results analysis of genomic sequences showed nucleosome free region nfr modules contain a higher percentage of conserved regions rna encoding sequences cpg islands splice sites and gc rich areas in contrast random in silico fragments revealed higher percentages of dna repeats and a lower conservation the larger conserved sequences from the vista enhancer browser veb showed a greater percentage of short dna sequence matches and rna coding regions in multiple species our model can predict small regulatory regions in the genome with prediction accuracy using nfr modules and prediction accuracy with veb elements ultimately this systems model can be applied to any organism to identify candidate tran scriptional modules on a genome scale availability the sequences used in this paper are available in supplementary table contact myar seas edu
background oxidative stress and excitotoxicity underlie the developmental neurotoxicity of numerous chemicals objectives we compared the effects of organophosphates chlorpyrifos and diazinon an organo chlorine dieldrin and a metal divalent nickel to determine how these mechanisms contribute to similar or dissimilar neurotoxic outcomes methods we used cells as a model of developing neurons and evaluated transcriptional profiles for genes for oxidative stress responses and glutamate receptors results chlorpyrifos had a greater effect on oxidative stress related genes in differentiating cells compared with the undifferentiated state chlorpyrifos and diazinon showed significant concordance in their effects on glutathione related genes but they were negatively correlated for effects on catalase and superoxide dismutase isoforms and had no concordance for effects on ionotropic glutamate receptors surprisingly the correlations were stronger between diazinon and dieldrin than between the two organophosphates the effects of were the least similar for genes related to oxidative stress but had significant concordance with dieldrin for effects on glutamate receptors conclusions our results point to underlying mechanisms by which different organophosphates produce disparate neurotoxic outcomes despite their shared property as cholinesterase inhibitors further apparently unrelated neurotoxicants may produce similar outcomes because of convergence on oxidative stress and excitotoxicity the combined use of cell cultures and microarrays points to specific end points that can distinguish similarities and disparities in the effects of diverse neurotoxicants
this study was carried out based on the assumption that brain derived neurotrophic factor bdnf may counterbalance the action of morphine in the brain morphine analgesic tolerance after daily administrations for six days was blocked by intracerebroventricular injection of anti bdnf igg on day but not by administrations on days chronic morphine treatment significantly increased the expression of exon i and iv bdnf transcripts indicating differential regulation of bdnf gene expression daily administration of the creb binding protein inhibitor curcumin abolished the upregulation of bdnf transcription and morphine analgesic tolerance these results suggest that curcumin might be a promising adjuvant to reduce morphine analgesic tolerance and that epigenetic control could be a new strategy useful for the control of problem
this report summarizes the results of an ambitious three year ethnographic study funded by the john d and catherine t macarthur foundation into how young people are living and learning with new media in varied settingsat home in after school programs and in online spaces it offers a condensed version of a longer treatment provided in the book hanging out messing around and geeking out mit press the authors present empirical data on new media in the lives of american youth in order to reflect upon the relationship between new media and learning in one of the largest qualitative and ethnographic studies of american youth culture the authors view the relationship of youth and new media not simply in terms of technology trends but situated within the broader structural conditions of childhood and the negotiations with adults that frame the experience of youth in the states
tea is particularly rich in polyphenols including catechins theaflavins and thearubigins which are thought to contribute to the health benefits of tea tea polyphenols act as antioxidants in vitro by scavenging reactive oxygen and nitrogen species and chelating redox active transition metal ions they may also function indirectly as antioxidants through inhibition of the redox sensitive transcription factors nuclear factor kappa b and activator protein inhibition of pro oxidant enzymes such as inducible nitric oxide synthase lipoxygenases cyclooxygenases and xanthine oxidase and induction of phase ii and antioxidant enzymes such as glutathione s transferases and superoxide dismutases the fact that catechins are rapidly and extensively metabolized emphasizes the importance of demonstrating their antioxidant activity in vivo animal studies offer a unique opportunity to assess the contribution of the antioxidant properties of tea and tea polyphenols to the physiological effects of tea administration in different models of oxidative stress most promising are the consistent findings in animal models of skin lung colon liver and pancreatic cancer that tea and tea polyphenol administration inhibit carcinogen induced increases in the oxidized dna base hydroxy deoxyguanosine in animal models of atherosclerosis green and black tea administration has resulted in modest improvements in the resistance of lipoproteins to ex vivo oxidation although limited data suggest that green tea or green tea catechins inhibit atherogenesis to determine whether tea polyphenols act as effective antioxidants in vivo future studies in animals and humans should employ sensitive and specific biomarkers of oxidative damage to lipids proteins dna
the proinflammatory effects of particulate pollutants including diesel exhaust particles dep are related to their content of redox cycling chemicals and their ability to generate oxidative stress in the respiratory tract an antioxidant defense pathway which involves phase ii enzyme expression protects against the pro oxidative and proinflammatory effects of dep the expression of enzymes including heme oxygenase ho and gst is dependent on the activity of a genetic antioxidant response element in their promoters in this study we investigated the mechanism by which redox cycling organic chemicals prepared from dep induce phase ii enzyme expression as a protective response we demonstrate that aromatic and polar dep fractions which are enriched in polycyclic aromatic hydrocarbons and quinones respectively induce the expression of ho gst and other phase ii enzymes in macrophages and epithelial cells we show that ho expression is mediated through accumulation of the bzip transcription factor in the nucleus and that gene targeting significantly weakens this response accumulation and subsequent activation of the antioxidant response element is regulated by the proteasomal degradation of this pathway is sensitive to pro oxidative and electrophilic dep chemicals and is also activated by ambient ultrafine particles we propose that mediated phase ii enzyme expression protects against the proinflammatory effects of particulate pollutants in the setting of allergic inflammation asthma
recently a number of papers have been published showing the benefits of column stores over row stores however the research comparing the two in an apples to apples way has left a number of unresolved questions in this paper we first discuss the factors that can affect the relative performance of each paradigm then we choose points within each of the factors to study further our study examines five tables with various characteristics and different query workloads in order to obtain a greater understanding and quantification of the relative performance of column stores and row stores we then add materialized views to the analysis and see how much they can help the performance of row stores finally we examine the performance of hash join operations in column stores and stores
scholars advertisers and political activists see massive online social networks as a representation of social interactions that can be used to study the propagation of ideas social bond dynamics and viral marketing among others but the linked structures of social networks do not reveal actual interactions among people scarcity of attention and the daily rythms of life and work makes people default to interacting with those few that matter and that reciprocate their attention a study of social interactions within twitter reveals that the driver of usage is a sparse and hidden network of connections underlying the declared set of friends followers
how should we understand why firms exist a prevailing view has been that they serve to keep in check the transaction costs arising from the self interested motivations of individuals we develop in this article the argument that what firms do better than markets is the sharing and transfer of the knowledge of individuals and groups within an organization this knowledge consists of information e g who knows what and of know how e g how to organize a research team what is central to our argument is that knowledge is held by individuals but is also expressed in regularities by which members cooperate in a social community i e group organization or network if knowledge is only held at the individual level then firms could change simply by employee turnover because we know that hiring new workers is not equivalent to changing the skills of a firm an analysis of what firms can do must understand knowledge as embedded in the organizing principles by which people cooperate within organizations based on this discussion a paradox is identified efforts by a firm to grow by the replication of its technology enhances the potential for imitation by considering how firms can deter imitation by innovation we develop a more dynamic view of how firms create new knowledge we build up this dynamic perspective by suggesting that firms learn new skills by recombining their current capabilities because new ways of cooperating cannot be easily acquired growth occurs by building on the social relationships that currently exist in a firm what a firm has done before tends to predict what it can do in the future in this sense the cumulative knowledge of the firm provides options to expand in new but uncertain markets in the future we discuss at length the example of the make buy decision and propose several testable hypotheses regarding the boundaries of the firm without appealing to the notion of opportunism abstract author
in bioinformatics we are familiar with the idea of curated data as a prerequisite for data integration we neglect often to our cost the curation and cataloguing of the processes that we use to integrate and analyse our data programmatic access to services for data and processes means that compositions of services can be made that represent the in silico experiments or processes that bioinformaticians perform data integration through workflows depends on being able to know what services exist and where to find those services the large number of services and the operations they perform their arbitrary naming and lack of documentation however mean that they can be difficult to use the workflows themselves are composite processes that could be pooled and reused but only if they too can be found and understood thus appropriate curation including semantic mark up would enable processes to be found maintained and consequently used more easily this broader view on semantic annotation is vital for full data integration that is necessary for the modern scientific analyses in biology this article will brief the community on the current state of the art and the current challenges for process curation both within and without the life bib
web based biomedical communities are becoming an increasingly popular vehicle for sharing information amongst researchers and are fast gaining an online presence however information organization and exchange in such communities is usually unstructured rendering interoperability between communities difficult furthermore specialized software to create such communities at low cost targeted at the specific common information requirements of biomedical researchers has been largely lacking at the same time a growing number of biological knowledge bases and biomedical resources are being structured for the semantic web several groups are creating reference ontologies for the biomedical domain actively publishing controlled vocabularies and making data available in resource description framework rdf language we have developed the science collaboration framework scf as a reusable platform for advanced structured online collaboration in biomedical research that leverages these ontologies and rdf resources scf supports structured web style community discourse amongst researchers makes heterogeneous data resources available to the collaborating scientist captures the semantics of the relationship among the resources and structures discourse around the resources the first instance of the scf framework is being used to create an open access online community for stem cell research stembook http www stembook org we believe that such a framework is required to achieve optimal productivity and leveraging of resources in interdisciplinary scientific research we expect it to be particularly beneficial in highly interdisciplinary areas such as neurodegenerative disease and neurorepair research as well as having broad utility across the natural bib
the web is now being used as a platform for publishing and linking life science data the web s linking architecture can be exploited to join heterogeneous data from multiple sources however as data are frequently being updated in a decentralized environment provenance information becomes critical to providing reliable and trustworthy services to scientists this article presents design patterns for representing and querying provenance information relating to mapping links between heterogeneous data from sources in the domain of functional genomics we illustrate the use of named resource description framework rdf graphs at different levels of granularity to make provenance assertions about linked data and demonstrate that these assertions are sufficient to support requirements including data currency integrity evidential support and historical bib
the biomedical literature can be seen as a large integrated but unstructured data repository extracting facts from literature and making them accessible is approached from two directions manual curation efforts develop ontologies and vocabularies to annotate gene products based on statements in papers text mining aims to automatically identify entities and their relationships in text using information retrieval and natural language processing techniques manual curation is highly accurate but time consuming and does not scale with the ever increasing growth of literature text mining as a high throughput computational technique scales well but is error prone due to the complexity of natural language how can both be married to combine scalability and accuracy here we review the state of the art text mining approaches that are relevant to annotation and discuss available online services analysing biomedical literature by means of text mining techniques which could also be utilised by annotation projects we then examine how far text mining has already been utilised in existing annotation projects and conclude how these techniques could be tightly integrated into the manual annotation process through novel authoring systems to scale up high quality curation
motivation micrornas mirnas play important roles in gene regulation and are regarded as key components in gene regulatory pathways systematically understanding functional roles of mirnas is essential to define core transcriptional units regulating key biological processes here we propose a method based on the probabilistic graphical model to identify the regulatory modules of mirnas and the core regulatory motifs involved in their ability to regulate gene expression results we applied our method to datasets of different sources from arabidopsis consisting of mirna target pair information upstream sequences of mirnas transcriptional regulatory motifs and gene expression profiles the graphical model used in this study can efficiently capture the relationship between mirnas and diverse conditions such as various developmental processes thus allowing us to detect functionally correlated mirna regulatory modules involved in specific biological processes furthermore this approach can reveal core transcriptional elements associated with their mirnas the proposed method found clusters of mirnas as well as putative regulators controlling the expression of mirnas which were highly related to diverse developmental processes of arabidopsis consequently our method can provide hypothetical mirna regulatory circuits for functional testing that represent transcriptional events of mirnas and transcriptional factors involved in gene regulatory pathways contact cornell edu supplementary information supplementary data are available at bioinformatics bioinformatics
motivation multiple sequence alignment is a cornerstone of comparative genomics much work has been done to improve methods for this task particularly for the alignment of small sequences and especially for amino acid sequences however less work has been done in making promising methods that work on the small scale practically for the alignment of much larger genomic sequences results we take the method of probabilistic consistency alignment and make it practical for the alignment of large genomic sequences in so doing we develop a set of new technical methods combined in a framework we term sequence progressive alignment because it allows us to iteratively compute an alignment by passing over the input sequences from left to right the result is that we massively decrease the memory consumption of the program relative to a naive implementation the general engineering of the challenges faced in scaling such a computationally intensive process offer valuable lessons for planning related large scale sequence analysis algorithms we also further show the strong performance of pecan using an extended analysis of ancient repeat alignments pecan is now one of the default alignment programs that has and is being used by a number of whole genome comparative genomic projects availability the pecan program is freely available at http www ebi ac uk approximately bjp pecan pecan whole genome alignments can be found in the ensembl genome browser contact benedict soe ucsc edu supplementary information supplementary data are available at online
abstract background high throughput genotyping technology has enabled cost effective typing of thousands of individuals in hundred of thousands of markers for use in genome wide studies this vast improvement in data acquisition technology makes it an informatics challenge to efficiently store and manipulate the data while spreadsheets and flat text files were adequate solutions earlier the increased data size mandates more efficient solutions results we describe a new binary file format for snp data together with a software library for file manipulation the file format stores genotype data together with any kind of additional data using a flexible serialisation mechanism the format is designed to be io efficient for the access patterns of most multi locus analysis methods conclusions the new file format has been very useful for our own studies where it has significantly reduced the informatics burden in keeping track of various secondary data and where the memory and io efficiency has greatly simplified analysis runs a main limitation with the file format is that it is only supported by the very limited set of analysis tools developed in our own lab this is alleviated by a scripting interfaces that makes it easy to write converters to and from format
regulatory cis acting variants account for a large proportion of gene expression variability in populations cis acting differences can be specifically measured by comparing relative levels of allelic transcripts within a sample allelic expression ae mapping for cis regulatory variant discovery has been hindered by the requirements of having informative or heterozygous single nucleotide polymorphisms snps within genes in order to assign the allelic origin of each transcript in this study we have developed an approach to systematically screen for heritable cis variants in common human haplotypes across genes in order to achieve the highest level of information per haplotype studied we carried out allelic expression measurements by using both intronic and exonic snps in primary transcripts we used a novel rna pooling strategy in immortalized lymphoblastoid cell lines lcls and primary human osteoblast cell lines hobs to allow for high throughput ae screening hits from rna pools were further validated by performing allelic expression mapping in individual samples our results indicate that of expressed genes in human lcls show genotype linked ae in addition we have validated cis acting variants in over genes linked with common disease susceptibility in recent genome wide studies more generally our results indicate that rna pooling coupled with ae read out by second generation sequencing or by other methods provides a high throughput tool for cataloging the impact of common noncoding variants in the genome
gr increasing read length is currently viewed as the crucial condition for fragment assembly with next generation sequencing technologies however introducing mate paired reads separated by a gap of length gaplength opens a possibility to transform short mate pairs into long mate reads of length gaplength and thus raises the question as to whether the read length as opposed to gaplength even matters we describe a new tool euler usr for assembling mate paired short reads and use it to analyze the question of whether the read length matters we further complement the ongoing experimental efforts to maximize read length by a new computational approach for increasing the effective read length while the common practice is to trim the error prone tails of the reads we present an approach that substitutes trimming with error correction using repeat graphs an important and counterintuitive implication of this result is that one may extend sequencing reactions that degrade with length past their prime to where the error rate grows above what is normally acceptable for assembly
feshbach resonances are the essential tool to control the interaction between atoms in ultracold quantum gases they have found numerous experimental applications opening up the way to important breakthroughs this review broadly covers the phenomenon of feshbach resonances in ultracold gases and their main applications this includes the theoretical background and models for the description of feshbach resonances the experimental methods to find and characterize the resonances a discussion of the main properties of resonances in various atomic species and mixed atomic species systems and an overview of key experiments with atomic bose einstein condensates degenerate fermi gases and molecules
abstract background the gene ontology is a controlled vocabulary for representing knowledge related to genes and proteins in a computable form the current effort of manually annotating proteins with the gene ontology is outpaced by the rate of accumulation of biomedical knowledge in literature which urges the development of text mining approaches to facilitate the process by automatically extracting the gene ontology annotation from literature the task is usually cast as a text classification problem and contemporary methods are confronted with unbalanced training data and the difficulties associated with multi label classification results in this research we investigated the methods of enhancing automatic multi label classification of biomedical literature by utilizing the structure of the gene ontology graph we have studied three graph based multi label classification algorithms including a novel stochastic algorithm and two top down hierarchical classification methods for multi label literature classification we systematically evaluated and compared these graph based classification algorithms to a conventional flat multi label algorithm the results indicate that through utilizing the information from the structure of the gene ontology graph the graph based multi label classification methods can significantly improve predictions of the gene ontology terms implied by the analyzed text furthermore the graph based multi label classifiers are capable of suggesting gene ontology annotations to curators that are closely related to the true annotations even if they fail to predict the true ones directly a software package implementing the studied algorithms is available for the research community conclusions through utilizing the information from the structure of the gene ontology graph the graph based multi label classification methods have better potential than the conventional flat multi label classification approach to facilitate protein annotation based on literature
information on protein protein interactions is of central importance for many areas of biomedical research at present no method exists to systematically and experimentally assess the quality of individual interactions reported in interaction mapping experiments to provide a standardized confidence scoring method that can be applied to tens of thousands of protein interactions we have developed an interaction tool kit consisting of four complementary high throughput protein interaction assays we benchmarked these assays against positive and random reference sets consisting of well documented pairs of interacting human proteins and randomly chosen protein pairs respectively a logistic regression model was trained using the data from these reference sets to combine the assay outputs and calculate the probability that any newly identified interaction pair is a true biophysical interaction once it has been tested in the tool kit this general approach will allow a systematic and empirical assignment of confidence scores to all individual protein protein interactions in networks
recently advances in information technology and an increased willingness to share primary biodiversity data are enabling unprecedented access to it by combining presences of species data with electronic cartography via a number of algorithms estimating niches of species and their areas of distribution becomes feasible at resolutions one to three orders of magnitude higher than it was possible a few years ago some examples of the power of that technique are presented for the method to work limitations such as lack of high quality taxonomic determination precise georeferencing of the data and availability of high quality and updated taxonomic treatments of the groups must be overcome these are discussed together with comments on the potential of these biodiversity informatics techniques not only for fundamental studies but also as a way for developing countries to apply state of the art bioinformatic methods and large quantities of data in practical ways to tackle issues of management
social tags are free text labels that are applied to items such as artists albums and songs captured in these tags is a great deal of information that is highly relevant to music information retrieval mir researchers including information about genre mood instrumentation and quality unfortunately there is also a great deal of irrelevant information and noise in the tags imperfect as they may be social tags are a source of human generated contextual knowledge about music that may become an essential part of the solution to many mir problems in this article we describe the state of the art in commercial and research social tagging systems for music we describe how tags are collected and used in current systems we explore some of the issues that are encountered when using tags and we suggest possible areas of exploration for research
two component spinors are the basic ingredients for describing fermions in quantum field theory in four space time dimensions we develop and review the techniques of the two component spinor formalism and provide a complete set of feynman rules for fermions using two component spinor notation these rules are suitable for practical calculations of cross sections decay rates and radiative corrections in the standard model and its extensions including supersymmetry and many explicit examples are provided the unified treatment presented in this review applies to massless weyl fermions and massive dirac and majorana fermions we exhibit the relation between the two component spinor formalism and the more traditional four component spinor formalism and indicate their connections to the spinor helicity method and techniques for the computation of amplitudes
neuroanatomical evidence indicates the human eyes visual field can be functionally divided into two vertical hemifields each specialized for specific functions the upper visual field uvf is specialized to support perceptual tasks in the distance while the lower visual field lvf is specialized to support visually guided motor tasks such as pointing we present a user study comparing mouse and touchscreen based pointing for items presented in the uvf and lvf on an interactive display consistent with the neuroscience literature we found that mouse and touchscreen pointing were faster and more accurate for items presented in the lvf when compared to pointing at identical targets presented in the uvf further analysis found previously unreported performance differences between the visual fields for touchscreen pointing that were not observed for mouse pointing this indicates that a placement of interactive items favorable to the lvf yields superior user performance especially for systems dependent on direct interactions
motivation complex diseases are generally thought to be under the influence of one or more mutated risk genes as well as genetic and environmental factors many traditional methods have been developed to identify susceptibility genes assuming a single gene disease model single locus methods pathway based approaches combined with traditional methods consider the joint effects of genetic factor and biologic network context with the accumulation of high throughput snp datasets and human biologic pathways it becomes feasible to search for risk pathways associated with complex diseases using bioinformatics methods by analyzing the contribution of genetic factor and biologic network context in kegg kyoto encyclopedia of genes and genomes pathways we proposed an approach to prioritize risk pathways for complex diseases prioritizing risk pathways fusing snps and pathways prp a risk scoring rs measurement was used to prioritize risk biologic pathways this could help to demonstrate the pathogenesis of complex diseases from a new perspective and provide new hypotheses we introduced this approach to five complex diseases and found that these five diseases not only share common risk pathways but also have their specific risk pathways which is verified by literature retrieval availability genotype frequencies of five case control samples were downloaded from the wtccc online system and the address is https www wtccc org uk info shtml contact chenlina ems hrbmu edu cn lixia hrbmu edu cn supplementary information supplementary data are available at bioinformatics bioinformatics
the gene expression omnibus geo at the national center for biotechnology information ncbi is the largest public repository for high throughput gene expression data additionally geo hosts other categories of high throughput functional genomic data including those that examine genome copy number variations chromatin structure methylation status and transcription factor binding these data are generated by the research community using high throughput technologies like microarrays and more recently next generation sequencing the database has a flexible infrastructure that can capture fully annotated raw and processed data enabling compliance with major community derived scientific reporting standards such as minimum information about a microarray experiment miame in addition to serving as a centralized data storage hub geo offers many tools and features that allow users to effectively explore analyze and download expression data from both gene centric and experiment centric perspectives this article summarizes the geo repository structure content and operating procedures as well as recently introduced data mining features geo is freely accessible at http www ncbi nlm nih geo
rna micrornas mirnas regulate the expression of mrnas in animals and plants through mirna containing ribonucleoprotein particles rnps at the core of these mirna silencing effector complexes are the argonaute ago proteins that bind mirnas and mediate target mrna recognition we generated cell lines stably expressing epitope tagged human ago proteins and other rna silencing related proteins and used these cells to purify mirna containing rnps mass spectrometric analyses of the proteins associated with different ago proteins revealed a common set of helicases and mrna binding proteins among them the three trinucleotide repeat containing proteins b c mrna microarray analyses of these mirna associated rnps revealed that ago and proteins bind highly similar sets of transcripts enriched in binding sites for highly expressed endogenous mirnas indicating that the proteins are a component of the mrna targeting mirna silencing complex together with the very similar proteomic composition of each ago complex this result suggests substantial functional redundancy within families of human ago and proteins our results further demonstrate that we have developed an effective biochemical approach to identify physiologically relevant human targets
after more than a decade of hope and hype researchers are finally making inroads into understanding the genetic basis of many common human diseases the use of genome wide association studies has broken the logjam enabling genetic variants at specific loci to be associated with particular diseases genetic association data are now providing new routes to understanding the aetiology of disease as well as new footholds on the long and difficult path to better treatment and prevention
the genetic variation that occurs naturally in a population is a powerful resource for studying how genotype affects phenotype each allele is a perturbation of the biological system and genetic crosses through the processes of recombination and segregation randomize the distribution of these alleles among the progeny of a cross the randomized genetic perturbations affect traits directly and indirectly and the similarities and differences between traits in their responses to common perturbations allow inferences about whether variation in a trait is a cause of a phenotype such as disease or whether the trait variation is instead an effect of that phenotype it is then possible to use this information about causes and effects to build models of probabilistic causal networks these networks are beginning to define the outlines of the map
society must respond to the growing demand for cognitive enhancement that response must start by rejecting the idea that enhancement is a dirty word argue henry greely and colleagues today on university campuses around the world students are striking deals to buy and sell prescription drugs such as adderall and ritalin not to get high but to get higher grades to provide an edge over their fellow students or to increase in some measurable way their capacity for learning these transactions are crimes in the united states punishable prison
background two inflammatory disorders type diabetes and celiac disease cosegregate in populations suggesting a common genetic origin since both diseases are associated with the hla class ii genes on chromosome we tested whether non hla loci are shared methods we evaluated the association between type diabetes and eight loci related to the risk of celiac disease by genotyping and statistical analyses of dna samples from patients with type diabetes control subjects and families providing parent child trios consisting of an affected child and both biologic parents we also investigated loci associated with type diabetes in patients with celiac disease and control subjects results three celiac disease loci on chromosome on chromosome and tagap on chromosome were associated with type diabetes p the bp insertion deletion variant on chromosome was newly identified as a type diabetes locus p and was also associated with celiac disease along with on chromosome and on chromosome bringing the total number of loci with evidence of a shared association to seven including on chromosome the effects of the and tagap alleles confer protection in type diabetes and susceptibility in celiac disease loci with distinct effects in the two diseases included ins on chromosome on chromosome and on chromosome in type diabetes and on and lpp on in celiac disease conclusions a genetic susceptibility to both type diabetes and celiac disease shares common alleles these data suggest that common biologic mechanisms such as autoimmunity related tissue damage and intolerance to dietary antigens may be etiologic features of diseases
wikipedia s success is often attributed to the large numbers of contributors who improve the accuracy completeness and clarity of articles while reducing bias however because of the coordination needed to write an article collaboratively adding contributors is costly we examined how the number of editors in wikipedia and the coordination methods they use affect article quality we distinguish between explicit coordination in which editors plan the article through communication and implicit coordination in which a subset of editors structure the work by doing the majority of it adding more editors to an article improved article quality only when they used appropriate coordination techniques and was harmful when they did not implicit coordination through concentrating the work was more helpful when many editors contributed but explicit coordination through communication was not both types of coordination improved quality more when an article was in a formative stage these results demonstrate the critical importance of coordination in effectively harnessing the wisdom of the crowd in online environments
perceptual experience consists of an enormous number of possible states previous fmri studies have predicted a perceptual state by classifying brain activity into prespecified categories constraint free visual image reconstruction is more challenging as it is impractical to specify brain activity for all possible images in this study we reconstructed visual images by combining local image bases of multiple scales whose contrasts were independently decoded from fmri activity by automatically selecting relevant voxels and exploiting their correlated patterns binary contrast x patch images possible states were accurately reconstructed without any image prior on a single trial or volume basis by measuring brain activity only for several hundred random images reconstruction was also used to identify the presented image among millions of candidates the results suggest that our approach provides an effective means to read out complex perceptual states from brain activity while discovering information representation in patterns
over the past several decades quantum information science has emerged to seek answers to the question can we gain some advantage by storing transmitting and processing information encoded in systems that exhibit unique quantum properties today it is understood that the answer is yes and many research groups around the world are working towards the highly ambitious technological goal of building a quantum computer which would dramatically improve computational power for particular tasks a number of physical systems spanning much of modern physics are being developed for quantum computation however it remains unclear which technology if any will ultimately prove successful here we describe the latest developments for each of the leading approaches and explain the major challenges for future
motivation a major challenge in regulatory genomics is the identification of associations between functional categories of genes e g tissues metabolic pathways and their regulating transcription factors tfs while for a limited number of categories the regulating tfs are already known still for many functional categories the responsible factors remain to be elucidated results we put forward a novel method pastaa for detecting transcriptions factors associated with functional categories which utilizes the prediction of binding affinities of a tf to promoters this binding strength information is compared to the likelihood of membership of the corresponding genes in the functional category under study coherence between the two ranked datasets is seen as an indicator of association between a tf and the category pastaa is applied primarily to the determination of tfs driving tissue specific expression we show that pastaa is capable of recovering many tfs acting tissue specifically and in addition provides novel associations so far not detected by alternative methods the application of pastaa to detect tfs involved in the regulation of tissue specific gene expression revealed a remarkable number of experimentally supported associations the validated success for various datasets implies that pastaa can directly be applied for the detection of tfs associated with newly derived gene sets availability the pastaa source code as well as a corresponding web interface is freely available at http trap molgen de
dramatic increases in the throughput of nucleotide sequencing machines and the promise of ever greater performance have thrust bioinformatics into the era of petabyte scale data sets sequence repositories which provide the feed for these data sets into the worldwide computational infrastructure are challenged by the impact of these data volumes the european nucleotide archive ena http www ebi ac uk embl comprising the embl nucleotide sequence database and the ensembl trace archive has identified challenges in the storage movement analysis interpretation and visualization of petabyte scale data sets we present here our new repository for next generation sequence data a brief summary of contents of the ena and provide details of major developments to submission pipelines high throughput rule based validation infrastructure and data approaches
collecting grasp data for learning and benchmarking purposes is very expensive it would be helpful to have a standard database of graspable objects along with a set of stable grasps for each object but no such database exists in this work we show how to automate the construction of a database consisting of several hands thousands of objects and hundreds of thousands of grasps using this database we demonstrate a novel grasp planning algorithm that exploits geometric similarity between a model and the objects in the database to synthesize form closure grasps our contributions are this algorithm and the database itself which we are releasing to the community as a tool for both grasp planning benchmarking
evolutionary pressures on proteins are often quantified by the ratio of substitution rates at non synonymous and synonymous sites the dn ds ratio was originally developed for application to distantly diverged sequences the differences among which represent substitutions that have fixed along independent lineages nevertheless the dn ds measure is often applied to sequences sampled from a single population the differences among which represent segregating polymorphisms here we study the expected dn ds ratio for samples drawn from a single population under selection and we find that in this context dn ds is relatively insensitive to the selection coefficient moreover the hallmark signature of positive selection over divergent lineages dn ds is violated within a population for population samples the relationship between selection and dn ds does not follow a monotonic function and so it may be impossible to infer selection pressures from dn ds these results have significant implications for the interpretation of dn ds measurements among population samples
abstract nbsp nbsp in this paper we will study the field through the problematic network built by scientific articles using actor network theory and consequently coword analysis as a model for scientific knowledge regarded as a social process growth scientometrics is an hybrid field made of invisible college and a lot of users thus controlled by both scientific research and final uses coword analysis gives the same weight to all articles cited or not and consequently computes the interaction network within all kind of authors according to already described network properties of scientific interaction coword analysis describes the dynamic of the field in accordance with what has been observed and suggest forecast for future
might dna sequence variation reflect germline genetic activity and underlying chromatin structure we investigated this question using medaka japanese killifish oryzias latipes by comparing the genomic sequences of two strains hd rr and hni and by mapping million nucleosome cores from hd rr blastulae and representative transcription start sites from six embryonic stages we observed a distinctive base pair bp periodic pattern of genetic variation downstream of transcription start sites the rate of insertions and deletions longer than bp peaked at positions of approximately and bp whereas the point mutation rate showed corresponding valleys this bp periodicity was correlated with the chromatin structure with nucleosome occupancy minimized at positions and bp these data exemplify the potential for genetic activity transcription and chromatin structure to contribute to molding the dna sequence on an evolutionary time science
newspapers and blogs express opinion of news entities people places things while reporting on recent events we present a system that assigns scores indicating positive or negative opinion to each distinct entity in the text corpus our system consists of a sentiment identification phase which associates expressed opinions with each relevant entity and a sentiment aggregation and scoring phase which scores each entity relative to others in the same class finally we evaluate the significance of our scoring techniques over large corpus of news blogs
cancer results from somatic alterations in key genes including point mutations copy number alterations and structural rearrangements a powerful way to discover cancer causing genes is to identify genomic regions that show recurrent copy number alterations gains and losses in tumor genomes recent advances in sequencing technologies suggest that massively parallel sequencing may provide a feasible alternative to dna microarrays for detecting copy number alterations here we present i a statistical analysis of the power to detect copy number alterations of a given size ii segseq an algorithm to segment equal copy numbers from massively parallel sequence data and iii analysis of experimental data from three matched pairs of tumor and normal cell lines we show that a collection of million aligned sequence reads from human cell lines has comparable power to detect events as the current generation of dna microarrays and has over twofold better precision for localizing breakpoints typically to kilobase
chandra observations of large samples of galaxy clusters detected in x rays by rosat provide a new robust determination of the cluster mass functions at low and high redshifts statistical and systematic errors are now sufficiently small and the redshift leverage sufficiently large for the mass function evolution to be used as a useful growth of a structure based dark energy probe in this paper we present cosmological parameter constraints obtained from chandra observations of clusters with z derived from rosat serendipitous survey and brightest z clusters detected in the all sky survey evolution of the mass function between these redshifts requires ol gt with a significance and constrains the dark energy equation of state parameter to w assuming a constant w and a flat universe cluster information also significantly improves constraints when combined with other methods fitting our cluster data jointly with the latest supernovae wilkinson microwave anisotropy probe and baryonic acoustic oscillation measurements we obtain w stat sys a factor of reduction in statistical uncertainties and nearly a factor of improvement in systematics compared with constraints that can be obtained without clusters the joint analysis of these four data sets puts a conservative upper limit on the masses of light neutrinos m n lt ev at cl we also present updated measurements of om h and from the low redshift cluster function
evaluation measures act as objective functions to be optimized by information retrieval systems such objective functions must accurately reflect user requirements particularly when tuning ir systems and learning ranking functions ambiguity in queries and redundancy in retrieved documents are poorly reflected by current evaluation measures in this paper we present a framework for evaluation that systematically rewards novelty and diversity we develop this framework into a specific evaluation measure based on cumulative gain we demonstrate the feasibility of our approach using a test collection based on the trec question track
next generation technologies enable massive scale cdna sequencing so called rna seq mainly because of the difficulty of aligning short reads on exon exon junctions no attempts have been made so far to use rna seq for building gene models de novo that is in the absence of a set of known genes and or splicing events we present g mo r se gene modelling using rna seq an approach aimed at building gene models directly from rna seq and demonstrate its utility on the genome
methylmercury mehg is a potent neurotoxicant and preferentially induces oxidative injury in astrocytes in neuronal tissues nuclear factor erythroid related factor is a key factor determining the protective antioxidant response against various environmental toxicants is subjected to regulation by many other signaling pathways the purpose of this study is to characterize its interaction with the phosphatidylinositol kinase in cultured rat neonatal primary astrocytes the results showed that at pathologically relevant concentrations exposure of primary astrocytes to mehg led to activation and upregulation of its downstream antioxidant genes inhibition of the kinase resulted in decreased activity decreased cellular glutathione and increased cell death to high dose mehg the functional interaction between the two signaling pathways underlined an important mechanism for astrocyte protection against mehg toxicity modulation of by pharmacological modalities should afford a treatment to attenuate mehg induced toxsci
background massively parallel dna sequencing instruments are enabling the decoding of whole genomes at significantly lower cost and higher throughput than classical sanger technology each of these technologies have been estimated to yield assemblies with more problematic features than the standard method these problems are of a different nature depending on the techniques used so an appropriate mix of technologies may help resolve most difficulties and eventually provide assemblies of high quality without requiring any sanger based input results we compared assemblies obtained using sanger data with those from different inputs from new sequencing technologies the assemblies were systematically compared with a reference finished sequence we found that the gsflx can efficiently produce high continuity when used at high coverage the potential to enhance continuity by scaffolding was tested using sequences from circularized genomic fragments finally we explore the use of solexa illumina short reads to polish the genome draft by implementing a technique to correct consensus errors conclusions high quality drafts can be produced for small genomes without any sanger data input we found that gsflx and solexa illumina show great complementarity in producing large contigs and supercontigs with a low rate
recommender systems provide users with personalized suggestions for products or services these systems often rely on collaborating filtering cf where past transactions are analyzed in order to establish connections between users and products the two more successful approaches to cf are latent factor models which directly prole both users and products and neighborhood models which analyze similarities between products or users in this work we introduce some innovations to both approaches the factor and neighborhood models can now be smoothly merged thereby building a more accurate combined model further accuracy improvements are achieved by extending the models to exploit both explicit and implicit feedback by the users the methods are tested on the netix data results are better than those previously published on that dataset in addition we suggest a new evaluation metric which highlights the differences among methods based on their performance at a top k task
in this article we review ethnographic research on the internet and computer mediated communication the technologically mediated environment prevents researchers from directly observing research participants and often makes the interaction anonymous in addition in the online environment direct interaction with participants is replaced by computer screen data that are largely textual but may include combinations of textual visual aural and kinetic components we show how the online environment requires adjustments in how ethnographers define the setting of their research conduct participant observation and interviews obtain access to settings and research subjects and deal with the ethical dilemmas posed by medium
background recognizing similarities and deriving relationships among protein molecules is a fundamental requirement in present day biology similarities can be present at various levels which can be detected through comparison of protein sequences or their structural folds in some cases similarities obscure at these levels could be present merely in the substructures at their binding sites inferring functional similarities between protein molecules by comparing their binding sites is still largely exploratory and not as yet a routine protocol one of the main reasons for this is the limitation in the choice of appropriate analytical tools that can compare binding sites with high sensitivity to benefit from the enormous amount of structural data that is being rapidly accumulated it is essential to have high throughput tools that enable large scale binding site comparison results here we present a new algorithm pocketmatch for comparison of binding sites in a frame invariant manner each binding site is represented by lists of sorted distances capturing shape and chemical nature of the site the sorted arrays are then aligned using an incremental alignment method and scored to obtain pmscores for pairs of sites a comprehensive sensitivity analysis and an extensive validation of the algorithm have been carried out a comparison with other site matching algorithms is also presented perturbation studies where the geometry of a given site was retained but the residue types were changed randomly indicated that chance similarities were virtually non existent our analysis also demonstrates that shape information alone is insufficient to discriminate between diverse binding sites unless combined with chemical nature of amino acids conclusion a new algorithm has been developed to compare binding sites in accurate efficient and high throughput manner though the representation used is conceptually simplistic we demonstrate that along with the new alignment strategy used it is sufficient to enable binding comparison with high sensitivity novel methodology has also been presented for validating the algorithm for accuracy and sensitivity with respect to geometry and chemical nature of the site the method is also fast and takes about second for one comparison on a single processor a parallel version on bluegene has also implemented
ncbi s conserved domain database cdd is a collection of multiple sequence alignments and derived database search models which represent protein domains conserved in molecular evolution the collection can be accessed at http www ncbi nlm nih gov structure cdd cdd shtml and is also part of ncbi s entrez query and retrieval system cross linked to numerous other resources cdd provides annotation of domain footprints and conserved functional sites on protein sequences precalculated domain annotation can be retrieved for protein sequences tracked in ncbi s entrez system and cdd s collection of models can be queried with novel protein sequences via the cd search service at http www ncbi nlm nih gov structure cdd wrpsb cgi starting with the latest version of cdd information from redundant and homologous domain models is summarized at a superfamily level and domain annotation on proteins is flagged as either specific identifying molecular function with high confidence or as non specific identifying superfamily only
calorie restriction extends lifespan in organisms ranging from yeast to mammals in yeast the gene mediates the life extending effects of calorie restriction here we show that the mammalian orthologue sirtuin activates a critical component of calorie restriction in mammals that is fat mobilization in white adipocytes upon food withdrawal protein binds to and represses genes controlled by the fat regulator ppar gamma peroxisome proliferator activated receptor gamma including genes mediating fat storage represses ppar gamma by docking with its cofactors ncor nuclear receptor co repressor and smrt silencing mediator of retinoid and thyroid hormone receptors mobilization of fatty acids from white adipocytes upon fasting is compromised in mice repression of ppar gamma by is also evident in adipocytes where overexpression of attenuates adipogenesis and rna interference of enhances it in differentiated fat cells upregulation of triggers lipolysis and loss of fat as a reduction in fat is sufficient to extend murine lifespan our results provide a possible molecular pathway connecting calorie restriction to life extension mammals
background each genome has a stable distribution of the combined frequency for each k mer and its reverse complement measured in sequence fragments as short as bps across the whole genome for k the collection of these k mer frequency distributions is unique to each genome and termed the genome s barcode results we found that for each genome the majority of its short sequence fragments have highly similar barcodes while sequence fragments with different barcodes typically correspond to genes that are horizontally transferred or highly expressed this observation has led to new and more effective ways for addressing two challenging problems metagenome binning problem and identification of horizontally transferred genes our barcode based metagenome binning algorithm substantially improves the state of the art in terms of both binning accuracies and the scope of applicability other attractive properties of genomes barcodes include a the barcodes have different and identifiable characteristics for different classes of genomes like prokaryotes eukaryotes mitochondria and plastids and b barcodes similarities are generally proportional to the genomes phylogenetic closeness conclusion these and other properties of genomes barcodes make them a new and effective tool for studying numerous genome and metagenome problems
nucleosome organization is critical for gene in living cells this organization is determined by multiple factors including the action of chromatin competition with site specific dna binding and the dna sequence preferences of the nucleosomes however it has been difficult to estimate the relative importance of each of these mechanisms in because in vivo nucleosome maps reflect the combined action of all influencing factors here we determine the importance of nucleosome dna sequence preferences experimentally by measuring the genome wide occupancy of nucleosomes assembled on purified yeast genomic dna the resulting map in which nucleosome occupancy is governed only by the intrinsic sequence preferences of nucleosomes is similar to in vivo nucleosome maps generated in three different growth conditions in vitro nucleosome depletion is evident at many transcription factor binding sites and around gene start and end sites indicating that nucleosome depletion at these sites in vivo is partly encoded in the genome we confirm these results with a micrococcal nuclease independent experiment that measures the relative affinity of nucleosomes for double stranded base pair oligonucleotides using our in vitro data we devise a computational model of nucleosome sequence preferences that is significantly correlated with in vivo nucleosome occupancy in caenorhabditis elegans our results indicate that the intrinsic dna sequence preferences of nucleosomes have a central role in determining the organization of nucleosomes vivo
background one mechanism to account for robustness against gene knockouts or knockdowns is through buffering by gene duplicates but the extent and general correlates of this process in organisms is still a matter of debate to reveal general trends of this process we provide a comprehensive comparison of gene essentiality duplication and buffering by duplicates across seven bacteria mycoplasma genitalium bacillus subtilis helicobacter pylori haemophilus influenzae mycobacterium tuberculosis pseudomonas aeruginosa escherichia coli and four eukaryotes saccharomyces cerevisiae yeast caenorhabditis elegans worm drosophila melanogaster fly mus musculus mouse results in nine of the eleven organisms duplicates significantly increase chances of survival upon gene deletion p value or but only by up to given that duplicates make up to of eukaryotic genomes the small contribution is surprising and points to dominant roles of other buffering processes such as alternative metabolic pathways the buffering capacity of duplicates appears to be independent of the degree of gene essentiality and tends to be higher for genes with high expression levels for example buffering capacity increases to amongst highly expressed genes in e coli sequence similarity and the number of duplicates per gene are weak predictors of the duplicate s buffering capacity in a case study we show that buffering gene duplicates in yeast and worm are somewhat more similar in their functions than non buffering duplicates and have increased transcriptional and translational activity conclusion in sum the extent of gene essentiality and buffering by duplicates is not conserved across organisms and does not correlate with the organisms apparent complexity this heterogeneity goes beyond what would be expected from differences in experimental approaches alone buffering by duplicates contributes to robustness in several organisms but to a small extent and the relatively large amount of buffering by duplicates observed in yeast and worm may be largely specific to these organisms thus the only common factor of buffering by duplicates between different organisms may be the by product of duplicate retention due to demands of dosage
b purpose b the purpose of this paper is to provide library managers with the ability to recognize and address world information issues to enhance their ability to develop management plans for the future b design methodology approach b this paper explores what world means to library managers in three ways three information dimensions are identified using models to examine world in a historical context an analysis is conducted of the different generations of users in world including their diverse attitudes beliefs experiences and skills and how these influence their engagement with the information environment four key characteristics of web are identified through an analysis of web in relation to world b findings b key findings in this paper are that three dimensions of information in world exist and can be used by library managers to help them understand the challenges and to facilitate the construction of strategic management plans that address them generational and organizational perspectives of world can influence how libraries engage web and should be considered when library managers make strategic management plans for the future the four characteristics of web create considerations for library managers during their planning processes b originality value b this paper is of interest because it provides library managers with a thorough understanding of world and how it may influence their libraries and their users so they can make more informed more successful choices
abstract illumina s genome analyzer generates ultra short sequence reads typically nucleotides in length and is primarily intended for resequencing we tested the potential of this technology for de novo sequence assembly on the mbp genome of pseudomonas syringae pv syringae with several freely available assembly software packages using an unpaired data set velvet assembled of the genome into contigs with an length of nucleotides and an error rate of edena generated smaller contigs was nucleotides and comparable error rates ssake and vcakeyielded shorter contigs with very high error rates assembly of paired end sequence data carrying bp inserts produced longer contigs up to nucleotides but with increased error rates contig length and error rate were very sensitive to the choice of parameter values noncoding rna genes were poorly resolved in de novo assemblies while of the protein coding genes were assembled with accuracy over their full length this study demonstrates that in practice de novo assembly of nucleotide reads can generate reasonably accurate assemblies from about x deep sequence data sets these draft assemblies are useful for exploring an organism s proteomic potential at a very economic cost
the flow of research data concerning the genetic basis of health and disease is rapidly increasing in speed and complexity in response many projects are seeking to ensure that there are appropriate informatics tools systems and databases available to manage and exploit this flood of information previous solutions such as central databases journal based publication and manually intensive data curation are now being enhanced with new systems for federated databases database publication and more automated management of data flows and quality control along with emerging technologies that enhance connectivity and data retrieval these advances should help to create a powerful knowledge environment for genotype information
gr micrornas mirnas are small noncoding rnas that control gene expression by inducing rna cleavage or translational inhibition most human mirnas are intragenic and are transcribed as part of their hosting transcription units we hypothesized that the expression profiles of mirna host genes and of their targets are inversely correlated and devised a novel procedure hoctar host gene oppositely correlated targets which ranks predicted mirna target genes based on their anti correlated expression behavior relative to their respective mirna host genes hoctar is the first tool for systematic mirna target prediction that utilizes the same set of microarray experiments to monitor the expression of both mirnas through their host genes and candidate targets we applied the procedure to human intragenic mirnas and found that it performs better than currently available prediction softwares in pinpointing previously validated mirna targets the high scoring hoctar predicted targets were enriched in gene ontology categories which were consistent with previously published data as in the case of mir and mir by means of overexpression and loss of function assays we also demonstrated that hoctar is efficient in predicting novel mirna targets and we identified by microarray and qrt pcr procedures and novel targets for mir and mir respectively overall we believe that the use of hoctar significantly reduces the number of candidate mirna targets to be tested compared to the procedures based solely on target sequence recognition finally our data further confirm that mirnas have a significant impact on the mrna levels of most of targets
motivation improving the usability of bioinformatics resources enables researchers to find interact with share compare and manipu late important information more effectively and efficiently it thus enables researchers to gain improved insights into biological processes with the potential ultimately of yielding new scientific results usability barriers can pose significant obstacles to a satisfactory user experience and force researchers to spend unnecessary time and effort to complete their tasks the number of online biological databases available is growing and there is an expanding community of diverse users in this context there is an increasing need to ensure the highest stan dards of usability results using state of the art usability evaluation methods we have identified and characterised a sample of usability issues potentially relevant to web bioinformatics resources in general these specifically concern the design of the navigation and search mechanisms available to the user the usability issues we have discovered in our substantial case studies are un dermining the ability of users to find the information they need in their daily research activities in addition to characterising these issues specific recommendations for improvements are proposed leveraging proven practices from web and usability engineering the methods and approach we exemplify can be readily adopted by the developers of bioinformatics resources supplementary information additional data about the usability methods and a summary of the results of the usability analyses presented in the paper is provided in a supplementary infor file
motivation while text mining technologies for biomedical research have gained popularity as a way to take advantage of the explosive growth of information in text form in biomedical papers selecting appropriate natural language processing nlp tools is still difficult for researchers who are not familiar with recent advances in nlp this article provides a comparative evaluation of several state of the art natural language parsers focusing on the task of extracting protein protein interaction ppi from biomedical papers we measure how each parser and its output representation contributes to accuracy improvement when the parser is used as a component in a ppi system results all the parsers attained improvements in accuracy of ppi extraction the levels of accuracy obtained with these different parsers vary slightly while differences in parsing speed are larger the best accuracy in this work was obtained when we combined miyao and tsujii s enju parser and charniak and johnson s reranking parser and the accuracy is better than the state of the art results on the same data availability the ppi extraction system used in this work akaneppi is available online at http www tsujii is s u tokyo ac jp downloads downloads cgi the evaluated parsers are also available online from each developer s site contact yusuke is s u tokyo ac bioinformatics
rapid and inexpensive sequencing technologies are making it possible to collect whole genome sequence data on multiple individuals from a population this type of data can be used to quickly identify genes that control important ecological and evolutionary phenotypes by finding the targets of adaptive natural selection and we therefore refer to such approaches as reverse ecology to quantify the power gained in detecting positive selection using population genomic data we compare three statistical methods for identifying targets of selection the test the mkprf method and a likelihood implementation for detecting dn ds gt because the first two methods use polymorphism data we expect them to have more power to detect selection however when applied to population genomic datasets from human fly and yeast the tests using polymorphism data were actually weaker in two of the three datasets we explore reasons why the simpler comparative method has identified more genes under selection and suggest that the different methods may really be detecting different signals from the same sequence data finally we find several statistical anomalies associated with the mkprf method including an almost linear dependence between the number of positively selected genes identified and the prior distributions used we conclude that interpreting the results produced by this method should be done with caution
bioinformatics motivation recognition of specific dna sequences is a central mechanism by which transcription factors tfs control gene expression many tf binding preferences however are unknown or poorly characterized in part due to the difficulty associated with determining their specificity experimentally and an incomplete understanding of the mechanisms governing sequence specificity new techniques that estimate the affinity of tfs to all possible k mers provide a new opportunity to study dnaprotein interaction mechanisms and may facilitate inference of binding preferences for members of a given tf family when such information is available for other family members results we employed a new dataset consisting of the relative preferences of mouse homeodomains for all eight base dna sequences in order to ask how well we can predict the binding profiles of homeodomains when only their protein sequences are given we evaluated a panel of standard statistical inference techniques as well as variations of the protein features considered nearest neighbour among functionally important residues emerged among the most effective methods our results underscore the complexity of tfdna recognition and suggest a rational approach for future analyses of tf families contact t hughes utorotno casupplementary information supplementary data are available at online
background thanks to recent high coverage mass spectrometry studies and reconstructed protein complexes we are now in an unprecedented position to study the evolution of biological systems gene duplications known to be a major source of innovation in evolution can now be readily examined in the context of protein complexes results we observe that paralogs operating in the same complex fulfill different roles mrna dosage increase for more than a hundred cytosolic ribosomal proteins mutually exclusive participation of at least paralogs resulting in alternative forms of complexes and proteins contributing to bona fide structural growth inspection of paralogous proteins participating in two independent complexes shows that an ancient pre duplication protein functioned in both multi protein assemblies and a gene duplication event allowed the respective copies to specialize and split their roles conclusions variants with conditionally assembled paralogous subunits likely have played a role in yeast s adaptation to anaerobic conditions in a number of cases the gene duplication has given rise to one duplicate that is no longer part of a protein complex and shows an accelerated rate of evolution such genes could provide the raw material for the evolution of functions
abstract background the amount of gene expression data in the public repositories such as ncbi gene expression omnibus geo has grown exponentially and provides a gold mine for bioinformaticians but has not been easily accessible by biologists and clinicians results we developed an automated approach to annotate and analyze all geo data sets including geo data sets from microarray types across species and performed group versus group comparisons of geo specified types we then built genechaser a web server that enables biologists and clinicians without bioinformatics skills to easily identify biological and clinical conditions in which a gene or set of genes was differentially expressed genechaser displays these conditions in graphs gives statistical comparisons allows sort filter functions and provides access to the original studies we performed a single gene search for nanog and a multiple gene search for nanog and confirmed their roles in embryonic stem cell development identified several drugs that regulate their expression and suggested their potential roles in sex determination abnormal sperm morphology malaria infection and cancer conclusions we demonstrated that genechaser is a powerful tool to elucidate information on function transcriptional regulation drug response and clinical implications for genes interest
label free chemical contrast is highly desirable in biomedical imaging spontaneous raman microscopy provides specific vibrational signatures of chemical bonds but is often hindered by low sensitivity here we report a three dimensional multiphoton vibrational imaging technique based on stimulated raman scattering srs the sensitivity of srs imaging is significantly greater than that of spontaneous raman microscopy which is achieved by implementing high frequency megahertz phase sensitive detection srs microscopy has a major advantage over previous coherent raman techniques in that it offers background free and readily interpretable chemical contrast we show a variety of biomedical applications such as differentiating distributions of omega fatty acids and saturated lipids in living cells imaging of brain and skin tissues based on intrinsic lipid contrast and monitoring drug delivery through the science
background recent genomic scale survey of epigenetic states in the mammalian genomes has shown that promoters and enhancers are correlated with distinct chromatin signatures providing a pragmatic way for systematic mapping of these regulatory elements in the genome with rapid accumulation of chromatin modification profiles in the genome of various organisms and cell types this chromatin based approach promises to uncover many new regulatory elements but computational methods to effectively extract information from these datasets are still limited results we present here a supervised learning method to predict promoters and enhancers based on their unique chromatin modification signatures we trained hidden markov models hmms on the histone modification data for known promoters and enhancers and then used the trained hmms to identify promoter or enhancer like sequences in the human genome using a simulated annealing sa procedure we searched for the most informative combination and the optimal window size of histone marks conclusion compared with the previous methods the hmm method can capture the complex patterns of histone modifications particularly from the weak signals cross validation and scanning the encode regions showed that our method outperforms the previous profile based method in mapping promoters and enhancers we also showed that including more histone marks can further boost the performance of our method this observation suggests that the hmm is robust and is capable of integrating information from multiple histone marks to further demonstrate the usefulness of our method we applied it to analyzing genome wide chip seq data in three mouse cell lines and correctly predicted active and inactive promoters with positive predictive values of more than the software is available at http http nash ucsd edu chromatin gz
gold standard datasets on protein complexes are key to inferring and validating protein protein interactions despite much progress in characterizing protein complexes in the yeast saccharomyces cerevisiae numerous researchers still use as reference the manually curated complexes catalogued by the munich information center of protein sequences database although this catalogue has served the community extremely well it no longer reflects the current state of knowledge here we report two catalogues of yeast protein complexes as results of systematic curation efforts the first one denoted as is a comprehensive catalogue of manually curated heteromeric protein complexes reliably backed by small scale experiments reported in the current literature this catalogue represents an up to date reference set for biologists interested in discovering protein interactions and protein complexes the second catalogue denoted as comprises high throughput complexes annotated with current literature evidence among them correspond at least partially to complexes evidence for interacting subunits is collected for complexes that have only partial or no overlap with complexes whereas no literature evidence was found for complexes some of these partially supported and as yet unsupported complexes may be interesting candidates for experimental follow up both catalogues are freely available at http org
abstract kegg spider is a web based tool for interpretation of experimentally derived gene lists in order to gain understanding of metabolism variations at a genomic level kegg spider implements a pathway free framework which overcomes a major bottleneck of enrichment analyses it provides global models uniting genes from different metabolic pathways analysing a number of experimentally derived gene lists we demonstrate that kegg spider provides deeper insights into metabolism variations in comparison to methods
interpreting brain image experiments requires analysis of complex multivariate data in recent years one analysis approach that has grown in popularity is the use of machine learning algorithms to train classifiers to decode stimuli mental states behaviours and other variables of interest from fmri data and thereby show the data contain information about them in this tutorial overview we review some of the key choices faced in using this approach as well as how to derive statistically significant results illustrating each point from a case study furthermore we show how in addition to answering the question of is there information about a variable of interest pattern discrimination classifiers can be used to tackle other classes of question namely where is the information pattern localization and how is that information encoded characterization
background tuberculosis still remains one of the largest killer infectious diseases warranting the identification of newer targets and drugs identification and validation of appropriate targets for designing drugs are critical steps in drug discovery which are at present major bottle necks a majority of drugs in current clinical use for many diseases have been designed without the knowledge of the targets perhaps because standard methodologies to identify such targets in a high throughput fashion do not really exist with different kinds of omics data that are now available computational approaches can be powerful means of obtaining short lists of possible targets for further experimental validation results we report a comprehensive in silico target identification pipeline targettb for mycobacterium tuberculosis the pipeline incorporates a network analysis of the protein protein interactome a flux balance analysis of the reactome experimentally derived phenotype essentiality data sequence analyses and a structural assessment of targetability using novel algorithms recently developed by us using flux balance analysis and network analysis proteins critical for survival of m tuberculosis are first identified followed by comparative genomics with the host finally incorporating a novel structural analysis of the binding sites to assess the feasibility of a protein as a target further analyses include correlation with expression data and non similarity to gut flora proteins as well as anti targets in the host leading to the identification of high confidence targets through phylogenetic profiling against pathogen genomes shortlisted targets have been further explored to identify broad spectrum antibiotic targets while also identifying those specific to tuberculosis targets that address mycobacterial persistence and drug resistance mechanisms are also analysed conclusion the pipeline developed provides rational schema for drug target identification that are likely to have high rates of success which is expected to save enormous amounts of money resources and time in the drug discovery process a thorough comparison with previously suggested targets in the literature demonstrates the usefulness of the integrated approach used in our study highlighting the importance of systems level analyses in particular the method has the potential to be used as a general strategy for target identification and validation and hence significantly impact most drug programmes
member of the sirtuin family is a nicotinamide adenosine dinucleotide nad dependent deacetylase that removes acetyl groups from various proteins a wide variety of proteins are substrates the list includes many transcription factors and cofactors deacetylation of these factors may lead to activation or inactivation of the factor thus impacting downstream gene expression in addition to direct deacetylation can modulate protein activity by other mechanisms although initial research focused on sirtuin s role in life span extension especially in lower organisms more recent studies show that activity can impact a wide array of proteins implicated in cardiovascular cv and metabolic diseases several patents have been published in the last years describing the application of sirtuin compounds in the treatment of metabolic diseases this review will focus on those modifiable proteins that have an impact on cv and metabolic diseases pharmacological agents that activate and thus impact the disease process will also reviewed
constraints in embryonic development are thought to bias the direction of evolution by making some changes less likely and others more likely depending on their consequences on ontogeny here we characterize the constraints acting on genome evolution in vertebrates we used gene expression data from two vertebrates zebrafish using a microarray experiment spanning stages of development and mouse using est counts for stages of development we show that in both species genes expressed early in development have a more dramatic effect of knock out or mutation and are more likely to revert to single copy after whole genome duplication relative to genes expressed late this supports high constraints on early stages of vertebrate development making them less open to innovations gene gain or gene loss results are robust to different sources of data gene expression from microarrays ests or in situ hybridizations and mutants from directed ko transgenic insertions point mutations or morpholinos we determine the pattern of these constraints which differs from the model used to describe vertebrate morphological conservation hourglass model while morphological constraints reach a maximum at mid development the phylotypic stage genomic constraints appear to decrease in a monotonous manner over time
background the effects of intensive glucose control on cardiovascular events in patients with long standing type diabetes mellitus remain uncertain methods we randomly assigned military veterans mean age years who had a suboptimal response to therapy for type diabetes to receive either intensive or standard glucose control other cardiovascular risk factors were treated uniformly the mean number of years since the diagnosis of diabetes was and of the patients had already had a cardiovascular event the goal in the intensive therapy group was an absolute reduction of percentage points in the glycated hemoglobin level as compared with the standard therapy group the primary outcome was the time from randomization to the first occurrence of a major cardiovascular event a composite of myocardial infarction stroke death from cardiovascular causes congestive heart failure surgery for vascular disease inoperable coronary disease and amputation for ischemic gangrene results the median follow up was years median glycated hemoglobin levels were in the standard therapy group and in the intensive therapy group the primary outcome occurred in patients in the standard therapy group and patients in the intensive therapy group hazard ratio in the intensive therapy group confidence interval ci to p there was no significant difference between the two groups in any component of the primary outcome or in the rate of death from any cause hazard ratio ci to p no differences between the two groups were observed for microvascular complications the rates of adverse events predominantly hypoglycemia were in the standard therapy group and in the intensive therapy group conclusions intensive glucose control in patients with poorly controlled type diabetes had no significant effect on the rates of major cardiovascular events death or microvascular complications with the exception of progression of albuminuria p added clinicaltrials number
when estimating binding affinities of a ligand which can exists in multiple forms for a target molecule one must consider all possible competing equilibria here a method is presented that estimates the contribution of the protonation equilibria of a ligand in solution to the measured or calculated binding affinity the method yields a correction to binding constants that are based on the total concentration of inhibitor the sum of all ionized forms of the inhibitor in solution to account for the complexed form of the inhibitor only the method is applied to the calculation of the difference in binding affinity of two inhibitors phosphoglycolate pga and its phoshonate analog phosphonopropionate for the glycolytic enzyme triosephosphate isomerase both inhibitors have three titrating sites and exist in solution as a mixture of different forms in this case the form that actually binds to the enzyme is present at relative low concentrations the contributions of the alternative forms to the difference in binding energies is estimated by means of molecular dynamics simulations and corrections the inhibitors undergo a pka shift upon binding that is estimated by ab initio calculations an interesting finding is that the affinity difference of the two inhibitors is not due to different interactions in the active site of the enzyme but rather due to the difference in the solvation properties of the inhibitors protein wiley inc
we describe the use of the social reference management website citeulike for recommending scientific articles to users based on their reference library we test three different collaborative filtering algorithms and find that user based filtering performs best a temporal analysis of the data indexed by citeulike shows that it takes about two years for the cold start problem to disappear and recommendation performance improve
integrated genome databases such as the ucsc ensembl and ncbi mapviewer databases and their associated data querying and visualization interfaces e g the genome browsers have transformed the way that molecular biologists geneticists and bioinformaticists analyze genomic data nevertheless because of the complexity of these tools many researchers take advantage of only a fraction of their capabilities in this tutorial using examples from medical genetics and alternative splicing i describe some of the biological questions that can be addressed with these techniques i also show why doing so typically is more effective than using alternative methods and indicate some of the resources available for learning more about the advanced capabilities of these tools
abstract background high throughput methods that allow for measuring the expression of thousands of genes or proteins simultaneously have opened new avenues for studying biochemical processes while the noisiness of the data necessitates an extensive pre processing of the raw data the high dimensionality requires effective statistical analysis methods that facilitate the identification of crucial biological features and relations for these reasons the evaluation and interpretation of expression data is a complex labor intensive multi step process while a variety of tools for normalizing analysing or visualizing expression profiles has been developed in the last years most of these tools offer only functionality for accomplishing certain steps of the evaluation pipeline results here we present a web based toolbox that provides rich functionality for all steps of the evaluation pipeline our tool genetrailexpress offers besides standard normalization procedures powerful statistical analysis methods for studying a large variety of biological categories and pathways furthermore an integrated graph visualization tool enables the user to draw the relevant biological pathways applying cutting edge graph layout algorithms conclusions our gene expression toolbox with its interactive visualization of the pathways and the expression values projected onto the nodes will simplify the analysis and interpretation of biochemical considerably
abstractcase study is a suitable research methodology for software engineering research since it studies contemporary phenomena in its natural context however the understanding of what constitutes a case study varies and hence the quality of the resulting studies this paper aims at providing an introduction to case study methodology and guidelines for researchers conducting case studies and readers studying reports of such studies the content is based on the authors own experience from conducting and reading case studies the terminology and guidelines are compiled from different methodology handbooks in other research domains in particular social science and information systems and adapted to the needs in software engineering we present recommended practices for software engineering case studies as well as empirically derived and evaluated checklists for researchers and readers of case research
david bioinformatics resources consists of an integrated biological knowledgebase and analytic tools aimed at systematically extracting biological meaning from large gene protein lists this protocol explains how to use david a high throughput and integrated data mining environment to analyze gene lists derived from high throughput genomic experiments the procedure first requires uploading a gene list containing any number of common gene identifiers followed by analysis using one or more text and pathway mining tools such as gene functional classification functional annotation chart or clustering and functional annotation table by following this protocol investigators are able to gain an in depth understanding of the biological themes in lists of genes that are enriched in genome studies
cholesterol mediated lipid interactions are thought to have a functional role in many membrane associated processes such as signalling although several experiments indicate their existence lipid nanodomains rafts remain controversial owing to the lack of suitable detection techniques in living the controversy is reflected in their putative size of spanning the range between the extent of a protein complex and the resolution limit of optical microscopy here we demonstrate the ability of stimulated emission depletion sted far field fluorescence to detect single diffusing lipid molecules in nanosized areas in the plasma membrane of living cells tuning of the probed area to spot sizes fold below the diffraction barrier reveals that unlike phosphoglycerolipids sphingolipids and glycosylphosphatidylinositol anchored proteins are transiently trapped in cholesterol mediated molecular complexes dwelling within nm diameter areas the non invasive optical recording of molecular time traces and fluctuation data in tunable nanoscale domains is a powerful new approach to study the dynamics of biomolecules in cells
by applying a method that combines end sequence profiling and massively parallel sequencing we obtained a sequence level map of chromosomal aberrations in the genome of the mcf breast cancer cell line a total of distinct somatic breakpoints of two distinct types dispersed and clustered were identified a total of breakpoints are evenly dispersed across the genome a majority of dispersed breakpoints are in regions of low copy repeats lcrs indicating a possible role for lcrs in chromosome breakage the remaining breakpoints form four distinct clusters of closely spaced breakpoints that coincide with the four highly amplified regions in mcf detected by array cgh located in the and chromosomal cytobands the clustered breakpoints are not significantly associated with lcrs sequences flanking most breakpoint junctions are consistent with double stranded dna break repair by nonhomologous end joining or template switching a total of known or predicted genes are involved in rearrangement events including fusions of coding exons from different genes and other rearrangements four fusions result in novel expressed chimeric mrna transcripts one of the four expressed fusion products and one gene truncation or involve genes coding for members of protein complexes responsible for homology driven repair of double stranded dna breaks another one of the four expressed fusion products involves a regulator of cell growth and angiogenesis we show that knock down of in cell lines causes tumorigenic phenotypes including increased proliferation enhanced survival and increased anchorage growth
abstract background protein structural alignment provides a fundamental basis for deriving principles of functional and evolutionary relationships it is routinely used for structural classification and functional characterization of proteins and for the construction of sequence alignment benchmarks however the available techniques do not fully consider the implications of protein structural diversity and typically generate a single alignment between sequences results we have taken alternative protein crystal structures and generated simulation snapshots to explicitly investigate the impact of structural changes on the alignments we show that structural diversity has a significant effect on structural alignment moreover we observe alignment inconsistencies even for modest spatial divergence implying that the biological interpretation of alignments is less straightforward than commonly assumed a salient example is the groes mobile loop where sub angstrom variations give rise to contradictory sequence alignments conclusions a comprehensive treatment of ambiguous alignment regions is crucial for further development of structural alignment applications and for the representation of alignments in general for this purpose we have developed an on line database containing our data and new ways of visualizing alignment inconsistencies which can be found at www ibi vu nl stralivari
pnas one of the most challenging problems in protein structure prediction is improvement of homology models structures within c rmsd of the native structure also known as the protein structure refinement problem it has been shown that improvement could be achieved using in vacuo energy minimization with molecular mechanics and statistically derived continuously differentiable hybrid knowledge based kb potential functions globular proteins however fold and function in aqueous solution in vivo and in vitro in this work we study the role of solvent in protein structure refinement molecular dynamics in explicit solvent and energy minimization in both explicit and implicit solvent were performed on a set of native proteins to test the various energy potentials a more stringent test for refinement was performed on near native decoys for each native protein we use a powerfully convergent energy minimization method to show that implicit solvent gbsa provides greater improvement for some proteins than the kb potential of proteins showing an average improvement of in c rmsd from the native structure with gbsa compared to just proteins with kb molecular dynamics in explicit solvent moved the structures further away from their native conformation than the initial unrefined decoys implicit solvent gives rise to a deep smooth potential energy attractor basin that pulls toward the structure
summary to analyse the vast amount of genome annotation data available today a visual representation of genomic features in a given sequence range is required we developed a c library which provides layout and drawing capabilities for annotation features it supports several common input and output formats and can easily be integrated into custom c applications to exemplify the use of annotationsketch in other languages we provide bindings to the scripting languages ruby python and lua availability the software is available under an open source license as part of genometools http genometools org annotationsketch html contact steinbiss zbh uni hamburg bioinformatics
this research uses adapted self assessment questionnaires to examine the relationships between the learning motivation learning strategies self efficacy attribution and learning results of distance learners the aim is to model the relationship between psychological characteristics and learning results of distance learners the outcomes of this study show that a relationship exists between psychological characteristics and learning scores of distance learners first there is a relationship between self efficacy learning strategies and learning results second there is a relationship between self efficacy internal attribution learning motivation and learning results learning motivation and learning strategies are clearly associated with positive and predictable effects on learning results the effect values are and respectively self efficacy and internal attribution have indirectly positive predictable effects on learning results the effect values are respectively
i sketch what it is supposed to mean to quantize gauge theory and how this can be made more concrete in perturbation theory and also by starting with a finite dimensional lattice approximation based on real experiments and computer simulations quantum gauge theory in four dimensions is believed to have a mass gap this is one of the most fundamental facts that makes the universe the way it is this article is the written form of a lecture presented at the conference geometric analysis past and future harvard university august september in honor of the birthday of s yau
the torrent of data emerging from the application of new technologies to functional genomics and systems biology can no longer be contained within the traditional modes of data sharing and publication with the consequence that data is being deposited in distributed across and disseminated through an increasing number of databases the resulting fragmentation poses serious problems for the model organism community which increasingly rely on data mining and computational approaches that require gathering of data from a range of sources in the light of these problems the european commission has funded a coordination action casimir coordination and sustainability of international mouse informatics resources with a remit to assess the technical and social aspects of database interoperability that currently prevent the full realization of the potential of data integration in mouse functional genomics in this article we assess the current problems with interoperability with particular reference to mouse functional genomics and critically review the technologies that can be deployed to overcome them we describe a typical use case where an investigator wishes to gather data on variation genomic context and metabolic pathway involvement for genes discovered in a genome wide screen we go on to develop an automated approach involving an in silico experimental workflow tool taverna using web services biomart and molgenis technologies for data retrieval finally we focus on the current impediments to adopting such an approach in a wider context and strategies to overcome bib
deregulation of kinase activity has emerged as a major mechanism by which cancer cells evade normal physiological constraints on growth and survival to date kinase inhibitors have received us food and drug administration approval as cancer treatments and there are considerable efforts to develop selective small molecule inhibitors for a host of other kinases that are implicated in cancer and other diseases herein we discuss the current challenges in the field such as designing selective inhibitors and developing strategies to overcome resistance mutations this review provides a broad overview of some of the approaches currently used to discover and characterize new inhibitors
in this paper we present a generative latent variable model for rating based collaborative filtering called the user rating profile model urp the generative process which underlies urp is designed to produce complete user rating profiles an assignment of one rating to each item for each user our model represents each user as a mixture of user attitudes and the mixing proportions are distributed according to a dirichlet random variable the rating for each item is generated by selecting a user attitude for the item and then selecting a rating according to the preference pattern associated with that attitude urp is related to several models including a multinomial mixture model the aspect model and lda but has clear advantages each
micrornas are small nt rnas that regulate gene expression and play important roles in both normal and disease physiology the use of microarrays for global characterization of microrna expression is becoming increasingly popular and has the potential to be a widely used and valuable research tool however microarray profiling of microrna expression raises a number of data analytic challenges that must be addressed in order to obtain reliable results we introduce here a universal reference microrna reagent set as well as a series of nonhuman spiked in synthetic microrna controls and demonstrate their use for quality control and between array normalization of microrna expression data we also introduce diagnostic plots designed to assess and compare various normalization methods we anticipate that the reagents and analytic approach presented here will be useful for improving the reliability of microrna microarray nar
one of the most provocative and exciting issues in cognitive science is how neural specificity for semantic categories of common objects arises in the functional architecture of the brain more than two decades of research on the neuropsychological phenomenon of category specific semantic deficits has generated detailed claims about the organization and representation of conceptual knowledge more recently researchers have sought to test hypotheses developed on the basis of neuropsychological evidence with functional imaging from those two fields the empirical generalization emerges that object domain and sensory modality jointly constrain the organization of knowledge in the brain at the same time research within the embodied cognition framework has highlighted the need to articulate how information is communicated between the sensory and motor systems and processes that represent and generalize abstract information those developments point toward a new approach for understanding category specificity in terms of the coordinated influences of diverse regions and cognitive systems copyright by reviews
recent growth of social classification systems due to steadily increasing popularity has established a multitude of heterogeneous isolated non integrated and non interoperable tag spaces contrary to current research predominantly focusing on single folksonomies we exploit cross space similarities to improve a variety of tagging use cases beyond the limits of one folksonomy this paper presents the results of practical studies concerning cross space analysis of co tag spaces of five well established social classification services for tagging of bookmarks del icio us bibsonomy bookmarks and publications bibsonomy publications citeulike connotea the studies are based on one month data sets of rss recent feeds from the same time scope we provide a profound motivation for cross space tagging and give insight into similarities and intersections of top ranking co tag spaces as well as convergence aspects time
abstract background microarray analysis allows the simultaneous measurement of thousands to millions of genes or sequences across tens to thousands of different samples the analysis of the resulting data tests the limits of existing bioinformatics computing infrastructure a solution to this issue is to use high performance computing hpc systems which contain many processors and more memory than desktop computer systems many biostatisticians use r to process the data gleaned from microarray analysis and there is even a dedicated group of packages bioconductor for this purpose however to exploit hpc systems r must be able to utilise the multiple processors available on these systems there are existing modules that enable r to use multiple processors but these are either difficult to use for the hpc novice or cannot be used to solve certain classes of problems a method of exploiting hpc systems using r but without recourse to mastering parallel programming paradigms is therefore necessary to analyse genomic data to its fullest results we have designed and built a prototype framework that allows the addition of parallelised functions to r to enable the easy exploitation of hpc systems the simple parallel r interface sprint is a wrapper around such parallelised functions their use requires very little modification to existing sequential r scripts and no expertise in parallel computing as an example we created a function that carries out the computation of a pairwise calculated correlation matrix this performs well with sprint when executed using sprint on an hpc resource of eight processors this computation reduces by more than three times the time r takes to complete it on one processor conclusions sprint allows the biostatistician to concentrate on the research problems rather than the computation while still allowing exploitation of hpc systems it is easy to use and with further development will become more useful as more functions are added to framework
background correlation networks are increasingly being used in bioinformatics applications for example weighted gene co expression network analysis is a systems biology method for describing the correlation patterns among genes across microarray samples weighted correlation network analysis wgcna can be used for finding clusters modules of highly correlated genes for summarizing such clusters using the module eigengene or an intramodular hub gene for relating modules to one another and to external sample traits using eigengene network methodology and for calculating module membership measures correlation networks facilitate network based gene screening methods that can be used to identify candidate biomarkers or therapeutic targets these methods have been successfully applied in various biological contexts e g cancer mouse genetics yeast genetics and analysis of brain imaging data while parts of the correlation network methodology have been described in separate publications there is a need to provide a user friendly comprehensive and consistent software implementation and an accompanying tutorial results the wgcna r software package is a comprehensive collection of r functions for performing various aspects of weighted correlation network analysis the package includes functions for network construction module detection gene selection calculations of topological properties data simulation visualization and interfacing with external software along with the r package we also present r software tutorials while the methods development was motivated by gene expression data the underlying data mining approach can be applied to a variety of different settings conclusion the wgcna package provides r functions for weighted correlation network analysis e g co expression network analysis of gene expression data the r package along with its source code and additional material are freely available at http www genetics ucla edu labs horvath coexpressionnetwork wgcna
comprehensive protein interaction mapping projects are underway for many model species and humans a key step in these projects is estimating the time cost and personnel required for obtaining an accurate and complete map here we modeled the cost of interaction map completion for various experimental designs we showed that current efforts may require up to independent tests covering each protein pair to approach completion we explored designs for reducing this cost substantially including prioritization of protein pairs probability thresholding and interaction prediction the best experimental designs lowered cost by fourfold overall and fold in early stages of mapping we demonstrate the best strategy in an ongoing project in drosophila melanogaster in which we mapped high confidence interactions using microtiter plates versus thousands of plates expected using current designs this study provides a framework for assessing the feasibility of interaction mapping projects and for future efforts to increase efficiency
several attempts have been made to systematically map protein protein interaction or interactome networks however it remains difficult to assess the quality and coverage of existing data sets here we describe a framework that uses an empirically based approach to rigorously dissect quality parameters of currently available human interactome maps our results indicate that high throughput yeast two hybrid ht interactions for human proteins are more precise than literature curated interactions supported by a single publication suggesting that ht is suitable to map a significant portion of the human interactome we estimate that the human interactome contains approximately binary interactions most of which remain to be mapped similar to estimates of dna sequence data quality and genome size early in the human genome project estimates of protein interaction data quality and interactome size are crucial to establish the magnitude of the task of comprehensive human interactome mapping and to elucidate a path toward goal
until not very long ago it was widely accepted that lens based far field optical microscopes cannot visualize details much finer than about half the wavelength of light the advent of viable physical concepts for overcoming the limiting role of diffraction in the early set off a quest that has led to readily applicable and widely accessible fluorescence microscopes with nanoscale spatial resolution here i discuss the principles of these methods together with their differences in implementation and operation finally i outline developments
how can we generate realistic networks in addition how can we do so with a mathematically tractable model that allows for rigorous analysis of network properties real networks exhibit a long list of surprising properties heavy tails for the in and out degree distribution heavy tails for the eigenvalues and eigenvectors small diameters and densification and shrinking diameters over time current network models and generators either fail to match several of the above properties are complicated to analyze mathematically or both here we propose a generative model for networks that is both mathematically tractable and can generate networks that have all the above mentioned structural properties our main idea here is to use a non standard matrix operation the kronecker product to generate graphs which we refer to as kronecker graphs first we show that kronecker graphs naturally obey common network properties in fact we rigorously prove that they do so we also provide empirical evidence showing that kronecker graphs can effectively model the structure of real networks we then present kronfit a fast and scalable algorithm for fitting the kronecker graph generation model to large real networks a naive approach to fitting would take super exponential time in contrast kronfit takes linear time by exploiting the structure of kronecker matrix multiplication and by using statistical simulation techniques experiments on a wide range of large real and synthetic networks show that kronfit finds accurate parameters that very well mimic the properties of target networks in fact using just four parameters we can accurately model several aspects of global network structure once fitted the model parameters can be used to gain insights about the network structure and the resulting synthetic graphs can be used for null models anonymization extrapolations and summarization
summary the assessment of data quality is a major concern in microarray analysis arrayqualitymetrics is a bioconductor package that provides a report with diagnostic plots for one or two colour microarray data the quality metrics assess reproducibility identify apparent outlier arrays and compute measures of signal to noise ratio the tool handles most current microarray technologies and is amenable to use in automated analysis pipelines or for automatic report generation as well as for use by individuals the diagnosis of quality remains in principle a context dependent judgement but our tool provides powerful automated objective and comprehensive instruments on which to base a decision availability arrayqualitymetrics is a free and open source package under lgpl license available from the bioconductor project at www bioconductor org a users guide and examples are provided with the package some examples of html reports generated by arrayqualitymetrics can be found at http www microarray org
the ability to manipulate nanoscopic matter precisely is critical for the development of active nanosystems optical are excellent tools for transporting particles ranging in size from several micrometres to a few hundred nanometres manipulation of dielectric objects with much smaller diameters however requires stronger optical confinement and higher intensities than can be provided by these diffraction systems here we present an approach to optofluidic transport that overcomes these limitations using sub wavelength liquid core slot the technique simultaneously makes use of near field optical forces to confine matter inside the waveguide and scattering adsorption forces to transport it the ability of the slot waveguide to condense the accessible electromagnetic energy to scales as small as allows us also to overcome the fundamental diffraction problem we apply the approach here to the trapping and transport of nm dielectric nanoparticles and dna molecules because trapping occurs along a line rather than at a point as with traditional point the method provides the ability to handle extended biomolecules directly we also carry out a detailed numerical analysis that relates the near field optical forces to release kinetics we believe that the architecture demonstrated here will help to bridge the gap between optical manipulation nanofluidics
the assembly of large recombinant dna encoding a whole biochemical pathway or genome represents a significant challenge here we report a new method dna assembler which allows the assembly of an entire biochemical pathway in a single step via in vivo homologous recombination in saccharomyces cerevisiae we show that dna assembler can rapidly assemble a functional d xylose utilization pathway approximately kb dna consisting of three genes a functional zeaxanthin biosynthesis pathway approximately kb dna consisting of five genes and a functional combined d xylose utilization and zeaxanthin biosynthesis pathway approximately kb consisting of eight genes with high efficiencies either on a plasmid or on a yeast chromosome as this new method only requires simple dna preparation and one step yeast transformation it represents a powerful tool in the construction of biochemical pathways for synthetic biology metabolic engineering and functional studies
our ability to reconstruct genome scale metabolic networks in microbial cells from genomic and high throughput data has grown substantially in recent years there are currently more than genome scale metabolic reconstructions of microbial cells and more are being produced each year an increasing number of research groups around the world are working on genome scale reconstructions of metabolism in their target organism there is no single source that practitioners can access to learn about and understand the reconstruction process this review details the data flows and work flows that underlie the reconstruction process and thus provides a basis for newcomers in the field biological network reconstructions continue to grow in scope and are expected to include transcriptional regulation and protein synthesis over the next few years expansion in scope will probably also include small rnas and two component signalling networks genome scale reconstructions are a common denominator in systems biology of microorganisms and are reaching an advanced stage of development which indicates that systems analysis of microbial functions and phenotypes will progress in the years come
background in cancer studies it is common that multiple microarray experiments are conducted to measure the same clinical outcome and expressions of the same set of genes an important goal of such experiments is to identify a subset of genes that can potentially serve as predictive markers for cancer development and progression analyses of individual experiments may lead to unreliable gene selection results because of the small sample sizes meta analysis can be used to pool multiple experiments increase statistical power and achieve more reliable gene selection the meta analysis of cancer microarray data is challenging because of the high dimensionality of gene expressions and the differences in experimental settings amongst different experiments results we propose a meta threshold gradient descent regularization mtgdr approach for gene selection in the meta analysis of cancer microarray data the mtgdr has many advantages over existing approaches it allows different experiments to have different experimental settings it can account for the joint effects of multiple genes on cancer and it can select the same set of cancer associated genes across multiple experiments simulation studies and analyses of multiple pancreatic and liver cancer experiments demonstrate the superior performance of the mtgdr conclusion the mtgdr provides an effective way of analyzing multiple cancer microarray studies and selecting reliable cancer genes
massively parallel tag based sequencing systems such as the solid system hold the promise of revolutionizing the study of whole genome gene expression due to the number of data points that can be generated in a simple and cost effective manner we describe the development of a transcriptome workflow for the solid system and demonstrate the advantages in sensitivity and dynamic range offered by this tag based application over traditional approaches for the study of whole genome gene expression end transcriptome analysis was used to study whole genome gene expression within a colon cancer cell line ht treated with the dna methyltransferase inhibitor aza deoxycytidine more than million base end tags were obtained from untreated and treated cells and matched to sequences within the human genome seventy three percent of the mapped unique tags were associated with refseq cdna sequences corresponding to approximately different protein coding genes in this single cell type the level of expression of these genes ranged from to transcripts per cell the sensitivity of a single sequence run of the solid platform was fold greater than that observed from sage data generated from the analysis of tags obtained by sanger sequencing the high resolution gene expression profiling presented in this study will not only provide novel insight into the transcriptional machinery but should also serve as a basis for a better understanding of biology
scientific research is increasingly digital some activities such as data analysis search and simulation can be accelerated by letting scientists write workflows and scripts that automate routine activities these capture pieces of the scientific method that scientists can share the taverna workbench a widely deployed scientific workflow management system together with the myexperiment social web site for sharing scientific experiments follow six principles of designing software for adoption by scientists and six principles of engagement
gene duplication provides much of the raw material from which functional diversity evolves two evolutionary mechanisms have been proposed that generate functional diversity neofunctionalization the de novo acquisition of function by one duplicate and subfunctionalization the partitioning of ancestral functions between gene duplicates with protein interactions as a surrogate for protein functions evidence of prodigious neofunctionalization and subfunctionalization has been identified in analyses of empirical protein interactions and evolutionary models of protein interactions however we have identified three phenomena that have contributed to neofunctionalization being erroneously identified as a significant factor in protein interaction network evolution first self interacting proteins are underreported in interaction data due to biological artifacts and design limitations in the two most common high throughput protein interaction assays second evolutionary inferences have been drawn from paralog analysis without consideration for concurrent and subsequent duplication events third the theoretical model of prodigious neofunctionalization is unable to reproduce empirical network clustering and relies on untenable parameter requirements in light of these findings we believe that protein interaction evolution is more persuasively characterized by subfunctionalization and interactions
despite the intense interest towards realizing the semantic web vision most existing rdf data management schemes are constrained in terms of efficiency and scalability still the growing popularity of the rdf format arguably calls for an effort to offset these drawbacks viewed from a relational database perspective these constraints are derived from the very nature of the rdf data model which is based on a triple format recent research has attempted to address these constraints using a vertical partitioning approach in which separate two column tables are constructed for each property however as we show this approach suffers from similar scalability drawbacks on queries that are not bound by rdf property value in this paper we propose an rdf storage scheme that uses the triple nature of rdf as an asset this scheme enhances the vertical partitioning idea and takes it to its logical conclusion rdf data is indexed in six possible ways one for each possible ordering of the three rdf elements each instance of an rdf element is associated with two vectors each such vector gathers elements of one of the other types along with lists of the third type resources attached to each vector element hence a sextuple indexing scheme emerges this format allows for quick and scalable general purpose query processing it confers significant advantages up to five orders of magnitude compared to previous approaches for rdf data management at the price of a worst case five fold increase in index space we experimentally document the advantages of our approach on real world and synthetic data sets with queries
background genomic imprinting is an epigenetic phenomenon that results in monoallelic gene expression many hypotheses have been advanced to explain why genomic imprinting evolved in mammals but few have examined how it arose the host defence hypothesis suggests that imprinting evolved from existing mechanisms within the cell that act to silence foreign dna elements that insert into the genome however the changes to the mammalian genome that accompanied the evolution of imprinting have been hard to define due to the absence of large scale genomic resources between all extant classes the recent release of the platypus genome has provided the first opportunity to perform comparisons between prototherian monotreme which appear to lack imprinting and therian marsupial and eutherian which have imprinting mammals results we compared the distribution of repeat elements known to attract epigenetic silencing across the entire genome from monotremes and therian mammals particularly focusing on the orthologous imprinted regions there is a significant accumulation of certain repeat elements within imprinted regions of therian mammals compared to the platypus conclusions our analyses show that the platypus has significantly fewer repeats of certain classes in the regions of the genome that have become imprinted in therian mammals the accumulation of repeats especially long terminal repeats and dna elements in therian imprinted genes and gene clusters is coincident with and may have been a potential driving force in the development of mammalian genomic imprinting these data provide strong support for the host hypothesis
at first sight selection is a simple notion and some consider it the most important evolutionary force but how important is selection is it really so trivial to understand and what are the alternatives here i discuss how genetics is crucial for addressing all of these questions genetics allowed the concept of natural selection to become viable it contributed to our understanding of the complexities of selection and it spurred the development of competing models of evolution understanding how and why selection acts has important potential applications from understanding the mechanisms of disease and microbial resistance to improving the design of transgenes drugs
to represent the sequence specificity of transcription factors the position weight matrix pwm is widely used in most cases each element is defined as a log likelihood ratio of a base appearing at a certain position which is estimated from a finite number of known binding sites to avoid bias due to this small sample size a certain numeric value called a pseudocount is usually allocated for each position and its fraction according to the background base composition is added to each element so far there has been no consensus on the optimal pseudocount value in this study we simulated the sampling process by artificially generating binding sites based on observed nucleotide frequencies in a public pwm database and then the generated matrix with an added pseudocount value was compared to the original frequency matrix using various measures although the results were somewhat different between measures in many cases we could find an optimal pseudocount value for each matrix these optimal values are independent of the sample size and are clearly correlated with the entropy of the original matrices meaning that larger pseudocount vales are preferable for less conserved binding sites as a simple representative we suggest the value of for practical nar
gad although micrornas mirnas are key regulators of gene expression in normal human physiology and disease transcriptional regulation of mirnas is poorly understood because most mirna promoters have not yet been characterized we identified the proximal promoters of human mirnas by combining nucleosome mapping with chromatin signatures for promoters we observe that one third of intronic mirnas have transcription initiation regions independent from their host promoters and present a list of rna polymerase ii and iii occupied mirnas nucleosome mapping and linker sequence analyses in mirna promoters permitted accurate prediction of transcription factors regulating mirna expression thus identifying nine mirnas regulated by the mitf transcription factor oncoprotein in melanoma cells furthermore dna sequences encoding mature mirnas were found to be preferentially occupied by positioned nucleosomes and the end sites of known genes exhibited nucleosome depletion the high throughput identification of mirna promoter and enhancer regulatory elements sheds light on evolution of mirna transcription and permits rapid identification of transcriptional networks mirnas
this paper describes how to automatically cross reference documents with wikipedia the largest knowledge base ever known it explains how machine learning can be used to identify significant terms within unstructured text and enrich it with links to the appropriate wikipedia articles the resulting link detector and disambiguator performs very well with recall and precision of almost this performance is constant whether the system is evaluated on wikipedia articles or real documents
we investigate the feasibility of high performance scientific computation using cloud computers as an alternative to traditional computational tools the availability of these large virtualized pools of compute resources raises the possibility of a new compute paradigm for scientific research with many advantages for research groups cloud computing provides convenient access to reliable high performance clusters and storage without the need to purchase and maintain sophisticated hardware for developers virtualization allows scientific codes to be optimized and pre installed on machine images facilitating control over the computational environment preliminary tests are presented for serial and parallelized versions of the widely used x ray spectroscopy and electronic structure code feff on the amazon elastic compute cloud including cpu and performance
abstract background biological studies involve a growing number of distinct high throughput experiments to characterize samples of interest there is a lack of methods to visualize these different genomic datasets in a versatile manner in addition genomic data analysis requires integrated visualization of experimental data along with constantly changing genomic annotation and statistical analyses results we developed genomegraphs as an add on software package for the statistical programming environment r to facilitate integrated visualization of genomic datasets genomegraphs uses the biomart package to perform on line annotation queries to ensembl and translates these to gene transcript structures in viewports of the grid graphics package this allows genomic annotation to be plotted together with experimental data genomegraphs can also be used to plot custom annotation tracks in combination with different experimental data types together in one plot using the same genomic coordinate system conclusions genomegraphs is a flexible and extensible software package which can be used to visualize a multitude of genomic datasets within the statistical programming r
when students answer an in class conceptual question individually using clickers discuss it with their neighbors and then revote on the same question the percentage of correct answers typically increases this outcome could result from gains in understanding during discussion or simply from peer influence of knowledgeable students on their neighbors to distinguish between these alternatives in an undergraduate genetics course we followed the above exercise with a second similar isomorphic question on the same concept that students answered individually our results indicate that peer discussion enhances understanding even when none of the students in a discussion group originally knows the correct science
chromatin immunoprecipitation chip followed by tag sequencing chip seq using high throughput next generation instrumentation is fast replacing chromatin immunoprecipitation followed by genome tiling array analysis chip chip as the preferred approach for mapping of sites of transcription factor binding and chromatin modification using two deeply sequenced data sets for human rna polymerase ii and each with matching input dna controls we describe a general scoring approach to address unique challenges in chip seq data analysis our approach is based on the observation that sites of potential binding are strongly correlated with signal peaks in the control likely revealing features of open chromatin we develop a two pass strategy called peakseq to compensate for this a two pass strategy compensates for signal caused by open chromatin as revealed by inclusion of the controls the first pass identifies putative binding sites and compensates for genomic variation in the mappability of sequences the second pass filters out sites not significantly enriched compared to the normalized control computing precise enrichments and significances our scoring procedure enables us to optimize experimental design by estimating the depth of sequencing required for a desired level of coverage and demonstrating that more than two replicates provides only a marginal gain information
pnas understanding the molecular determinants of specificity in proteinprotein interaction is an outstanding challenge of postgenome biology the availability of large protein databases generated from sequences of hundreds of bacterial genomes enables various statistical approaches to this problem in this context covariance based methods have been used to identify correlation between amino acid positions in interacting proteins however these methods have an important shortcoming in that they cannot distinguish between directly and indirectly correlated residues we developed a method that combines covariance analysis with global inference analysis adopted from use in statistical physics applied to a set of representatives of the bacterial two component signal transduction system the combination of covariance with global inference successfully and robustly identified residue pairs that are proximal in space without resorting to ad hoc tuning parameters both for heterointeractions between sensor kinase sk and response regulator rr proteins and for homointeractions between rr proteins the spectacular success of this approach illustrates the effectiveness of the global inference approach in identifying direct interaction based on sequence information alone we expect this method to be applicable soon to interaction surfaces between proteins present in only copy per genome as the number of sequenced genomes continues to expand use of this method could significantly increase the potential targets for therapeutic intervention shed light on the mechanism of proteinprotein interaction and establish the foundation for the accurate prediction of interacting partners
in clinical practice guidelines cpgs the medical information is stored in a narrative way a large part of this information occurs in a negated form the detection of negation in cpgs is an important task since it helps medical personnel to identify not occurring symptoms and diseases as well as treatment actions that should not be accomplished we developed algorithms capable of negation detection in this kind of medical documents according to our results we are convinced that the involvement of syntactical methods can improve negation detection not only in medical writings but also in arbitrary texts
genome wide pervasive transcription has been reported in many eukaryotic revealing a highly interleaved transcriptome organization that involves hundreds of previously unknown non coding these recently identified transcripts either exist stably in cells stable unannotated transcripts suts or are rapidly degraded by the rna surveillance pathway cryptic unstable transcripts cuts one characteristic of pervasive transcription is the extensive overlap of suts and cuts with previously annotated features which prompts questions regarding how these transcripts are generated and whether they exert single gene studies have shown that transcription of suts and cuts can be functional through mechanisms involving the generated or their generation so far a complete transcriptome architecture including suts and cuts has not been described in any organism knowledge about the position and genome wide arrangement of these transcripts will be instrumental in understanding their here we provide a comprehensive analysis of these transcripts in the context of multiple conditions a mutant of the exosome machinery and different strain backgrounds of saccharomyces cerevisiae we show that both suts and cuts display distinct patterns of distribution at specific locations most of the newly identified transcripts initiate from nucleosome free regions nfrs associated with the promoters of other transcripts mostly protein coding genes or from nfrs at the ends of protein coding genes likewise about half of all coding transcripts initiate from nfrs associated with promoters of other transcripts these data change our view of how a genome is transcribed indicating that bidirectionality is an inherent feature of promoters such an arrangement of divergent and overlapping transcripts may provide a mechanism for local spreading of regulatory signalsthat is coupling the transcriptional regulation of neighbouring genes by means of transcriptional interference or modification
bioinformatics summary scaffolded and corrected assembly of roche scarf is a next generation sequence assembly tool for evolutionary genomics that is designed especially for assembling est sequences against high quality reference sequences from related species the program was created to knit together contigs that do not assemble during traditional de novo assembly using a reference sequence library to orient the sequences availability scarf is freely available at http msbarker com software htm and is released under the open source license http www opensource org licenses gpl html contact msbarker indiana edusupplementary information supplementary data are available at online
an rna enzyme that catalyzes the rna templated joining of rna was converted to a format whereby two enzymes catalyze each other s synthesis from a total of four oligonucleotide substrates these cross replicating rna enzymes undergo self sustained exponential amplification in the absence of proteins or other biological materials amplification occurs with a doubling time of about hour and can be continued indefinitely populations of various cross replicating enzymes were constructed and allowed to compete for a common pool of substrates during which recombinant replicators arose and grew to dominate the population these replicating rna enzymes can serve as an experimental model of a genetic system many such model systems could be constructed allowing different selective outcomes to be related to the underlying properties of the genetic science
this is the first in a series of articles serving as an introduction to clinicians wishing to read and critically appraise genetic association studies we summarize the key concepts in genetics that clinicians must understand to review these studies including the structure of dna transcription and translation patterns of inheritance hardy weinberg equilibrium and linkage disequilibrium we review the types of dna variation including single nucleotide polymorphisms snps insertions and deletions and how these can affect protein function we introduce the idea of genetic association for both single candidate gene and genome wide association studies in which thousands of genetic variants are tested for association with disease we use the apoe polymorphism and its association with dementia as a case study to demonstrate the concepts and introduce the terminology used in this field the second and third articles will focus on issues of validity applicability
characterization of the evolutionary constraints acting on cis regulatory sequences is crucial to comparative genomics and provides key insights on the evolution of organismal diversity we study the relationships among orthologous cis regulatory modules crms in drosophila species especially with respect to the evolution of transcription factor binding sites and report statistical evidence in favor of key evolutionary hypotheses binding sites are found to have position specific substitution rates however the selective forces at different positions of a site do not act independently and the evidence suggests that constraints on sites are often based on their exact binding affinities binding site loss is seen to conform to a molecular clock hypothesis the rate of site loss is transcription factorspecific and depends on the strength of binding and in some cases the presence of other binding sites in close proximity our analysis is based on a novel computational method for aligning orthologous crms on a tree which rigorously accounts for alignment uncertainties and exploits binding site predictions through a unified probabilistic framework finally we report weak purifying selection on short deletions providing important clues about overall spatial constraints on crms our results present a complex picture of regulatory sequence evolution with substantial plasticity that depends on a number of factors the insights gained in this study will help us to understand the combinatorial control of gene regulation and how it evolves they will pave the way for theoretical models that are cognizant of the important determinants of regulatory sequence evolution and will be critical in genome wide identification of non coding sequences under purifying or selection
one of the few commonly believed principles of molecular evolution is that functionally more important genes or dna sequences evolve more slowly than less important ones this principle is widely used by molecular biologists in daily practice however recent genomic analysis of a diverse array of organisms found only weak negative correlations between the evolutionary rate of a gene and its functional importance typically measured under a single benign lab condition a frequently suggested cause of the above finding is that gene importance determined in the lab differs from that in an organism s natural environment here we test this hypothesis in yeast using gene importance values experimentally determined in lab conditions or computationally predicted for nutritional conditions in no single condition or combination of conditions did we find a much stronger negative correlation which is explainable by our subsequent finding that always essential enzyme genes do not evolve significantly more slowly than sometimes essential or always nonessential ones furthermore we verified that functional density approximated by the fraction of amino acid sites within protein domains is uncorrelated with gene importance thus neither the lab nature mismatch nor a potentially biased among gene distribution of functional density explains the observed weakness of the correlation between gene importance and evolutionary rate we conclude that the weakness is factual rather than artifactual in addition to being weakened by population genetic reasons the correlation is likely to have been further weakened by the presence of multiple nontrivial rate determinants that are independent from gene importance these findings notwithstanding we show that the principle of slower evolution of more important genes does have some predictive power when genes with vastly different evolutionary rates are compared explaining why the principle can be practically useful despite the weakness of correlation
motivation significant efforts have been made to acquire data under different conditions and to construct static networks that can explain various gene regulation mechanisms however gene regulatory networks are dynamic and condition specific under different conditions networks exhibit different regulation patterns accompanied by different transcriptional network topologies thus an investigation on the topological changes in transcriptional networks can facilitate the understanding of cell development or provide novel insights into the pathophysiology of certain diseases and help identify the key genetic players that could serve as biomarkers or drug targets results here we report a differential dependency network ddn analysis to detect statistically significant topological changes in the transcriptional networks between two biological conditions we pro pose a local dependency model to represent the local structures of a network by a set of conditional probabilities we develop an efficient learning algorithm to learn the local dependency model using the lasso technique a permutation test is subsequently performed to estimate the statistical significance of each learned local structure in testing on a simulation dataset the proposed algorithm accurately detected all the genes with network topological changes the me thod was then applied to the estrogen dependent t er breast cancer cell line datasets and human and mouse embryonic stem cell datasets in both experiments using real microarray datasets the proposed method produced biologically meaningful results we ex pect ddn to emerge as an important bioinformatics tool in transcrip tional network analyses while we focus specifically on transcrip tional networks the ddn method we introduce here is generally applicable to other biological networks with similar characteristics availability the ddn matlab toolbox and experiment data are available at http www cbil ece vt edu software htm contact yuewang vt edu supplementary information supplementary data are available at bioinformatics bioinformatics
background short regulating rnas guide many cellular processes compared with transcription factor proteins they appear to provide more specialized control and their deletions are less frequently lethal results we find large differences between computationally predicted lists of human microrna mirna target pairs instead of integrating these lists we use the two most accurate of them next we construct the co regulation network of human mirnas as nodes by computing the correlation link weight between the gene silencing scores of individual mirnas in this network we locate groups of tightly co regulating nodes modules despite explicitly allowing overlaps the co regulation modules of mirnas are well separated we use the modules and mirna co expression data to define and compute mirna essentiality instead of focusing on particular biological functions we identify a mirna as essential if it has a low co expression with the mirnas in its module this may be thought of as having many workers performing the same tasks together in one place non essential mirnas as opposed to a single worker performing those tasks alone essential mirna conclusions on the system level we quantitatively confirm previous findings about the specialized control provided by mirnas for knock out tests we list the groups of our predicted most and least essential mirnas in addition we provide possible explanations for i the low number of individually essential mirnas in caenorhabdtits elegans and ii the high number of ubiquitous mirnas influencing cell and tissue specific mirna expression patterns in mouse human
summary many genomic and proteomic analyses generate as a result a tree of genes or proteins these trees are often large containing tens of thousands of nodes and edges and need a visualization tool to fully display all the information contained in the tree clustering analysis can be performed on these trees to obtain clusters of proteins and we need an efficient way to visualize the clustering results we present a novel tree visualization tool to help with such analyses availability http renci org jeff software bin proteinvis zip contact jeff renci org xguan org
motivation yeast two hybrid screens are an important method to map pairwise protein interactions this method can generate spurious interactions false discoveries and true interactions can be missed false negatives previously we reported a capture recapture estimator for bait specific precision and recall here we present an improved method that better accounts for heterogeneity in bait specific error rates result for yeast worm and fly screens we estimate the overall false discovery rates fdrs to be and and the false negative rates fnrs to be and bait specific fdrs and the estimated protein degrees are then used to identify protein categories that yield more or fewer false positive interactions and more or fewer interaction partners while membrane proteins have been suggested to have elevated fdrs the current analysis suggests that intrinsic membrane proteins may actually have reduced fdrs hydrophobicity is positively correlated with decreased error rates and fewer interaction partners these methods will be useful for future two hybrid screens which could use ultra high throughput sequencing for deeper sampling of interacting bait prey pairs availability all software c source and datasets are available as supplemental files and at http www baderzone org under the lesser gpl license
we propose semantic texton forests efficient and powerful new low level features these are ensembles of decision trees that act directly on image pixels and therefore do not need the expensive computation of filter bank responses or local descriptors they are extremely fast to both train and test especially compared with k means clustering and nearest neighbor assignment of feature descriptors the nodes in the trees provide i an implicit hierarchical clustering into semantic textons and ii an explicit local classification estimate our second contribution the bag of semantic textons combines a histogram of semantic textons over an image region with a region prior category distribution the bag of semantic textons is computed over the whole image for categorization and over local rectangular regions for segmentation including both histogram and region prior allows our segmentation algorithm to exploit both textural and semantic context our third contribution is an image level prior for segmentation that emphasizes those categories that the automatic categorization believes to be present we evaluate on two datasets including the very challenging voc segmentation dataset our results significantly advance the state of the art in segmentation accuracy and furthermore our use of efficient decision forests gives at least a five fold increase in speed
the drosha complex also known as microprocessor is essential for microrna mirna maturation drosha functions as the catalytic subunit while also known as pasha recognizes the rna substrate although the action mechanism of this complex has been intensively studied it remains unclear how drosha and are regulated and if these proteins have any additional role s apart from mirna processing here we report that drosha and regulate each other posttranscriptionally the drosha complex cleaves the hairpin structures embedded in the mrna and thereby destabilizes the mrna we further find that stabilizes the drosha protein via protein protein interaction this crossregulation between drosha and may contribute to the homeostatic control of mirna biogenesis furthermore microarray analyses suggest that a number of mrnas may be downregulated in a microprocessor dependent mirna independent manner our study reveals a previously unsuspected function of microprocessor in mrna control
microrna mirna control has emerged as a critical regulatory principle in the mammalian immune system genetic ablation of the mirna machinery as well as loss or deregulation of certain individual mirnas severely compromises immune development and response and can lead to immune disorders like autoimmunity and cancer although individual mirnas modulate protein output from hundreds of target genes they may impact physiological processes by regulating the concentrations of just a few key cellular proteins that may be components of a single or of functionally interrelated pathways in a given cellular context main text introductionthe generation of the mammalian immune system from hematopoietic stem cells hscs involves ordered events of lineage commitment differentiation proliferation and cell migration this includes developmental programs of ordered immunoglobulin ig and t cell receptor tcr gene segments to equip each lymphocyte with a single antigen receptorbcr in the case of b cells and tcr in the case of t cells it also includes the differentiation of the cells into distinct cellular subsets with distinct effector functions and innate and adaptive responses to antigens in which cell interactions between lymphocytes of different subsets and of lymphocytes with other cells of the hematopoietic system such as dendritic cells play a critical role and which may be accompanied by the generation of immunological memory because of its unique experimental accessibility this system is perhaps the best characterized developmental system in mammals in terms of surface markers of cell subsets and of signaling pathways and transcriptional programs underlying developmental progression cellular selection and cell migration the immune system has also been well characterized with respect to differentiation processes and effector mechanisms involved in acquired and innate immune responses given the detailed conceptual and experimental framework of our present understanding of the immune system it came as a surprise that there is a previously unrecognized layer of control impacting at least some of the salient features of the development and physiology of the system namely control exerted by micrornas mirnas mirnas are endogenously encoded single stranded rnas of about nt in length that play essential roles in animals and plants in a large variety of physiological contexts ambros bartel and bushati and cohen hundreds of mirnas many of them evolutionarily conserved have been identified in mammals mirbase http microrna sanger ac uk and many of these molecules exhibit highly specific regulated patterns of expression chen et al landgraf et al merkerova et al monticelli et al and neilson et al which can be transcriptionally or posttranscriptionally controlled many mirnas derive from independent transcription units but mirna genes can also be located in the introns of protein coding genes rodriguez et al a large fraction of mirnas are clustered in the genome with arrangement and expression patterns suggesting that they are transcribed as polycistronic primary transcripts bartel the primary transcript of a mirna gene the pri mirna is cleaved by the rnaase iii endonuclease drosha in the nucleus to release a nt stem loop intermediate known as the pre mirna the pre mirna is subsequently exported to the cytoplasm where it is further cleaved by dicer another rnase iii endonuclease to produce a double stranded rna duplex which contains the mature mirna and its antisense strand the dicer cleavage process is coupled with the integration of the mature mirna into the rna induced silencing complex risc whose core components are the argonaute family proteins the mirna then directs the risc to its target mrna which it recognizes through partial sequence complementarity a major determinant in this recognition process is a perfect match in the so called seed region of nt at the end of the mirna the usual consequence of mirna mrna interaction is the downregulation of protein expression by translational repression mrna cleavage or promotion of mrna decay kim recent proteomic experiments in mammalian cells have demonstrated that single mirnas can directly repress the production of hundreds of proteins that repression is typically mild and that it is mostly but not always due to both downregulation of mrna levels and translation inhibition baek et al and selbach et al there is some evidence that under certain conditions mirnas can also upregulate the translation of target mrnas vasudevan et al or even directly interfere with gene transcription kim et al the physiological impact of such modes of regulation remains to be determined below we discuss studies of the immune system exemplifying that this multitarget subtle mode of control turns out to be highly efficient and versatile in mammalian cells through the targeting of multiple components of regulatory networks inside the cell in this context a special aspect of mirna control is of particular interest namely its rapid evolvability because of their simple structure and mode of target recognition mirnas as well as their targets can evolve rapidly chen and rajewsky gardner and vinther lu et al and niwa and slack as rapid evolution is a characteristic feature and essential element of the interplay of the immune system with pathogens one might thus predict that mirnas may play a prominent role in the control of host pathogen interactions and that the immune system may thus offer particularly rich opportunities to study mechanisms of mirna control emerging experimental evidence supports this view in our discussion we mainly focus on experiments in which genetic evidence obtained in vivo provides unequivocal evidence for the impact of mirna control mirnas control development and responses of the immune systemafter a first demonstration that overexpression of a mirna in hscs affected b lymphopoiesis in vivo upon hsc transplantation chen et al it became apparent from experiments in which dicer the key enzyme in mirna biogenesis was conditionally inactivated in t or b lymphocytes of the mouse that mirnas are critical for lymphocyte development and differentiation in the case of t cell development deletion of dicer in immature thymocytes at the so called double negative stage led to a fold reduction in total thymocyte numbers with a drastic reduction of the more mature double positive and single positive or thymocytes probably because of increased cell death although very few peripheral t cells were detectable in these animals cobb et al the peripheral t cells were only reduced fold when dicer was deleted at the double positive stage of thymocyte differentiation the mutant cells exhibited reduced proliferation and increased cell death upon activation in vitro when cultured under conditions that favor differentiation toward t helper and lineages they showed a bias toward differentiation reflecting the failure of these cells to repress interferon ifn expression muljo et al the t cell subset most affected by the deletion of dicer at the double positive stage are the t regulatory cells whose fold reduction in both thymus and periphery likely resulted in the severe immunopathology developing in the mutant mice characterized by splenomegaly enlarged intestinal lymph nodes and colitis cobb et al that mirnas are essential for the homeostasis and suppressor function of t regulatory cells was corroborated by dicer and drosha deletion in those cells and the ensuing fatal autoimmunity indistinguishable from that caused by deficiency in the master transcription factor controlling regulatory t cell differentiation chong et al liston et al and zhou et al regulatory t cells require stronger signals from their tcr to be positively selected than do t cells of other subsets zheng and rudensky hinting at the possibility that mirnas may be involved in the control of the sensitivity of t cells to signals from the tcr as discussed further below this seems indeed to be the case that a deficiency in mirnas also affects b cell development was first suggested by the conditional deletion of a component of risc in hematopoietic cells which resulted in a partial deficiency in mirnas and compromised development of b and erythroid cells o carroll et al when dicer was deleted in the b cell lineage from the earliest stage of b cell development an almost complete block at the pro to pre b transition resulted which was at least in part due to apoptosis of dicer deficient pre b cells koralov et al in an attempt to elucidate the molecular basis of this phenotype gene expression profiles of dicer deficient and proficient pro b cells were established and a bioinformatic search was conducted for conserved nucleotide hexamers complementary to mirna seed regions in the untranslated regions utrs of genes upregulated in dicer deficient pro b cells surprisingly only a few such motifs were identified with a corresponding small group of mirnas predicted to be critical players at the pro b cell stage most of these mirnas belonged to three interrelated mirna clusters one of which mir has emerged as a critical regulator of cellular development in a variety of cellular contexts as will be further discussed below the six mirnas encoded in the mir cluster share four distinct seed regions and when the annotated utrs of genes in the mouse were grouped according to the number of corresponding conserved target sites two genes on top of the list were immediately apparent as potentially causally involved in the induction of apoptosis in dicer deficient pre b cells the genes encoding the proapoptotic family member bim and the tumor suppressor pten which can positively control bim expression when this bioinformatic prediction was tested bim and pten were indeed found to be strongly upregulated in dicer deficient b cell progenitors and most importantly b cell development could be partially rescued by transgenic or bim deficiency koralov et al these experiments exemplify a way to identify mirnas and their targets in a particular biological context by combining bioinformatic and experimental approaches and also reveal that targeting of multiple components of a regulatory network as one of the emerging principles of mirna control this latter notion will be discussed in more detail below in a broader sense however the inactivation of components of the mirna machinery in lymphocytes has made us realize that although mirna control does not seem to affect lymphocyte development in every respectv d j recombination for example appears to function properly in dicer deficient b cells in a first approximationit operates at multiple levels in lymphocyte development and physiology and will have to be studied separately for individual mirnas it is important to keep in mind in this context that dicer ablation by conditional gene targeting may well fail to uncover mirna control in certain situations because of the persistence of dicer protein or of mirnas whose decay in cells is known to vary substantially between individual mirna species after dicer gene deletion cobb et al koralov et al muljo et al and ramachandran and chen on the other hand dicer deficiency may reveal functions of this enzyme outside of mirna control such as transcriptional gene silencing through the production of endogenous small interfering rnas sirnas silencing of centromeric repeats through such a mechanism has been demonstrated in yeast verdel et al and volpe et al and there was indeed an indication in dicer deficient b cells for a derepression of a set of d elements in the igh locus which exhibit a similar repeat structure chakraborty et al and koralov et al whether dicer dependent mechanisms of epigenetic gene silencing play a significant role in mammalian cells remains to be elucidated that the genetic inactivation of individual mammalian mirnas can produce discernible phenotypes became apparent early in from four papers rodriguez et al thai et al van rooij et al and zhao et al two of which addressed a mirna specifically expressed in the hematopoietic system like most other cells cells in the hematopoietic system express a multitude of mirna species which may or may not be shared between different cell types or developmental stages but only a few of which are specific for the hematopoietic system merkerova et al monticelli et al neilson et al and wu et al knockout of a mirna of the latter kind mir affected a large spectrum of immune reactions ranging from cytokine production by t and b cells to antigen presentation by dendritic cells and the germinal center b cell response rodriguez et al and thai et al there is also evidence that mir together with others is involved in the control of innate immune reactions expression profiling showed that stimulation of monocytes with lipopolysaccharide lps induced the expression of mir mir and mir o connell et al and taganov et al mir targets and components of the toll like receptor tlr signaling pathway that is activated by lps suggesting a negative feedback loop taganov et al the functional importance of lps induced mir expression in myeloid cells was further supported by the work of tili et al and the myeloproliferative disorder that develops in mice reconstituted with bone marrow progenitors that had been retrovirally transduced to express mir o connell et al induction of mir expression by lps could thus be a mechanism through which the immune system rapidly expands the myeloid cell population during inflammatory responses o connell et al other mirnas control additional processes in the immune system thus genetic ablation of mir led to the expansion of the b cell subset involved in natural defense against pathogens in spleen and peritoneal cavity increased steady state serum immunoglobulin levels and enhanced t dependent immune responses xiao et al mir seems to control tcr signaling thresholds in t lymphocytes as discussed further below and may also participate in the control of the antibody response li et al and de ybenes et al mir is a myeloid specific mirna whose deletion leads to an increase in the numbers of neutrophil progenitors and mature neutrophils as well as an increased neutrophil response to fungal infection demonstrating that this mirna plays important roles in the development and function of the myeloid lineage and innate immunity johnnidis et al taking these findings together mirna control has emerged as a general regulatory mechanism in immune development and the acquired and innate immune response in the sections below we focus on what is known about mechanisms by which mirnas exert their function dose dependent modulation of key targets by mirnasstudies are now revealing that mirnas modulate the concentration of key target proteins over a narrow range in a dose dependent manner in general the net outcome of mirna binding to its target mrnas is a reduction of the amounts of protein produced from these mrnas baek et al and selbach et al addressing this issue quantitatively xiao et al studied the interaction between mir and its chief target c myb in the context of lymphocyte development in vivo mir is highly expressed in mature b and t cells but not in their progenitors whereas c myb is highly expressed in lymphocyte progenitors but downregulated upon maturation genetic studies had demonstrated that c myb plays a critical role at multiple stages of lymphocyte development including the transition from pro to pre b cells bender et al and thomas et al when mir was ectopically expressed at graded concentrations in lymphocyte progenitors in vivo with the highest concentration at about of that in mature wild type t cells a dose dependent downregulation of c myb protein levels was observed with a reduction caused by the highest concentration of the transgenic mirna the transgenic mice exhibited a dose dependent developmental block at the pro to pre b transition and a reduction of splenic b cell numbers at young age figure that the developmental block was mainly caused by the reduction in c myb protein levels was supported by a similar phenotype in c myb heterozygous mice which displayed a similar reduction of c myb levels in b cell progenitors these experiments suggested a few basic principles of the relationship between mirnas and their target genes first the cellular concentration of a mirna dictates the protein output of its target genes and is therefore of key importance in mirna mediated control second small changes in the concentration of key cellular proteins as effected by mirna control can have significant biological consequences in line with the many examples of pathogenic hemizygous null mutations in man and mouse santarosa and ashworth seidman and seidman and smilenov mirnas may have evolved to modulate the concentrations of such key cellular proteins third although a single mirna can repress protein production from hundreds of genes only a few of these proteins may be critical for a particular biological process in studies in caenorhabditis elegans and drosophila melanogaster the control of single key target genes also appeared to largely explain the functions of individual mirnas karres et al lee et al teleman et al varghese and cohen and wightman et al as in the case of mir in mice these claims were largely based on the similarity between the phenotypes caused by loss and or gain of function of individual mirnas and manipulating the protein levels of target genes to similar degrees this clearly leaves room for a contribution of additional target genes to the observed phenotypes targeted mutagenesis of mirna binding sites in target mrnas is ultimately required to directly address this issue as exemplified further below for mir and its target gene aid setting signaling thresholds by modulating negative regulators mir accommodate the differential needs of developing and mature t cells in their response to antigen the former being positively and negatively selected by self antigens the latter responding to foreign antigens thymocytes exhibit a higher sensitivity than mature t cells to the signal triggered by tcr engagement of peptide mhc complexes this is despite the fact that the tcr uses the same set of molecules for signaling at different developmental stages germain and stefanova recent experiments indicate that mir is critically involved in this developmental control through its predominant expression in thymocytes and its ability to downregulate the protein levels of multiple phosphatases that are negative regulators of distinct steps of the tcr signaling pathway li et al figure the protein tyrosine phosphatase dephosphorylates and inactivates lck and two crucial components of proximal tcr signaling wu et al dual specificity phosphatases and dephosphorylate and inactivate the kinase erk with the former acting in the nucleus and the latter acting in the cytoplasm owens and keyse and tanzola and kersh and are essential for the positive selection of double positive thymocytes and this event is very sensitive to the dosage of erk activity targeted deletion of both and leads to a complete block of positive selection whereas retroviral expression of a dominant negative mutant of which increases erk activity enhances positive selection bettini and kersh and fischer et al overexpression of mir in mature t cells similarly increases erk activity and this converts antagonist peptides into agonists treatment of thymocytes with antagomir which degrades its endogenous counterpart reduces erk activity and impairs positive selection of thymocytes in a fetal thymic organ culture system li et al significantly the regulation of the response of t cells to signals from the tcr by mir could not be fully recapitulated by knockdown of the protein levels of any one of its proposed targets with shrnas whereas restoration of the protein levels of individual targets partly or completely reversed the effects of mir overexpression this suggests that the repression of a given target was essential but not sufficient for the function of mir li et al although these experiments still await confirmation in vivo they suggest that mirnas may target multiple components in a common regulatory pathway a concept that is also supported by the functional analysis of mir and the mir cluster see below conceptually similar to the additive effect on tumor development of double heterozygosity of functionally related mutant tumor suppressor genes smilenov this regulatory principle may contribute in a major way to the overall impact of mirna control despite moderate effects on the concentrations of individual target proteins nonsynergistic control of distinct regulatory pathways mir gain and loss of function studies of mir which is upregulated in b and t cells upon activation have demonstrated that this mirna plays a positive role in the control of the germinal center reaction in vivo in addition to its manifold other regulatory roles rodriguez et al thai et al and vigorito et al curiously a potential target of this mirna is the enzyme aid a central player in the germinal center reaction mediating class switch recombination csr and somatic hypermutation shm chaudhuri and alt di noia and neuberger and honjo et al aid is also however causally involved in oncogenic reciprocal translocations between igh and c myc c myc igh an unwanted byproduct of csr in the course of the germinal center reaction muramatsu et al and ramiro et al this calls for a careful control of aid activity during this process to analyze whether mir is involved in this control a knockin mouse strain carrying a mutation in the evolutionary conserved mir binding site in the utr of the aid gene was generated dorsett et al aid protein levels were increased to fold in both activated mir and b cells demonstrating that aid is a direct target of mir in vivo the latter was also suggested by the work of teng et al who made use of a similarly mutated aid transgene interestingly although shm was affected by neither the aid gene mutation nor the mir knockout the generation of class switched b cells was increased in the former and reduced in the latter case thus in mir b cells the effect of increased aid protein levels on csr must be offset by changes in other target genes indeed the work of vigorito et al has shown that the protein level of pu another mir target gene is increased in mir b cells and that this negatively affects the differentiation of cells undergoing csr vigorito et al in contradistinction when c myc igh translocation was analyzed activated b cells exhibited a to fold increased translocation frequency compared to control cells whereas the translocation frequency was increased fold in mir deficient b cells together these data suggested that mir regulates aid and some additional component s in a pathway minimizing aid mediated oncogenic translocations and in addition positively controls the generation of class switched b cells in the germinal center reaction figure paradoxically the latter goes together with downregulation of the key mediator of csr aid perhaps mir had originally evolved as a safeguard against aid mediated oncogenic translocations and had to accommodate downregulation of aid acquired for this purpose in a separate functional context namely the promotion of csr the particular risk associated with aid expression may also be the reason for its control by yet another mirna mir perhaps at a different stage of b cell activation de ybenes et al cooperation and redundancy of coexpressed mirnas mir to bioinformatic predictions the utrs of mirna target genes often contain binding sites for several different mirnas krek et al and lewis et al on the other hand mirnas encoded by different genomic loci can have very similar mature sequences and there are families of mirnas sharing seed regions can coexpressed mirnas regulate the expression of target mrnas in a cooperative manner do members of the same mirna family regulate different sets of targets because of sequence differences outside the seed region or are they functionally redundant recent studies of the mir cluster and related mirna clusters begin to answer some of these questions the mir transcript encoded by mouse chromosome is the precursor of six mirnas mir mir mir mir mir and mir not including mir additionally this cluster is homologous to the mir cluster on the x chromosome and the mir cluster on chromosome together these three clusters contain mirna stem loops giving rise to distinct mature mirnas which fall into four mirna families mir mir mir and mir figures and this genomic organization is highly conserved in all vertebrates from which complete genome sequences are available tanzer and stadler the mir and mir clusters are ubiquitously expressed whereas expression of the mir cluster was undetectable in the tissues examined ventura et al during lymphocyte development mir mirnas are highly expressed in progenitor cells with the expression level decreasing to fold upon maturation ventura et al and xiao et al addressing the biological functions of these mirnas ventura and colleagues showed that although deficiency of mir or mir did not produce a detectable phenotype mice deficient of mir were smaller than their wild type littermates and died within minutes after birth because of lung hypoplasia and a ventricular septal defect of the heart in the hematopoietc system there was a severe block of b cell development at the pro to pre b transition with increased apoptosis of pro b cells when both the mir and mir cluster were deleted the mice died before embryonic day with the mutant embryos displaying severe cardiac developmental abnormalities and apoptosis in specific regions of the central nervous system and the fetal liver the b cell developmental block also became more severe in the absence of these two mirna clusters ventura et al this demonstrates functional cooperation between the two mirna clusters in humans the mir mirna cluster is located at chromosome a genomic region that is frequently amplified in lymphomas and solid tissue cancers and the mature mirnas encoded by this locus are expressed in high amounts in those cancer cells hayashita et al lu et al ota et al tagawa and seto and zhang et al he and colleagues showed that retroviral expression of mir on the e myc transgenic background accelerated c myc mediated lymphomagenesis he et al to address whether mir overexpression by itself can reproduce some of these pathologies we generated transgenic mice expressing the human mir cluster specifically in b and t lymphocytes at levels comparable to those found in human lymphoma cell lines to study the consequences of elevated mir expression xiao et al the mutant mice exhibited spontaneous activation and pronounced expansion of b and t lymphocytes as well as symptoms typical of lymphoproliferative and autoimmune disease and died prematurely lymphoma development may have been precluded in the animals simply because of their short life span reporter assays in cell culture revealed that as predicted bioinformatically see koralov et al pten and bim were functional targets of mirnas in the mir cluster indeed mice doubly heterozygous for pten and bim partly reproduced the phenotype of the mir transgenics demonstrating that downregulation of pten and bim did contribute to the transgenic phenotype but deregulation of other targets must also be involved xiao et al such additional targets have been identified in other cellular contexts as discussed below these studies demonstrated additional principles of mirna control first homologous mirnas with similar expression patterns can be functionally redundant as exemplified by the lack of an obvious phenotype in mice deficient for mir whose function is apparently largely compensated by mir the phenotypic difference caused by deletion of mir and mir may be due to the fact that only the mir cluster contains members of the mir and mir families previous studies of the let family of mirnas in c elegans came to a similar conclusion abbott et al second there is a dose effect among coexpressed homologous mirnas as demonstrated by the more severe phenotype produced by combined mir and mir deficiency than produced by mir deficiency alone third mirnas organized in a cluster can regulate not only multiple components of a single pathway but also components of functionally interrelated pathways thus mir mirnas target pten and bim in the pathway xiao et al and a few critical cell cycle regulators such as and as suggested by studies in various cellular contexts lu et al o donnell et al petrocca et al sylvestre et al and wang et al figure and regulate cell cycle progression at the to s transition balomenos et al cobrinik field et al massague murga et al santiago raber et al yamasaki et al and zhu et al the pathway promotes cell cycle progression through at least two mechanisms akt mediated inactivation of which phosphorylates and destabilizes cyclin d an essential cyclin for the to s transition and akt mediated phosphorylation of foxo transcription factors and their subsequent translocation to the cytoplasm from the nucleus where they drive transcription of and two negative regulators of the to s transition manning and cantley the net outcome of regulating these targets is enhanced cell survival and proliferation finally there was an indication that coexpressed mirnas can regulate a common target in a cooperative manner in accord with earlier work in cell culture systems doench and sharp grimson et al and krek et al in the case of the control of pten by mir and mir the upregulation of a reporter gene containing the pten utr caused by knockdown of both mir and mir was more pronounced than that caused by knockdown of either mir or mir alone xiao et al mirnas control host virus interactionsas mentioned the ability of mirnas and their targets to rapidly evolve makes mirnas ideal candidates for the control of host pathogen interactions with virus control on the top of the list as viruses entirely depend on the host cellular machinery for their survival and propagation accordingly many viruses express mirnas and a plethora of regulatory mechanisms is emerging in which host or virus encoded mirnas interact with virus or host mrnas respectively in addition to their own mrnas scaria et al viral mirnas can help viruses to evade the host immune response by regulating cellular and viral gene expression sullivan thus mir encoded by human cytomegalovirus hcmv downregulates the expression of major histocompatibility complex class i related chain b micb a stress induced ligand of the natural killer nk cell activating receptor that is critical for the nk cell killing of virus infected cells stern ginossar et al interestingly the same mirna binding sites in micb seem to be exploited by cellular mirnas to maintain the expression of micb protein under a certain threshold in normal tissues and to help tumor cells to escape immune attack stern ginossar et al mirnas encoded by simian virus accumulate at late stages of infection are perfectly complementary to early viral mrnas and target these mrnas for cleavage this reduces the expression of viral t antigens encoded by the early viral mrnas enabling the infected cells to evade killing by cytotoxic t cells sullivan et al mirnas expressed by herpes simplex virus hsv facilitate the establishment and maintenance of viral latency through a similar mechanism umbach et al in turn cellular mirnas in addition to their normal regulatory roles can participate in antiviral defense thus mir counteracts the accumulation of primate foamy virus type pfv in human cells by directly targeting the pfv genome and causing translation inhibition lecellier et al some viruses overcome mirna mediated antiviral defense by encoding proteins that suppress rna silencing through mechanisms ranging from sequestering small rnas to disrupting functions of key proteins involved in mirna processing haasnoot et al viruses can also take advantage of host mirnas a liver specific mirna mir interacts with sequences in the noncoding region of the hepatitis c virus hcv rna and this interaction is required for viral replication and maintains high viral rna abundance in liver cells the tissue specificity of mir expression also helps hcv to establish its tissue selectivity jopling et al and jopling et al although general rules of mirna control are unlikely to emerge from the analysis of host virus interactions such studies should lead to major new insights into mechanisms of viral latency tropism and escape from innate and adaptive immune responses and thus potentially new therapeutic approaches mirnas and hematopoietic malignanciesderegulation of mirna expression has become a recurrent theme in the cancer field gene expression profiling of human cancer cells has revealed specific mirna expression signatures in many human cancers calin et al and lu et al a global downregulation of mature mirnas was frequently observed in such experiments often accompanied by upregulation of specific groups of mirnas calin and croce and lu et al in addition mirna genes are frequently located at fragile sites and genomic regions involved in carcinogenesis calin et al and recent evidence indicates that mirnas can directly contribute to various aspects of this process including tumor metastasis and cancer stem cell maintenance huang et al kent and mendell ma et al tavazoie et al and yu et al mirnas appear also to be heavily involved in the pathogenesis of leukemias and lymphomas this topic has been recently reviewed garzon and croce and we limit our discussion in this context to the two cases in which genetic evidence in mouse models points to a causal role of mirnas in these diseases thus mir and the mirnas of the mir cluster also called oncomir are highly expressed in many kinds of human cancers including lymphomas retroviral expression of mir in mice on the e myc transgenic background accelerated c myc mediated lymphomagenesis he et al and transgenic mice with elevated mir expression in lymphocytes died prematurely from a lymphoproliferative and autoimmune disease as discussed earlier xiao et al in the case of mir transgenic mice overexpressing this mirna in b cells developed polyclonal pre b cell proliferation followed by b cell malignancy costinean et al in addition a subset of human patients with acute myeloid leukemia aml have elevated mir expression in their bone marrow and retroviral expression of mir in bone marrow progenitor cells caused a myeloproliferative disorder in mice o connell et al although this field is still in its infancy the mechanistic study of the role of mirnas in malignancies of the hematopoietic system promises not only to lead to a better understanding of mirna control in normal physiology but also to allow the problem of hematopoietic malignancies to be approached from a new clinical angle conclusionsmirnas have emerged in the immune system as regulatory elements involved in the control of cellular development homeostasis and response in highly specific ways key features of this control are a dose dependent regulation of target protein concentrations over a mostly modest range and the targeting of multiple functionally related proteins subtle changes of intracellular protein concentrations can have profound physiological effects as documented by the many pathologies arising from haploinsufficiency and the targeting of several components of a functional network may further enhance the functional impact of mirna control it is therefore not surprising that the genetic ablation as well as ectopic or overexpression of individual mirnas can have severe physiological consequences ranging in the immune system from cell death and impairment of immune functions to autoimmunity lymphoproliferation and cancer a major challenge in the field is the identification of the critical mirna targets in a particular biological in vivo context this can ultimately only be done by combining bioinformatic biochemical and genetic approaches using bioinformatics to predict possible targets and to organize targets into functional networks using proteomic and gene expression analysis as well as reporter assays and the still challenging isolation of mirna mrna complexes to identify real mirna targets and using genetic experiments like the overexpression or inactivation of target genes or of mirna target sequences within these genes to address the biological relevance of such targets in a particular biological context interdisciplinary approaches have thus become indispensable in the pursuit of the biological impact of mirna control the unique ability of mirnas and their target sequences to rapidly evolve seems to be reflected in a plethora of mirna based control mechanisms operating in host virus interactions in the medical context this together with the notion that mirnas control in a dose dependent manner cellular states or responses rather than single or collections of functionally unrelated proteins makes these molecules attractive candidates or targets for future medical therapies acknowledgmentswe thank h von boehmer and n rajewsky for critical reading of this manuscript k r is supported by grants from the national institutes of health and the european union through mugen c x was supported by a cancer research institute postdoctoral fellowship and a training grant awarded to the joint program in hematology and transfusion medicine at harvard school
it is generally assumed that stabilizing selection promoting a phenotypic optimum acts to shape variation in quantitative traits across individuals and species although gene expression represents an intensively studied molecular phenotype the extent to which stabilizing selection limits divergence in gene expression remains contentious in this study we present a theoretical framework for the study of stabilizing and directional selection using data from between species divergence of continuous traits this framework based upon brownian motion is analytically tractable and can be used in maximum likelihood or bayesian parameter estimation we apply this model to gene expression levels in species of drosophila and find that gene expression divergence is substantially curtailed by stabilizing selection however we estimate the selective effect s of gene expression change to be very small approximately equal to ns for a change of one standard deviation where n is the effective population size these findings highlight the power of natural selection to shape phenotype even when the fitness effects of mutations are in the nearly range
recurrent gene fusions typically associated with haematological malignancies and rare bone and soft tissue have recently been described in common solid here we use an integrative analysis of high throughput long and short read transcriptome sequencing of cancer cells to discover novel gene fusions as a proof of concept we successfully used integrative transcriptome sequencing to re discover the ref gene fusion in a chronic myelogenous leukaemia cell line and the gene fusion in a prostate cancer cell line and tissues additionally we nominated and experimentally validated novel gene fusions resulting in chimaeric transcripts in cancer cell lines and tumours taken together this study establishes a robust pipeline for the discovery of novel gene chimaeras using high throughput sequencing opening up an important class of cancer related mutations for characterization
preparing and manipulating quantum states of mechanical resonators is a highly interdisciplinary undertaking that now receives enormous interest for its far reaching potential in fundamental and applied up to now only nanoscale mechanical devices achieved operation close to the quantum we report a new micro optomechanical resonator that is laser cooled to a level of thermal quanta this is equivalent to the best nanomechanical devices however with a mass more than four orders of magnitude larger versus and at more than two orders of magnitude higher environment temperature versus despite the large laser added cooling factor of and the cryogenic environment our cooling performance is not limited by residual absorption effects these results pave the way for the preparation of m scale objects in the quantum regime possible applications range from quantum limited optomechanical sensing devices to macroscopic tests quantum
polycomb group pcg and trithorax group trxg proteins are conserved chromatin factors that regulate key developmental genes throughout development in drosophila pcg and trxg factors bind to regulatory dna elements called pcg and trxg response elements pres and tres several dna binding proteins have been suggested to recruit pcg proteins to pres but the dna sequences necessary and sufficient to define pres are largely unknown here we used chromatin immunoprecipitation chip on chip assays to map the chromosomal distribution of drosophila pcg proteins the n and c terminal fragments of the trithorax trx protein and four candidate dna binding factors for pcg recruitment in addition we mapped histone modifications associated with pcg dependent silencing and trx mediated activation pcg proteins colocalize in large regions that may be defined as polycomb domains and colocalize with recruiters to form several hundreds of putative pres strikingly the majority of pcg recruiter binding sites are associated with and not with pcg binding suggesting that recruiter proteins have a dual function in activation as well as silencing one major discriminant between activation and silencing is the strong binding of pleiohomeotic pho to silenced regions whereas its homolog pleiohomeotic like phol binds preferentially to active promoters in addition the c terminal fragment of trx trx c showed high affinity to pcg binding sites whereas the n terminal fragment trx n bound mainly to active promoter regions trimethylated on our results indicate that dna binding proteins serve as platforms to assist pcg and trxg binding furthermore several dna sequence features discriminate between pcg and trx n bound regions indicating that underlying dna sequence contains critical information to drive pres and tres towards silencing activation
abstract background biologists need to perform complex queries often across a variety of databases typically each data resource provides an advanced query interface each of which must be learnt by the biologist before they can begin to query them frequently more than one data source is required and for high throughput analysis cutting and pasting results between websites is certainly very time consuming therefore many groups rely on local bioinformatics support to process queries by accessing the resource s programmatic interfaces if they exist this is not an efficient solution in terms of cost and time instead it would be better if the biologist only had to learn one generic interface biomart provides such a solution results biomart enables scientists to perform advanced querying of biological data sources through a single web interface the power of the system comes from integrated querying of data sources regardless of their geographical locations once these queries have been defined they may be automated with its scripting at the click of a button functionality biomart s capabilities are extended by integration with several widely used software packages such as bioconductor das galaxy cytoscape taverna in this paper we describe all aspects of biomart from a user s perspective and demonstrate how it can be used to solve real biological use cases such as snp selection for candidate gene screening or annotation of microarray results conclusions biomart is an easy to use generic and scalable system and therefore has become an integral part of large data resources including ensembl uniprot hapmap wormbase gramene dictybase pride msd and reactome biomart is freely accessible to use at www org
this paper reviews classifies and compares recent models for social networks that have mainly been published within the physics oriented complex networks literature the models fall into two categories those in which the addition of new links is dependent on the typically local network structure network evolution models nems and those in which links are generated based only on nodal attributes nodal attribute models nams an exponential random graph model ergm with structural dependencies is included for comparison we fit models from each of these categories to two empirical acquaintance networks with respect to basic network properties we compare higher order structures in the resulting networks with those in the data with the aim of determining which models produce the most realistic network structure with respect to degree distributions assortativity clustering spectra geodesic path distributions and community structure subgroups with dense internal connections we find that the nodal attribute models successfully produce assortative networks and very clear community structure however they generate unrealistic clustering spectra and peaked degree distributions that do not match empirical data on large social networks on the other hand many of the network evolution models produce degree distributions and clustering spectra that agree more closely with data they also generate assortative networks and community structure although often not to the same extent as in the data the ergm model which turned out to be near degenerate in the parameter region best fitting our data produces the weakest structure
analyses of similarities and changes in protein conformation can provide important information regarding protein function and evolution many scores including the commonly used root mean square deviation have therefore been developed to quantify the similarities of different protein conformations however instead of examining individual conformations it is in many cases more relevant to analyse ensembles of conformations that have been obtained either through experiments or from methods such as molecular dynamics simulations we here present three approaches that can be used to compare conformational ensembles in the same way as the root mean square deviation is used to compare individual pairs of structures the methods are based on the estimation of the probability distributions underlying the ensembles and subsequent comparison of these distributions we first validate the methods using a synthetic example from molecular dynamics simulations we then apply the algorithms to revisit the problem of ensemble averaging during structure determination of proteins and find that an ensemble refinement method is able to recover the correct distribution of conformations better than standard single refinement
gr the recent arrival of large scale cap analysis of gene expression cage data sets in mammals provides a wealth of quantitative information on coding and noncoding rna polymerase ii transcription start sites tss genome wide cage studies reveal that a large fraction of tss exhibit peaks where the vast majority of associated tags map to a particular location whereas other active regions contain a broader distribution of initiation events the presence of a strong single peak suggests that transcription at these locations may be mediated by position specific sequence features we therefore propose a new model for single peaked tss based solely on known transcription factors tfs and their respective regions of positional enrichment this probabilistic model leads to near perfect classification results in cross validation auroc and performance in genomic scans demonstrates that tss prediction with both high accuracy and spatial resolution is achievable for a specific but large subgroup of mammalian promoters the interpretable model structure suggests a dna code in which canonical sequence features such as tata box initiator and gc content do play a significant role but many additional tfs show distinct spatial biases with respect to tss location and are important contributors to the accurate prediction of single peak transcription initiation sites the model structure also reveals that cage tag clusters distal from annotated gene starts have distinct characteristics compared to those close to gene ends using this high resolution single peak model we predict tss for of mammalian micrornas based on currently data
pnas we have combined ultrasensitive magnetic resonance force microscopy mrfm with image reconstruction to achieve magnetic resonance imaging mri with resolution nm the image reconstruction converts measured magnetic force data into a map of nuclear spin density taking advantage of the unique characteristics of the resonant slice that is projected outward from a nanoscale magnetic tip the basic principles are demonstrated by imaging the h spin density within individual tobacco mosaic virus particles sitting on a nanometer thick layer of adsorbed hydrocarbons this result which represents a million fold improvement in volume resolution over conventional mri demonstrates the potential of mrfm as a tool for elementally selective imaging on the scale
cloud computing has become another buzzword after web however there are dozens of different definitions for cloud computing and there seems to be no consensus on what a cloud is on the other hand cloud computing is not a completely new concept it has intricate connection to the relatively new but thirteen year established grid computing paradigm and other relevant technologies such as utility computing cluster computing and distributed systems in general this paper strives to compare and contrast cloud computing with grid computing from various angles and give insights into the essential characteristics both
complex genetic disease is caused by the interaction between genetic and environmental variables and is the predominant cause of mortality globally recognition that susceptibility arises through the combination of multiple genetic pathways that influence liability factors in a nonlinear manner suggests that a process of decanalization contributes to the epidemic nature of common genetic diseases the rapid evolution of the human genome combined with marked environmental and cultural perturbation in the past two generations might lead to the uncovering of cryptic genetic variation that is a major source of susceptibility
since the discovery in of the first small silencing rna a dizzying number of small rna classes have been identified including micrornas mirnas small interfering rnas sirnas and piwi interacting rnas pirnas these classes differ in their biogenesis their modes of target regulation and in the biological pathways they regulate there is a growing realization that despite their differences these distinct small rna pathways are interconnected and that small rna pathways compete and collaborate as they regulate genes and protect the genome from external and threats
accurate genome wide identification of orthologs is a central problem in comparative genomics a fact reflected by the numerous orthology identification projects developed in recent years however only a few reports have compared their accuracy and indeed several recent efforts have not yet been systematically evaluated furthermore orthology is typically only assessed in terms of function conservation despite the phylogeny based original definition of fitch we collected and mapped the results of nine leading orthology projects and methods cog kog inparanoid orthomcl ensembl compara homologene roundup eggnog and oma and two standard methods bidirectional best hit and reciprocal smallest distance we systematically compared their predictions with respect to both phylogeny and function using six different tests this required the mapping of millions of sequences the handling of hundreds of millions of predicted pairs of orthologs and the computation of tens of thousands of trees in phylogenetic analysis or in functional analysis where high specificity is required we find that oma and homologene perform best at lower functional specificity but higher coverage level orthomcl outperforms ensembl compara and to a lesser extent inparanoid lastly the large coverage of the recent eggnog can be of interest to build broad functional grouping but the method is not specific enough for phylogenetic or detailed function analyses in terms of general methodology we observe that the more sophisticated tree reconstruction reconciliation approach of ensembl compara was at times outperformed by pairwise comparison approaches even in phylogenetic tests furthermore we show that standard bidirectional best hit often outperforms projects with more complex algorithms first the present study provides guidance for the broad community of orthology data users as to which database best suits their needs second it introduces new methodology to verify orthology and third it sets performance standards for current and approaches
micrornas mirnas are fundamental regulatory elements of animal and plant gene expression although rapid progress in our understanding of mirna biogenesis has been achieved by experimentation computational approaches have also been influential in determining the general principles that are thought to govern mirna target recognition and mode of action we discuss how these principles are being progressively challenged by genetic and biochemical studies in addition we discuss the role of target site specific endonucleolytic cleavage which is the hallmark of experimental rna interference and a mechanism that is used by plant mirnas and a few animal mirnas generally thought to be merely a degradation mechanism we propose that this might also be a biogenesis mechanism for biologically functional non coding fragments
motivation full length dna and protein sequences that span the entire length of a gene are ideally used for multiple sequence alignments msas and the subsequent inference of their relationships frequently however msas contain a substantial amount of missing data for example expressed sequence tags ests which are partial sequences of expressed genes are the predominant source of sequence data for many organisms the patterns of missing data typical for est derived alignments greatly compromise the accuracy of estimated phylogenies results we present a statistical method for inferring phylogenetic trees from est based incomplete msa data we propose a class of hierarchical models for modeling pairwise distances between the sequences and develop a fully bayesian approach for estimation of the model parameters once the distance matrix is estimated the phylogenetic tree may be constructed by applying neighbor joining or any other algorithm of choice we also show that maximizing the marginal likelihood from the bayesian approach yields similar results to a profile likelihood estimation the proposed methods are illustrated using simulated protein families for which the true phylogeny is known and one real protein family availability r code for fitting these models are available from http people bu edu gupta software htm contact gupta bu edu supplementary information supplemantary data are available at bioinformatics bioinformatics
background microarray technology is commonly used as a simple screening tool with a focus on selecting genes that exhibit extremely large differential expressions between different phenotypes it lacks the ability to select genes that change their relationships with other genes in different biological conditions differentially correlated genes we intend to enrich the above procedure by proposing a nonparametric selection procedure that selects differentially correlated genes results using both simulations and resampling techniques we found that our procedure correctly detected genes that were not differentially expressed but differentially correlated we also applied our procedure to a set of biological data and found some potentially important genes that were not selected by the traditional method discussion and conclusion microarray technology yields multidimensional information on the function of the whole genome rather than treating intergene correlation as a nuisance to the traditional gene selection procedures which are essentially univariate our method utilizes the rich information contained in the correlation as a new selection criterion it can provide additional useful candidate genes for biologists
comparative developmental evidence indicates that reorganizations in developmental gene regulatory networks grns underlie evolutionary changes in animal morphology including body plans we argue here that the nature of the evolutionary alterations that arise from regulatory changes depends on the hierarchical position of the change within a grn this concept cannot be accomodated by microevolutionary nor macroevolutionary theory it will soon be possible to investigate these ideas experimentally by assessing the effects of grn changes on evolution
much effort and interest have focused on assessing the importance of natural selection particularly positive natural selection in shaping the human genome although scans for positive selection have identified candidate loci that may be associated with positive selection in humans such scans do not indicate whether adaptation is frequent in general in humans studies based on the reasoning of the macdonaldkreitman test which in principle can be used to evaluate the extent of positive selection suggested that adaptation is detectable in the human genome but that it is less common than in drosophila or escherichia coli both positive and purifying natural selection at functional sites should affect levels and patterns of polymorphism at linked nonfunctional sites here we search for these effects by analyzing patterns of neutral polymorphism in humans in relation to the rates of recombination functional density and functional divergence with chimpanzees we find that the levels of neutral polymorphism are lower in the regions of lower recombination and in the regions of higher functional density or divergence these correlations persist after controlling for the variation in gc content density of simple repeats selective constraint mutation rate and depth of sequencing coverage we argue that these results are most plausibly explained by the effects of natural selection at functional siteseither recurrent selective sweeps or background selectionon the levels of linked neutral polymorphism natural selection at both coding and regulatory sites appears to affect linked neutral polymorphism reducing neutral polymorphism by genome wide and by in the gene rich half of the human genome these findings suggest that the effects of natural selection at linked sites cannot be ignored in the study of neutral polymorphism
background surgery has become an integral part of global health care with an estimated million operations performed yearly surgical complications are common and often preventable we hypothesized that a program to implement a item surgical safety checklist designed to improve team communication and consistency of care would reduce complications and deaths associated with surgery methods between october and september eight hospitals in eight cities toronto canada new delhi india amman jordan auckland new zealand manila philippines ifakara tanzania london england and seattle wa representing a variety of economic circumstances and diverse populations of patients participated in the world health organization s safe surgery saves lives program we prospectively collected data on clinical processes and outcomes from consecutively enrolled patients years of age or older who were undergoing noncardiac surgery we subsequently collected data on consecutively enrolled patients after the introduction of the surgical safety checklist the primary end point was the rate of complications including death during hospitalization within the first days after the operation results the rate of death was before the checklist was introduced and declined to afterward p inpatient complications occurred in of patients at baseline and in after introduction of the checklist p conclusions implementation of the checklist was associated with concomitant reductions in the rates of death and complications among patients at least years of age who were undergoing noncardiac surgery in a diverse group hospitals
among cellular organisms symbiotic bacteria provide the extreme examples of genome degradation and reduction however only isolated snapshots of eroding symbiont genomes have previously been available we documented the dynamics of symbiont genome evolution by sequencing seven strains of buchnera aphidicola from pea aphid hosts we estimated a spontaneous mutation rate of at least x substitutions per site per replication which is more than times as high as the rates previously estimated for any bacteria we observed a high rate of small insertions and deletions associated with abundant dna homopolymers and occasional larger deletions although purifying selection eliminates many mutations some persist resulting in ongoing loss of genes and dna from this already tiny genome our results provide a general model for the stepwise process leading to genome science
to act as computational devices neurons must perform mathematical operations as they transform synaptic and modulatory input into output firing rate experiments and theory indicate that neuronal firing typically represents the sum of synaptic inputs an additive operation but multiplication of inputs is essential for many computations multiplication by a constant produces a change in the slope or gain of the input output relationship amplifying or scaling down the sensitivity of the neuron to changes in its input such gain modulation occurs in vivo during contrast invariance of orientation tuning attentional scaling translation invariant object recognition auditory processing and coordinate transformations moreover theoretical studies highlight the necessity of gain modulation in several of these tasks although potential cellular mechanisms for gain modulation have been identified they often rely on membrane noise and require restrictive conditions to work because nonlinear components are used to scale signals in electronics we examined whether synaptic nonlinearities are involved in neuronal gain modulation we used synaptic stimulation and the dynamic clamp technique to investigate gain modulation in granule cells in acute slices of rat cerebellum here we show that when excitation is mediated by synapses with short term depression std neuronal gain is controlled by an inhibitory conductance in a noise independent manner allowing driving and modulatory inputs to be multiplied together the nonlinearity introduced by std transforms inhibition mediated additive shifts in the input output relationship into multiplicative gain changes when granule cells were driven with bursts of high frequency mossy fibre input as observed in vivo larger inhibition mediated gain changes were observed as expected with greater std simulations of synaptic integration in more complex neocortical neurons suggest that std based gain modulation can also operate in neurons with large dendritic trees our results establish that neurons receiving depressing excitatory inputs can act as powerful multiplicative devices even when integration of postsynaptic conductances linear
autonomous and self sustained oscillator circuits mediating the periodic induction of specific target genes are minimal genetic time keeping devices found in the central and peripheral circadian they have attracted significant attention because of their intriguing dynamics and their importance in controlling critical and signalling the precise molecular mechanism and expression dynamics of this mammalian circadian clock are still not fully understood here we describe a synthetic mammalian oscillator based on an auto regulated senseantisense transcription control circuit encoding a positive and a time delayed negative feedback loop enabling autonomous self sustained and tunable oscillatory gene expression after detailed systems design with experimental analyses and mathematical modelling we monitored oscillating concentrations of green fluorescent protein with tunable frequency and amplitude by time lapse microscopy in real time in individual chinese hamster ovary cells the synthetic mammalian clock may provide an insight into the dynamics of natural periodic processes and foster advances in the design of prosthetic networks in future gene and therapies
two related developments are currently changing traditional approaches to computational systems biology modelling first stochastic models are being used increasingly in preference to deterministic models to describe biochemical network dynamics at the single cell level second sophisticated statistical methods and algorithms are being used to fit both deterministic and stochastic models to time course and other experimental data both frameworks are needed to adequately describe observed noise variability and heterogeneity of biological systems over a range of scales of organization
a conspicuous ability of the brain is to seamlessly assimilate and process spatial and temporal features of sensory stimuli this ability is indispensable for the recognition of natural stimuli yet a general computational framework for processing spatiotemporal stimuli remains elusive recent theoretical and experimental work suggests that spatiotemporal processing emerges from the interaction between incoming stimuli and the internal dynamic state of neural networks including not only their ongoing spiking activity but also their hidden neuronal states such as short term plasticity
facile and meaningful integration of data from disparate resources is the holy grail of bioinformatics some resources have begun to address this problem by providing their data using semantic web standards specifically the resource description framework rdf and the web ontology language owl unfortunately adoption of semantic web standards has been slow overall and even in cases where the standards are being utilized interconnectivity between resources is rare in response we have seen the emergence of centralized semantic warehouses that collect public data from third parties integrate it translate it into owl rdf and provide it to the community as a unified and queryable resource one limitation of the warehouse approach is that queries are confined to the resources that have been selected for inclusion a related problem perhaps of greater concern is that the majority of bioinformatics data exists in the deep web that is the data does not exist until an application or analytical tool is invoked and therefore does not have a predictable web address the inability to utilize uniform resource identifiers uris to address this data is a barrier to its accessibility via uri centric semantic web technologies here we examine the state of the union for the adoption of semantic web standards in the health care and life sciences domain by key bioinformatics resources explore the nature and connectivity of several community driven semantic warehousing projects and report on our own progress with the cardioshare moby project which aims to make the resources of the deep web transparently accessible through sparql bib
this paper aims to quantify two common assumptions about social tagging that tags are meaningful and that the tagging process is influenced by tag suggestions for we analyze the semantic properties of tags and the relationship between the tags and the content of the tagged page our analysis is based on a corpus of search keywords contents titles and tags applied to several thousand popular web pages among other results we find that the more popular tags of a page tend to be the more meaningful ones for we develop a model of how the influence of tag suggestions can be measured from a user study with over participants we conclude that roughly one third of the tag applications may be induced by the suggestions our results would be of interest for designers of social tagging systems and are a step towards understanding how to best leverage social tags for applications such as search and extraction
retroviruses have the potential to acquire host cell derived genetic material during reverse transcription and can integrate into the genomes of larger more complex dna viruses in contrast rna viruses were believed not to integrate into the host s genome under any circumstances we found that illegitimate recombination between an exogenous nonretroviral rna virus lymphocytic choriomeningitis virus and the endogenous intracisternal a type particle iap retrotransposon occurred and led to reverse transcription of exogenous viral rna the resulting complementary dna was integrated into the host s genome with an iap element thus rna viruses should be closely scrutinized for any capacity to interact with endogenous retroviral elements before their approval for therapeutic use humans
abstract background recent advances in experimental and computational technologies have fueled the development of many sophisticated bioinformatics programs the correctness of such programs is crucial as incorrectly computed results may lead to wrong biological conclusion or misguide downstream experimentation common software testing procedures involve executing the target program with a set of test inputs and then verifying the correctness of the test outputs however due to the complexity of many bioinformatics programs it is often difficult to verify the correctness of the test outputs therefore our ability to perform systematic software testing is greatly hindered results we propose to use a novel software testing technique metamorphic testing mt to test a range of bioinformatics programs instead of requiring a mechanism to verify whether an individual test output is correct the mt technique verifies whether a pair of test outputs conform to a set of domain specific properties called metamorphic relations mrs thus greatly increases the number and variety of test cases that can be applied to demonstrate how mt is used in practice we applied mt to test two open source bioinformatics programs namely gnlab and seqmap in particular we show that mt is simple to implement and is effective in detecting faults in a real life program and some artificially fault seeded programs further we discuss how mt can be applied to test programs from various domains of bioinformatics conclusions this paper describes the application of a simple effective and automated technique to systematically test a range of bioinformatics programs we show how mt can be implemented in practice through two real life case studies since many bioinformatics programs particularly those for large scale simulation and data analysis are hard to test systematically their developers may benefit from using mt as part of the testing strategy therefore our work represents a significant step towards software reliability bioinformatics
to analyze the vast number and variety of microorganisms inhabiting the human intestine emerging metagenomic technologies are extremely powerful the intestinal microbes are taxonomically complex and constitute an ecologically dynamic community microbiota that has long been believed to possess a strong impact on human physiology furthermore they are heavily involved in the maturation and proliferation of human intestinal cells helping to maintain their homeostasis and can be causative of various diseases such as inflammatory bowel disease and obesity a simplified animal model system has provided the mechanistic basis for the molecular interactions that occur at the interface between such microbes and host intestinal epithelia through metagenomic analysis it is now possible to comprehensively explore the genetic nature of the intestinal microbiome the mutually interacting system comprising the host cells and the residing microbial community the human microbiome project was recently launched as an international collaborative research effort to further promote this newly developing field and to pave the way to a new frontier of human biology which will provide new strategies for the maintenance of health
language acquisition and processing are governed by genetic constraints a crucial unresolved question is how far these genetic constraints have coevolved with language perhaps resulting in a highly specialized and species specific language module and how much language acquisition and processing redeploy preexisting cognitive machinery in the present work we explored the circumstances under which genes encoding language specific properties could have coevolved with language itself we present a theoretical model implemented in computer simulations of key aspects of the interaction of genes and language our results show that genes for language could have coevolved only with highly stable aspects of the linguistic environment a rapidly changing linguistic environment does not provide a stable target for natural selection thus a biological endowment could not coevolve with properties of language that began as learned cultural conventions because cultural conventions change much more rapidly than genes we argue that this rules out the possibility that arbitrary properties of language including abstract syntactic principles governing phrase structure case marking and agreement have been built into a language module by natural selection the genetic basis of human language acquisition and processing did not coevolve with language but primarily predates the emergence of language as suggested by darwin the fit between language and its underlying mechanisms arose because language has evolved to fit the human brain rather than reverse
a computer program is described for calculating the environment and the exposure to solvent of atoms of a protein the computation is based on the atomic co ordinates of the protein and on assumptions like those of lee richards results for lysozyme and insulin are presented changes in exposure to solvent and in the nature of contacts that develop through folding association reactions and crystallization are described numerically the computations suggest several generalizations a lattice contacts within the protein crystal are characterized by a significantly smaller involvement of non polar side chains and a proportionately greater involvement of ionizable side chains than is found for protein folding or for protein association reactions important for biological function b in helical regions the carbonyl oxygen of the first residue in the helix has high probability of being shielded from solvent c glycine is among the residues having exposure least affected by folding this accords with the expectation that it lies at bends of the peptide chain on the surface of molecule
bioinformatics summary jalview version is a system for interactive wysiwyg editing analysis and annotation of multiple sequence alignments core features include keyboard and mouse based editing multiple views and alignment overviews and linked structure display with jmol jalview is available in two forms a lightweight java applet for use in web applications and a powerful desktop application that employs web services for sequence alignment secondary structure prediction and the retrieval of alignments sequences annotation and structures from public databases and any das compliant sequence or annotation server availability the jalview desktop application and jalviewlite applet are made freely available under the gpl and can be downloaded from www jalview orgcontact g j barton dundee uk
the increasingly multicultural fabric of modern societies has given rise to many new issues and conflicts as ethnic and national minorities demand recognition and support for their cultural identity this book presents a new conception of the rights and status of minority cultures it argues br that certain collective rights of minority cultures are consistent with liberal democratic principles and that standard liberal objections to such rights can be answered however the author emphasizes that no single formula can be applied to all groups and that the needs and aspirations of br immigrants are very different from those of indigenous peoples and national minorities he looks at issues such as language rights group representation religious education federalism and secession issues central to an understanding of multicultural politics but which have been neglected in br contemporary liberal theory scholars of political theory and philosophy as well as the general reader will find this work to be the most comprehensive analysis to date of this crucial issue
it is widely accepted that the growth and regeneration of tissues and organs is tightly controlled although experimental studies are beginning to reveal molecular mechanisms underlying such control there is still very little known about the control strategies themselves here we consider how secreted negative feedback factors chalones may be used to control the output of multistage cell lineages as exemplified by the actions of and activin in a self renewing neural tissue the mammalian olfactory epithelium oe we begin by specifying performance objectiveswhat precisely is being controlled and to what degreeand go on to calculate how well different types of feedback configurations feedback sensitivities and tissue architectures achieve control ultimately we show that many features of the oethe number of feedback loops the cellular processes targeted by feedback even the location of progenitor cells within the tissuefit with expectations for the best possible control in so doing we also show that certain distinctions that are commonly drawn among cells and moleculessuch as whether a cell is a stem cell or transit amplifying cell or whether a molecule is a growth inhibitor or stimulatormay be the consequences of control and not a reflection of intrinsic differences in cellular or character
we study the problem of answering ambiguous web queries in a setting where there exists a taxonomy of information and that both queries and documents may belong to more than one category according to this taxonomy we present a systematic approach to diversifying results that aims to minimize the risk of dissatisfaction of the average user we propose an algorithm that well approximates this objective in general and is provably optimal for a natural special case furthermore we generalize several classical ir metrics including ndcg mrr and map to explicitly account for the value of diversification we demonstrate empirically that our algorithm scores higher in these generalized metrics compared to results produced by commercial engines
rfam is a collection of rna sequence families represented by multiple sequence alignments and covariance models cms the primary aim of rfam is to annotate new members of known rna families on nucleotide sequences particularly complete genomes using sensitive blast filters in combination with cms a minority of families with a very broad taxonomic range e g trna and rrna provide the majority of the sequence annotations whilst the majority of rfam families e g snornas and mirnas have a limited taxonomic range and provide a limited number of annotations recent improvements to the website methodologies and data used by rfam are discussed rfam is freely available on the web at http rfam sanger ac uk and http rfam janelia nar
data mining of available biomedical data and information has greatly boosted target discovery in the omics era target discovery is the key step in the biomarker and drug discovery pipeline to diagnose and fight human diseases in biomedical science the target is a broad concept ranging from molecular entities such as genes proteins and mirnas to biological phenomena such as molecular functions pathways and phenotypes within the context of biomedical science data mining refers to a bioinformatics approach that combines biological concepts with computer tools or statistical methods that are mainly used to discover select and prioritize targets in response to the huge demand of data mining for target discovery in the omics era this review explicates various data mining approaches and their applications to target discovery with emphasis on text and microarray data analysis two emerging data mining approaches chemogenomic data mining and proteomic data mining are briefly introduced also discussed are the limitations of various data mining approaches found in the level of database integration the quality of data annotation sample heterogeneity and the performance of analytical and mining tools tentative strategies of integrating different data sources for target discovery such as integrated text mining with high throughput data analysis and integrated mining with pathway databases introduced
genome wide association studies are revolutionizing the search for the genes underlying human complex diseases the main decisions to be made at the design stage of these studies are the choice of the commercial genotyping chip to be used and the numbers of case and control samples to be genotyped the most common method of comparing different chips is using a measure of coverage but this fails to properly account for the effects of sample size the genetic model of the disease and linkage disequilibrium between snps in this paper we argue that the statistical power to detect a causative variant should be the major criterion in study design because of the complicated pattern of linkage disequilibrium ld in the human genome power cannot be calculated analytically and must instead be assessed by simulation we describe in detail a method of simulating case control samples at a set of linked snps that replicates the patterns of ld in human populations and we used it to assess power for a comprehensive set of available genotyping chips our results allow us to compare the performance of the chips to detect variants with different effect sizes and allele frequencies look at how power changes with sample size in different populations or when using multi marker tags and genotype imputation approaches and how performance compares to a hypothetical chip that contains every snp in hapmap a main conclusion of this study is that marked differences in genome coverage may not translate into appreciable differences in power and that when taking budgetary considerations into account the most powerful design may not always correspond to the chip with the highest coverage we also show that genotype imputation can be used to boost the power of many chips up to the level obtained from a hypothetical complete chip containing all the snps in hapmap our results have been encapsulated into an r software package that allows users to design future association studies and our methods provide a framework with which new chip sets can evaluated
twin studies have provided the basis for genetic and epidemiological studies in human complex traits as epigenetic factors can contribute to phenotypic outcomes we conducted a dna methylation analysis in white blood cells wbc buccal epithelial cells and gut biopsies of monozygotic mz twins as well as wbc and buccal epithelial cells of dizygotic dz twins using cpg island microarrays here we provide the first annotation of epigenetic metastability of approximately unique genomic regions in mz twins an intraclass correlation icc based comparison of matched mz and dz twins showed significantly higher epigenetic difference in buccal cells of dz co twins p x although such higher epigenetic discordance in dz twins can result from dna sequence differences our in silico snp analyses and animal studies favor the hypothesis that it is due to epigenomic differences in the zygotes suggesting that molecular mechanisms of heritability may not be limited to dna differences
charles darwin believed that all traits of organisms have been honed to near perfection by natural selection the empirical basis underlying darwin s conclusions consisted of numerous observations made by him and other naturalists on the exquisite adaptations of animals and plants to their natural habitats and on the impressive results of artificial selection darwin fully appreciated the importance of heredity but was unaware of the nature and in fact the very existence of genomes a century and a half after the publication of the origin we have the opportunity to draw conclusions from the comparisons of hundreds of genome sequences from all walks of life these comparisons suggest that the dominant mode of genome evolution is quite different from that of the phenotypic evolution the genomes of vertebrates those purported paragons of biological perfection turned out to be veritable junkyards of selfish genetic elements where only a small fraction of the genetic material is dedicated to encoding biologically relevant information in sharp contrast genomes of microbes and viruses are incomparably more compact with most of the genetic material assigned to distinct biological functions however even in these genomes the specific genome organization gene order is poorly conserved the results of comparative genomics lead to the conclusion that the genome architecture is not a straightforward result of continuous adaptation but rather is determined by the balance between the selection pressure that is itself dependent on the effective population size and mutation rate the level of recombination and the activity of selfish elements although genes and in many cases multigene regions of genomes possess elaborate architectures that ensure regulation of expression these arrangements are evolutionarily volatile and typically change substantially even on short evolutionary scales when gene sequences diverge minimally thus the observed genome architectures are mostly products of neutral processes or epiphenomena of more general selective processes such as selection for genome streamlining in successful lineages with large populations selection for specific gene arrangements elements of genome architecture seems only to modulate the results of processes
we propose a method for characterizing large complex networks by introducing a new matrix structure unique for a given network which encodes structural information provides useful visualization even for very large networks and allows for rigorous statistical comparison between networks dynamic processes such as percolation can be visualized using animations applications to graph theory are discussed as are generalizations to weighted networks real world network similarity testing and applicability to the graph problem
advancements in the field of dna sequencing are changing the scientific horizon and promising an era of personalized medicine for elevated human health although platforms are improving at the rate of moore s law thereby reducing the sequencing costs by a factor of two or three each year we find ourselves at a point in history where individual genomes are starting to appear but where the cost is still too high for routine sequencing of whole genomes these needs will be met by miniaturized and parallelized platforms that allow a lower sample and template consumption thereby increasing speed and reducing costs current massively parallel state of the art systems are providing significantly improved throughput over sanger systems and future single molecule approaches will continue the exponential improvements in field
the erbb signaling pathways which regulate diverse physiological responses such as cell survival proliferation and motility have been subjected to extensive molecular analysis nonetheless it remains poorly understood how different ligands induce different responses and how this is affected by oncogenic mutations to quantify signal flow through erbb activated pathways we have constructed trained and analyzed a mass action model of immediate early signaling involving receptors egfr and and the mapk and akt cascades we find that parameter sensitivity is strongly dependent on the feature e g erk or akt activation or condition e g egf or heregulin stimulation under examination and that this context dependence is informative with respect to mechanisms of signal propagation modeling predicts log linear amplification so that significant erk and akt activation is observed at ligand concentrations far below the k d for receptor binding however mapk and akt modules isolated from the erbb model continue to exhibit switch like responses thus key system wide features of erbb signaling arise from nonlinear interaction among signaling elements the properties of which appear quite different in context and isolation
haemodynamic signals underlying functional brain imaging for example functional magnetic resonance imaging fmri are assumed to reflect metabolic demand generated by local neuronal activity with equal increases in haemodynamic signal implying equal increases in the underlying neuronal few studies have compared neuronal and haemodynamic signals in alert to test for this assumed correspondence here we present evidence that brings this assumption into question using a dual wavelength optical imaging that independently measures cerebral blood volume and oxygenation continuously in alert behaving monkeys we find two distinct components to the haemodynamic signal in the alert animals primary visual cortex one component is reliably predictable from neuronal responses generated by visual input the other componentof almost comparable strengthis a hitherto unknown signal that entrains to task structure independently of visual input or of standard neural predictors of haemodynamics this latter component shows predictive timing with increases of cerebral blood volume in anticipation of trial onsets even in darkness this trial locked haemodynamic signal could be due to an accompanying arterial pumping mechanism closely matched in time with peaks of arterial dilation entrained to predicted trial onsets these findings tested in two animals challenge the current understanding of the link between brain haemodynamics and local neuronal activity they also suggest the existence of a novel preparatory mechanism in the brain that brings additional arterial blood to cortex in anticipation of tasks
in eukaryotes small non coding rnas regulate gene expression helping to control cellular metabolism growth and differentiation to maintain genome integrity and to combat viruses and mobile genetic elements these pathways involve two specialized ribonucleases that control the production and function of small regulatory rnas the enzyme dicer cleaves double stranded rna precursors generating short interfering rnas and micrornas in the cytoplasm these small rnas are transferred to argonaute proteins which guide the sequence specific silencing of messenger rnas that contain complementary sequences by either enzymatically cleaving the mrna or repressing its translation the molecular structures of dicer and the argonaute proteins free and bound to small rnas have offered exciting insights into the molecular mechanisms that are central to rna pathways
the finding that sequence specific gene silencing occurs in response to the presence of double stranded rnas has had an enormous impact on biology uncovering an unsuspected level of regulation of gene expression this process known as rna interference rnai or rna silencing involves small non coding rnas which associate with nuclease containing regulatory complexes and then pair with complementary messenger rna targets thereby preventing the expression of these mrnas remarkable progress has been made towards understanding the underlying mechanisms of rnai raising the prospect of deciphering the rnai code that like transcription factors allows the fine tuning and networking of complex suites of gene activity thereby specifying cellular physiology development
background ontology term labels can be ambiguous and have multiple senses while this is no problem for human annotators it is a challenge to automated methods which identify ontology terms in text classical approaches to word sense disambiguation use co occurring words or terms however most treat ontologies as simple terminologies without making use of the ontology structure or the semantic similarity between terms another useful source of information for disambiguation are metadata here we systematically compare three approaches to word sense disambiguation which use ontologies and metadata respectively results the closest sense method assumes that the ontology defines multiple senses of the term it computes the shortest path of co occurring terms in the document to one of these senses the term cooc method defines a log odds ratio for co occurring terms including co occurrences inferred from the ontology structure the metadata approach trains a classifier on metadata it does not require any ontology but requires training data which the other methods do not to evaluate these approaches we defined a manually curated training corpus of documents for seven ambiguous terms from the gene ontology and mesh all approaches over all conditions achieve success rate on average the metadata approach performed best with when trained on high quality data its performance deteriorates as quality of the training data decreases the term cooc approach performs better on gene ontology success than on mesh success as mesh is not a strict is a part of but rather a loose is related to hierarchy the closest sense approach achieves on average success rate conclusion metadata is valuable for disambiguation but requires high quality training data closest sense requires no training but a large consistently modelled ontology which are two opposing conditions term cooc achieves greater success given a consistently modelled ontology overall the results show that well structured ontologies can play a very important role to improve disambiguation availability the three benchmark datasets created for the purpose of disambiguation are available in file
oxidation of proteins by reactive oxygen species is associated with aging oxidative stress and many diseases although free and protein bound methionine residues are particularly sensitive to oxidation to methionine sulfoxide derivatives these oxidations are readily repaired by the action of methionine sulfoxide reductase msra to gain a better understanding of the biological roles of msra in metabolism we have created a strain of mouse that lacks the msra gene compared with the wild type this mutant i exhibits enhanced sensitivity to oxidative stress exposure to oxygen ii has a shorter lifespan under both normal and hyperoxic conditions iii develops an atypical tip toe walking pattern after months of age iv accumulates higher tissue levels of oxidized protein carbonyl derivatives under oxidative stress and v is less able to up regulate expression of thioredoxin reductase under oxidative stress it thus seems that msra may play an important role in aging and disorders
the technological advances in dna sequencing over the past five years have changed our approaches to gene expression analysis fundamentally altering the basic methods used and in most cases driving a shift from hybridization based approaches to sequencing based approaches quantitative tag based studies of gene expression were one of the earliest applications of these next generation technologies but the tremendous depth of sequencing facilitates de novo transcript discovery which replaces traditional expressed sequence tag est sequencing in addition these technologies have created new opportunities for understanding the generation stability and decay of rna and the impacts of chromatin differences on gene expression as we review the impact of these methods on plant biology we also mention published studies from animal systems when the methods are broadly applicable we can anticipate that the published work over the past few years is a harbinger of much broader studies that are yet to be published and are sure to further advance our understanding of plant genomes in a field changing at a pace
summaryeukaryotic dna is bound and interpreted by numerous protein complexes in the context of chromatin a description of the full set of proteins that regulate specific loci is critical to understanding regulation here we describe a protocol called proteomics of isolated chromatin segments pich that addresses this issue pich uses a specific nucleic acid probe to isolate genomic dna with its associated proteins in sufficient quantity and purity to allow identification of the bound proteins purification of human telomeric chromatin using pich identified the majority of known telomeric factors and uncovered a large number of novel associations wecompared proteins found at telomeres maintained by the alternative lengthening of telomeres alt pathway to proteins bound at telomeres maintained by telomerase we identified and validated several proteins including orphan nuclear receptors that specifically bind to alt telomeres establishing pich as a useful tool for characterizing chromatin composition introductiondespite substantial characterization efforts chromosomes remain poorly understood cellular organelles kornberg and lorch in part due to an inability to purify given chromatin segments in a manner that would allow the identification of bound factors during the past years various chromatin isolation strategies have been pursued to establish locus specific protein composition boffa etal ghirlando and felsenfeld griesenbeck etal jasinskas and hamkalo workman and langmore and zhang and horz while each achieved enrichment of the targeted region none gave material of sufficient amount and purity to allow identification of bound factors other methods have been developed that connect specific dna sequences to the proteins that directly associate with them these include yeast one hybrid li and herskowitz and nucleic acid affinity capture kadonaga and tjian which identify sequence specific dna binding proteins these methods either use a bait dna sequence outside of its endogenous context or an invitro capture approach while these methods are useful they do not provide a complete description of what is found at the loci invivo chromatin immunoprecipitation chip is a powerful technology to assess whether a protein of interest is bound to a given genomic region chip relies on the use of antibodies and thus is limited to analysis of the factors that are tested and does not establish a complete description of composition to gain insight into locus specific composition we sought to develop a strategy to purify an endogenous segment of chromatin in sufficient quantity and purity to identify the associated proteins such a technology would permit a detailed correlation between composition at a locus and phenotype leading to a deeper understanding of chromosome biology we wanted this method to be direct relatively quantitative and achievable without the requirement for genetic engineering since the dna sequence provides a universal means of discriminating a specific chromatin locus from others we used nucleic acid hybridization as the basis for purification we demonstrate below that we are able to isolate specific formaldehyde crosslinked chromatin regions and identify the proteins bound to those loci using mass spectrometric analysis ms we call this method proteomics of isolated chromatin segments pich because it uses the dna to retrieve the protein information this is opposite to chip which uses protein antigens to capture the associated dna results pich strategyto determine whether a protein of interest is localized to a specific genomic region a common approach is to combine immunostaining and dna insitu hybridization immuno fish on fixed nuclei we sought to develop a biochemical variation of this method to affinity purify the proteins from a genomic chromatin locus using nucleic acid hybridization and use ms to identify the associated factors pich see initial attempts to retrieve the target chromatin using conventional dna capture fish probes and common fish reagents suffered from very low yields and high levels of contamination from nonspecific proteins data not shown to maximize the specific enrichment of telomere associated proteins optimization of the pich procedure was necessary the resulting protocol is outlined in in brief cells were fixed the chromatin was solubilized a specific probe was hybridized to the chromatin the hybridized chromatin was captured on magnetic beads the hybrids were eluted and the associated proteins were identified extensive crosslinking with formaldehyde was used to preserve protein dna and protein protein interactions unlike strategies based upon antibody antigen affinity nucleic acid hybridization is insensitive to the presence of ionic detergents rose etal which allows the use of these detergents throughout to limit contamination to increase the stability of the probe chromatin interactions we used locked nucleic acid lna containing oligonucleotides as probes because lna residues have an altered backbone that favors base stacking thereby significantly increasing their melting temperature vester and wengel to minimize the steric hindrance detrimental for yields observed upon immobilization of chromatin griesenbeck etal and sandaltzopoulos etal we used a very long spacer morocho etal between the immobilization tag and the lna probe finally we limited the coelution of nonspecific factors by using desthiobiotin a biotin analog with weaker affinity for avidin permitting a competitive gentle elution using biotin hirsch etal use of telomeric chromatin as a proof of principleto establish this technology we chose to look at telomeres telomeres are an attractive target to test pich because they are abundant there are approximately telomeres per cell thus reducing the amount of starting material required to obtain sufficient purified material for analysis moreover since telomeres are of significant biological interest they have been well characterized allowing us to validate the technology telomeres must be maintained to support continued cell division a characteristic feature of stem cells and cancer cells stewart and weinberg maintenance is normally achieved by theactivation of telomerase a reverse transcriptase able to compensate for sequence loss inherent to linear dna replication in a subset of cancers primarily those of mesenchymal origin and in several invitro immortalized cell lines telomeres are maintained in the absence of telomerase by an alternative lengthening of telomeres pathway alt that involves a poorly understood mechanism based on recombination cesare and reddel alt telomeres exhibit heterogeneous length and can be found associated with pml nuclear bodies pml nbs for unknown reasons molenaar etal we used pich to compare composition at alt telomeres with composition at telomerase maintained telomeres these data were used to validate pich by asking whether pich could identify the majority of previously characterized associations with these telomeres and identify novel proteins whose association with telomeres could be verified using independent means including fluorescence microscopy pich reveals telomere compositionwe used pich on three human cell lines with two distinct types of telomere two hela clones that are telomerase positive and show different telomere length and and the alt cell line we used a probe designed to hybridize with telomere sequences and as a control a probe with the same base composition but in a scrambled order referred to as scrambled below among the hits not present in the scrambled purification pich identified proteins associated with hela telomeres and proteins associated with alt telomeres of the cells see tables and consistent ms results were obtained in replicate purifications i e out of proteins were found in only one out of two alt pulldowns see table ninety eight proteins about half were found at both types of telomeres these associations were specific to the telomere probe since these proteins were not retrieved when pich was performed using the scrambled probe see table most of the proteins previously shown to bind telomeres including low abundance proteins such as apollo lenain etal and van overbeek and de lange were found in the telomere pich see table in total proteins previously shown to associate with telomeres were found table we failed to identify four known or expected components of telomere chromatin the tankyrase poly adp ribose polymerase smith etal the helicase tarsounas etal the werner syndrome helicase wrn crabbe etal although wrn interactors such as the whip atpase and the tera protein were found and the deacetylase michishita etal we also failed to identify the telomerase reverse transcriptase tert and dyskerin at hela telomeres these proteins are unlikely to represent a constitutive component of telomere chromatin based upon previous analyses so would not be expected to be telomere bound in a significant percentage of cells especially in these nonsynchronized populations in any case lack of detection might reflect low abundance at telomeres or might reflect inefficient crosslinking although formaldehyde was used by others to demonstrate the presence of certain of these proteins at telomeres tankyrase and wrn we conclude that pich is robust sincea single experiment suffices to retrieve most of the known components of the targeted locus about to further validate the technology we asked whether other proteins present in these preparations are bona fide telomere associated proteins validation of telomere associationsto determine the utility of pich novel associations must be demonstrated to be bound to telomeres using independent methods we present several lines of evidence that suggest that pich is reliable first we assembled a rank ordered list of factors identified from the purified material based upon abundance calculated using the ratio of peptide number to protein size among the top scoring proteins from each of the two lists hela and are known or expected telomere interacting proteins we also detected maintenance type specific associations see tables and and references therein for instance protein components of the telomerase holoenzyme and were only found at hela telomeres their absence at alt telomeres is consistent with the absence of htr and htert expression in the cell line bryan etal similarly the pml protein the complex the complex the blm helicase or topoisomerase iii alpha proteins previously shown to be associated with alt telomeres nabetani etal potts and yu stavropoulos etal and tsai etal were only detected in the pich preparations again confirming that maintenance specific associations can be identified finally ob fold containing protein is a putative human ortholog of yeast a critical telomere binding protein in this organism is found at both types of telomeres by pich consistent with the anticipated role for this protein at human telomeres martin etal we next validated the association of a set of proteins from the list that had not previously been reported to interact with telomeres as a stringent criterion for validity of the listed proteins we analyzed two of the five lowest ranked proteins from the alt list the fanconi anemia factor j fanc j a helicase ranking in the last position and the transcriptional corepressor we also analyzed the localization of nxp a protein that ranked in the middle of the alt list consistent with our results from pich all three proteins were found to associate with alt telomeres by coimmunostaining but were not found to associate with hela telomeres data not shown while we found some cells harboring a pattern with enriched at most telomeres this pattern was uncommon because this was seen in multiple cells is a valid association we then examined homeobox containing protein a putative transcription factor that had not been previously reported as binding to telomeres chen etal but was identified by pich in all cells tested hela and see tables and and data not shown this protein is highly conserved in vertebrates but not in lower eukaryotes interestingly the two closest related sequences in c elegans retrieved the and ceh proteins the latter is known to be telomeric in this organism which harbors a similar telomeric repeat sequence ttaggc kim etal exogenously expressed flag colocalized with the telomere marker in about of nuclei we could detect at some telomeres in the hela cell line but in only of nuclei with the majority of staining being nontelomeric thus we cannot rigorously exclude the possibility that it represents a false positive from the hela pich however its ms identification at both types of telomeres and telomeric localization observed by immunostaining of a majority of nuclei suggest that it is a true association in both cell lines therefore of the eight novel telomere associated proteins identified by pich that we tested including four described below seven showed clear association and one showed ambiguous but likely telomere association this indicates that the false positive rate for telomere proteins identified by pich is likely to be low orphan receptors at alt telomeresshelterin proteins and histones are expected to be the most abundant components at long telomeres because they play a structural role at that locus in addition to these proteins in the alt telomere preparations we were surprised to identify orphan receptors the two most abundant of these receptors were of the class coup and which both had coverage by ms sequencing thus ranking higher than two shelterin proteins and see table other proteins were also identified coup and these proteins belong to the nuclear hormone receptor superfamily and are well characterized transcription factors it is unlikely that they originate from contaminating promoter elements adjacent to telomeres since these factors were not found at hela telomeres short or long we were intrigued that these factors were highly enriched at regions that have not been described as containing classical promoters the specificity for these associations was confirmed by chip using antibodies against coup and and by immunoblot analysis on pich purified telomeres moreover these proteins were specifically enriched at telomeres compared to other nuclear regions as shown by immuno fish and costaining experiments in the alt cell line coup was also found at alt telomeres by costaining but at a lower frequency consistent with it having significantly lower coverage in the ms table in contrast none of these proteins could be detected at hela telomeres see table figures and by immunoblot we found that while coup and are expressed at higher levels in the alt cell line coup is more abundant in hela cells and yet is not found at telomeres the specific association of orphan receptors with alt telomeres thus doesnot appear to reflect differences in expression levels in the cell lines tested but rather an association that is specific to alt telomeres to determine if these associations are unique to the particular alt cell line analyzed we performed immuno fish experiments in two other alt cell lines os and sa os in contrast to the cell line originating from immortalization invitro os and sa os are human osteosarcoma tumor cell lines both coup and were detected at telomeres in os cells while only could be observed at sa os telomeres see we suggest that orphan receptor association with telomeres might be a common feature of alt cells since this was detected in three distinct alt cell lines since a growing number of orphan receptors including park etal are shown to be targeted to the pml nb or to discrete nuclear bodies chalkiadaki and talianidis chen etal and wu etal their presence at alt telomeres could be due to the nonspecific crosslinking of orphan receptors to telomeres localized at the pml nb we found however that orphan receptors could be found associated with telomeres in the absence of any pml signal arrowheads in arguing against this possibility furthermore we examined colocalization of orphan receptors and telomeres using cells in metaphase this experiment also addresses the possibility that orphan receptors associate with extrachromosomal telomere repeats ectr alt cells are characterized by the presence of extrachromosomal telomere dna repeats ectr of unknown function tokutake etal despite the observation that the cells that we used have negligible amounts of ectr ford etal and tokutake etal the recent proposal that ectr localizes at pml bodies fasching etal raised the formal possibility that orphan receptors are associated only with ectr if that were true they would not be expected to be colocalized with telomeres at the ends of chromosomes immunostaining of metaphase chromosomes revealed prominent signals for orphan receptors colocalized with shelterin proteins at the tips of chromosomes we observed a similar pattern of chromosomal tip localization for orphan receptors in the os cell line not shown we conclude that orphan receptors associate with telomeric dna located at the tips of chromosomes leading us to wonder what impact these nuclear receptors might have on alt telomere maintenance orphan nuclear receptor proteins may underlie telomere pml nb colocalizationit has been proposed that a critical step in alt maintenance occurs in the pml nb because disruption of pml expression suppresses alt jiang etal and several proteins potentially involved in telomere recombination are localized at pml nbs in alt cells henson etal we wondered if the binding of orphan receptors to variant sequences known to be at telomeres see discussion might cause relocalization of telomeres to pml bodies and thus might impact telomere maintenance according to the pich data there are at least five different orphan receptors bound to telomeres knocking down the expression of all five of these proteins at the same time is technically challenging so we examined whether knocking down theexpression of the highest ranked associated protein coup would produce an effect on the localization of telomeres to pml nbs using shrna we derived stable cell lines in which coup had been knocked down in the absence of knockdown an average of of telomeres as identified by immunostaining are colocalized with the pml protein in the clones in which coup levels were reduced telomeres were significantly delocalized from pml nb colocalization see we conclude that coup expression levels are related to telomere pml nb association frequency in the cell line to address whether delocalization of telomeres from pml nb impacts telomere maintenance we measured telomere length inthe orphan receptor depleted cell lines a subtle shortening of telomeres was observed in the two cell lines that also showed knockdown of coup as indicated by the smearing down of telomere restriction fragments from these cell lines despite this phenotype we did not notice an obvious proliferation defect however this analysis is complicated by the observation that becomes upregulated in many of these stable cell lines and by the appearance of revertants in the cellular population data not shown our results are consistent with a possible redundant function of orphan receptor action at telomeres a full phenotypic effect total disruption of pml nb association and thus alt maintenance might require knocking down multiple orphan receptors found at telomeres nonetheless we conclude that there is a subtle but measurable impact on telomeres caused by knocking down coup offering further validation for a genuine association of this family of receptors with alt telomeres insights into telomeric chromatin structurein addition to providing information regarding the identity of proteins bound to a locus pich provides information on the relative levels of abundant proteins bound to a given sequence in distinct cell types for example in the process of developing pich for use on telomeres we noticed fewer histones in telomeric preparations from hela cells than in telomeric preparations from hela cells we pursued this finding because of its possible relationship to the silencing of subtelomeric regions known as telomere position effect tpe which increases with the length of telomeres baur etal tpe is a classic example of gene silencing mediated by chromatin indeed histone modifications and chromatin structure have been suggested to play a major role in tpe in mammals blasco in addition we were interested in validating the efficacy of pich in measuring protein level the telomeres from hela cells are longer than those in cells kb on average versus kb thus the observation that the shorter telomeres have less histones might reflect a difference in nucleosome density or might simply reflect the fact that there is less total telomeric material in cells when we loaded the same amount of shelterin proteins expected to represent an equal loading of telomeric dna loayza and de lange we observed greater nucleosome signal at the longer telomeres chip with an anti histone antibody confirmed the lower nucleosomal density at hela telomeres about fold less nucleosomes kb of telomeric dna moreover immunoblots performed on pich purified telomere material normalized for shelterin abundance also confirmed the paucity of nucleosomes at the short hela telomeres this depletion of nucleosomes might explain the previously observed smeary mnase digestion of short human telomeres while longer telomeres show a more discrete pattern tommerup etal these results suggest that in the size range analyzed here nucleosomal density increases with telomere length consistent with the greater tpe observed with longer telomeres baur etal this is also consistent with our failure to detect proteins athela telomeres while these factors were readily identified in the other cell line harboring longer telomeres we hypothesize that a close relationship exists between histone density heterochromatin protein associations telomere length and tpe importantly these data indicate that nucleosomal chromatin is not refractory to pich in addition to canonical histones and histone variants and testis specific were identified by pich in the cell lines that contain longer telomeres hela and we confirmed the presence of in these telomere preparations by western blot on pich material we noticed an underrepresentation of the linker compare the input with the purified telomere lanes consistent with previous findings suggesting that and linker may not coexist at specific loci abbott etal we confirmed the paucity of histone at telomeres using chip to compare the relative abundance of to that of telomeres or at alu repeats about to fold less the underrepresentation of in the telomeric nucleosomal arrays might explain the bp shorter nucleosomal repeat length previously observed at telomeres lejnine etal we conclude that for ubiquitous chromatin proteins expected at most loci such as histones pich reflects relative abundance of bound proteins and provides insights into chromatin structure discussion pich as a tool for the characterization of chromatin compositionthe identification of the proteins that interact with genomic regions of interest is critical to the understanding of genome biology these questions have primarily been studied using genetics biochemical characterization of soluble complexes structural studies chromatin immunoprecipitation and cell biology these methods are powerful but do not provide information concerning the complete composition of a locus by establishing the chromatin formula of factors bound at specific loci pich has the potential to advance the characterization of chromosomes by providing a method to examine the entire set of interacting proteins and how composition changes during regulation we used pich to study telomeres and identified most previously known factors and also new proteins that might be relevant to telomere biology these studies validate the potential of pich a major limitation for pich is the need for a sufficient amount of purified protein for identification by ms which can require starting with large amounts of material using conservative estimates a standard ms requires about a picomole of a given protein if the purification of a locus bound by a single protein is considered and if that locus is unique ms necessitates at least half a picomole of diploid cells amounting to several hundreds of liters of a standard mammalian cell line to circumvent issues related to handling such large amounts of cells we chose to target human telomeres which represent a relatively abundant locus focusing on human telomeric chromatin decreases the need for starting material by about fold as compared to single copy loci of similar individual size since each chromosome has two telomeres in terms of relative abundance telomere sequences constitute to of the genome in each cell a technology that works on this type of target can immediately be applied to other repeat sequences or low copy elements found in smaller genomes for example corresponds to a and kb locus in the yeast and fly genomes respectively indicating that pich as described here might be suitable for the purification of single copy segments in those organisms subsequent optimization might extend the use of pich to smaller segments or low repeat single copy elements in mammalian genomes pich relies on nucleic acid hybridization to crosslinked chromatin and thus has the advantage that no genetic engineering is required there are however potential limitations avoiding crosshybridization to other genomic regions is an issue that requires a careful capture probe design moreover our failure to identify a few of the known telomere interactors suggests that either these factors are not abundant at telomeres or these factors fail to crosslink stably we used formaldehyde a crosslinker in this study the use of other crosslinking compounds might help resolve such issues by capturing loosely bound but biologically important proteins complexity of proteins found at telomeresthe relatively high number of proteins found at each class of telomeres underscores the diversity of events occurring at this locus while it is possible that telomeres are continually bound by a large number of proteins this diversity might more likely be caused by each individual telomere harboring distinct composition at a given time the cells used for these experiments were not synchronized and it is therefore expected that pich retrieved telomeres during various phases of the cell cycle for instance the bi modal distribution of fanc j in alt cells detected by immunflourescence might reflect this and might explain its weak ms detection in our samples the results from pich as performed here therefore give an average composition of this locus the use of synchronized cultures and pich might be informative in this regard it is inevitable that some proteins retrieved by pich are bound throughout the genome and thus do not play a specific role at telomeres for example replication factors presumably fall into this category although their enrichment at telomeres might also have a specific biological meaning gilson and geli on the other hand pich has the ability to identify factors that would be difficult to uncover using genetics because they either play vital roles elsewhere or are redundant as might be the case for orphan receptors it will be interesting to purify telomeres from more sources to correlate variations in the telomere formula with phenotypic changes the purification of telomeres from other organisms will also be critical for a greater understanding of telomere biology since fundamental protection or maintenance mechanisms seem to be conserved orphan receptors at alt telomeresnumerous proteins were seen that were specific to alt telomeres some of which were expected based upon previous literature e g proteins involved in recombination and some of which were unexpected e g orphan receptors we focused on the interaction between alt telomeres and a subfamily of orphan nuclear receptors as characterizing this unexpected finding was important to validating the efficacy of pich the presence of orphan receptors at alt telomeres might be viewed as a by product of the alt pathway e g promoted by the deprotection of telomeres or as one of the causes for alt we found that knocking down coup one of several orphan receptors had a measurable impact on association of alt telomeres with pml bodies and on overall telomere length while this is consistent with a potential effector role for orphan receptors in alt maintenance more work is needed to test this hypothesis why might orphan receptors bind to telomeres the proximal kb of human telomeres harbor variant sequences interspersed with the normal ttaggg repeats or other telomere variant motifs baird etal one of these variant sequences the c type variant hexanucleotide repeat tcaggg creates a high affinity binding site for orphan receptors it is possible that alt telomeres are enriched in this motif alternatively these sites could pre exist in non alt cells but be occluded by chromatin regulators benetti etal the mechanisms that promote orphan receptor binding and function at alt telomeres require further investigation while pich has been proven effective for the characterization of the composition of telomeres its applicability extends to other loci we have used pich to purify mouse pericentric heterochromatin and have obtained a distinct protein formula data not shown this approach can be used to characterize other nucleoprotein complexes including nonchromatin targets although technically challenging we hope to extend pich to single copy elements the ability to identify proteins bound to a given regulatory sequence based solely upon the identity of that dna sequence will allow the unbiased discovery of regulatory interactions at key genomic loci experimental procedures telomeric chromatin isolation by pich chromatin sample preparationfrom l hela cell equivalent cells ml cells were crosslinked in formaldehyde for min at room temperature rt and washed four times in pbs cells were then equilibrated in sucrose buffer and dounced with a tight pestle cells were equilibrated in glycerol buffer and pellet was collected following centrifugation at g for min at the pellet was resuspended into the same volume of glycerol buffer the pellet was frozen into liquid nitrogen and stored at or used immediately for the next step the following volumes and numbers are given for one purification that is cell equivalent the material was centrifuged at g for min at rt and the pellet was resuspended into the same pellet volume of triton x and l of rnasea qiagen mg ml were added the mixture was incubated for min at rt with shaking then at for hr and was washed six times in pbs the material was equilibrated in lbjd solution and the pellet was resuspended into pellet volume of lbjd solution samples were sonicated micro tip misonix using the following parameters power setting watts s constant pulse and s pause for a min total process time sample was collected by centrifugation at g for min at rt chromatin sample was then applied to sephacryl s hr spin columns and incubated at for min lbjdls pre equilibrated streptavidin beads were added pierce ultralink streptavidin ml and the sample was incubated for hr at rt beads were discarded and supernatant was saved chromatin capturesamples were centrifuged min at g at rt and final volume of sds was added together with the lna probe m final concentration hybridization was conducted as follows for min for min for min for min for for min for min final temperature the sample was centrifuged at g for min and the supernatant was diluted twice with milliq water and the lbjd pre equilibrated myone beads solution was added typically ml the sample was incubated for hr at rt before increasing the volume to ml with lbjd beads were washed seven times with ml with lbjd at rt and two times with ml with lbjd at beads were resuspended into ml of elution buffer the sample was incubated for hr at rt with shaking and temperature was raised to for min the eluate was precipitated using tri chloroacetic acid final and the pellet was resuspended into l of crosslinking reversal solution the sample was incubated at for min the proteins were separated using a bis tris acrylamide pre cast gel invitrogen or stored at the proteins were revealed using the silverquest staining kit invitrogen or colloidal blue invitrogen according to manufacturer s instructions colloidal blue stained gels were used for subsequent ms analysis lanes were sliced in regions according to banding pattern and submitted to ms analysis buffers composition for pichpbs mm mm mm nacl mm kcl sucrose buffer m sucrose mm hepes naoh ph triton x mm mgoac glycerol buffer glycerol mm hepes naoh ph mm edta mm egta mm mgoac lbjd mm hepes naoh ph mm nacl mm edta ph mm egta sds sarkosyl protease inhibitors lbjdls mm hepes naoh ph mm nacl mm edta ph mm egta ph sds sarkosyl protease inhibitors elution buffer mm biotin invitrogen cat mm hepes naoh ph mm nacl mm edta ph mm egta ph sds sarkosyl crosslinking reversal solution mm tris ph sds m mercaptoethanol capture probes are as follows telomere desthiobiotin desthiobiotin capitalized letters are lna residues and small letters are dna residues synthesis was performed by fidelity systems chromatin immunoprecipitationschip assays have been performed essentially as described lee etal antibodies used for chip were anti abcam anti clone ae santa cruz anti perseus proteomics and anti coup abcam immunoprecipitated dna was spoted on a slot blot apparatus and probed with a telomere specific probe or an alu specific probe error bars represent standard deviation sd of enrichments values obtained from independent experiments cell lines culture conditions shrna treatment and plasmid constructshela cells were grown in joklik s modified mem with calf serum by the national cell culture center hela os and sa were cultured in dmem calf serum the clone was obtained from the invitrogen human ultimate orf collection and cloned in frame with flag in a pegfp vector clontech lacking the gfp sequence flag sequence was introduced by pcr using the following primer pair aagcttcaccatgcttagttcctttccagtgg and flag gcggccgctcactacttgtcgtcatcgtccttgtagtcagcagcagcgtcatcatccagggcctctgtcttgatttc the pcr product was ligated into a pgem t easy vector and sequenced the gene was then inserted into the pegfp vector at hindiii noti sites shrna constructs were obtained from sigma mission shrna collection of the five sequences tested by transient transfection assays only the following was efficiently knocking down coup expression coup ccgggccgtatatggcaattcaatactcgagtattgaattgccatatacggcttttt control shrna is the scramble sequence available from sigma microscopycells were grown in two chambers slides lab tekii nunc cells were washed in pbs and fixed in pbs containing triton x and formaldehyde for min after two washes in pbs cells were blocked in pbs milk bsa containing tween and np for hr at rt incubation with primary antibody was in blocking solution for hr at rt or overnight at two min washes were performed after antibody incubation using pbs solutions containing tween np and then mm nacl secondary antibodies conjugated with or were from jackson laboratories slides were counterstained with dapi and mounted in mowiol antibodies used in this study are abcam santa cruz or abcam pml santa cruz or coup abcam and coup perseus proteomics flag sigma fanc j sigma and abcam ab a standard fish procedure was used whenever mentioned after post fixation of the antibody staining using the capture probe at m in a standard hybridization buffer an anti biotin fitc vectorslab was used to detect probe binding metaphase chromosome spreads and staining were performed as described sullivan and schwartz stainings were analyzed using a zeiss epifluorescence microscope and a hammamatsu ccd camera images were obtained using the openlab software and the photoshop cs software was used to color panels and prepare the figures acknowledgmentswe would like to thank mike blower for suggestions on the metaphase spread experiments matthew simon rebecca dunn and karim bouazoune for critically reading the manuscript the national cell culture center for providing large scale hela cultures titia de lange for providing the hela cell line elliott kieff for providing the ha construct joe garlick for technical assistance and the taplin mass spectrometric facility ross tomaino atharvard medical school for protein identifications j d was supported by an embo long term fellowship this work is supported by nih grant to r e k this work is dedicated to the memory of wolfram hrz a gentleman scientist
two prehistoric migrations peopled the pacific one reached new guinea and australia and a second more recent migration extended through melanesia and from there to the polynesian islands these migrations were accompanied by two distinct populations of the specific human pathogen helicobacter pylori called hpsahul and hspmaori respectively hpsahul split from asian populations of h pylori to years ago in concordance with archaeological history the hpsahul populations in new guinea and australia have diverged sufficiently to indicate that they have remained isolated for the past to years the second human expansion from taiwan years ago dispersed one of several subgroups of the austronesian language family along with one of several hspmaori clades into melanesia and polynesia where both language and parasite have continued to science
our understanding of the genetic basis of phenotypic diversity is limited by the paucity of examples in which multiple interacting loci have been identified we show that natural variation in the efficiency of sporulation the program in yeast that initiates the sexual phase of the life cycle between oak tree and vineyard strains is due to allelic variation between four nucleotide changes in three transcription factors and furthermore we identified that selection has shaped quantitative variation in yeast sporulation between strains these results illustrate how genetic interactions between transcription factors are a major source of phenotypic diversity within science
transcription factors tfs regulate the expression of genes through sequence specific interactions with dna binding sites however despite recent progress in identifying in vivo tf binding sites by microarray readout of chromatin immunoprecipitation chip chip nearly half of all known yeast tfs are of unknown dna binding specificities and many additional predicted tfs remain uncharacterized to address these gaps in our knowledge of yeast tfs and their cis regulatory sequences we have determined high resolution binding profiles for known and predicted yeast tfs over more than million gapped and ungapped bp sequences k mers we report new or significantly different direct dna binding site motifs for yeast dna binding proteins and motifs for eight proteins for which only a consensus sequence was previously known in total this corresponds to over a increase in the number of yeast dna binding proteins with experimentally determined dna binding specificities among other novel regulators we discovered proteins that bind the pac polymerase a and c motif gatgag and regulate ribosomal rna rrna transcription and processing core cellular processes that are constituent to ribosome biogenesis in contrast to earlier data types these comprehensive k mer binding data permit us to consider the regulatory potential of genomic sequence at the individual word level these k mer data allowed us to reannotate in vivo tf binding targets as direct or indirect and to examine tfs potential effects on gene expression in environmental and cellular conditions these approaches could be adapted to identify tfs and cis regulatory elements in eukaryotes
small rnas of nucleotides can target both chromatin and transcripts and thereby keep both the genome and the transcriptome under extensive surveillance recent progress in high throughput sequencing has uncovered an astounding landscape of small rnas in eukaryotic cells various small rnas of distinctive characteristics have been found and can be classified into three classes based on their biogenesis mechanism and the type of argonaute protein that they are associated with micrornas mirnas endogenous small interfering rnas endo sirnas or esirnas and piwi interacting rnas pirnas this review summarizes our current knowledge of how these intriguing molecules are generated in cells
summary at a glance the major advance offered by next generation sequencing ngs technologies is the ability to produce in some cases in excess of one billion short reads per instrument run which makes them useful for many biological applications the variety of ngs features makes it likely that multiple platforms will coexist in the marketplace with some having clear advantages for particular applications over others the leading ngs platforms use clonally amplified templates which are not affected by the arbitrary losses of genomic sequences that are inherent in bacterial cloning methods an important advantage of single molecule template platforms is that pcr is not required pcr can create mutations that masquerade as sequence variants and amplification bias that underrepresents at rich and gc rich regions in target sequences there are four primary ngs chemistry methods cyclic reversible termination sequencing by ligation pyrosequencing and real time sequencing which are described in this review to call sequence variants in genomes ngs reads are aligned to a reference sequence using various bioinformatics mapping tools whole genome sequencing using current ngs platforms is still expensive but targeting regions of interest may provide an interim solution to analysing hundreds if not thousands of samples to date the sequences of twelve human genomes have been published using a number of ngs platforms marking the beginning of personalized genomics ngs costs will continue to drop in the foreseeable future although the cost reduction should be weighed against the quality of the produced sequence
genetic screens are powerful methods for the discovery of genephenotype associations however a systems biology approach to genetics must leverage the massive amount of omics data to enhance the power and speed of functional gene discovery in vivo thus far few computational methods for gene function prediction have been rigorously tested for their performance on a genome wide scale in vivo in this work we demonstrate that integrating genome wide computational gene prioritization with large scale genetic screening is a powerful tool for functional gene discovery to discover genes involved in neural development in drosophila we extend our strategy for the prioritization of human candidate disease genes to functional prioritization in drosophila we then integrate this prioritization strategy with a large scale genetic screen for interactors of the proneural transcription factor atonal using genomic deficiencies and mutant and rnai collections using the prioritized genes validated in our genetic screen we describe a novel genetic interaction network for atonal lastly we prioritize the whole drosophila genome and identify candidate gene associations for ten receptor signaling pathways this novel database of prioritized pathway candidates as well as a web application for functional prioritization in drosophila called e ndeavour h igh f ly and the atonal network are publicly available resources a systems genetics approach that combines the power of computational predictions with in vivo genetic screens strongly enhances the process of gene function and genegene discovery
a major goal of system biology is the characterization of transcription factors and micrornas mirnas and the transcriptional programs they regulate we present allegro a method for de novo discovery of cis regulatory transcriptional programs through joint analysis of genome wide expression data and promoter or utr sequences the algorithm uses a novel log likelihood based non parametric model to describe the expression pattern shared by a group of co regulated genes we show that allegro is more accurate and sensitive than existing techniques and can simultaneously analyze multiple expression datasets with more than conditions we apply allegro on datasets from several species and report on the transcriptional modules it uncovers our analysis reveals a novel motif over represented in the promoters of genes highly expressed in murine oocytes and several new motifs related to fly development finally using stem cell expression profiles we identify three mirna families with pivotal roles in human nar
the advent and maturation of algorithms for estimating species treesphylogenetic trees that allow gene tree heterogeneity and whose tips represent lineages populations and species as opposed to genesrepresent an exciting confluence of phylogenetics phylogeography and population genetics and ushers in a new generation of concepts and challenges for the molecular systematist in this essay i argue that to better deal with the large multilocus datasets brought on by phylogenomics and to better align the fields of phylogeography and phylogenetics we should embrace the primacy of species trees not only as a new and useful practical tool for systematics but also as a long standing conceptual goal of systematics that largely due to the lack of appropriate computational tools has been eclipsed in the past few decades i suggest that phylogenies as gene trees are a local optimum for systematics and review recent advances that will bring us to the broader optimum inherent in species trees in addition to adopting new methods of phylogenetic analysis and ideally reserving the term phylogeny for species trees rather than gene trees the new paradigm suggests shifts in a number of practices such as sampling data to maximize not only the number of accumulated sites but also the number of independently segregating genes routinely using coalescent or other models in computer simulations to allow gene tree heterogeneity and understanding better the role of concatenation in influencing topologies and confidence in phylogenies by building on the foundation laid by concepts of gene trees and coalescent theory and by taking cues from recent trends in multilocus phylogeography molecular systematics stands to be enriched many of the challenges and lessons learned for estimating gene trees will carry over to the challenge of estimating species trees although adopting the species tree paradigm will clarify many issues such as the nature of polytomies and the star tree paradox raise conceptually new challenges or provide new answers to questions
originally founded to link students at harvard university the social networking application facebook has evolved into the most visited social networking site in the world with over million active users specializing in regional and scholastic networks facebook boasts an market share at universities and colleges in the united states and a recent study of more than university of florida medical students and residents determined that use facebook as academic health sciences libraries explore social networking technologies to create and market library services facebook provides a flexible space to interface with a large number of students homegrown applications for facebook have been created by libraries to answer reference questions search online public access catalogs and host multimedia collections for health sciences libraries whose users are often widely dispersed facebook offers several opportunities for outreach and instruction for example self organizing groups of users i e medical student class of pharmaceutical sciences undergraduates afford targeted marketing opportunities despite their distributed locations i e teaching hospitals rural clinics commercial pharmaceutical laboratories additionally facebook encourages developers to create applications that could be useful in a health sciences setting i e pubmed search application form affinity groups i e medical library association facebook group and fashion library pages
copy number variants cnvs contribute to human genetic and phenotypic diversity however the distribution of larger cnvs in the general population remains largely unexplored we identify large variants in approximately individuals by using illumina snp data with an emphasis on hotspots prone to recurrent mutations we find variants larger than kb in of individuals and variants greater than mb in in contrast to previous studies we find limited evidence for stratification of cnvs in geographically distinct human populations importantly our sample size permits a robust distinction between truly rare and polymorphic but low frequency copy number variation we find that a significant fraction of individual cnvs larger than kb are rare and that both gene density and size are strongly anticorrelated with allele frequency thus although large cnvs commonly exist in normal individuals which suggests that size alone can not be used as a predictor of pathogenicity such variation is generally deleterious considering these observations we combine our data with published cnvs from more than individuals contrasting control and neurological disease collections this analysis identifies known disease loci and highlights additional cnvs e g and for further investigation this study provides one of the first analyses of large rare cnvs in the general population with insights relevant to future analyses of disease
background molecular biology data exist on diverse scales from the level of molecules to omics at the same time the data at each scale can be categorised into multiple layers such as the genome transcriptome proteome metabolome and biochemical pathways due to the highly multi layer and multi dimensional nature of biological information software interfaces for database browsing should provide an intuitive interface that allows for rapid migration across different views and scales the zoomable user interface zui and tabbed browsing have proven successful for this purpose in other areas especially to navigate the vast information in the world wide web results this paper presents genome projector a web based gateway for genomics information with a zoomable user interface using google maps api equipped with four seamlessly accessible and searchable views a circular genome map a traditional genome map a biochemical pathways map and a dna walk map the web application for bacterial genomes is available at http www g language org genomeprojector all data and software including the source code documentations and development api are freely available under the gnu general public license zoomable maps can be easily created from any image file using the development api and an online data mapping service for genome projector is also available at our web site conclusion genome projector is an intuitive web application for browsing genomics information implemented with a zoomable user interface and tabbed browsing utilising google maps api and asynchronous javascript and xml technology
the most time consuming and expensive task in machine learning is the gathering of labeled data to train the model or to estimate its parameters in the real world scenario the availability of labeled data is scarce and we have limited resources to label the abundantly available unlabeled data hence it makes sense to pick only the most informative instances from the unlabeled data and request an expert to provide the label for that instance active learning algorithms aim at minimizing the amount of labeled data required to achieve the goal of the machine learning task in hand by strategically selecting the data instance to be labeled by the expert a lot of research has been conducted in this area over the past two decades leading to great improvements in performance of several existing machine learning algorithms and has also been applied to diverse fields like text classification information retrieval computer vision and bioinformatics to name a few this survey aims at providing an insight into the research in this area and categorizes the diverse algorithms proposed based on main characteristics we also provides a desk where different active learning algorithms can be compared by evaluation on datasets
pervasive and hidden transcription is widespread in eukaryotes but its global level the mechanisms from which it originates and its functional significance are unclear cryptic unstable transcripts cuts were recently described as a principal class of rna polymerase ii transcripts in saccharomyces cerevisiae these transcripts are targeted for degradation immediately after synthesis by the action of the exosome tramp complexes although cut degradation mechanisms have been analysed in detail the genome wide distribution at the nucleotide resolution and the prevalence of cuts are unknown here we report the first high resolution genomic map of cuts in yeast revealing a class of potentially functional cuts and the intrinsic bidirectional nature of eukaryotic promoters an rna fraction highly enriched in cuts was analysed by a long sage serial analysis of gene expression approach adapted to deep sequencing the resulting detailed genomic map of cuts revealed that they derive from extremely widespread and very well defined transcription units and do not result from unspecific transcriptional noise moreover the transcription of cuts predominantly arises within nucleosome free regions most of which correspond to promoter regions of bona fide genes some of the cuts start upstream from messenger rnas and overlap their end our study of glycolysis genes as well as recent results from the literature indicate that such concurrent transcription is potentially associated with regulatory mechanisms our data reveal numerous new cuts with such a potential regulatory role however most of the identified cuts corresponded to transcripts divergent from the promoter regions of genes indicating that they represent by products of divergent transcription occurring at many and possibly most promoters eukaryotic promoter regions are thus intrinsically bidirectional a fundamental property that escaped previous analyses because in most cases divergent transcription generates short lived unstable transcripts present at very low steady levels
background the number of genome wide association studies gwas is growing rapidly leading to the discovery and replication of many new disease loci combining results from multiple gwas datasets may potentially strengthen previous conclusions and suggest new disease loci pathways or pleiotropic genes however no database or centralized resource currently exists that contains anywhere near the full scope of gwas results methods we collected available results from gwas articles into a database of significant snp phenotype associations and accompanying information making this database freely available here in doing so we met and describe here a number of challenges to creating an open access database of gwas results through preliminary analyses and characterization of available gwas we demonstrate the potential to gain new insights by querying a database across gwas results using a genomic bin based density analysis to search for highly associated regions of the genome positive control loci e g mhc loci were detected with high sensitivity likewise an analysis of highly repeated snps across gwas identified replicated loci e g apoe lpl at the same time we identified novel highly suggestive loci for a variety of traits that did not meet genome wide significant thresholds in prior analyses in some cases with strong support from the primary medical genetics literature suggesting these genes merit further study additional adjustment for linkage disequilibrium within most regions with a high density of gwas associations did not materially alter our findings having a centralized database with standardized gene annotation also allowed us to examine the representation of functional gene categories gene ontologies containing one or more associations among top gwas results genes relating to cell adhesion functions were highly over represented among significant associations p x a finding which was not perturbed by a sensitivity analysis conclusion we provide access to a full gene annotated gwas database which could be used for further querying analyses or integration with other genomic information we make a number of general observations of reported associated snps lie within the boundaries of a refseq gene and are within kb of one indicating a bias toward gene centricity in the findings we found considerable heterogeneity in information available from gwas suggesting the wider community could benefit from standardization and centralization of reporting
species distribution models sdms use spatial environmental data to make inferences on species range limits and habitat suitability conceptually these models aim to determine and map components of a species ecological niche through space and time and they have become important tools in pure and applied ecology and evolutionary biology most approaches are correlative in that they statistically link spatial data to species distribution records an alternative strategy is to explicitly incorporate the mechanistic links between the functional traits of organisms and their environments into sdms here we review how the principles of biophysical ecology can be used to link spatial data to the physiological responses and constraints of organisms this provides a mechanistic view of the fundamental niche which can then be mapped to the landscape to infer range constraints we show how physiologically based sdms can be developed for different organisms in different environmental contexts mechanistic sdms have different strengths and weaknesses to correlative approaches and there are many exciting and unexplored prospects for integrating the two approaches as physiological knowledge becomes better integrated into sdms we will make more robust predictions of range shifts in novel or non equilibrium contexts such as invasions translocations climate change and shifts
determining protein small molecule binding affinity is a key component of present day rational drug discovery to circumvent the time labor and materials costs associated with experimental protein small molecule binding assays a variety of structure based computational methods have been developed for determining protein small molecule binding affinities these methods can be placed in one of two classes accurate but slow class and fast but approximate class class methods which explicitly take into account protein flexibility and include an atomic level description of solvation are capable of quantitatively reproducing experimental protein small molecule absolute binding free energies however class computational requirements make screening thousands to millions of small molecules against a protein as required for rational drug design infeasible for the foreseeable future class methods on the contrary are sufficiently fast to perform such inhibitor screening yet they suffer from limited descriptions of protein flexibility and solvation which in turn limit their ability to select and rank order small molecules by computed binding affinities this review presents an overview of class and class methods and avenues of research in class methods aimed at bringing them closer to accuracy
energy policies may lead to important industrial outcomes this paper investigates the impacts of energy policies on industry growth in renewable energy research tools employed include the commercialization process value chain analysis and empirical case studies different renewable energy technologies and geographical regions are considered covering over of the world markets of the technology fields considered market deployment measures that enhance home markets of domestic industries will in most cases lead to growing industrial activities irrespective of the domestic market situation pure investment or r d support alone to already strong industries in related fields may be powerful to help with diversification into sustainable energy business several exogenous factors such as timing size factors geography etc will influence both the industrial and policy positioning in practice the results indicate that there are increased industrial opportunities in renewable energy to be captured not only by large countries or through large public resources but also smaller countries can gain success through clever policies and optimal managing of the process
the purpose of this study was to investigate the extent to which social networking tools are being used in the curricula of medical and nursing schools as new internet technology tools are introduced educators in health related disciplines have the opportunity to incorporate these new tools into the curriculum to enhance instruction and the learning process wikis blogs and other social networking tools may all be used both to augment the educational method and to increase efficacy
we present a candidate quantum field theory of gravity with dynamical critical exponent equal to z in the uv as in condensed matter systems z measures the degree of anisotropy between space and time this theory which at short distances describes interacting nonrelativistic gravitons is power counting renormalizable in dimensions when restricted to satisfy the condition of detailed balance this theory is intimately related to topologically massive gravity in three dimensions and the geometry of the cotton tensor at long distances this theory flows naturally to the relativistic value z and could therefore serve as a possible candidate for a uv completion of einsteins general relativity or an infrared modification thereof the effective speed of light the newton constant and the cosmological constant all emerge from relevant deformations of the deeply nonrelativistic z theory at distances
bioinformatics summary biodbnet is an online web resource that provides interconnected access to many types of biological databases it has integrated many of the most commonly used biological databases and in its current state has database identifiers nodes covering all aspects of biology including genes proteins pathways and other biological concepts biodbnet offers various ways to work with these databases including conversions extensive database reports custom navigation and has various tools to enhance the quality of the results importantly the access to biodbnet is updated regularly providing access to the most recent releases of each individual database availability http biodbnet abcc ncifcrf govcontact stephensr mail nih govsupplementary information supplementary data are available at online
abstract background in the context of systems biology few sparse approaches have been proposed so far to integrate several data sets it is however an important and fundamental issue that will be widely encountered in post genomic studies when simultaneously analyzing transcriptomics proteomics and metabolomics data using different platforms the goal here is to understand the mutual interactions between the different data sets in this high dimensional setting variable selection is crucial to give interpretable results we focus on a sparse partial least squares approach spls to handle two block data sets where the relationship between the two types of variables is known to be symmetric sparse pls has been developed either for a regression or a canonical correlation framework and includes a built in procedure to select variables while integrating data to illustrate the canonical mode approach we analyzed the data sets where two different platforms cdna and affymetrix chips were used to study the transcriptome of sixty cancer cell lines results we compare the results obtained with two other sparse or related canonical correlation approaches cca with elastic net penalization cca en and co inertia analysis cia the latter does not include a built in procedure for variable selection and requires a two step analysis we stress the lack of statistical criteria to evaluate canonical correlation methods which makes biological interpretation absolutely necessary to compare the different gene selections we also propose comprehensive graphical representations of both samples and variables to facilitate the interpretation of the results conclusions spls and cca en selected highly relevant genes and complementary findings from the two data sets which enabled a detailed understanding of the molecular characteristics of several groups of cell lines these two approaches were found to bring similar results although they highlighted the same phenomenons with a different priority they outperformed cia that tended to select information
the transcriptomes of eukaryotic cells are incredibly complex individual non coding rnas dwarf the number of protein coding genes and include classes that are well understood as well as classes for which the nature extent and functional roles are obscure deep sequencing of small rnas nucleotides from human hela and cells revealed a remarkable breadth of species these arose both from within annotated genes and from unannotated intergenic regions overall small rnas tended to align with cage cap analysis of gene expression tags which mark the ends of capped long rna transcripts many small rnas including the previously described promoter associated small rnas appeared to possess cap structures members of an extensive class of both small rnas and cage tags were distributed across internal exons of annotated protein coding and non coding genes sometimes crossing exon exon junctions here we show that processing of mature mrnas through an as yet unknown mechanism may generate complex populations of both long and short rnas whose apparently capped ends coincide supplying synthetic promoter associated small rnas corresponding to the c myc transcriptional start site reduced myc messenger rna abundance the studies presented here expand the catalogue of cellular small rnas and demonstrate a biological impact for at least one class of non canonical rnas
summary we present a tool designed for visualization of large scale genetic and genomic data exemplified by results from genome wide association studies this software provides an integrated framework to facilitate the interpretation of snp association studies in genomic context gene annotations can be retrieved from ensembl linkage disequilibrium data downloaded from hapmap and custom data imported in bed or wig format associationviewer integrates functionalities that enable the aggregation or intersection of data tracks it implements an efficient cache system and allows the display of several very large scale genomic datasets availability the java code for associationviewer is distributed under the gnu general public licence and has been tested on microsoft windows xp macosx and gnu linux operating systems it is available from the sourceforge repository this also includes java webstart documentation and datafiles
recently approaches have been developed to sample the genetic content of heterogeneous environments metagenomics however by what means these sequences link distinct environmental conditions with specific biological processes is not well understood thus a major challenge is how the usage of particular pathways and subnetworks reflects the adaptation of microbial communities across environments and habitats i e how network dynamics relates to environmental features previous research has treated environments as discrete somewhat simplified classes e g terrestrial vs marine and searched for obvious metabolic differences among them i e treating the analysis as a typical classification problem however environmental differences result from combinations of many factors which often vary only slightly therefore we introduce an approach that employs correlation and regression to relate multiple continuously varying factors defining an environment to the extent of particular microbial pathways present in a geographic site moreover rather than looking only at individual correlations one to one we adapted canonical correlation analysis and related techniques to define an ensemble of weighted pathways that maximally covaries with a combination of environmental variables many to many which we term a metabolic footprint applied to available aquatic datasets we identified footprints predictive of their environment that can potentially be used as biosensors for example we show a strong multivariate correlation between the energy conversion strategies of a community and multiple environmental gradients e g temperature moreover we identified covariation in amino acid transport and cofactor synthesis suggesting that limiting amounts of cofactor can partially explain increased import of amino acids in nutrient conditions
a re parameterization of the standard water model for use with ewald techniques is introduced providing an overall global improvement in water properties relative to several popular nonpolarizable and polarizable water potentials using high precision simulations and careful application of standard analytical corrections we show that the new ew potential has a density maximum at a circ c and reproduces experimental bulk densities and the enthalpy of vaporization hvap from a to a circ c at atm with an absolute average error of less than structural properties are in very good agreement with x ray scattering intensities at temperatures between and a circ c and dynamical properties such as self diffusion coefficient are in excellent agreement with experiment the parameterization approach used can be easily generalized to rehabilitate any water force field using available experimental data over a range of points
computer simulations have become an invaluable tool to study the sometimes counterintuitive temporal dynamics of bio chemical systems in particular stochastic simulation methods have attracted increasing interest recently in contrast to the well known deterministic approach based on ordinary differential equations they can capture effects that occur due to the underlying discreteness of the systems and random fluctuations in molecular numbers numerous stochastic approximate stochastic and hybrid simulation methods have been proposed in the literature in this article they are systematically reviewed in order to guide the researcher and help her find the appropriate method for a specific bib
for more than a century the origin of metazoan animals has been debated one aspect of this debate has been centered on what the hypothetical urmetazoon bauplan might have been the morphologically most simply organized metazoan animal the placozoan trichoplax adhaerens resembles an intriguing model for one of several urmetazoon hypotheses the placula hypothesis clear support for a basal position of placozoa would aid in resolving several key issues of metazoan specific inventions including for example headfoot axis symmetry and coelom and would determine a root for unraveling their evolution unfortunately the phylogenetic relationships at the base of metazoa have been controversial because of conflicting phylogenetic scenarios generated while addressing the question here we analyze the sum of morphological evidence the secondary structure of mitochondrial ribosomal genes and molecular sequence data from mitochondrial and nuclear genes that amass over phylogenetically informative characters from to taxa together with mitochondrial dna genome structure and sequence analyses and hox like gene expression patterns these data provide evidence that placozoa are basal relative to all other diploblast phyla and spark a modernized hypothesis
gr inter individual and regional variability in recombination rates cannot be fully explained by the dna sequence itself epigenetic mechanisms might be one additional factor affecting recombination a biochemical approach to studying human germline methylation is difficult we used the density of the nonredundant methylation associated snps msnps in the derived allele hapmap data set as a surrogate marker for germline dna methylation we validated our methodology by demonstrating that the msnp density confirmed known patterns of genomic methylation including the hypermutability of methylated cytosine and hypomethylation of cpg islands using this approach we found a genome wide positive correlation between germline methylation and regional recombination rate kb windows this remained significant with multiple correlations correcting for sequence features known to affect recombination such as gc content and cpg dinucleotides kb windows using the encode data set for increased resolution we found a positive correlation between germline dna methylation and recombination rate kb windows this correlation was further strengthened when corrected for sequence features affecting recombination kb windows in the human epigenome project data set there was increased dna methylation in regions within recombination hot spots in male germ cells vs the relationship we observed between germline dna methylation and recombination could be explained in two ways that are not mutually exclusive dna methylation could indicate preferred sites for recombination or methylation following recombination could inhibit further recombination perhaps by being part of the enigmatic molecular pathway mediating interference
cloud computing systems fundamentally provide access to large pools of data and computational resources through a variety of interfaces similar in spirit to existing grid and hpc resource management and programming systems these types of systems offer a new programming target for scalable application developers and have gained popularity over the past few years however most cloud computing systems in operation today are proprietary rely upon infrastructure that is invisible to the research community or are not explicitly designed to be instrumented and modified by systems researchers in this work we present eucalyptus an open source software framework for cloud computing that implements what is commonly referred to as infrastructure as a service iaas systems that give users the ability to run and control entire virtual machine instances deployed across a variety physical resources we outline the basic principles of the eucalyptus design detail important operational aspects of the system and discuss architectural trade offs that we have made in order to allow eucalyptus to be portable modular and simple to use on infrastructure commonly found within academic settings finally we provide evidence that eucalyptus enables users familiar with existing grid and hpc systems to explore new cloud computing functionality while maintaining access to existing familiar application development software and grid ware
abstract background as sequencing costs have decreased whole genome sequencing has become a viable and integral part of biological laboratory research however the tools with which genes can be found and functionally characterized have not been readily adapted to be part of the everyday biological sciences toolkit most annotation pipelines remain as a service provided by large institutions or come as an unwieldy conglomerate of independent components each requiring their own setup and maintenance results to address this issue we have created the genome reverse compiler an easy to use open source automated annotation tool the grc is independent of third party software installs and only requires a linux operating system this stands in contrast to most annotation packages which typically require installation of relational databases sequence similarity software and a number of other programming language modules we provide details on the methodology used by grc and evaluate its performance on several groups of prokaryotes using grc s built in comparison module conclusions traditionally to perform whole genome annotation a user would either set up a pipeline or take advantage of an online service with grc the user need only provide the genome he or she wants to annotate and the function resource files to use the result is high usability and a very minimal learning curve for the intended audience of life science researchers and bioinformaticians we believe that the grc fills a valuable niche in allowing users to perform explorative whole annotation
systems biology aims to develop mathematical models of biological systems by integrating experimental and theoretical techniques during the last decade many systems biological approaches that base on genome wide data have been developed to unravel the complexity of gene regulation this review deals with the reconstruction of gene regulatory networks grns from experimental data through computational methods standard grn inference methods primarily use gene expression data derived from microarrays however the incorporation of additional information from heterogeneous data sources e g genome sequence and proteindna interaction data clearly supports the network inference process this review focuses on promising modelling approaches that use such diverse types of molecular biological information in particular approaches are discussed that enable the modelling of the dynamics of gene regulatory systems the review provides an overview of common modelling schemes and learning algorithms and outlines current challenges in modelling
pnas social networks exhibit strikingly systematic patterns across a wide range of human contexts although genetic variation accounts for a significant portion of the variation in many complex social behaviors the heritability of egocentric social network attributes is unknown here we show that of these attributes in degree transitivity and centrality are heritable we then develop a mirror network method to test extant network models and show that none account for observed genetic variation in human social networks we propose an alternative attract and introduce model with two simple forms of heterogeneity that generates significant heritability and other important network features we show that the model is well suited to real social networks in humans these results suggest that natural selection may have played a role in the evolution of social networks they also suggest that modeling intrinsic variation in network attributes may be important for understanding the way genes affect human behaviors and the way these behaviors spread from person to er
a complete understanding of a protein folding mechanism requires description of the distribution of microscopic pathways that connect the folded and unfolded states this distribution can in principle be described by computer simulations and theoretical models of protein folding but is hidden in conventional experiments on large ensembles of molecules because only average properties are measured a long term goal of single molecule fluorescence studies is to time resolve the structural events as individual molecules make transitions between folded and unfolded states although such studies are still in their infancy the work till now shows great promise and has already produced novel and important information on current issues in protein folding that has been impossible or difficult to obtain from measurements
given the complexity of microarray based gene expression studies guidelines encourage transparent design and public data availability several journals require public data deposition and several public databases exist however not all data are publicly available and even when available it is unknown whether the published results are reproducible by independent scientists here we evaluated the replication of data analyses in articles on microarray based gene expression profiling published in nature genetics in one table or figure from each article was independently evaluated by two teams of analysts we reproduced two analyses in principle and six partially or with some discrepancies ten could not be reproduced the main reason for failure to reproduce was data unavailability and discrepancies were mostly due to incomplete data annotation or specification of data processing and analysis repeatability of published microarray studies is apparently limited more strict publication rules enforcing public data availability and explicit description of data processing and analysis should considered
pnas in densely populated urban areas wifi routers form a tightly interconnected proximity network that can be exploited as a substrate for the spreading of malware able to launch massive fraudulent attacks in this article we consider several scenarios for the deployment of malware that spreads over the wireless channel of major urban areas in the us we develop an epidemiological model that takes into consideration prevalent security flaws on these routers the spread of such a contagion is simulated on real world data for georeferenced wireless routers we uncover a major weakness of wifi networks in that most of the simulated scenarios show tens of thousands of routers infected in as little as weeks with the majority of the infections occurring in the first h we indicate possible containment and prevention measures and provide computational estimates for the rate of encrypted routers that would stop the spreading of the epidemics by placing the system below the threshold
for the past years it has been known that alterations in dna methylation dnam occur in cancer including hypomethylation of oncogenes and hypermethylation of tumor suppressor genes however most studies of cancer methylation have assumed that functionally important dnam will occur in promoters and that most dnam changes in cancer occur in cpg islands here we show that most methylation alterations in colon cancer occur not in promoters and also not in cpg islands but in sequences up to kb distant which we term cpg island shores cpg island shore methylation was strongly related to gene expression and it was highly conserved in mouse discriminating tissue types regardless of species of origin there was a notable overlap of the locations of colon cancerrelated methylation changes with those that distinguished normal tissues with hypermethylation enriched closer to the associated cpg islands and hypomethylation enriched further from the associated cpg island and resembling that of noncolon normal tissues thus methylation changes in cancer are at sites that vary normally in tissue differentiation consistent with the epigenetic progenitor model of cancer which proposes that epigenetic alterations affecting tissue specific differentiation are the predominant mechanism by which epigenetic changes cancer
the continued increase in web usage in particular participation in folksonomies reveals a trend towards a more dynamic and interactive web where individuals can organise and share resources tagging has emerged as the de facto standard for the organisation of such resources providing a versatile and reactive knowledge management mechanism that users find easy to use and understand it is common nowadays for users to have multiple profiles in various folksonomies thus distributing their tagging activities in this paper we present a method for the automatic consolidation of user profiles across two popular social networking sites and subsequent semantic modelling of their interests utilising wikipedia as a multi domain model we evaluate how much can be learned from such sites and in which domains the knowledge acquired is focussed results show that far richer interest profiles can be generated for users when multiple tag clouds combined
this paper presents flickr distance which is a novel measurement of the relationship between semantic concepts objects scenes in visual domain for each concept a collection of images are obtained from flickr based on which the improved latent topic based visual language model is built to capture the visual characteristic of this concept then flickr distance between different concepts is measured by the square root of jensen shannon js divergence between the corresponding visual language models comparing with wordnet flickr distance is able to handle far more concepts existing on the web and it can scale up with the increase of concept vocabularies comparing with google distance which is generated in textual domain flickr distance is more precise for visual domain concepts as it captures the visual relationship between the concepts instead of their co occurrence in text search results besides unlike google distance flickr distance satisfies triangular inequality which makes it a more reasonable distance metric both subjective user study and objective evaluation show that flickr distance is more coherent to human perception than google distance we also design several application scenarios such as concept clustering and image annotation to demonstrate the effectiveness of this proposed distance in image applications
motivation statistical methods are used to test for the differential expression of genes in microarray experiments the most widely used methods successfully test whether the true differential expression is different from zero but give no assurance that the differences found are large enough to be biologically meaningful results we present a method t tests relative to a threshold treat that allows researchers to test formally the hypothesis with associated p values that the differential expression in a microarray experiment is greater than a given biologically meaningful threshold we have evaluated the method using simulated data a dataset from a quality control experiment for microarrays and data from a biological experiment investigating histone deacetylase inhibitors when the magnitude of differential expression is taken into account treat improves upon the false discovery rate of existing methods and identifies more biologically relevant genes availability r code implementing our methods is contributed to the software package limma available at http www bioconductor org contact smyth wehi au
genes that have experienced accelerated evolutionary rates on the human lineage during recent evolution are candidates for involvement in human specific adaptations to determine the forces that cause increased evolutionary rates in certain genes we analyzed alignments of human genes to their orthologues in chimpanzee and macaque using a likelihood ratio test we identified protein coding sequences with an accelerated rate of base substitutions along the human lineage exons evolving at a fast rate in humans have a significant tendency to contain clusters of at to gc weak to strong biased substitutions this pattern is also observed in noncoding sequence flanking rapidly evolving exons accelerated exons occur in regions with elevated male recombination rates and exhibit an excess of nonsynonymous substitutions relative to the genomic average we next analyzed genes with significantly elevated ratios of nonsynonymous to synonymous rates of base substitution d n d s along the human lineage and those with an excess of amino acid replacement substitutions relative to human polymorphism these genes also show evidence of clusters of weak to strong biased substitutions these findings indicate that a recombination associated process such as biased gene conversion bgc is driving fixation of gc alleles in the human genome this process can lead to accelerated evolution in coding sequences and excess amino acid replacement substitutions thereby generating significant results for tests of selection
zesch mueller gurevych at tk informatik tu darmstadt de recently collaboratively constructed resources such as wikipedia and wiktionary have been discovered as valuable lexical semantic knowledge bases with a high potential in diverse natural language processing nlp tasks collaborative knowledge bases however significantly differ from traditional linguistic knowledge bases in various respects and this constitutes both an asset and an impediment for research in nlp this paper addresses one such major impediment namely the lack of suitable programmatic access mechanisms to the knowledge stored in these large semantic knowledge bases we present two application programming interfaces for wikipedia and wiktionary which are especially designed for mining the rich lexical semantic information dispersed in the knowledge bases and provide efficient and structured access to the available knowledge as we believe them to be of general interest to the nlp community we have made them freely available for purposes
how should ecologists and evolutionary biologists analyze nonnormal data that involve random effects nonnormal data such as counts or proportions often defy classical statistical procedures generalized linear mixed models glmms provide a more flexible approach for analyzing nonnormal data when random effects are present the explosion of research on glmms in the last decade has generated considerable uncertainty for practitioners in ecology and evolution despite the availability of accurate techniques for estimating glmm parameters in simple cases complex glmms are challenging to fit and statistical inference such as hypothesis testing remains difficult we review the use and misuse of glmms in ecology and evolution discuss estimation and inference and summarize best practice data analysis procedures for scientists facing challenge
identifying interaction sites in proteins provides important clues to the function of a protein and is becoming increasingly relevant in topics such as systems biology and drug discovery although there are numerous papers on the prediction of interaction sites using information derived from structure there are only a few case reports on the prediction of interaction residues based solely on protein sequence here a sliding window approach is combined with the random forests method to predict protein interaction sites using i a combination of sequence and structure derived parameters and ii sequence information alone for sequence based prediction we achieved a precision of with a recall and an f measure of when combined with structural information the prediction performance increases to a precision of and a recall of with an f measure of we also present an attempt to rationalize the sliding window size and demonstrate that a nine residue window is the most suitable for predictor construction finally we demonstrate the applicability of our prediction methods by modeling the rasraf complex using predicted interaction sites as target binding interfaces our results suggest that it is possible to predict protein interaction sites with quite a high accuracy using only information
background domains are the building blocks of proteins during evolution they have been duplicated fused and recombined to produce proteins with novel structures and functions structural and genome scale studies have shown that pairs or groups of domains observed together in a protein are almost always found in only one n to c terminal order and are the result of a single recombination event that has been propagated by duplication of the multi domain unit previous studies of domain organisation have used graph theory to represent the co occurrence of domains within proteins we build on this approach by adding directionality to the graphs and connecting nodes based on their relative order in the protein most of the time the linear order of domains is conserved however using the directed graph representation we have identified non linear features of domain organization that are over represented in genomes recognising these patterns and unravelling how they have arisen may allow us to understand the functional relationships between domains and understand how the protein repertoire has evolved results we identify groups of domains that are not linearly conserved but instead have been shuffled during evolution so that they occur in multiple different orders we consider genomes across all three kingdoms of life and use domain and protein annotation to understand their functional significance to identify these features and assess their statistical significance we represent the linear order of domains in proteins as a directed graph and apply graph theoretical methods we describe two higher order patterns of domain organisation clusters and bi directionally associated domain pairs and explore their functional importance and phylogenetic conservation conclusion taking into account the order of domains we have derived a novel picture of global protein organization we found that all genomes have a higher than expected degree of clustering and more domain pairs in forward and reverse orientation in different proteins relative to random graphs with identical degree distributions while these features were statistically over represented they are still fairly rare looking in detail at the proteins involved we found strong functional relationships within each cluster in addition the domains tended to be involved in protein protein interaction and are able to function as independent structural units a particularly striking example was the human jak stat signalling pathway which makes use of a set of domains in a range of orders and orientations to provide nuanced signaling functionality this illustrated the importance of functional and structural constraints or lack thereof on organisation
genetic incompatibilities resulting from interactions between two loci represent a potential source of postzygotic barriers and may be an important factor in evolution when they impair the outcome of interspecific crosses we show that in crosses between strains of the plant arabidopsis thaliana loci interact epistatically controlling a recessive embryo lethality this interaction is explained by divergent evolution occurring among paralogs of an essential duplicate gene for which the functional copy is not located at the same locus in different accessions these paralogs demonstrate genetic heterogeneity in their respective evolutionary trajectories which results in widespread incompatibility among strains our data suggest that these passive mechanisms gene duplication and extinction could represent an important source of genetic incompatibilities across all science
doi an increasing number of studies have reported computations of the standard absolute binding free energy of small ligands to proteins using molecular dynamics md simulations and explicit solvent molecules that are in good agreement with experiments this encouraging progress suggests that physics based approaches hold the promise of making important contributions to the process of drug discovery and optimization in the near future two types of approaches are principally used to compute binding free energies with md simulations the most widely known is the alchemical double decoupling method in which the interaction of the ligand with its surroundings are progressively switched off it is also possible to use a potential of mean force pmf method in which the ligand is physically separated from the protein receptor for both of these computational approaches restraining potentials may be activated and released during the simulation for sampling efficiently the changes in translational rotational and conformational freedom of the ligand and protein upon binding because such restraining potentials add bias to the simulations it is important that their effects be rigorously removed to yield a binding free energy that is properly unbiased with respect to the standard state a review of recent results is presented and differences in computational methods are discussed examples of computations with lysozyme mutants domain and cytochrome are discussed and compared remaining difficulties and challenges highlighted
gr multiple sequence alignments have become one of the most commonly used resources in genomics research most algorithms for multiple alignment of whole genomes rely either on a reference genome against which all of the other sequences are laid out or require a one to one mapping between the nucleotides of the genomes preventing the alignment of recently duplicated regions both approaches have drawbacks for whole genome comparisons in this paper we present a novel symmetric alignment algorithm the resulting alignments not only represent all of the genomes equally well but also include all relevant duplications that occurred since the divergence from the last common ancestor our algorithm implemented as a part of the vista genome pipeline vgp was used to align seven vertebrate and six genomes the resulting whole genome alignments demonstrate a higher sensitivity and specificity than the pairwise alignments previously available through the vgp and have higher exon alignment accuracy than comparable public whole genome alignments of the multiple alignment methods tested ours performed the best at aligning genes from multigene familiesperhaps the most challenging test for whole genome alignments our whole genome multiple alignments are available through the vista at
this paper presents a method for measuring the semantic similarity of texts using corpus based and knowledge based measures of similarity previous work on this problem has focused mainly on either large documents e g text classification information retrieval or individual words e g synonymy tests given that a large fraction of the information available today on the web and elsewhere consists of short text snippets e g abstracts of scientific documents imagine captions product descriptions in this paper we focus on measuring the semantic similarity of short texts through experiments performed on a paraphrase data set we show that the semantic similarity method outperforms methods based on simple lexical matching resulting in up to error rate reduction with respect to the traditional vector based metric
the interaction of a multitude of transcription factors and other chromatin proteins with the genome can influence gene expression and subsequently cell differentiation and function thus systematic identification of binding targets of transcription factors is key to unraveling gene regulation networks the recent development of chip seq has revolutionized mapping of dna protein interactions now protein binding can be mapped in a truly genome wide manner with extremely high resolution this review discusses chip seq technology its possible pitfalls data analysis and several early applications j cell biochem c wiley inc
understanding the molecular mechanisms underlying synergistic potentiative and antagonistic effects of drug combinations could facilitate the discovery of novel efficacious combinations and multi targeted agents in this article we describe an extensive investigation of the published literature on drug combinations for which the combination effect has been evaluated by rigorous analysis methods and for which relevant molecular interaction profiles of the drugs involved are available analysis of the drug combinations identified reveals general and specific modes of action and highlights the potential value of molecular interaction profiles in the discovery of novel therapies
genome wide rna expression data provide a detailed view of an organism s biological state hence a dataset measuring expression variation between genetically diverse individuals eqtl data may provide important insights into the genetics of complex traits however with data from a relatively small number of individuals it is difficult to distinguish true causal polymorphisms from the large number of possibilities the problem is particularly challenging in populations with significant linkage disequilibrium where traits are often linked to large chromosomal regions containing many genes here we present a novel method lirnet that automatically learns a regulatory potential for each sequence polymorphism estimating how likely it is to have a significant effect on gene expression this regulatory potential is defined in terms of regulatory features including the function of the gene and the conservation type and position of genetic polymorphisms that are available for any organism the extent to which the different features influence the regulatory potential is learned automatically making lirnet readily applicable to different datasets organisms and feature sets we apply lirnet both to the human hapmap eqtl dataset and to a yeast eqtl dataset and provide statistical and biological results demonstrating that lirnet produces significantly better regulatory programs than other recent approaches we demonstrate in the yeast data that lirnet can correctly suggest a specific causal sequence variation within a large linked chromosomal region in one example lirnet uncovered a novel experimentally validated connection between a sequence specific rna binding protein and p bodies cytoplasmic structures that regulate translation and rna stability as well as the particular causative polymorphism a snp in that induces the variation in pathway
a new development in the dynamical behavior of elementary quantum systems is the surprising discovery that correlation between two quantum units of information called qubits can be degraded by environmental noise in a way not seen previously in studies of dissipation this new route for dissipation attacks quantum entanglement the essential resource for quantum information as well as the central feature in the einstein podolsky rosen so called paradox and in discussions of the fate of schr o inger s cat the effect has been labeled esd which stands for early stage disentanglement or more frequently entanglement sudden death we review recent progress in studies focused on phenomenon
the challenges associated with the structural characterization of disordered proteins have resulted in the application of a host of biophysical methods to such systems nmr spectroscopy is perhaps the most readily suited technique for providing high resolution structural information on disordered protein states in solution optical methods solid state nmr esr and x ray scattering can also provide valuable information regarding the ensemble of conformations sampled by disordered states finally computational studies have begun to assume an increasingly important role in interpreting and extending the impact of experimental data obtained for such systems this article discusses recent advances in the applications of these methods to intrinsically proteins
researching experiential phenomena is a challenging undertaking given the sheer variety of experiences that are described by gamers and missing a formal taxonomy flow immersion boredom excitement challenge and fun these informal terms require scientific explanation which amounts to providing measurable criteria for different experiential states this paper reports the results of an experimental psychophysiological study investigating different traits of gameplay experience using subjective and objective measures participants played three half life game modifications while being measured with electroencephalography electrocardiography electromyography galvanic skin response and eye tracking equipment in addition questionnaire responses were collected after each play session a level designed for combat oriented flow experience demonstrated measurable high arousal positive affect emotions the positive correlation between subjective and objective indicators of gameplay experience shows the great potential of the method presented here for providing real time emotional profiles of gameplay that may be correlated with self reported descriptions
anxious doctoral researchers can now call on a proliferation of advice books telling them how to produce their dissertations this article analyzes some characteristics of this self help genre including the ways it produces an expert novice relationship with readers reduces dissertation writing to a series of linear steps reveals hidden rules and asserts a mix of certainty and fear to position readers correctly the authors argue for a more complex view of doctoral writing both as text work identity work and as a discursive social practice they reject transmission pedagogies that normalize the power saturated relations of protege and master and point to alternate pedagogical approaches that position doctoral researchers as colleagues engaged in a shared unequal and practice
rna profiling mirna levels in cells with mirna microarrays is becoming a widely used technique although normalization methods for mrna gene expression arrays are well established mirna array normalization has so far not been investigated in detail in this study we investigate the impact of normalization on data generated with the agilent mirna array platform we have developed a method to select nonchanging mirnas invariants and use them to compute linear regression normalization coefficients or variance stabilizing normalization vsn parameters we compared the invariants normalization to normalization by scaling quantile and vsn with default parameters as well as to no normalization using samples with strong differential expression of mirnas heartbrain comparison and samples where only a few mirnas are affected by overexpression in squamous carcinoma cells versus control all normalization methods performed better than no normalization normalization procedures based on the set of invariants and quantile were the most robust over all experimental conditions tested our method of invariant selection and normalization is not limited to agilent mirna arrays and can be applied to other data sets including those from one color mirna microarray platforms focused gene expression arrays and gene expression analysis using pcr
itunes university a website with downloadable educational podcasts can provide students the opportunity to obtain professors lectures when students are unable to attend class to determine the effectiveness of audio lectures in higher education undergraduate general psychology students participated in one of two conditions in the lecture condition participants listened to a min lecture given in person by a professor using powerpoint slides copies of the slides were given to aid note taking in the podcast condition participants received a podcast of the same lecture along with the powerpoint handouts participants in both conditions were instructed to keep a running log of study time and activities used in preparing for an exam one week from the initial session students returned to take an exam on lecture content results indicated that students in the podcast condition who took notes while listening to the podcast scored significantly higher than the lecture condition the impact of mobile learning on classroom performance discussed
there is growing recognition that mammalian cells produce many thousands of large intergenic transcripts however the functional significance of these transcripts has been particularly controversial although there are some well characterized examples most show little evidence of evolutionary conservation and have been suggested to represent transcriptional noise here we report a new approach to identifying large non coding rnas using chromatin state maps to discover discrete transcriptional units intervening known protein coding loci our approach identified approximately large multi exonic rnas across four mouse cell types in sharp contrast to previous collections these large intervening non coding rnas lincrnas show strong purifying selection in their genomic loci exonic sequences and promoter regions with greater than showing clear evolutionary conservation we also developed a functional genomics approach that assigns putative functions to each lincrna demonstrating a diverse range of roles for lincrnas in processes from embryonic stem cell pluripotency to cell proliferation we obtained independent functional validation for the predictions for over lincrnas using cell based assays in particular we demonstrate that specific lincrnas are transcriptionally regulated by key transcription factors in these processes such as nfkappab also known as and nanog together these results define a unique collection of functional lincrnas that are highly conserved and implicated in diverse processes
protein tyrosine phosphatases ptps play a critical role in regulating cellular functions by selectively dephosphorylating their substrates here we present human ptp crystal structures that together with prior structural knowledge enable a comprehensive analysis of the classical ptp family despite their largely conserved fold surface properties of ptps are strikingly diverse a potential secondary substrate binding pocket is frequently found in phosphatases and this has implications for both substrate recognition and development of selective inhibitors structural comparison identified four diverse catalytic loop wpd conformations and suggested a mechanism for loop closure enzymatic assays revealed vast differences in ptp catalytic activity and identified and hdptp as catalytically inert protein phosphatases we propose a head to toe dimerization model for rptpgamma zeta that is distinct from the inhibitory wedge model and that provides a molecular basis for inhibitory regulation this phosphatome resource gives an expanded insight into intrafamily ptp diversity catalytic activity substrate recognition and autoregulatory association
motivation with the increasing use of post genomics techniques to examine a wide variety of biological systems in laboratories throughout the world scientists are often presented with lists of genes that they must make sense of a consistently challenging problem is that of defining co regulated genes within those gene lists in recent years micrornas have emerged as a mechanism for regulating several cellular processes in this article we report on how gene lists and microrna targets data may be integrated to test for significant associations between gene lists and micrornas results we discuss corna a package written in r and released under the gnu gpl which allows users to test gene lists for significant microrna target associations using one of three separate statistical tests to link microrna targets to functional annotation and to visualize quantitative data associated with those data availability corna is available as an r package from http corna sf net contact xikun wu bbsrc ac bioinformatics
micrornas mirnas interact with target sites located in the untranslated regions utrs of mrnas to downregulate their expression when the appropriate mirna is bound to target mrna to establish the functional importance of target site localization in the utr we modified the stop codon to extend the coding region of the transgene reporter through the mirna target sequence as a result the mirnas lost their ability to inhibit translation but retained their ability to function as small interfering rnas in mammalian cells in culture and in vivo the addition of rare but not optimal codons upstream of the extended opening reading frame orf made the mirna target site more accessible and restored mirna induced translational knockdown taken together these results suggest that active translation impedes mirna programmed risc association with target mrnas and support a mechanistic explanation for the localization of most mirna target sites in noncoding regions of mrnas mammals
in the last decade it became apparent that a large number of the most interesting structures and phenomena of the world can be described by networks separable elements with connections or interactions between certain pairs of them these huge networks pose exciting challenges for the mathematician graph theory the mathematical theory of networks faces novel unconventional problems these very large networks like the internet are never completely known in most cases they are not even well defined data about them can be collected only by indirect means like random local sampling dense networks in which a node is adjacent to a positive percent of others nodes and sparse networks in which a node has a bounded number of neighbors show very different behavior from a practical point of view sparse networks are more important but at present we have more complete theoretical results for dense networks the paper surveys relations with probability algebra extrema graph theory analysis
background dna microarrays provide data for genome wide patterns of expression between observation classes microarray studies often have small samples sizes however due to cost constraints or specimen availability this can lead to poor random error estimates and inaccurate statistical tests of differential expression we compare the performance of the standard t test fold change and four small n statistical test methods designed to circumvent these problems we report results of various normalization methods for empirical microarray data and of various random error models for simulated data results three empirical bayes methods cybert brb and limma t statistics were the most effective statistical tests across simulated and both colour cdna and affymetrix experimental data the cybert regularized t statistic in particular was able to maintain expected false positive rates with simulated data showing high variances at low gene intensities although at the cost of low true positive rates the local pooled error lpe test introduced a bias that lowered false positive rates below theoretically expected values and had lower power relative to the top performers the standard two sample t test and fold change were also found to be sub optimal for detecting differentially expressed genes the generalized log transformation was shown to be beneficial in improving results with certain data sets in particular high variance cdna data conclusion pre processing of data influences performance and the proper combination of pre processing and statistical testing is necessary for obtaining the best results all three empirical bayes methods assessed in our study are good choices for statistical tests for small n microarray studies for both affymetrix and cdna data choice of method for a particular study will depend on software and preferences
background with the growing availability of full text articles online scientists and other consumers of the life sciences literature now have the ability to go beyond searching bibliographic records title abstract metadata to directly access full text content motivated by this emerging trend i posed the following question is searching full text more effective than searching abstracts this question is answered by comparing text retrieval algorithms on medline abstracts full text articles and spans paragraphs within full text articles using data from the trec genomics track evaluation two retrieval models are examined and the ranking algorithm implemented in the open source lucene search engine results experiments show that treating an entire article as an indexing unit does not consistently yield higher effectiveness compared to abstract only search however retrieval based on spans or paragraphs sized segments of full text articles consistently outperforms abstract only search results suggest that highest overall effectiveness may be achieved by combining evidence from spans and full articles conclusion users searching full text are more likely to find relevant articles than searching only abstracts this finding affirms the value of full text collections for text retrieval and provides a starting point for future work in exploring algorithms that take advantage of rapidly growing digital archives experimental results also highlight the need to develop distributed text retrieval algorithms since full text articles are significantly longer than abstracts and may require the computational resources of multiple machines in a cluster the mapreduce programming model provides a convenient framework for organizing computations
the escherichia coli species represents one of the best studied model organisms but also encompasses a variety of commensal and pathogenic strains that diversify by high rates of genetic change we uniformly re annotated the genomes of commensal and pathogenic e coli strains and one strain of e fergusonii the closest e coli related species including seven that we sequenced to completion within the approximately families of orthologous genes we found approximately common to all strains although recombination rates are much higher than mutation rates we show both theoretically and using phylogenetic inference that this does not obscure the phylogenetic signal which places the phylogenetic group and one group d strain at the basal position based on this phylogeny we inferred past evolutionary events of gain and loss of genes identifying functional classes under opposite selection pressures we found an important adaptive role for metabolism diversification within group and shigella strains but identified few or no extraintestinal virulence specific genes which could render difficult the development of a vaccine against extraintestinal infections genome flux in e coli is confined to a small number of conserved positions in the chromosome which most often are not associated with integrases or trna genes core genes flanking some of these regions show higher rates of recombination suggesting that a gene once acquired by a strain spreads within the species by homologous recombination at the flanking genes finally the genome s long scale structure of recombination indicates lower recombination rates but not higher mutation rates at the terminus of replication the ensuing effect of background selection and biased gene conversion may thus explain why this region is a t rich and shows high sequence divergence but low sequence polymorphism overall despite a very high gene flow genes co exist in an genome
pnas reliable structure prediction methods for membrane proteins are important because the experimental determination of high resolution membrane protein structures remains very difficult especially for eukaryotic proteins however membrane proteins are typically longer than aa and represent a formidable challenge for structure prediction we have developed a method for predicting the structures of large membrane proteins by constraining helixhelix packing arrangements at particular positions predicted from sequence or identified by experiments we tested the method on membrane proteins of diverse topologies and functions with lengths ranging between and residues enforcing a single constraint during the folding simulations enriched the population of near native models for proteins in of the cases in which the constraint was predicted from the sequence of the lowest energy models was superimposable within on the native structure near native structures could also be selected for heme binding and pore forming domains from simulations in which pairs of conserved histidine chelating hemes and one experimentally determined salt bridge were constrained respectively these results suggest that models within of the native structure can be achieved for complex membrane proteins if even limited information on residue residue interactions can be obtained from protein structure databases experiments
making sense of rapidly evolving evidence on genetic associations is crucial to making genuine advances in human genomics and the eventual integration of this information into the practice of medicine and public health assessment of the strengths and weaknesses of this evidence and hence the ability to synthesize it has been limited by inadequate reporting of results the strengthening the reporting of genetic association studies strega initiative builds on the strengthening the reporting of observational studies in epidemiology strobe statement and provides additions to of the items on the strobe checklist the additions concern population stratification genotyping errors modeling haplotype variation hardy weinberg equilibrium replication selection of participants rationale for choice of genes and variants treatment effects in studying quantitative traits statistical methods relatedness reporting of descriptive and outcome data and issues of data volume that are important to consider in genetic association studies the strega recommendations do not prescribe or dictate how a genetic association study should be designed but seek to enhance the transparency of its reporting regardless of choices made during design conduct analysis
background analysis of microarray and other high throughput data on the basis of gene sets rather than individual genes is becoming more important in genomic studies correspondingly a large number of statistical approaches for detecting gene set enrichment have been proposed but both the interrelations and the relative performance of the various methods are still very much unclear results we conduct an extensive survey of statistical approaches for gene set analysis and identify a common modular structure underlying most published methods based on this finding we propose a general framework for detecting gene set enrichment this framework provides a meta theory of gene set analysis that not only helps to gain a better understanding of the relative merits of each embedded approach but also facilitates a principled comparison and offers insights into the relative interplay of the methods conclusions we use this framework to conduct a computer simulation comparing different variants of gene set enrichment procedures and to analyze two experimental data sets based on the results we offer recommendations for best practices regarding the choice of effective procedures for gene set analysis
abstract background since the inception of the go annotation project a variety of tools have been developed that support exploring and searching the go database in particular a variety of tools that perform go enrichment analysis are currently available most of these tools require as input a target set of genes and a background set and seek enrichment in the target set compared to the background set a few tools also exist that support analyzing ranked lists the latter typically rely on simulations or on union bound correction for assigning statistical significance to the results results gorilla is a web based application that identifies enriched go terms in ranked lists of genes without requiring the user to provide explicit target and background sets this is particularly useful in many typical cases where genomic data may be naturally represented as a ranked list of genes e g by level of expression or of differential expression gorilla employs a flexible threshold statistical approach to discover go terms that are significantly enriched at the top of a ranked gene list building on a complete theoretical characterization of the underlying distribution called mhg gorilla computes an exact p value for the observed enrichment taking threshold multiple testing into account without the need for simulations this enables rigorous statistical analysis of thousands of genes and thousands of go terms in order of seconds the output of the enrichment analysis is visualized as a hierarchical structure providing a clear view of the relations between enriched go terms conclusions gorilla is an efficient go analysis tool with unique features that make a useful addition to the existing repertoire of go enrichment tools gorilla s unique features and advantages over other threshold free enrichment tools include rigorous statistics fast running time and an effective graphical representation gorilla is publicly available at http cbl gorilla cs technion il
wikipedia has become one of the most important information resources on the web by promoting peer collaboration and enabling virtually anyone to edit anything however this mutability also leads many to distrust it as a reliable source of information although there have been many attempts at developing metrics to help users judge the trustworthiness of content it is unknown how much impact such measures can have on a system that is perceived as inherently unstable here we examine whether a visualization that exposes hidden article information can impact readers perceptions of trustworthiness in a wiki environment our results suggest that surfacing information relevant to the stability of the article and the patterns of editor behavior can have a significant impact on users trust across a variety of types
pnas for comparison of whole genome genic nongenic sequences multiple sequence alignment of a few selected genes is not appropriate one approach is to use an alignment free method in which feature or mer frequency profiles ffp of whole genomes are used for comparisona variation of a text or book comparison method using word frequency profiles in this approach it is critical to identify the optimal resolution range of mers for the given set of genomes compared the optimum ffp method is applicable for comparing whole genomes or large genomic regions even when there are no common genes with high homology we outline the method in stages we first show how the optimal resolution range can be determined with english books which have been transformed into long character strings by removing all punctuation and spaces next we test the robustness of the optimized ffp method at the nucleotide level using a mutation model with a wide range of base substitutions and rearrangements finally to illustrate the utility of the method phylogenies are reconstructed from concatenated mammalian intronic genomes the ffp derived intronic genome topologies for each within the optimal range are all very similar the topology agrees with the established mammalian phylogeny revealing that intron regions contain a similar level of phylogenic signal as do regions
we describe a complete implementation of all atom protein molecular dynamics running entirely on a graphics processing unit gpu including all standard force field terms integration constraints and implicit solvent we discuss the design of our algorithms and important optimizations needed to fully take advantage of a gpu we evaluate its performance and show that it can be more than times faster than a conventional implementation running on a single cpu core wiley periodicals inc j chem
sciminer is a web based literature mining and functional analysis tool that identifies genes and proteins using a context specific analysis of medline abstracts and full texts sciminer accepts a free text query pubmed entrez search or a list of pubmed identifiers as input sciminer uses both regular expression patterns and dictionaries of gene symbols and names compiled from multiple sources ambiguous acronyms are resolved by a scoring scheme based on the co occurrence of acronyms and corresponding description terms which incorporates optional user defined filters functional enrichment analyses are used to identify highly relevant targets genes and proteins go gene ontology terms mesh medical subject headings terms pathways and protein protein interaction networks by comparing identified targets from one search result with those from other searches or to the full hgnc hugo human genome organization gene nomenclature committee gene set the performance of gene protein name identification was evaluated using the biocreative critical assessment of information extraction systems in biology version year gene normalization task as a gold standard sciminer achieved recall precision and f measure sciminer s literature mining performance coupled with functional enrichment analyses provides an efficient platform for retrieval and summary of rich biological information from corpora of users interests availability http jdrf neurology med umich edu sciminer a server version of the sciminer is also available for download and enables users to utilize their institution s journal subscriptions supplementary information supplementary data are available at online
the design of mobile navigation systems adapting to limited resources will be an important future challenge since typically several different means of transportation have to be combined in order to reach a destination the user interface of such a system has to adapt to the user s changing situation this applies especially to the alternating use of different technologies to detect the user s position which should be as seamless as possible this article presents a hybrid navigation system that relies on different technologies to determine the user s location and that adapts the presentation of route directions to the limited technical resources of the output device and the limited cognitive resources of user
changes in the biochemical wiring of oncogenic cells drives phenotypic transformations that directly affect disease outcome here we examine the dynamic structure of the human protein interaction network interactome to determine whether changes in the organization of the interactome can be used to predict patient outcome an analysis of hub proteins identified intermodular hub proteins that are co expressed with their interacting partners in a tissue restricted manner and intramodular hub proteins that are co expressed with their interacting partners in all or most tissues substantial differences in biochemical structure were observed between the two types of hubs signaling domains were found more often in intermodular hub proteins which were also more frequently associated with oncogenesis analysis of two breast cancer patient cohorts revealed that altered modularity of the human interactome may be useful as an indicator of breast prognosis
the mutation rate is known to vary between adjacent sites within the human genome as a consequence of context the most well studied example being the influence of cpg dinucelotides we investigated whether there is additional variation by testing whether there is an excess of sites at which both humans and chimpanzees have a single nucleotide polymorphism snp we found a highly significant excess of such sites and we demonstrated that this excess is not due to neighbouring nucleotide effects ancestral polymorphism or natural selection we therefore infer that there is cryptic variation in the mutation rate however although this variation in the mutation rate is not associated with the adjacent nucleotides we show that there are highly nonrandom patterns of nucleotides that extend base pairs on either side of sites with coincident snps suggesting that there are extensive and complex context effects finally we estimate the level of variation needed to produce the excess of coincident snps and show that there is a similar or higher level of variation in the mutation rate associated with this cryptic process than there is associated with adjacent nucleotides including the cpg effect we conclude that there is substantial variation in the mutation that has until now been hidden view
we design and implement mars a mapreduce framework on graphics processors gpus mapreduce is a distributed programming framework originally proposed by google for the ease of development of web search applications on a large number of commodity cpus compared with cpus gpus have an order of magnitude higher computation power and memory bandwidth but are harder to program since their architectures are designed as a special purpose co processor and their programming interfaces are typically for graphics applications as the first attempt to harness gpu s power for mapreduce we developed mars on an nvidia gpu which contains over one hundred processors and evaluated it in comparison with phoenix the state of the art mapreduce framework on multi core cpus mars hides the programming complexity of the gpu behind the simple and familiar mapreduce interface it is up to times faster than its cpu based counterpart for six common web applications on a quad machine
high dietary fat intake leads to insulin resistance in skeletal muscle and this represents a major risk factor for type diabetes and cardiovascular disease mitochondrial dysfunction and oxidative stress have been implicated in the disease process but the underlying mechanisms are still unknown here we show that in skeletal muscle of both rodents and humans a diet high in fat increases the emitting potential of mitochondria shifts the cellular redox environment to a more oxidized state and decreases the redox buffering capacity in the absence of any change in mitochondrial respiratory function furthermore we show that attenuating mitochondrial emission either by treating rats with a mitochondrial targeted antioxidant or by genetically engineering the overexpression of catalase in mitochondria of muscle in mice completely preserves insulin sensitivity despite a high fat diet these findings place the etiology of insulin resistance in the context of mitochondrial bioenergetics by demonstrating that mitochondrial emission serves as both a gauge of energy balance and a regulator of cellular redox environment linking intracellular metabolic balance to the control of sensitivity
the metabolism of oxygen although central to life produces reactive oxygen species ros that have been implicated in processes as diverse as cancer cardiovascular disease and ageing it has recently been shown that central nervous system stem cells and haematopoietic stem cells and early progenitors contain lower levels of ros than their more mature progeny and that these differences are critical for maintaining stem cell function we proposed that epithelial tissue stem cells and their cancer stem cell csc counterparts may also share this property here we show that normal mammary epithelial stem cells contain lower concentrations of ros than their more mature progeny cells notably subsets of cscs in some human and murine breast tumours contain lower ros levels than corresponding non tumorigenic cells ntcs consistent with ros being critical mediators of ionizing radiation induced cell killing cscs in these tumours develop less dna damage and are preferentially spared after irradiation compared to ntcs lower ros levels in cscs are associated with increased expression of free radical scavenging systems pharmacological depletion of ros scavengers in cscs markedly decreases their clonogenicity and results in radiosensitization these results indicate that similar to normal tissue stem cells subsets of cscs in some tumours contain lower ros levels and enhanced ros defences compared to their non tumorigenic progeny which may contribute to radioresistance
variation in patterns of methylations of histone tails reflects and modulates chromatin structure and function to provide a framework for the analysis of chromatin function in caenorhabditis elegans we generated a genome wide map of histone tail methylations we find that c elegans genes show distributions of histone modifications that are similar to those of other organisms with near transcription start sites in the body of genes and enriched on silent genes we also observe a novel pattern exons are preferentially marked with relative to introns exon marking is dependent on transcription and is found at lower levels in alternatively spliced exons supporting a splicing related marking mechanism we further show that the difference in marking between exons and introns is evolutionarily conserved in human and mouse we propose that exon marking in chromatin provides a dynamic link between transcription splicing
ever since the integration of mendelian genetics into evolutionary biology in the early century evolutionary geneticists have for the most part treated genes and mutations as generic entities however recent observations indicate that all genes are not equal in the eyes of evolution evolutionarily relevant mutations tend to accumulate in hotspot genes and at specific positions within genes genetic evolution is constrained by gene function the structure of genetic networks and population biology the genetic basis of evolution may be predictable to some extent and further understanding of this predictability requires incorporation of the specific functions and characteristics of genes into theory
the social sciences investigate human and social dynamics and organization at all levels of analysis consilience including cognition decision making behavior groups organizations societies and the world system computational social science is the integrated interdisciplinary pursuit of social inquiry with emphasis on information processing and through the medium of advanced computation the main computational social science areas are automated information extraction systems social network analysis social geographic information systems gis complexity modeling and social simulation models just like galileo exploited the telescope as the key instrument for observing and gaining a deeper and empirically truthful understanding of the physical universe computational social scientists are learning to exploit the advanced and increasingly powerful instruments of computation to see beyond the visible spectrum of more traditional disciplinary analyses copyright john wiley sons inc for further resources related to this article please the
the bacteria and archaea are the most genetically diverse superkingdoms of life and techniques for exploring that diversity are only just becoming widespread taxonomists classify these organisms into species in much the same way as they classify eukaryotes but differences in their biology including horizontal gene transfer between distantly related taxa and variable rates of homologous recombination mean that we still do not understand what a bacterial species is this is not merely a semantic question evolutionary theory should be able to explain why species exist at all levels of the tree of life and we need to be able to define species for practical applications in industry agriculture and medicine recent studies have emphasized the need to combine genetic diversity and distinct ecology in an attempt to define species in a coherent and convincing fashion the resulting data may help to discriminate among the many theories of prokaryotic species that have been produced to science
we report a genome wide assessment of single nucleotide polymorphisms snps and copy number variants cnvs in schizophrenia we investigated snps using patients and controls following up the top hits in four independent cohorts comprising patients and controls all of european origin we found no genome wide significant associations nor could we provide support for any previously reported candidate gene or genome wide associations we went on to examine cnvs using a subset of cases and controls of european ancestry and a further set of cases and controls of african ancestry we found that eight cases and zero controls carried deletions greater than mb of which two at and are newly reported here a further evaluation of controls identified no deletions greater than mb suggesting a high prior probability of disease involvement when such deletions are observed in cases we also provide further evidence for some smaller previously reported schizophrenia associated cnvs such as those in and we could not provide strong support for the hypothesis that schizophrenia patients have a significantly greater load of large kb rare cnvs nor could we find common cnvs that associate with schizophrenia finally we did not provide support for the suggestion that schizophrenia associated cnvs may preferentially disrupt genes in neurodevelopmental pathways collectively these analyses provide the first integrated study of snps and cnvs in schizophrenia and support the emerging view that rare deleterious variants may be more important in schizophrenia predisposition than common polymorphisms while our analyses do not suggest that implicated cnvs impinge on particular key pathways we do support the contribution of specific genomic regions in schizophrenia presumably due to recurrent mutation on balance these data suggest that very few schizophrenia patients share identical genomic causation potentially complicating efforts to personalize regimens
recent successful discoveries of potentially causal single nucleotide polymorphisms snps for complex diseases hold great promise and commercialization of genomics in personalized medicine has already begun the hope is that genetic testing will benefit patients and their families and encourage positive lifestyle changes and guide clinical decisions however for many complex diseases it is arguable whether the era of genomics in personalized medicine is here yet we focus on the clinical validity of genetic testing with an emphasis on two popular statistical methods for evaluating markers the two methods logistic regression and receiver operating characteristic roc curve analysis are applied to our age related macular degeneration dataset by using an additive model of the cfh and variants the odds ratios are and with p values of and respectively the area under the roc curve auc is but assuming prevalences of and which are realistic for age groups y y and y and older respectively only and of the group classified as high risk are cases additionally we present examples for four other diseases for which strongly associated variants have been discovered in type diabetes our classification model of snps has an auc of only and two snps achieve an auc of only for prostate cancer nine snps were not sufficient to improve the discrimination power over that of nongenetic predictors for risk of cardiovascular events finally in crohn s disease a model of five snps one with a quite low odds ratio of has an auc of only our analyses and examples show that strong association although very valuable for establishing etiological hypotheses does not guarantee effective discrimination between cases and controls the scientific community should be cautious to avoid overstating the value of association findings in terms of personalized medicine before time
background information extraction from microarrays has not yet been widely used in diagnostic or prognostic decision support systems due to the diversity of results produced by the available techniques their instability on different data sets and the inability to relate statistical significance with biological relevance thus there is an urgent need to address the statistical framework of microarray analysis and identify its drawbacks and limitations which will enable us to thoroughly compare methodologies under the same experimental set up and associate results with confidence intervals meaningful to clinicians in this study we consider gene selection algorithms with the aim to reveal inefficiencies in performance evaluation and address aspects that can reduce uncertainty in algorithmic validation results a computational study is performed related to the performance of several gene selection methodologies on publicly available microarray data three basic types of experimental scenarios are evaluated i e the independent test set and the fold cross validation cv using maximum and average performance measures feature selection methods behave differently under different validation strategies the performance results from cv do not mach well those from the independent test set except for the support vector machines svm and the least squares svm methods however these wrapper methods achieve variable often low performance whereas the hybrid methods attain consistently higher accuracies the use of an independent test set within cv is important for the evaluation of the predictive power of algorithms the optimal size of the selected gene set also appears to be dependent on the evaluation scheme the consistency of selected genes over variation of the training set is another aspect important in reducing uncertainty in the evaluation of the derived gene signature in all cases the presence of outlier samples can seriously affect algorithmic performance conclusion multiple parameters can influence the selection of a gene signature and its predictive power thus possible biases in validation methods must always be accounted for this paper illustrates that independent test set evaluation reduces the bias of cv and case specific measures reveal stability characteristics of the gene signature over changes of the training set moreover frequency measures on gene selection address the algorithmic consistency in selecting the same gene signature under different training conditions these issues contribute to the development of an objective evaluation framework and aid the derivation of statistically consistent gene signatures that could eventually be correlated with biological relevance the benefits of the proposed framework are supported by the evaluation results and methodological comparisons performed for several gene selection algorithms on three publicly datasets
conventional statistical analysis methods for functional magnetic resonance imaging fmri data are very successful at detecting brain regions that are activated as a whole during specific mental activities the overall activation of a region is usually taken to indicate involvement of the region in the task however such activation analysis does not consider the multivoxel patterns of activity within a brain region these patterns of activity which are thought to reflect neuronal population codes can be investigated by pattern information analysis in this framework a region s multivariate pattern information is taken to indicate representational content this tutorial introduction motivates pattern information analysis explains its underlying assumptions introduces the most widespread methods in an intuitive way and outlines the basic sequence of steps
as social computing systems persist over time the user experiences and interactions they support may change one type of social computing system social network sites snss are becoming more popular across broad segments of internet users facebook in particular has very broad participation amongst college attendees and has been growing in other populations as well this paper looks at how use of facebook has changed over time as indicated by three consecutive years of survey data and interviews with a subset of survey respondents reported uses of the site remain relatively constant over time but the perceived audience for user profiles and attitudes about the site show differences over the period
social networking sites e g myspace and facebook are popular online communication forms among adolescents and emerging adults yet little is known about young people s activities on these sites and how their networks of friends relate to their other online e g instant messaging and offline networks in this study college students responded in person and online to questions about their online activities and closest friends in three contexts social networking sites instant messaging and face to face results showed that participants often used the internet especially social networking sites to connect and reconnect with friends and family members hence there was overlap between participants online and offline networks however the overlap was imperfect the pattern suggested that emerging adults may use different online contexts to strengthen different aspects of their offline connections information from this survey is relevant to concerns about young people s life online contains figures tables
recent developments in the quantitative analysis of complex networks based largely on graph theory have been rapidly translated to studies of brain network organization the brain s structural and functional systems have features of complex networks such as small world topology highly connected hubs and modularity both at the whole brain scale of human neuroimaging and at a cellular scale in non human animals in this article we review studies investigating complex brain networks in diverse experimental modalities including structural and functional mri diffusion tensor imaging magnetoencephalography and electroencephalography in humans and provide an accessible introduction to the basic principles of graph theory we also highlight some of the technical challenges and key questions to be addressed by future developments in this rapidly field
molecular recognition between proteins and their interacting partners underlies the biochemistry of living organisms specificity in this recognition is thought to be essential whereas promiscuity is often associated with unwanted side effects poor catalytic properties and errors in biological function recent experimental evidence suggests that promiscuity not only in interactions but also in the actual function of proteins is not as rare as was previously thought this has implications not only for our fundamental understanding of molecular recognition and how protein function has evolved over time but also in the realm of biotechnology understanding protein promiscuity is becoming increasingly important not only to optimize protein engineering applications in areas as diverse as synthetic biology and metagenomics but also to lower attrition rates in drug discovery programs identify drug interaction surfaces less susceptible to escape mutations and potentiate the power polypharmacology
targeting genomic loci by massively parallel sequencing requires new methods to enrich templates to be sequenced we developed a capture method that uses biotinylated rna baits to fish targets out of a pond of dna fragments the rna is transcribed from pcr amplified oligodeoxynucleotides originally synthesized on a microarray generating sufficient bait for multiple captures at concentrations high enough to drive the hybridization we tested this method with mer baits that target coding exons mb and four regions mb total using illumina sequencing as read out about of uniquely aligning bases fell on or near bait sequence up to lay on exons proper the uniformity was such that of target bases in the exonic catch and in the regional catch had at least half the mean coverage one lane of illumina sequence was sufficient to call high confidence genotypes for of the targeted space
over the last decade or so there have been large numbers of methods published on approaches for normalization variable gene selection classification and clustering of microarray data as indicated in the scope document for bioinformatics this requires papers describing new methods for these problems to meet a very high standard showing important improvement in results for real biological data as well as novelty in this editorial we describe some standards that need to be met for papers in these areas to be seriously considered we ask that prospective authors consider these points carefully before submission of their papers bioinformatics
pnas influenza a incidence peaks during winter in temperate regions the basis for this pronounced seasonality is not understood nor is it well documented how influenza a transmission principally occurs previous studies indicate that relative humidity rh affects both influenza virus transmission ivt and influenza virus survival ivs here we reanalyze these data to explore the effects of humidity on ivt and ivs we find that absolute humidity ah constrains both transmission efficiency and ivs much more significantly than rh in the studies presented of ivt variability and of ivs variability are explained by ah whereas respectively only and are explained by rh in temperate regions both outdoor and indoor ah possess a strong seasonal cycle that minimizes in winter this seasonal cycle is consistent with a wintertime increase in ivs and ivt and may explain the seasonality of influenza thus differences in ah provide a single coherent more physically sound explanation for the observed variability of ivs ivt and influenza seasonality in temperate regions this hypothesis can be further tested through future additional laboratory epidemiological and studies
recommender systems based on collaborative filtering suggest to users items they might like such as movies songs scientific papers or jokes based on the ratings based on the ratings provided by users about items they first find users similar to the users receiving the recommendations and then suggest to her items appreciated in past by those like minded users however given the ratable items are many and the ratings provided by each users only a tiny fraction the step of finding similar users often fails we propose to replace this step with the use of a trust metric an algorithm able to propagate trust over the trust network in order to find users that can be trusted by the active user items appreciated by these trustworthy users can then be recommended to the active user an empirical evaluation on a large dataset crawled from epinions com shows that recommender systems that make use of trust information are the most effective in term of accuracy while preserving a good coverage this is especially evident on users who provided few ratings so that trust is able to alleviate the cold start problem and other weaknesses that beset collaborative filtering systems
biologists have long been fascinated by the exceptionally high diversity displayed by some evolutionary groups adaptive radiation in such clades is not only spectacular but is also an extremely complex process influenced by a variety of ecological genetic and developmental factors and strongly dependent on historical contingencies using modeling approaches we identify general patterns concerning the temporal spatial and genetic morphological properties of adaptive radiation some of these are strongly supported by empirical work whereas for others empirical support is more tentative in almost all cases more data are needed future progress in our understanding of adaptive radiation will be most successful if theoretical and empirical approaches are integrated as has happened in other areas of biology
the predominant organizational theme by which the transcription machinery and chromatin regulators are positioned within promoter regions or throughout genes in a genome is largely unknown we mapped the genomic location of diverse representative components of the gene regulatory machinery in saccharomyces cerevisiae to an experimental resolution of bp sequence specific gene regulators chromatin regulators mediator and rna polymerase pol ii were found primarily near the downstream border from the nucleosome which abuts against the approximately bp nucleosome free promoter region nfr general transcription factors tfiia b d e f h were located near the downstream edge from the nfr the nucleosome dissociated upon pol ii recruitment but not upon recruitment of only tbp and tfiib the position of many sequence specific regulators in promoter regions correlated with the position of specific remodeling complexes potentially reflecting functional interactions taken together the findings suggest that the combined action of activators and chromatin remodeling complexes remove the nucleosome after the preinitiation complex pic has partially assembled but before or concomitant with pol ii recruitment we find pic assembly which includes pol ii recruitment to be a significant rate limiting step during transcription but that additional gene specific rate limiting steps associated with pol ii occur recruitment
in the cerebral cortex the activity levels of neuronal populations are continuously fluctuating when neuronal activity as measured using functional mri fmri is temporally coherent across populations those populations are said to be functionally connected functional connectivity has previously been shown to correlate with structural anatomical connectivity patterns at an aggregate level in the present study we investigate with the aid of computational modeling whether systems level properties of functional networks including their spatial statistics and their persistence across time can be accounted for by properties of the underlying anatomical network we measured resting state functional connectivity using fmri and structural connectivity using diffusion spectrum imaging tractography in the same individuals at high resolution structural connectivity then provided the couplings for a model of macroscopic cortical dynamics in both model and data we observed i that strong functional connections commonly exist between regions with no direct structural connection rendering the inference of structural connectivity from functional connectivity impractical ii that indirect connections and interregional distance accounted for some of the variance in functional connectivity that was unexplained by direct structural connectivity and iii that resting state functional connectivity exhibits variability within and across both scanning sessions and model runs these empirical and modeling results demonstrate that although resting state functional connectivity is variable and is frequently present between regions without direct structural linkage its strength persistence and spatial statistics are nevertheless constrained by the large scale anatomical structure of the human cortex
in a series of five lectures i review inflationary cosmology i begin with a description of the initial conditions problems of the friedmann robertson walker frw cosmology and then explain how inflation an early period of accelerated expansion solves these problems next i describe how inflation transforms microscopic quantum fluctuations into macroscopic seeds for cosmological structure formation i present in full detail the famous calculation for the primordial spectra of scalar and tensor fluctuations i then define the inverse problem of extracting information on the inflationary era from observations of cosmic microwave background fluctuations the current observational evidence for inflation and opportunities for future tests of inflation are discussed finally i review the challenge of relating inflation to fundamental physics by giving an account of inflation in theory
identifying the tissues in which a microrna is expressed could enhance the understanding of the functions the biological processes and the diseases associated with that microrna however the mechanisms of microrna biogenesis and expression remain largely unclear and the identification of the tissues in which a microrna is expressed is limited here we present a machine learning based approach to predict whether an intronic microrna show high co expression with its host gene by doing so we could infer the tissues in which a microrna is high expressed through the expression profile of its host gene our approach is able to achieve an accuracy of in the leave one out cross validation and on an independent testing dataset we further estimated our method through comparing the predicted tissue specific micrornas and the tissue specific micrornas identified by biological experiments this study presented a valuable tool to predict the co expression patterns between human intronic micrornas and their host genes which would also help to understand the microrna expression and regulation mechanisms finally this framework can be easily extended to species
we have developed a new bioinformatics approach called ecmfinder evolutionary conserved motif finder this program searches for a given dna motif within the entire genome of one species and uses the gene association information of a potential transcription factor binding site tfbs to screen the homologous regions of a second and third species if multiple species have this potential tfbs in homologous positions this program recognizes the identified tfbs as an evolutionary conserved motif ecm this program outputs a list of ecms which can be uploaded as a custom track in the ucsc genome browser and can be visualized along with other available data the feasibility of this approach was tested by searching the genomes of three mammals human mouse and cow with the dna binding motifs of and ctcf this program successfully identified many clustered and ctcf binding sites that are conserved among these species but were previously undetected in particular this program identified ctcf binding sites that are located close to the and imprinted genes individual chip experiments confirmed the in vivo binding of the and ctcf proteins to most of these newly discovered binding sites demonstrating the feasibility and usefulness of nar
the microblogging service twitter is in the process of being appropriated for conversational interaction and is starting to be used for collaboration as well in an attempt to determine how well twitter supports user to user exchanges what people are using twitter for and what usage or design modifications would make it more usable as a tool for collaboration this study analyzes a corpus of naturally occurring public twitter messages tweets focusing on the functions and uses of the sign and the coherence of exchanges the findings reveal a surprising degree of conversationality facilitated especially by the use of as a marker of addressivity and shed light on the limitations of twitter s current design for use
background biology has increasingly recognized the necessity to build and utilize larger phylogenies to address broad evolutionary questions large phylogenies have facilitated the discovery of differential rates of molecular evolution between trees and herbs they have helped us understand the diversification patterns of mammals as well as the patterns of seed evolution in addition to these broad evolutionary questions there is increasing awareness of the importance of large phylogenies for addressing conservation issues such as biodiversity hotspots and response to global change two major classes of methods have been employed to accomplish the large tree building task supertrees and supermatrices although these methods are continually being developed they have yet to be made fully accessible to comparative biologists making extremely large trees rare results here we describe and demonstrate a modified supermatrix method termed mega phylogeny that uses databased sequences as well as taxonomic hierarchies to make extremely large trees with denser matrices than supermatrices the two major challenges facing large scale supermatrix phylogenetics are assembling large data matrices from databases and reconstructing trees from those datasets the mega phylogeny approach addresses the former as the latter is accomplished by employing recently developed methods that have greatly reduced the run time of large phylogeny construction we present an algorithm that requires relatively little human intervention the implemented algorithm is demonstrated with a dataset and phylogeny for asterales within campanulidae containing species and sites and an rbcl matrix for green plants viridiplantae with species and sites conclusion by examining much larger phylogenies patterns emerge that were otherwise unseen the phylogeny of viridiplantae successfully reconstructs major relationships of vascular plants that previously required many more genes these demonstrations underscore the importance of using large phylogenies to uncover important evolutionary patterns and we present a fast and simple method for constructing phylogenies
mobile phones are becoming increasingly popular as a means of information access while on the go mobile users are likely to be interested in locating different types of content however the mobile space presents a number of key challenges many of which go beyond issues with device characteristics such as screen size and input capabilities in particular changing contexts such as location time activity and social interactions are likely to impact on the types of information needs that arise in order to offer personalized effective mobile services we need to understand mobile users in more detail thus we carried out a four week diary study of mobile information needs looking in particular at the goal intent behind mobile information needs the topics users are interested in and the impact of mobile contexts such as location and time on needs
using content specific models to guide information retrieval and extraction can provide richer interfaces to end users for both understanding the context of news events and navigating related news articles in this paper we discuss a system brussell that uses semantic models to organize retrieval and extraction results generating both storylines explaining how news event situations unfold and also biographical sketches of the situation participants we generalize these models to introduce a new category of knowledge representation an explanatory structure that can scale up to include information from hundreds of documents yet still provide model based ui support to end users an informal survey of business news suggests the broad prevalence of news event situations indicating brussell s potential utility while an evaluation quantifies its performance in finding situations
while recommender systems tell users what items they might like explanations of recommendations reveal why they might like them explanations provide many benefits from improving user satisfaction to helping users make better decisions this paper introduces tagsplanations which are explanations based on community tags tagsplanations have two key components tag relevance the degree to which a tag describes an item and tag preference the user s sentiment toward a tag we develop novel algorithms for estimating tag relevance and tag preference and we conduct a user study exploring the roles of tag relevance and tag preference in promoting effective tagsplanations we also examine which types of tags are most useful tagsplanations
do new anatomical structures arise de novo or do they evolve from pre existing structures advances in developmental genetics palaeontology and evolutionary developmental biology have recently shed light on the origins of some of the structures that most intrigued charles darwin including animal eyes tetrapod limbs and giant beetle horns in each case structures arose by the modification of pre existing genetic regulatory circuits established in early metazoans the deep homology of generative processes and cell type specification mechanisms in animal development has provided the foundation for the independent evolution of a great variety structures
motivation numerous microarray studies of aging have been conducted yet given the noisy nature of gene expression changes with age elucidating the transcriptional features of aging and how these relate to physiological biochemical and pathological changes remains a critical problem results we performed a meta analysis of age related gene expression profiles using datasets from mice rats and humans our results reveal several common signatures of aging including genes consistently overexpressed with age the most significant of which was apod and genes underexpressed with age we characterized the biological processes associated with these signatures and found that age related gene expression changes most notably involve an overexpression of inflammation and immune response genes and of genes associated with the lysosome an underexpression of collagen genes and of genes associated with energy metabolism particularly mitochondrial genes as well as alterations in the expression of genes related to apoptosis cell cycle and cellular senescence biomarkers were also observed by employing a new method that emphasizes sensitivity our work further reveals previously unknown transcriptional changes with age in many genes processes and functions we suggest these molecular signatures reflect a combination of degenerative processes but also transcriptional responses to the process of aging overall our results help to understand how transcriptional changes relate to the process of aging and could serve as targets for future studies availability http genomics senescence info uarrays signatures htmlcontact jp senescence infosupplementary information supplementary data are available at online
it is generally accepted that the extent of phenotypic change between human and great apes is dissonant with the rate of molecular change between these two groups proteins are virtually identical cytogenetically there are few rearrangements that distinguish ape human chromosomes and rates of single base pair change and retrotransposon activity have slowed particularly within hominid lineages when compared to rodents or monkeys studies of gene family evolution indicate that gene loss and gain are enriched within the primate lineage here we perform a systematic analysis of duplication content of four primate genomes macaque orang utan chimpanzee and human in an effort to understand the pattern and rates of genomic duplication during hominid evolution we find that the ancestral branch leading to human and african great apes shows the most significant increase in duplication activity both in terms of base pairs and in terms of events this duplication acceleration within the ancestral species is significant when compared to lineage specific rate estimates even after accounting for copy number polymorphism and homoplasy we discover striking examples of recurrent and independent gene containing duplications within the gorilla and chimpanzee that are absent in the human lineage our results suggest that the evolutionary properties of copy number mutation differ significantly from other forms of genetic mutation and in contrast to the hominid slowdown of single base pair mutations there has been a genomic burst of duplication activity at this period during evolution
a major yet unresolved quest in decoding the human genome is the identification of the regulatory sequences that control the spatial and temporal expression of genes distant acting transcriptional enhancers are particularly challenging to uncover because they are scattered among the vast non coding portion of the genome evolutionary sequence constraint can facilitate the discovery of enhancers but fails to predict when and where they are active in vivo here we present the results of chromatin immunoprecipitation with the enhancer associated protein followed by massively parallel sequencing and map several thousand in vivo binding sites of in mouse embryonic forebrain midbrain and limb tissue we tested of these sequences in a transgenic mouse assay which in nearly all cases demonstrated reproducible enhancer activity in the tissues that were predicted by binding our results indicate that in vivo mapping of binding is a highly accurate means for identifying enhancers and their associated activities and suggest that such data sets will be useful to study the role of tissue specific enhancers in human biology and disease on a genome scale
automatically clustering web pages into semantic groups promises improved search and browsing on the web in this paper we demonstrate how user generated tags from large scale social bookmarking websites such as del icio us can be used as a complementary data source to page text and anchor text for improving automatic clustering of web pages this paper explores the use of tags in k means clustering in an extended vector space model that includes tags as well as page text and a novel generative clustering algorithm based on latent dirichlet allocation that jointly models text and tags we evaluate the models by comparing their output to an established web directory we find that the naive inclusion of tagging data improves cluster quality versus page text alone but a more principled inclusion can substantially improve the quality of all models with a statistically significant absolute f score increase of the generative model outperforms k means with another f increase
tagging has emerged as a popular means to annotate on line objects such as bookmarks photos and videos tags vary in semantic meaning and can describe different aspects of a media object tags describe the content of the media as well as locations dates people and other associated meta data being able to automatically classify tags into semantic categories allows us to understand better the way users annotate media objects and to build tools for viewing and browsing the media objects in this paper we present a generic method for classifying tags using third party open content resources such as wikipedia and the open directory our method uses structural patterns that can be extracted from resource meta data we describe the implementation of our method on wikipedia using wordnet categories as our classification schema and ground truth two structural patterns found in wikipedia are used for training and classification categories and templates we apply our system to classifying flickr tags compared to a wordnet baseline our method increases the coverage of the flickr vocabulary by we can classify many important entities that are not covered by wordnet such as london eye big island ronaldinho geocaching wii
comprehensive identification of polymorphisms among individuals within a species is essential both for studying the genetic basis of phenotypic differences and for elucidating the evolutionary history of the species large scale polymorphism surveys have recently been reported for human mouse and arabidopsis thaliana here we report a nucleotide level survey of genomic variation in a diverse collection of saccharomyces cerevisiae strains sampled from different ecological niches beer bread vineyards immunocompromised individuals various fermentations and nature and from locations on different continents we hybridized genomic dna from each strain to whole genome tiling microarrays and detected million single nucleotide polymorphisms which were grouped into distinct segregating sites we also identified deletion events of length base pairs among the surveyed strains we analysed the genome wide patterns of nucleotide polymorphism and deletion variants and measured the extent of linkage disequilibrium in s cerevisiae these results and the polymorphism resource we have generated lay the foundation for genome wide association studies in yeast we also examined the population structure of s cerevisiae providing support for multiple domestication events as well as insight into the origins of strains
provided certain obstacles are overcome we believe cloud computing has the potential to transform a large part of the it industry making software even more attractive as a service and shaping the way it hardware is designed and purchased developers with innovative ideas for new interactive internet services no longer require the large capital outlays in hardware to deploy their service or the human expense to operate it they need not be concerned about over provisioning for a service whose popularity does not meet their predictions thus wasting costly resources or under provisioning for one that becomes wildly popular thus missing potential customers and revenue moreover companies with large batch oriented tasks can get their results as quickly as their programs can scale since using servers for one hour costs no more than using one server for hours this elasticity of resources without paying a premium for large scale is unprecedented in the history of it the economies of scale of very large scale datacenters combined with pay as you go resource usage has heralded the rise of cloud computing it is now attractive to deploy an innovative new internet service on a third party s internet datacenter rather than your own infrastructure and to gracefully scale its resources as it grows or declines in popularity and revenue expanding and shrinking daily in response to normal diurnal patterns could lower costs even further cloud computing transfers the risks of over provisioning or under provisioning to the cloud computing provider who mitigates that risk by statistical multiplexing over a much larger set of users and who offers relatively low prices due better utilization and from the economy of purchasing at a larger scale we define terms present an economic model that quantifies the key buy vs pay as you go decision offer a spectrum to classify cloud computing providers and give our view of the top obstacles and opportunities to the growth of computing
over the past decade there has been an explosion of interest in network research across the physical and social sciences for social scientists the theory of networks has been a gold mine yielding explanations for social phenomena in a wide variety of disciplines from psychology to economics here we review the kinds of things that social scientists have tried to explain using social network analysis and provide a nutshell description of the basic assumptions goals and explanatory mechanisms prevalent in the field we hope to contribute to a dialogue among researchers from across the physical and social sciences who share a common interest in understanding the antecedents and consequences of network science
infection by human rhinoviruses hrvs is a major cause of upper and lower respiratory tract disease worldwide and displays significant phenotypic variation we examined diversity by completing the genome sequences for all known serotypes n superimposition of capsid crystal structure and optimal energy rna configurations established alignments and phylogeny these revealed conserved motifs clade specific diversity including a potential new species hrv d mutations in field isolates and recombination in analogy with poliovirus a hypervariable utr tract may affect virulence a configuration consistent with nonscanning internal ribosome entry was found in all hrvs and may account for rapid translation the data density from complete sequences of the reference hrvs provided high resolution for this degree of modeling and serves as a platform for full genome based epidemiologic studies and antiviral or vaccine science
defining the transcriptome the repertoire of transcribed regions encoded in the genome is a challenging experimental task current approaches relying on sequencing of ests or cdna libraries are expensive and labor intensive here we present a general approach for ab initio discovery of the complete transcriptome of the budding yeast based only on the unannotated genome sequence and millions of short reads from a single massively parallel sequencing run using novel algorithms we automatically construct a highly accurate transcript catalog our approach automatically and fully defines of the genes expressed under the given conditions and discovers previously undescribed transcription units of bp or longer it correctly demarcates the and utr boundaries of and of expressed genes respectively the method further identifies of known splice junctions in expressed genes and discovers previously uncharacterized introns including cases of condition dependent intron retention our framework is applicable to poorly understood organisms and can lead to greater understanding of the transcribed elements in an genome
knowing the precise locations of nucleosomes in a genome is key to understanding how genes are regulated recent next generation chipchip and chipseq technologies have accelerated our understanding of the basic principles of chromatin organization here we discuss what high resolution genome wide maps of nucleosome positions have taught us about how nucleosome positioning demarcates promoter regions and transcriptional start sites and how the composition and structure of promoter nucleosomes facilitate or inhibit transcription a detailed picture is starting to emerge of how diverse factors including underlying dna sequences and chromatin remodelling complexes influence positioning
the effective size of a population ne determines the rate of change in the composition of a population caused by genetic drift which is the random sampling of genetic variants in a finite population ne is crucial in determining the level of variability in a population and the effectiveness of selection relative to drift this article reviews the properties of ne in a variety of different situations of biological interest and the factors that influence it in particular the action of selection means that ne varies across the genome and advances in genomic techniques are giving new insights into how selection ne
techniques for systematically monitoring protein translation have lagged far behind methods for measuring messenger rna mrna levels here we present a ribosome profiling strategy that is based on the deep sequencing of ribosome protected mrna fragments and enables genome wide investigation of translation with subcodon resolution we used this technique to monitor translation in budding yeast under both rich and starvation conditions these studies defined the protein sequences being translated and found extensive translational control in both determining absolute protein abundance and responding to environmental stress we also observed distinct phases during translation that involve a large decrease in ribosome density going from early to late peptide elongation as well as widespread regulated initiation at non adenine uracil guanine aug codons ribosome profiling is readily adaptable to other organisms making high precision investigation of protein translation experimentally science
we discuss how collaborative annotations can be exploited to simplify and improve the management of context and re sources in the context aware retrieval eld we apply this approach to our context aware browser a general purpose solution to web content perusal by means of mobile devices based on the users context instead of relying on a pool of experts and on a rigid categorization as it is usually done in the context aware eld our solution allows the crowd of users to model control and manage the contextual knowl edge through collaboration and participation we propose two models and we outline an example application
the scholarly literature on innovation was for a long time not very voluminous but as shown in the paper this is now rapidly changing new journals professional associations and organizational units within universities focusing on innovation have also been formed this paper explores the cognitive and organizational characteristics of this emerging field of social science and considers its prospects and challenges the research reported in this paper is based on a web survey in which more than one thousand scholars worldwide part
with a genome size of kb and approximately protein coding regions mycoplasma genitalium is one of the smallest known self replicating organisms and additionally has extremely fastidious nutrient requirements the reduced genomic content of m genitalium has led researchers to suggest that the molecular assembly contained in this organism may be a close approximation to the minimal set of genes required for bacterial growth here we introduce a systematic approach for the construction and curation of a genome scale in silico metabolic model for m genitalium key challenges included estimation of biomass composition handling of enzymes with broad specificities and the lack of a defined medium computational tools were subsequently employed to identify and resolve connectivity gaps in the model as well as growth prediction inconsistencies with gene essentiality experimental data the curated model m genitalium i reactions metabolites is accurate in recapitulating in vivo gene essentiality results for m genitalium approaches and tools described herein provide a roadmap for the automated construction of in silico metabolic models of organisms
changes in gene expression may represent an important mode of human adaptation however to date there are relatively few known examples in which selection has been shown to act directly on levels or patterns of gene expression in order to test whether single nucleotide polymorphisms snps that affect gene expression in cis are frequently targets of positive natural selection in humans we analyzed genome wide snp and expression data from cell lines associated with the international hapmap project using a haplotype based test for selection that was designed to detect incomplete selective sweeps we found that snps showing signals of selection are more likely than random snps to be associated with gene expression levels in cis this signal is significant in the yoruba which is the population that shows the strongest signals of selection overall and shows a trend in the same direction in the other hapmap populations our results argue that selection on gene expression levels is an important type of human adaptation finally our work provides an analytical framework for tackling a more general problem that will become increasingly important namely testing whether selection signals overlap significantly with snps that are associated with phenotypes interest
motivation the solution of high dimensional inference and prediction problems in computational biology is almost always a compromise between mathematical theory and practical constraints such as limited computational resources as time progresses computational power increases but well established inference methods often remain locked in their initial suboptimal solution results we revisit the approach of segal et al to infer regulatory modules and their condition specific regulators from gene expression data in contrast to their direct optimization based solution we use a more representative centroid like solution extracted from an ensemble of possible statistical models to explain the data the ensemble method automatically selects a subset of most informative genes and builds a quantitatively better model for them genes which cluster together in the majority of models produce functionally more coherent modules regulators which are consistently assigned to a module are more often supported by literature but a single model always contains many regulator assignments not supported by the ensemble reliably detecting condition specific or combinatorial regulation is particularly hard in a single optimum but can be achieved using ensemble averaging availability all software developed for this study is available from http bioinformatics psb ugent be software contact tom michoel psb ugent be supplementary information supplementary data and figures are available from http bioinformatics psb ugent be bioinformatics
resequencing is an emerging tool for identification of rare disease associated mutations rare mutations are difficult to tag with snp genotyping as genotyping studies are designed to detect common variants however studies have shown that genetic heterogeneity is a probable scenario for common diseases in which multiple rare mutations together explain a large proportion of the genetic basis for the disease thus we propose a weighted sum method to jointly analyse a group of mutations in order to test for groupwise association with disease status for example such a group of mutations may result from resequencing a gene we compare the proposed weighted sum method to alternative methods and show that it is powerful for identifying disease associated genes both on simulated and encode data using the weighted sum method a resequencing study can identify a disease associated gene with an overall population attributable risk par of even when each individual mutation has much lower par using to affected and unaffected individuals depending on the underlying genetic model this study thus demonstrates that resequencing studies can identify important genetic associations provided that specialised analysis methods such as the weighted sum method used
summary standard dna alignment programs are inadequate to manage the data produced by new generation dna sequencers to answer this problem we developed pass with the objective of improving execution time and sensitivity when compared with other available programs pass performs fast gapped and ungapped alignments of short dna sequences onto a reference dna typically a genomic sequence it is designed to handle a huge amount of reads such as those generated by solexa solid or technologies the algorithm is based on a data structure that holds in ram the index of the genomic positions of seed words typically bases as well as an index of the precomputed scores of short words typically bases aligned against each other after building the genomic index the program scans every query sequence performing steps finds matching seed words in the genome for every match checks the precomputed alignment of the short flanking regions if passes step then it performs an exact dynamic alignment of a narrow region around the match the performance of the program is very striking both for sensitivity and speed for instance gap alignment is achieved hundreds of times faster than blast and several times faster than soap especially when gaps are allowed furthermore pass has a higher sensitivity when compared with the other available programs ravailability and implementation source code and binaries are freely available for download at http pass cribi unipd it implemented in c and supported on linux and windows contact pass cribi it
motivation high density tiling microarrays are increasingly used in combination with chip assays to study transcriptional regulation to ease the analysis of the large amounts of data generated by this approach we have developed chip on chip analysis suite cocas a standalone software suite which implements optimized chip on chip data normalization improved peak detection as well as quality control reports our software allows dye swap replicate correlation and connects easily with genome browsers and other peak detection algorithms cocas can readily be used on the latest generation of agilent high density arrays also the implemented peak detection methods are suitable for other datasets including chip seq output availability the software is available for download along with a sample dataset at http www ciml univ mrs fr software ferrier htm supplementary information supplementary data are available at online
progress of research efforts in a novel tech nology is contingent on having a rigorous organization of its knowledge domain and a comprehensive understanding of all the relevant components of this technology and their relationships cloud computing is one contemporary technology in which the research community has recently embarked manifesting itself as the descendant of several other computing research areas such as service oriented architecture distributed and grid computing and virtual ization cloud computing inherits their advancements and limitations towards the end goal of a thorough comprehension of the field of cloud computing and a more rapid adoption from the scientific community we propose in this paper an ontology of this area which demonstrates a dissection of the cloud into five main layers and illustrates their inter relations as well as their inter dependency on preceding technologies the contribution of this paper lies in being one of the first attempts to establish a detailed ontology of the cloud better comprehension of the technology would enable the community to design more efficient portals and gateways for the cloud and facilitate the adoption of this novel computing approach in scientific environments in turn this will assist the scientific community to expedite its contributions and insights into this evolving field
sec title background title p the impact of scientific publications has traditionally been expressed in terms of citation counts however scientific activity has moved online over the past decade to better capture scientific impact in the digital era a variety of new impact measures has been proposed on the basis of social network analysis and usage log data here we investigate how these new measures relate to each other and how accurately and completely they express scientific impact p sec sec title methodology title p we performed a principal component analysis of the rankings produced by existing and proposed measures of scholarly impact that were calculated on the basis of both citation and usage log data p sec sec title conclusions title p our results indicate that the notion of scientific impact is a multi dimensional construct that can not be adequately measured by any single indicator although some measures are more suitable than others the commonly used citation impact factor is not positioned at the core of this construct but at its periphery and should thus be used with caution sec
summary question why are so many leading modern scientists so dull and lacking in scientific ambition answer because the science selection process ruthlessly weeds out interesting and imaginative people at each level in education training and career progression there is a tendency to exclude smart and creative people by preferring conscientious and agreeable people the progressive lengthening of scientific training and the reduced independence of career scientists have tended to deter vocational revolutionary scientists in favour of industrious and socially adept individuals better suited to incremental normal science high general intelligence iq is required for revolutionary science but educational attainment depends on a combination of intelligence and the personality trait of conscientiousness and these attributes do not correlate closely therefore elite scientific institutions seeking potential revolutionary scientists need to use iq tests as well as examination results to pick out high iq under achievers as well as high iq revolutionary science requires high creativity creativity is probably associated with moderately high levels of eysencks personality trait of psychoticism psychoticism combines qualities such as selfishness independence from group norms impulsivity and sensation seeking with a style of cognition that involves fluent associative and rapid production of many ideas but modern science selects for high conscientiousness and high agreeableness therefore it enforces low psychoticism and low creativity yet my counter proposal to select elite revolutionary scientists on the basis of high iq and moderately high psychoticism may sound like a recipe for disaster since resembles a formula for choosing gifted charlatans and confidence tricksters a further vital ingredient is therefore necessary devotion to the transcendental value of truth elite revolutionary science should therefore be a place that welcomes brilliant impulsive inspired antisocial oddballs so long as they are also dedicated seekers
abstract nbsp nbsp in recent years technological advances in high throughput techniques and efficient data gathering methods coupled with a world wide effort in computational biology have resulted in an enormous amount of life science data available in repositories devoted to biomedical literature these repositories lack the ability to attain an effective and accurate search using semantic technologies as the key for interoperation enables searching and processing of biomedical literature in a more efficient way however emerging semantic applications take for granted specific knowledge that biomedical researchers may not have this paper presents design principles for easy to use biomedical semantic applications by means of ontology based annotations and faceted search the proposed approach is backed with a usable prototype that shows the breakthroughs of adding these principles to a biomedical digital library where identifying and searching information are critical aspects for non semantic experts
para this paper describes an adaptive computational intelligence system for learning trading rules the trading rules are represented using a fuzzy logic rule base and using an artificial evolutionary process the system learns to form rules that can perform well in dynamic market conditions a comprehensive analysis of the results of applying the system for portfolio construction using portfolio evaluation tools widely accepted by both the financial industry and academia is para
a convergence of different commercial and publicly accessible chemical informatics databases and social networking tools is positioned to change the way that research collaborations are initiated maintained and expanded particularly in the realm of neglected diseases a community based platform that combines traditional drug discovery informatics with features in secure groups is believed to be the key to facilitating richer instantaneous collaborations involving sensitive drug discovery data and intellectual property heterogeneous chemical and biological data from low throughput or high throughput experiments are archived mined and then selectively shared either just securely between specifically designated colleagues or openly on the internet in standardized formats we will illustrate several case studies for anti malarial research enabled by this platform which we suggest could be easily expanded more broadly for pharmaceutical research general
comparative genomics and systems biology offer unprecedented opportunities for testing central tenets of evolutionary biology formulated by darwin in the origin of species in and expanded in the modern synthesis years later evolutionary genomic studies show that natural selection is only one of the forces that shape genome evolution and is not quantitatively dominant whereas non adaptive processes are much more prominent than previously suspected major contributions of horizontal gene transfer and diverse selfish genetic elements to genome evolution undermine the tree of life concept an adequate depiction of evolution requires the more complex concept of a network or forest of life there is no consistent tendency of evolution towards increased genomic complexity and when complexity increases this appears to be a non adaptive consequence of evolution under weak purifying selection rather than an adaptation several universals of genome evolution were discovered including the invariant distributions of evolutionary rates among orthologous genes from diverse genomes and of paralogous gene family sizes and the negative correlation between gene expression level and sequence evolution rate simple non adaptive models of evolution explain some of these universals suggesting that a new synthesis of evolutionary biology might become feasible in a not so future
there is accumulating evidence to implicate the importance of n methyl d aspartate nmda receptors to the induction and maintenance of central sensitization during pain states however nmda receptors may also mediate peripheral sensitization and visceral pain nmda receptors are composed of a b c and d and a and b subunits which determine the functional properties of native nmda receptors among nmda receptor subtypes the subunit containing receptors appear particularly important for nociception thus leading to the possibility that selective antagonists may be useful in the treatment of pain
abstract background differential coexpression is a change in coexpression between genes that may reflect rewiring of transcriptional networks it has previously been hypothesized that such changes might be occurring over time in the lifespan of an organism while both coexpression and differential expression of genes have been previously studied in life stage change or aging differential coexpression has not generalizing differential coexpression analysis to many time points presents a methodological challenge here we introduce a method for analyzing changes in coexpression across multiple ordered groups e g over time and extensively test its validity and usefulness results our method is based on the use of the haar basis set to efficiently represent changes in coexpression at multiple time scales and thus represents a principled and generalizable extension of the idea of differential coexpression to life stage data we used published microarray studies categorized by age to test the methodology we validated the methodology by testing our ability to reconstruct gene ontology go categories using our measure of differential coexpression and compared this result to using coexpression alone our method allows significant improvement in characterizing these groups of genes further we examine the statistical properties of our measure of differential coexpression and establish that the results are significant both statistically and by an improvement in semantic similarity in addition we found that our method finds more significant changes in gene relationships compared to several other methods of expressing temporal relationships between genes such as coexpression over time conclusions differential coexpression over age generates significant and biologically relevant information about the genes producing it our haar basis methodology for determining age related differential coexpression performs better than other tested methods the haar basis set also lends itself to ready interpretation in terms of both evolutionary and physiological mechanisms of aging and can be seen as a natural generalization of two category differential coexpression supplementary data http www chibi ubc ca diffexage contact paul bioinformatics ca
motor skills can take weeks to months to acquire and can diminish over time in the absence of continued practice thus strategies that enhance skill acquisition or retention are of great scientific and practical interest here we investigated the effect of noninvasive cortical stimulation on the extended time course of learning a novel and challenging motor skill task a skill measure was chosen to reflect shifts in the task s speed accuracy tradeoff function saf which prevented us from falsely interpreting variations in position along an unchanged saf as a change in skill subjects practiced over consecutive days while receiving transcranial direct current stimulation tdcs over the primary motor cortex using the skill measure we assessed the impact of anodal relative to sham tdcs on both within day online and between day offline effects and on the rate of forgetting during a month follow up long term retention there was greater total online plus offline skill acquisition with anodal tdcs compared to sham which was mediated through a selective enhancement of offline effects anodal tdcs did not change the rate of forgetting relative to sham across the month follow up period and consequently the skill measure remained greater with anodal tdcs at months this prolonged enhancement may hold promise for the rehabilitation of brain injury furthermore these findings support the existence of a consolidation mechanism susceptible to anodal tdcs which contributes to offline effects but not to online effects or long retention
a study of user profile generation from folksonomies pensato per la recommendation improntato alla precisione dei profili utente analizzano i dati personali nelle folskonomie e investigano se si possono generare dei profili accurati basandosi su questi dati scoprono che tutti gli utenti hanno interessi multipli fanno un algoritmo per generare profili che rappresentano bene questi interessi multipli discutono anche come questi profili possano essere usati per raccomandare pagine web e organizzare i dati personali sistemi per suggerire guardano la history degli utenti per fare dei profili guarda i documenti collezionati dall utente collaborative tagging systems folksonomy uso di tag per descrivere i siti ed emerge uno schema di classificazione che pu dipendere dagli interessi dell utente costruicono profili basandosi su dati dei sistemi di tagging collaborativo e di solito solo un insieme di tag popolari usato per rappresentare gli interessi dell utente solo un insieme di tag non sufficiente a modellare gli interessi dell utente qui si propone una network analysis technique fatta sulla personimia di un utente per vedere i diversi interessi dell utente funziona bene nell individuare diversi ambiti di interesse folksonomie e personimie folksonomie utenti tags e risorse personimia tags e risorse associate ad un utente particolare personimia network un grafo bipartito tags nodi e documenti dell utente esiste un arco tag documento se un tag assegnato a quel documento si pu fare anche una matrice x tagxdocumento con sulla cella se tag assegnato al documento altrimenti piego il grafo in una rete one mode costruendo d x x che il repository dell utente mostra la relazione tra documenti celle con pesi pi alti si presentano se i due documenti sono considerati correlati t x x stessa cosa per i tag una rete semantica che mostra associazione tra i tags il vocabolario personale una ontologia semplice per il singolo utente analisi delle personimie dataset utenti di delicious andando su delicious recent distribuiti secondo power law risultati simili a diversi interessi degli utenti se un utente usa tutti i suoi tag per taggare quasi tutti i suoi segnalibri allora pu essere che abbia un interesse molto specifico perch quei tag coprono quasi tutti i segnalibri se la maggior parte dei tag coprono solo una piccola parte di bookmarks allora forse l utente ha molti interessi diversi misura matematica tag utilisation la media della frazione di bookmark per quali un tag usato la variet degli interessi si pu anche misurare con le co occorrenze tra i tag se i tag sono sempre usati uno con l altro allora riguarderanno argomenti simili e l utente potrebbe avere un interesse specifico average co occurrence ratio algoritmo valutazioni lavori futuro
this paper discusses the concept of cloud computing to achieve a complete definition of what a cloud is using the main characteristics typically associated with this paradigm in the literature more than definitions have been studied allowing for the extraction of a consensus definition as well as a minimum definition containing the essential characteristics this paper pays much attention to the grid paradigm as it is often confused with cloud technologies we also describe the relationships and distinctions between the grid and approaches
variation in gene expression is an important mechanism underlying susceptibility to complex disease the simultaneous genome wide assay of gene expression and genetic variation allows the mapping of the genetic factors that underpin individual differences in quantitative levels of expression expression qtls eqtls the availability of systematically generated eqtl information could provide immediate insight into a biological basis for disease associations identified through genome wide association gwa studies and can help to identify networks of genes involved in disease pathogenesis although there are limitations to current eqtl maps understanding of disease will be enhanced with novel technologies and international efforts that extend to a wide range of new samples tissues
in mammals and other eukaryotes most of the genome is transcribed in a developmentally regulated manner to produce large numbers of long non coding rnas ncrnas here we review the rapidly advancing field of long ncrnas describing their conservation their organization in the genome and their roles in gene regulation we also consider the medical implications and the emerging recognition that any transcript regardless of coding potential can have an intrinsic function as rna
human synuclein accumulates in dopaminergic neurons as intraneuronal inclusions lewy bodies which are characteristic of idiopathic parkinson s disease pd here we suggest that modulation of the functional activity of the dopamine transporter dat by synuclein may be a key factor in the preferential degeneration of mesencephalic dopamine da synthesizing neurons in pd in cotransfected ltk hek and sk n mc cells synuclein induced a decrease in da uptake biotinylated dat levels were decreased by in cotransfected cells relative to cells expressing only dat dat was colocalized with synuclein in mesencephalic neurons and cotransfected ltk cells coimmunoprecipitation studies showed the existence of a complex between synuclein and dat in specific rat brain regions and cotransfected cells through specific amino acid motifs of both proteins the attenuation of dat function by synuclein was cytoprotective because da mediated oxidative stress and cell death were reduced in cotransfected cells the neurotoxin mpp methyl phenylpyridinium oxidative stress or impairment of cell adhesion ablated the synuclein mediated inhibition of dat activity which caused increased uptake of da and increased biotinylated dat levels in both mesencephalic neurons and cotransfected cells these studies suggest a novel normative role for synuclein in regulating da synaptic availability and homeostasis which is relevant to the pathophysiology of pd key words parkinson s disease synucleinopathies mpp neurodegeneration lewy fj
understanding the effect of genetic sequence variation on phenotype is a major challenge that lies at the heart of genetics we developed golph genomic linkage to phenotype a statistical method to identify genetic interactions and used it to characterize the landscape of genetic interactions between gene expression quantitative trait loci our results reveal that allele specific interactions in which a gene only exerts an influence on the phenotype in the presence of a particular allele at the primary locus are widespread and that genetic interactions are predominantly nonadditive the data portray a complex picture in which interacting loci influence the expression of modules of coexpressed genes involved in coherent biological processes and pathways we show that genetic variation at a single gene can have a major impact on the global transcriptional response altering interactions between genes through shutdown or activation of pathways thus different cellular states occur not only in response to the external environment but also result from intrinsic variation
summary cluster analysis plays an important role in the analysis of gene expression data since the early beginning of microarray studies and is routinely used to find groups of genes with common expression pattern in order to make cluster analysis helpful for users visualization of cluster solutions is of utmost importance here we present the new r package gcexplorer for the interactive exploration of gene clusters gcexplorer facilitates the interpretation of cluster results and allows to investigate extensive information about clusters availability the latest release of gcexplorer is always availableat the comprehensive r archive network cran http cran r project org package gcexplorer see the readme file in the package for detailed installation instructions contact theresa scharl ci tuwien at
the amount of knowledge and talent dispersed among the human race has always outstripped our capacity to harness it crowdsourcing corrects thatbut in doing so it also unleashes the forces of creative destruction from first identified by journalist jeff howe in a june wired article crowdsourcing describes the process by which the power of the many can be leveraged to accomplish feats that were once the province of the specialized few howe reveals that the crowd is more than wiseits talented creative and stunningly productive activates the transformative power of todays technology liberating the latent potential within us all its a perfect meritocracy where age gender race education and job history no longer matter the quality of work is all that counts and every field is open to people of every imaginable background if you can perform the service design the product or solve the problem youve got the job but crowdsourcing has also triggered a dramatic shift in the way work is organized talent is employed research is conducted and products are made and marketed as the crowd comes to supplant traditional forms of labor pain and disruption are inevitable jeff howe delves into both the positive and negative consequences of this intriguing phenomenon through extensive reporting from the front lines of this revolution he employs a brilliant array of stories to look at the economic cultural business and political implications of crowdsourcing how were a bunch of part time dabblers in finance able to help an investment company consistently beat the market why does procter gamble repeatedly call on enthusiastic amateurs to solve scientific and technical challenges how can companies as diverse as istockphoto and threadless employ just a handful of people yet generate millions of dollars in revenue every year the answers lie within these pages the blueprint for crowdsourcing originated from a handful of computer programmers who showed that a community of like minded peers could create better products than a corporate behemoth like microsoft jeff howe tracks the amazing migration of this new model of production showing the potential of the internet to create human networks that can divvy up and make quick work of otherwise overwhelming tasks one of the most intriguing ideas of is that the knowledge to solve intractable problemsa cure for cancer for instancemay already exist within the warp and weave of this infinite and as yet largely untapped resource but first howe proposes we need to banish preconceived notions of how such problems are solved the very concept of crowdsourcing stands at odds with centuries of practice yet for the digital natives soon to enter the workforce the technologies and principles behind crowdsourcing are perfectly intuitive this generation collaborates shares remixes and creates with a fluency and ease the rest of us can hardly understand just now starting to emerge will in a short time simply be the way things done
heat shock factor is essential for protecting cells from protein damaging stress associated with misfolded proteins and regulates the insulin signaling pathway and aging here we show that human is inducibly acetylated at a critical residue that negatively regulates dna binding activity activation of the deacetylase and longevity factor prolonged binding to the heat shock promoter by maintaining in a deacetylated dna binding competent state conversely down regulation of accelerated the attenuation of the heat shock response hsr and release of from its cognate promoter elements these results provide a mechanistic basis for the requirement of in the regulation of life span and establish a role for in protein homeostasis and the science
anomaly detection is an important problem that has been researched within diverse research areas and application domains many anomaly detection techniques have been specifically developed for certain application domains while others are more generic this survey tries to provide a structured and comprehensive overview of the research on anomaly detection we have grouped existing techniques into different categories based on the underlying approach adopted by each technique for each category we have identified key assumptions which are used by the techniques to differentiate between normal and anomalous behavior when applying a given technique to a particular domain these assumptions can be used as guidelines to assess the effectiveness of the technique in that domain for each category we provide a basic anomaly detection technique and then show how the different existing techniques in that category are variants of the basic technique this template provides an easier and more succinct understanding of the techniques belonging to each category further for each category we identify the advantages and disadvantages of the techniques in that category we also provide a discussion on the computational complexity of the techniques since it is an important issue in real application domains we hope that this survey will provide a better understanding of the different directions in which research has been done on this topic and how techniques developed in one area can be applied in domains for which they were not intended to with
motivation identifying single nucleotide polymorphisms snps that underlie common and complex human diseases such as cancer is of major interest in current molecular epidemiology nevertheless the tremendous number of snps on the human genome requires computational methods for prioritizing snps according to their potentially deleterious effects to human health and as such for expediting genotyping and analysis as of yet little has been done to quantitatively assess the possible deleterious effects of snps for effective association studies results we propose a new integrative scoring system for prioritizing snps based on their possible deleterious effects within a probabilistic framework we applied our system to disease susceptibility genes obtained from the omim online mendelian inheritance in man database which is one of the most widely used databases of human genes and genetic disorders the scoring results clearly show that the distribution of the functional significance fs scores for already known disease related snps is significantly different from that of neutral snps in addition we summarize distinct features of potentially deleterious snps based on their fs score such as functional genomic regions where they occur or bio molecular functions that they mainly affect we also demonstrate through a comparative study that our system improves upon other function assessment systems for snps by assigning significantly higher fs scores to already known disease related snps than to neutral snps availability http compbio cs queensu ca f snp and http compbio cs queensu ca rankingsnps default html contact lee cs queensu bioinformatics
summary comparative approach is one of the most essential methods for extracting functional and evolutionary information from genomic sequences so far a number of sequence comparison tools have been developed and most are either for on site use requiring program installation but providing a wide variety of analyses or for the online search of user s sequences against given databases on a server we newly devised an asynchronous javascript and xml ajax based system for comparative genomic analyses cgas with highly interactive interface within a browser requiring no software installation the current version cgas version provides functionality for viewing similarity relationships between user s sequences including a multiple dot plot between sequences with their annotation information the scrollbar less draggable interface of cgas is implemented with google maps api version the annotation information associated with the genomic sequences compared is synchronously displayed with the comparison view the multiple comparison viewer is one of the unique functionalities of this system to allow the users to compare the differences between different pairs of sequences in this viewer the system tells orthologous correspondences between the sequences compared interactively this web based tool is platform independent and will provide biologists having no computational skills with opportunities to analyze their own data without software installation and customization of the computer system availability and implementation cgas is available at http cgas ist hokudai jp
gr divergence of gene expression can result in phenotypic variation which contributes to the evolution of new species although the influence of and regulatory mutations is well known the genome wide impact of changes in genomic neighborhood of genes on expression divergence between species remains largely unexplored here we compare the neighborhood of orthologous genes within a window of in human and chimpanzee with the expression levels of their transcripts from several equivalent tissues and demonstrate that genes with altered neighborhood are more likely to undergo expression divergence than genes with conserved neighborhood we observe the same trend when expression divergence data was analyzed from six different brain parts that are equivalent between human and chimpanzee additionally we find an enrichment for genes with altered neighbourhood to be expressed in a tissue specific manner in the human brain these results suggest that expression divergence induced by this mechanism could have contributed to the phenotypic differences between humans and chimpanzee we propose that in addition to other molecular mechanisms change in genomic neighborhood is an important factor that drives evolution
previous investigations into the impact of open access journals on subsequent citations confounded open and electronic access and failed to track availability over time with new data we separated these effects we demonstrate that a journal receives a modest increase in citations when it comes online freely but the jump is larger when it first comes online through commercial sources this effect reverses for poor countries where free access articles are much more likely to be cited together findings suggest that free internet access widens the circle of those who read and make use of scientists science
motivation development of high throughput technology makes it possible to measure expressions of thousands of genes simultaneously genes have the inherent pathway structure where pathways are composed of multiple genes with coordinated biological functions it is of great interest to identify differential gene pathways that are associated with the variations of phenotypes results we propose the following approach for detecting differential gene pathways first we construct gene pathways using databases such as kegg or go second for each pathway we extract a small number of representative features which are linear combinations of gene expressions and or their transformations specifically we propose using i principal components pcs of gene expression sets ii pcs of expanded gene expression sets and iii expanded sets of pcs of gene expressions as the representative features third we identify differential gene pathways as those with representative features significantly associated with the variations of phenotypes particularly disease clinical outcomes in regression models the false discovery rate approach is used to adjust for multiple comparisons analysis of three gene expression datasets suggests that i the proposed approach can effectively identify differential gene pathways ii pcs that explain only a small amount of variations of gene expressions may bear significant associations between gene pathways and phenotypes iii including second order terms of gene expressions may lead to identification of new differential gene pathways iv the proposed approach is relatively insensitive to additional noises and v the proposed approach can identify gene pathways missed by alternative approaches supplementary information supplementary data are available at online
onthefly is a web based application that applies biological named entity recognition to enrich microsoft office pdf and plain text documents the input files are converted into the html format and then sent to the reflect tagging server which highlights biological entity names like genes proteins and chemicals and attaches to them javascript code to invoke a summary pop up window the window provides an overview of relevant information about the entity such as a protein description the domain composition a link to the structure and links to other relevant online resources onthefly is also able to extract the bioentities mentioned in a set of files and to produce a graphical representation of the networks of the known and predicted associations of these entities by retrieving the information from the stitch database availability http onthefly embl de http onthefly embl de faq htmlcontact pavlopou embl desupplementary information supplementary data are available at online
to a large extent progress in neuroscience has been driven by the study of single cell responses averaged over several repetitions of stimuli or behaviours however the brain typically makes decisions based on single events by evaluating the activity of large neuronal populations therefore to further understand how the brain processes information it is important to shift from a single neuron multiple trial framework to multiple neuron single trial methodologies two related approaches decoding and information theory can be used to extract single trial information from the activity of neuronal populations such population analysis can give us more information about how neurons encode stimulus features than traditional single studies
abstract background researchers in the field of bioinformatics often face a challenge of combining several ordered lists in a proper and efficient manner rank aggregation techniques offer a general and flexible framework that allows one to objectively perform the necessary aggregation with the rapid growth of high throughput genomic and proteomic studies the potential utility of rank aggregation in the context of meta analysis becomes even more apparent one of the major strengths of rank based aggregation is the ability to combine lists coming from different sources and platforms for example different microarray chips which may or may not be directly comparable otherwise results the rankaggreg package provides two methods for combining the ordered lists the cross entropy method and the genetic algorithm two examples of rank aggregation using the package are given in the manuscript one in the context of clustering based on gene expression and the other one in the context of meta analysis of prostate cancer microarray experiments conclusions the two examples described in the manuscript clearly show the utility of the rankaggreg package in the current bioinformatics context where ordered lists are routinely produced as a result of modern high technologies
allosteric proteins bind an effector molecule at one site resulting in a functional change at a second site we hypothesize that allosteric communication in proteins relies upon networks of quaternary collective rigid body and tertiary residueresidue contact motions we argue that cyclic topology of these networks is necessary for allosteric communication an automated algorithm identifies rigid bodies from the displacement between the inactive and the active structures and constructs quaternary networks from these rigid bodies and the substrate and effector ligands we then integrate quaternary networks with a coarse grained representation of contact rearrangements to form global communication networks gcns the gcn reveals allosteric communication among all substrate and effector sites in of multidomain and multimeric proteins while tertiary and quaternary networks exhibit such communication in only and of these proteins respectively furthermore in of the proteins connected by the gcn or more of the substrate effector paths via the gcn are interdependent paths that do not exist via either the tertiary or the quaternary network substrate effector pathways typically are not linear but rather consist of polycyclic networks of rigid bodies and clusters of rearranging residue contacts these results argue for broad applicability of allosteric communication based on structural changes and demonstrate the utility of the gcn global communication networks may inform a variety of experiments on allosteric proteins as well as the design of allostery into non proteins
the pan genome denotes the set of all genes present in the genomes of a group of organisms here we extend the pan genome concept to higher taxonomic units using sequenced genomes we estimate the size of the bacterial pan genome based on the frequency of occurrences of genes among sampled genomes using gene and genome centered approaches we characterize three distinct pools of gene families that comprise the bacterial pan genome each evolving under different evolutionary constraints our findings indicate that the pan genome of the bacterial domain is of infinite size the bacteria as a whole have an open pan genome and that genes per genome belong to the extended bacterial genome
motivation advances in technology have made different microarray platforms available among the many illumina beadarrays are relatively new and have captured significant market share with beadarray technology high data quality is generated from low sample input at reduced cost however the analysis methods for illumina beadarrays are far behind those for affymetrix oligonucleotide arrays and so need to be improved results in this article we consider the problem of background correction for beadarray data one distinct feature of beadarrays is that for each array the noise is controlled by over bead types conjugated with non specific oligonucleotide sequences we extend the robust multi array analysis rma background correction model to incorporate the information from negative control beads and consider three commonly used approaches for parameter estimation namely non parametric maximum likelihood estimation mle and bayesian estimation the proposed approaches as well as the existing background correction methods are compared through simulation studies and a data example we find that the maximum likelihood and bayes methods seem to be the most promising supplementary information supplementary data are available at online
the study of complex information processing systems requires appropriate theoretical tools to help unravel their underlying design principles information theory is one such tool and has been utilized extensively in the study of the neural code although much progress has been made in information theoretic methodology there is still no satisfying answer to the question what is the information that a given property of the neural population activity e g the responses of single cells within the population carries about a set of stimuli here we answer such questions via the minimum mutual information minmi principle we quantify the information in any statistical property of the neural response by considering all hypothetical neuronal populations that have the given property and finding the one that contains the minimum information about the stimuli all systems with higher information values necessarily contain additional information processing mechanisms and thus the minimum captures the information related to the given property alone minmi may be used to measure information in properties of the neural response such as that conveyed by responses of small subsets of cells e g singles or pairs in a large population and cooperative effects between subunits in networks we show how the framework can be used to study neural coding in large populations and to reveal properties that are not discovered by other information methods
visual working memory provides an essential link between perception and higher cognitive functions allowing for the active maintenance of information about stimuli no longer in view research suggests that sustained activity in higher order prefrontal parietal inferotemporal and lateral occipital areas supports visual maintenance and may account for the limited capacity of working memory to hold up to items because higher order areas lack the visual selectivity of early sensory areas it has remained unclear how observers can remember specific visual features such as the precise orientation of a grating with minimal decay in performance over delays of many seconds one proposal is that sensory areas serve to maintain fine tuned feature information but early visual areas show little to no sustained activity over prolonged delays here we show that orientations held in working memory can be decoded from activity patterns in the human visual cortex even when overall levels of activity are low using functional magnetic resonance imaging and pattern classification methods we found that activity patterns in visual areas could predict which of two oriented gratings was held in memory with mean accuracy levels upwards of even in participants whose activity fell to baseline levels after a prolonged delay these orientation selective activity patterns were sustained throughout the delay period evident in individual visual areas and similar to the responses evoked by unattended task irrelevant gratings our results demonstrate that early visual areas can retain specific information about visual features held in working memory over periods of many seconds when no physical stimulus present
population coding is widely regarded as an important mechanism for achieving reliable behavioral responses despite neuronal variability however standard reinforcement learning slows down with increasing population size as the global reward signal becomes less and less related to the performance of any single neuron we found that learning speeds up with increasing population size if in addition to global reward feedback about the population response modulates plasticity
the pathogenic mechanisms underlying idiopathic parkinson s disease pd remain enigmatic recent findings suggest that inflammatory processes are associated with several neurodegenerative disorders including pd enhanced expression of the proinflammatory cytokine tumor necrosis factor tnf a has been found in association with glial cells in the substantia nigra of patients with pd to determine the potential role for tnf a in pd we examined the effects of the methyl phenyl tetrahydropyridine mptp a dopaminergic neurotoxin that mimics some of the key features associated with pd using transgenic mice lacking tnf receptors administration of mptp to wild type mice resulted in a time dependent expression of tnf a in striatum which preceded the loss of dopaminergic markers and reactive gliosis in contrast transgenic mice carrying homozygous mutant alleles for both the tnf receptors tnfr dko but not the individual receptors were completely protected against the dopaminergic neurotoxicity of mptp the data indicate that the proinflammatory cytokine tnf a is an obligatory component of dopaminergic neurodegeneration moreover because tnf a is synthesized predominantly by microglia and astrocytes our findings implicate the participation of glial cells in mptp induced neurotoxicity similar mechanisms may underlie the etiopathogenesis of pd key words brain neurodegeneration neuroprotection fj
summary current short read mapping programs are based on the reasonable premise that most sequencing errors occur near the end of the read these programs map reads with either a small number of mismatches in the entire read or a small number of mismatches in the segment remaining after trimming bases from the end or a single base from the end though multiple sequencing errors most likely occur near the end of the reads they can still occur at the end of the reads trimming from the end will not be able to map these reads we have developed a program maximum oligonucleotide mapping mom based on the concept of query matching that is designed to capture a maximal length match within the short read satisfying the user defined error parameters this query matching approach thus accommodates multiple sequencing errors at both ends we demonstrate that this technique achieves greater sensitivity and a higher percentage of uniquely mapped reads when compared to existing programs such as soap maq and shrimp software and test data availability http mom csbc vcu edu contact ygao vcu edu hleaves vcu bioinformatics
several genome wide association studies gwas have been published on various complex diseases although new loci are found to be associated with these diseases still only very little of the genetic risk for these diseases can be explained as gwas are still underpowered to find small main effects and gene gene interactions are likely to play a role the data might currently not be analyzed to its full potential in this study we evaluated alternative methods to study gwas data instead of focusing on the single nucleotide polymorphisms snps with the highest statistical significance we took advantage of prior biological information and tried to detect overrepresented pathways in the gwas data we evaluated whether pathway classification analysis can help prioritize the biological pathways most likely to be involved in the disease etiology in this study we present the various benefits and limitations of pathway classification tools in analyzing gwas data we show multiple differences in outcome between pathway tools analyzing the same dataset furthermore analyzing randomly selected snps always results in significantly overrepresented pathways large pathways have a higher chance of becoming statistically significant and the bioinformatics tools used in this study are biased toward detecting well defined pathways as an example we analyzed data from two gwas on type diabetes the diabetes genetics initiative dgi and the wellcome trust case control consortium wtccc occasionally the results from the dgi and the wtccc gwas showed concordance in overrepresented pathways but discordance in the corresponding genes thus incorporating gene networks and pathway classification tools into the analysis can point toward significantly overrepresented molecular pathways which cannot be picked up using traditional single locus analyses however the limitations discussed in this study need to be addressed before these methods can be used
a single molecule method for sequencing dna that does not require fluorescent labelling could reduce costs and increase sequencing speeds an exonuclease enzyme might be used to cleave individual nucleotide molecules from the dna and when coupled to an appropriate detection system these nucleotides could be identified in the correct order here we show that a protein nanopore with a covalently attached adapter molecule can continuously identify unlabelled nucleoside monophosphate molecules with accuracies averaging methylated cytosine can also be distinguished from the four standard dna bases guanine adenine thymine and cytosine the operating conditions are compatible with the exonuclease and the kinetic data show that the nucleotides have a high probability of translocation through the nanopore and therefore of not being registered twice this highly accurate tool is suitable for integration into a system for sequencing nucleic acids and for analysing modifications
rna is not only a messenger operating between dna and protein transcription of essentially the entire eukaryotic genome generates a myriad of non protein coding rna species that show complex overlapping patterns of expression and regulation although long noncoding rnas lncrnas are among the least well understood of these transcript species they cannot all be dismissed as merely transcriptional noise here we review the evolution of lncrnas and their roles in transcriptional regulation epigenetic gene regulation disease
social tagging is the process by which many users add metadata in the form of keywords to annotate and categorize information items songs pictures web links products etc collaborative tagging systems recommend tags to users based on what tags other users have used for the same items aiming to develop a common consensus about which tags best describe an item however they fail to provide appropriate tag recommendations because i users may have different interests for an information item and ii information items may have multiple facets in contrast to the current tag recommendation algorithms our approach develops a unified framework to model the three types of entities that exist in a social tagging system users items and tags these data is represented by a order tensor on which latent semantic analysis and dimensionality reduction is performed using the higher order singular value decomposition hosvd technique we perform experimental comparison of the proposed method against two state of the art tag recommendations algorithms with two real data sets last fm and bibsonomy our results show significant improvements in terms of effectiveness measured through precision
we study the problem of personalized interactive tag recommendation for flickr while a user enters selects new tags for a particular picture the system suggests related tags to her based on the tags that she or other people have used in the past along with some of the tags already entered the suggested tags are dynamically updated with every additional tag entered selected we describe a new algorithm called hybrid which can be applied to this problem and show that it outperforms previous algorithms it has only a single tunable parameter which we found to be robust
abstract personal genomics endeavors such as the genomes project are generating maps of genomic structural variants by analyzing ends of massively sequenced genome fragments to process these we developed paired end mapper pemer http sv gersteinlab org pemer this comprises an analysis pipeline compatible with several next generation sequencing platforms simulation based error models yielding confidence values for each structural variant and a back end database the simulations demonstrated high structural variant reconstruction efficiency for pemer s coverage adjusted multi cutoff scoring strategy and showed its relative insensitivity to base errors
bistable epigenetic switches are fundamental for cell fate determination in unicellular and multicellular organisms regulatory proteins associated with bistable switches are often present in low numbers and subject to molecular noise it is becoming clear that noise in gene expression can influence cell fate although the origins and consequences of noise have been studied the stochastic and transient nature of rna errors during transcription has not been considered in the origin or modeling of noise nor has the capacity for such transient errors in information transfer to generate heritable phenotypic change been discussed we used a classic bistable memory module to monitor and capture transient rna errors the lac operon of escherichia coli comprises an autocatalytic positive feedback loop producing a heritable all or none epigenetic switch that is sensitive to molecular noise using single cell analysis we show that the frequency of epigenetic switching from one expression state to the other is increased when the fidelity of rna transcription is decreased due to error prone rna polymerases or to the absence of auxiliary rna fidelity factors grea and greb functional analogues of eukaryotic tfiis therefore transcription infidelity contributes to molecular noise and can effect heritable phenotypic change in genetically identical cells in the same environment whereas dna errors allow genetic space to be explored rna errors may allow epigenetic or expression space to be sampled thus rna infidelity should also be considered in the heritable origin of altered or aberrant behaviour
identifying protein protein interactions is crucial for understanding cellular functions genomic data provides opportunities and challenges in identifying these interactions we uncover the rules for predicting protein protein interactions using a frequent pattern tree fpt approach modified to generate a minimum set of rules mfpt with rule attributes constructed from the interaction features of the yeast genomic data the mfpt prediction accuracy is benchmarked against other commonly used methods such as bayesian networks and logistic regressions under various statistical measures our study indicates that mfpt outranks other methods in predicting the protein protein interactions for the database used we predict a new protein protein interaction complex whose biological function is related to premrna splicing and new protein protein interactions within existing complexes based on the rules generated our method is general and can be used to discover the underlying rules for protein protein interactions genomic interactions structure function relationships and other fields research
pnas sequence alignment and database searching are essential tools in biology because a protein s function can often be inferred from homologous proteins standard sequence comparison methods use substitution matrices to find the alignment with the best sum of similarity scores between aligned residues these similarity scores do not take the local sequence context into account here we present an approach that derives context specific amino acid similarities from short windows centered on each query sequence residue our results demonstrate that the sequence context contains much more information about the expected mutations than just the residue itself by employing our context specific similarities cs blast in combination with ncbi blast we increase the sensitivity more than fold on a difficult benchmark set without loss of speed alignment quality is likewise improved significantly furthermore we demonstrate considerable improvements when applying this paradigm to sequence profiles two iterations of csi blast our context specific version of psi blast are more sensitive than iterations of psi blast the paradigm for biological sequence comparison presented here is very general it can replace substitution matrices in sequence and profile based alignment and search methods for both protein and nucleotide er
we investigate how to organize a large collection of geotagged photos working with a dataset of about million images collected from flickr our approach combines content analysis based on text tags and image data with structural analysis based on geospatial data we use the spatial distribution of where people take photos to define a relational structure between the photos that are taken at popular places we then study the interplay between this structure and the content using classification methods for predicting such locations from visual textual and temporal features of the photos we find that visual and temporal features improve the ability to estimate the location of a photo compared to using just textual features we illustrate using these techniques to organize a large photo collection while also revealing various interesting properties about popular cities and landmarks at a scale
we survey some of the concepts methods and applications of community detection which has become an increasingly important area of network science to help ease newcomers into the field we provide a guide to available methodology and open problems and discuss why scientists from diverse backgrounds are interested in these problems as a running theme we emphasize the connections of community detection to problems in statistical physics and optimization
we extend the definition of spectral dimension usually defined for fractal and lattice geometries to theories on smooth spacetimes with anisotropic scaling we show that in quantum gravity dominated by a lifshitz point with dynamical critical exponent z in d spacetime dimensions the spectral dimension of spacetime is equal to d z in the case of gravity in dimensions presented in a href abs arxiv a which is dominated by z in the uv and flows to z in the ir the spectral dimension of spacetime flows from at large scales to at short distances remarkably this is the qualitative behavior of found numerically by ambjorn jurkiewicz and loll in their causal dynamical triangulations approach to gravity
the volume of publicly available genomic scale data is increasing genomic datasets in public repositories are annotated with free text fields describing the pathological state of the studied sample these annotations are not mapped to concepts in any ontology making it difficult to integrate these datasets across repositories we have previously developed methods to map text annotations of tissue microarrays to concepts in the nci thesaurus and snomed ct in this work we generalize our methods to map text annotations of gene expression datasets to concepts in the umls we demonstrate the utility of our methods by processing annotations of datasets in the gene expression omnibus we demonstrate that we enable ontology based querying and integration of tissue and gene expression microarray data we enable identification of datasets on specific diseases across both repositories our approach provides the basis for ontology driven data integration for translational research on gene and protein expression data based on this work we have built a prototype system for ontology based annotation and indexing of biomedical data the system processes the text metadata of diverse resource elements such as gene expression data sets descriptions of radiology images clinical trial reports and pubmed article abstracts to annotate and index them with concepts from appropriate ontologies the key functionality of this system is to enable users to locate biomedical data resources related to particular concepts
motivation in this article we show that the classification of human precursor microrna pre mirnas hairpins from both genome pseudo hairpins and other non coding rnas ncrnas is a common and essential requirement for both comparative and non comparative computational recognition of human mirna genes however the existing computational methods do not address this issue completely or successfully here we present the development of an effective classifier system named as micropred for this classification problem by using appropriate machine learning techniques our approach includes the introduction of more representative datasets extraction of new biologically relevant features feature selection handling of class imbalance problem in the datasets and extensive classifier performance evaluation via systematic cross validation methods results our micropred classifier yielded higher and especially much more reliable classification results in terms of both sensitivity and specificity than the exiting pre mirna classification methods when validated with non human animal pre mirnas and virus pre mirnas from mirbase micropred resulted in and recognition respectively
pnas according to thomas hobbes touchstone new york english ed the life of man is solitary poor nasty brutish and short and it would need powerful social institutions to establish social order in reality however social cooperation can also arise spontaneously based on local interactions rather than centralized control the self organization of cooperative behavior is particularly puzzling for social dilemmas related to sharing natural resources or creating common goods such situations are often described by the prisoner s dilemma here we report the sudden outbreak of predominant cooperation in a noisy world dominated by selfishness and defection when individuals imitate superior strategies and show success driven migration in our model individuals are unrelated and do not inherit behavioral traits they defect or cooperate selfishly when the opportunity arises and they do not know how often they will interact or have interacted with someone else moreover our individuals have no reputation mechanism to form friendship networks nor do they have the option of voluntary interaction or costly punishment therefore the outbreak of prevailing cooperation when directed motion is integrated in a game theoretical model is remarkable particularly when random strategy mutations and random relocations challenge the formation and survival of cooperative clusters our results suggest that mobility is significant for the evolution of social order and essential for its stabilization maintenance
at the foundation of amazon s cloud computing are infrastructure services such as amazon s simple storage service simpledb and elastic compute cloud that provide the resources for constructing internet scale computing platforms and a great variety of applications the requirements placed on these infrastructure services are very strict they need to score high marks in the areas of security scalability availability performance and cost effectiveness and they need to meet these requirements while serving millions of customers around the continuously
clustering is ubiquitously applied in bioinformatics with hierarchical clustering and k means partitioning being the most popular methods numerous improvements of these two clustering methods have been introduced as well as completely different approaches such as grid based density based and model based clustering for improved bioinformatics analysis of data it is important to match clusterings to the requirements of a biomedical application in this article we present a set of desirable clustering features that are used as evaluation criteria for clustering algorithms we review different clustering algorithms of all approaches and datatypes we compare algorithms on the basis of desirable clustering features and outline algorithms benefits and drawbacks as a basis for matching them to biomedical bib
bib the unanimous agreement that cellular processes are largely governed by interactions between proteins has led to enormous community efforts culminating in overwhelming information relating to these proteins to the regulation of their interactions to the way in which they interact and to the function which is determined by these interactions these data have been organized in databases and servers however to make these really useful it is essential not only to be aware of these but in particular to have a working knowledge of which tools to use for a given problem what are the tool advantages and drawbacks and no less important how to combine these for a particular goal since usually it is not one tool but some combination of tool modules that is needed this is the goal of review
abstract background many difficult problems in evolutionary genomics are related to mutations that have weak effects on fitness as the consequences of mutations with large effects are often simple to predict current systems biology has accumulated much data on mutations with large effects and can predict the properties of knockout mutants in some systems however experimental methods are too insensitive to observe small effects results here i propose a novel framework that brings together evolutionary theory and current systems biology approaches in order to quantify small effects of mutations and their epistatic interactions in silico central to this approach is the definition of fitness correlates that can be computed in some current systems biology models employing the rigorous algorithms that are at the core of much work in computational systems biology the framework exploits synergies between the realism of such models and the need to understand real systems in evolutionary theory this framework can address many longstanding topics in evolutionary biology by defining various levels of the adaptive landscape addressed topics include the distribution of mutational effects on fitness as well as the nature of advantageous mutations epistasis and robustness combining corresponding parameter estimates with population genetics models raises the possibility of testing evolutionary hypotheses at a new level of realism conclusions evosysbio is expected to lead to a more detailed understanding of the fundamental principles of life by combining knowledge about well known biological systems from several disciplines this will benefit both evolutionary theory and current systems biology understanding robustness by analysing distributions of mutational effects and epistasis is pivotal for drug design cancer research responsible genetic engineering in synthetic biology and many other applications
background new rapid high throughput sequencing technologies have sparked the creation of a new class of assembler since all high throughput sequencing platforms incorporate errors in their output short read assemblers must be designed to account for this error while utilizing all available data results we have designed and implemented an assembler quality value guided short read assembler created to take advantage of quality value scores as a further method of dealing with error compared to previous published algorithms our assembler shows significant improvements not only in speed but also in output quality conclusion qsra generally produced the highest genomic coverage while being faster than vcake qsra is extremely competitive in its longest contig and contig lengths producing results of similar quality to those of edena and velvet qsra provides a step closer to the goal of de novo assembly of complex genomes improving upon the original vcake algorithm by not only drastically reducing runtimes but also increasing the viability of the assembly algorithm through further error capabilities
this article presents a novel scale and rotation invariant detector and descriptor coined surf speeded up robust features surf approximates or even outperforms previously proposed schemes with respect to repeatability distinctiveness and robustness yet can be computed and compared much faster this is achieved by relying on integral images for image convolutions by building on the strengths of the leading existing detectors and descriptors specifically using a hessian matrix based measure for the detector and a distribution based descriptor and by simplifying these methods to the essential this leads to a combination of novel detection description and matching steps the paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters we conclude the article with surfs application to two challenging yet converse goals camera calibration as a special case of image registration and object recognition our experiments underline surfs usefulness in a broad range of topics in vision
over the past years a range of methods have been developed that are able to learn human like estimates of the semantic relatedness between terms from the way in which these terms are distributed in a corpus of unannotated natural language text these methods have also been evaluated in a number of applications in the cognitive science computational linguistics and the information retrieval literatures in this paper we review the available methodologies for derivation of semantic relatedness from free text as well as their evaluation in a variety of biomedical and other applications recent methodological developments and their applicability to several existing applications are discussed
cells respond to stimuli by changes in various processes including signaling pathways and gene expression efforts to identify components of these responses increasingly depend on mrna profiling and genetic library screens by comparing the results of these two assays across various stimuli we found that genetic screens tend to identify response regulators whereas mrna profiling frequently detects metabolic responses we developed an integrative approach that bridges the gap between these data using known molecular interactions thus highlighting major response pathways we used this approach to reveal cellular pathways responding to the toxicity of alpha synuclein a protein implicated in several neurodegenerative disorders including parkinson s disease for this we screened an established yeast model to identify genes that when overexpressed alter alpha synuclein toxicity bridging these data and data from mrna profiling provided functional explanations for many of these genes and identified previously unknown relations between alpha synuclein toxicity and basic pathways
determining the genetic architecture of complex traits is challenging because phenotypic variation arises from interactions between multiple environmentally sensitive alleles we quantified genome wide transcript abundance and phenotypes for six ecologically relevant traits in d melanogaster wild derived inbred lines we observed genetically variable transcripts and high heritabilities for all organismal phenotypes the transcriptome is highly genetically intercorrelated forming transcriptional modules modules are enriched for transcripts in common pathways gene ontology categories tissue specific expression and transcription factor binding sites the high degree of transcriptional connectivity allows us to infer genetic networks and the function of predicted genes from annotations of other genes in the network regressions of organismal phenotypes on transcript abundance implicate several hundred candidate genes that form modules of biologically meaningful correlated transcripts affecting each phenotype overlapping transcripts in modules associated with different traits provide insight into the molecular basis of pleiotropy between traits
summary the modeling tool promot facilitates the efficient and comprehensible setup and editing of modular models coupled with customizable visual representations since its last major publication in promot has gained new functionality in particular support of logical models efficient editing visual exploration model validation and support for sbml availability promot is an open source project and freely available at http www mpi magdeburg mpg de projects promot contact mirschel mpi magdeburg mpg de mirschel mpi magdeburg mpg de supplementary information supplementary data are available at bioinformatics bioinformatics
in a wide variety of organisms synonymous codons are used with different frequencies a phenomenon known as codon bias population genetic studies have shown that synonymous sites are under weak selection and that codon bias is maintained by a balance between selection mutation and genetic drift it appears that the major cause for selection on codon bias is that certain preferred codons are translated more accurately and or efficiently however additional and sometimes maybe even contradictory selective forces appear to affect codon usage as well in this review we discuss the current understanding of the ways in which natural selection participates in the creation and maintenance of codon bias we also raise several open questions i is natural selection weak independently of the level of codon bias it is possible that selection for preferred codons is weak only when codon bias approaches equilibrium and may be quite strong on genes with codon bias levels that are much lower and or above equilibrium ii what determines the identity of the major codons iii how do shifts in codon bias occur iv what is the exact nature of selection on codon bias we discuss these questions in depth and offer some ideas on how they can be addressed using a combination of computational and analyses
over the last decade approximately nucleotide rna molecules have emerged as critical regulators in the expression and function of eukaryotic genomes two primary categories of these small rnas short interfering rnas sirnas and micrornas mirnas act in both somatic and germline lineages in a broad range of eukaryotic species to regulate endogenous genes and to defend the genome from invasive nucleic acids recent advances have revealed unexpected diversity in their biogenesis pathways and the regulatory mechanisms that they access our understanding of sirna and mirna based regulation has direct implications for fundamental biology as well as disease etiology treatment
human genomic data of many types are readily available but the complexity and scale of human molecular biology make it difficult to integrate this body of data understand it from a systems level and apply it to the study of specific pathways or genetic disorders an investigator could best explore a particular protein pathway or disease if given a functional map summarizing the data and interactions most relevant to his or her area of interest using a regularized bayesian integration system we provide maps of functional activity and interaction networks in over areas of human cellular biology each including information from genome scale experiments pertaining to human genes key to these analyses is the ability to efficiently summarize this large data collection from a variety of biologically informative perspectives prediction of protein function and functional modules cross talk among biological processes and association of novel genes and pathways with known genetic disorders in addition to providing maps of each of these areas we also identify biological processes active in each data set experimental investigation of five specific genes and has confirmed novel roles for these proteins in the proper initiation of macroautophagy in amino acid starved human fibroblasts our functional maps can be explored using hefalmp human experimental functional mapper a web interface allowing interactive visualization and investigation of this large body information
abstract background although most of the current disease candidate gene identification and prioritization methods depend on functional annotations the coverage of the gene functional annotations is a limiting factor in the current study we describe a candidate gene prioritization method that is entirely based on protein protein interaction network ppin analyses results for the first time extended versions of the pagerank and hits algorithms and the k step markov method are applied to prioritize disease candidate genes in a training test schema using a list of known disease related genes from our earlier study as a training set seeds and the rest of the known genes as a test list we perform large scale cross validation to rank the candidate genes and also evaluate and compare the performance of our approach under appropriate settings for example a back probability of for pagerank with priors and hits with priors and step size for k step markov method the three methods achieved a comparable auc value suggesting a similar performance conclusions even though network based methods are generally not as effective as integrated functional annotation based methods for disease candidate gene prioritization in a one to one comparison ppin based candidate gene prioritization performs better than all other gene features or annotations additionally we demonstrate that methods used for studying both social and web networks can be successfully used for disease candidate prioritization
we have developed cluego an easy to use cytoscape plug in that strongly improves biological interpretation of large lists of genes cluego integrates gene ontology go terms as well as kegg biocarta pathways and creates a functionally organized go pathway term network it can analyze one or compare two lists of genes and comprehensively visualizes functionally grouped terms a one click update option allows cluego to automatically download the most recent go kegg release at any time cluego provides an intuitive representation of the analysis results and can be optionally used in conjunction with the golorize in
trust and betrayal of trust are ubiquitous in human societies recent behavioral evidence shows that the neuropeptide oxytocin increases trust among humans thus offering a unique chance of gaining a deeper understanding of the neural mechanisms underlying trust and the adaptation to breach of trust we examined the neural circuitry of trusting behavior by combining the intranasal double blind administration of oxytocin with fmri we find that subjects in the oxytocin group show no change in their trusting behavior after they learned that their trust had been breached several times while subjects receiving placebo decrease their trust this difference in trust adaptation is associated with a specific reduction in activation in the amygdala the midbrain regions and the dorsal striatum in subjects receiving oxytocin suggesting that neural systems mediating fear processing amygdala and midbrain regions and behavioral adaptations to feedback information dorsal striatum modulate oxytocin s effect on trust these findings may help to develop deeper insights into mental disorders such as social phobia and autism which are characterized by persistent fear or avoidance of interactions
summary the development of rna sequencing rna seq makes it possible for us to measure transcription at an unprecedented precision and throughput however challenges remain in understanding the source and distribution of the reads modeling the transcript abundance and developing efficient computational methods in this article we develop a method to deal with the isoform expression estimation problem the count of reads falling into a locus on the genome annotated with multiple isoforms is modeled as a poisson variable the expression of each individual isoform is estimated by solving a convex optimization problem and statistical inferences about the parameters are obtained from the posterior distribution by importance sampling our results show that isoform expression inference in rna seq is possible by employing appropriate statistical methods contact whwong stanford edusupplementary information supplementary data are available at online
word sense disambiguation wsd is the ability to identify the meaning of words in context in a computational manner wsd is considered an ai complete problem that is a task whose solution is at least as hard as the most difficult problems in artificial intelligence we introduce the reader to the motivations for solving the ambiguity of words and provide a description of the task we overview supervised unsupervised and knowledge based approaches the assessment of wsd systems is discussed in the context of the senseval semeval campaigns aiming at the objective evaluation of systems participating in several different disambiguation tasks finally applications open problems and future directions discussed
life expectancy has increased dramatically in the united states and in much of the world in recent years and decades the factors underlying this increase are incompletely understood and are undoubtedly complex a question that drives current research is whether life expectancy can be further extended using current knowledge of modifiable risk factors a still more challenging research focus is on the possibility that life expectancy might be further increased through knowledge gained from studies of the basic biology of aging and its genetic and modifiers
background both genetic and environmental factors contribute to human diseases most common diseases are influenced by a large number of genetic and environmental factors most of which individually have only a modest effect on the disease though genetic contributions are relatively well characterized for some monogenetic diseases there has been no effort at curating the extensive list of environmental etiological factors results from a comprehensive search of the mesh annotation of medline articles we identified environmental etiological factors associated with diseases we also identified genes associated with complex diseases from the nih genetic association database gad a database of genetic association studies diseases have both genetic and environmental etiological factors available integrating genetic and environmental factors results in the etiome which we define as the comprehensive compendium of disease etiology clustering of environmental factors may alert clinicians of the risks of added exposures or synergy in interventions to alter these factors clustering of both genetic and environmental etiological factors puts genes in the context of environment in a quantitative manner conclusion in this paper we obtained a comprehensive list of associations between disease and environmental factors using mesh annotation of medline articles it serves as a summary of current knowledge between etiological factors and diseases by combining the environmental etiological factors and genetic factors from gad we computed the etiome profile for diseases comparing diseases across these profiles may have utility for clinical medicine basic science research and population science
gr no single experimental method can discover all connections in the interactome a computational approach can help by integrating data from multiple often unrelated proteomics and genomics pipelines reconstructing global networks of functional coupling fc faces the challenges of scale and heterogeneityhow to efficiently integrate huge amounts of diverse data from multiple organisms yet ensuring high accuracy we developed funcoup an optimized bayesian framework to resolve these issues because interactomes comprise functional coupling of many types funcoup annotates network edges with confidence scores in support of different kinds of interactions physical interaction protein complex member metabolic or signaling link this capability boosted overall accuracy on the whole the constructed framework was comprehensively tested to optimize the overall confidence and ensure seamless automated incorporation of new data sets of heterogeneous types using over data sets in seven organisms and extensively transferring information between orthologs funcoup predicted global networks in eight eukaryotes for the ciona intestinalis network only orthologous information was used and it recovered a significant number of experimental facts funcoup predictions were validated on independent cancer mutation data we show how funcoup can be used for discovering candidate members of the parkinson and alzheimer pathways cross species pathway conservation analysis provided further support to observations
widespread adoption of massively parallel deoxyribonucleic acid dna sequencing instruments has prompted the recent development of de novo short read assembly algorithms a common shortcoming of the available tools is their inability to efficiently assemble vast amounts of data generated from large scale sequencing projects such as the sequencing of individual human genomes to catalog natural genetic variation to address this limitation we developed abyss assembly by short sequences a parallelized sequence assembler as a demonstration of the capability of our software we assembled billion paired end reads from the genome of an african male publicly released by illumina inc approximately million contigs base pairs bp in length were created with an size of bp representing of the reference human genome analysis of these contigs identified polymorphic and novel sequences not present in the human reference assembly which were validated by alignment to alternate human assemblies and to other genomes
mapreduce is emerging as an important programming model for large scale data parallel applications such as web indexing data mining and scientific simulation hadoop is an open source implementation of mapreduce enjoying wide adoption and is often used for short jobs where low response time is critical hadoops performance is closely tied to its task scheduler which implicitly assumes that cluster nodes are homogeneous and tasks make progress linearly and uses these assumptions to decide when to speculatively re execute tasks that appear to be stragglers in practice the homogeneity assumptions do not always hold an especially compelling setting where this occurs is a virtualized data center such as amazons elastic compute cloud we show that hadoops scheduler can cause severe performance degradation in heterogeneous environments we design a new scheduling algorithm longest approximate time to end late that is highly robust to heterogeneity late can improve hadoop response times by a factor of in clusters of virtual on
background metagenomics is a rapidly growing field of research that aims at studying uncultured organisms to understand the true diversity of microbes their functions cooperation and evolution in environments such as soil water ancient remains of animals or the digestive system of animals and humans the recent development of ultra high throughput sequencing technologies which do not require cloning or pcr amplification and can produce huge numbers of dna reads at an affordable cost has boosted the number and scope of metagenomic sequencing projects increasingly there is a need for new ways of comparing multiple metagenomics datasets and for fast and user friendly implementations of such approaches results this paper introduces a number of new methods for interactively exploring analyzing and comparing multiple metagenomic datasets which will be made freely available in a new comparative version of the stand alone metagenome analysis tool megan conclusion there is a great need for powerful and user friendly tools for comparative analysis of metagenomic data and megan will help to fill gap
abstract background r is the leading open source statistics software with a vast number of biostatistical and bioinformatical analysis packages to exploit the advantages of r extensive scripting programming skills are required results we have developed a software tool called r gui generator rgg which enables the easy generation of graphical user interfaces guis for the programming language r by adding a few extensible markup language xml tags rgg consists of an xml based gui definition language and a java based gui engine guis are generated in runtime from defined gui tags that are embedded into the r script user gui input is returned to the r code and replaces the xml tags rgg files can be developed using any text editor the current version of rgg is available as a stand alone software rggrunner and as a plug in for jgr conclusions rgg is a general gui framework for r that has the potential to introduce r statistics r packages built in functions and scripts to users with limited programming skills and helps to bridge the gap between r developers and gui dependent users rgg aims to abstract the gui development from individual gui toolkits by using an xml based gui definition language thus rgg can be easily integrated in any software the rgg project further includes the development of a web based repository for rgg guis rgg is an open source project licensed under the lesser general public license lgpl and can be downloaded freely at http rgg r forge r org
micrornas are important regulators of gene expression that control both physiological and pathological processes such as development and cancer although their mode of action has attracted great attention the principles governing their expression and activity are only beginning to emerge recent studies have introduced a paradigm shift in our understanding of the microrna biogenesis pathway which was previously believed to be universal to all micrornas maturation steps specific to individual micrornas have been uncovered and these offer a plethora of regulatory options after transcription with multiple proteins affecting microrna processing efficiency here we review the recent advances in knowledge of the microrna biosynthesis pathways and discuss their impact on post transcriptional microrna regulation during development
youtube has become the most successful internet website providing a new generation of short video sharing service since its establishment in early youtube has a great impact on internet traffic nowadays yet itself is suffering from a severe problem of scalability therefore understanding the characteristics of youtube and similar sites is essential to network traffic engineering and to their sustainable development to this end we have crawled the youtube site for four months collecting more than million youtube videos data in this paper we present a systematic and in depth measurement study on the statistics of youtube videos we have found that youtube videos have noticeably different statistics compared to traditional streaming videos ranging from length and access pattern to their growth trend and active life span we investigate the social networking in youtube videos as this is a key driving force toward its success in particular we find that the links to related videos generated by uploaders choices have clear small world characteristics this indicates that the videos have strong correlations with each other and creates opportunities for developing novel techniques to enhance the quality
this paper presents a comparative evaluation of google scholar and other bibliographic databases academic search elite ageline articlefirst econlit geobase medline pais international popline social sciences abstracts social sciences citation index and socindex focusing on search performance within the multidisciplinary field of later life migration the results of simple keyword searches are evaluated with reference to a set of relevant articles identified in advance in terms of both recall and precision google scholar performs better than most of the subscription databases this finding based on a rigorous evaluation procedure is contrary to the impressions of many early reviewers the paper concludes with a discussion of a new approach to document relevance in educational settings an approach that accounts for the instructors goals as well as the students assessments of relevance contains tables figures notes
allostery has come of age the number breadth and functional roles of documented protein allostery cases are rising quickly since all dynamic proteins are potentially allosteric and allostery plays crucial roles in all cellular pathways sorting and classifying allosteric mechanisms in proteins should be extremely useful in understanding and predicting how the signals are regulated and transmitted through the dynamic multi molecular cellular organizations classification organizes the complex information thereby unraveling relationships and patterns in molecular activation and repression in signaling current classification schemes consider classes of molecules according to their functions for example epinephrine and norepinephrine secreted by the central nervous system are classified as neurotransmitters other schemes would account for epinephrine when secreted by the adrenal medulla to be hormone like yet such classifications account for the global function of the molecule not for the molecular mechanism of how the signal transmission initiates and how it is transmitted here we provide a unified view of allostery and the first classification framework we expect that a classification scheme would assist in comprehension of allosteric mechanisms in prediction of signaling on the molecular level in better comprehension of pathways and regulation of the complex signals in translating them to the cascading events and in allosteric drug design we further provide a range of examples illustrating mechanisms in protein allostery and their classification from the cellular standpoint
pnas protein functions require conformational motions we show here that the dominant conformational motions are slaved by the hydration shell and the bulk solvent the protein contributes the structure necessary for function we formulate a model that is based on experiments insights from the physics of glass forming liquids and the concepts of a hierarchically organized energy landscape to explore the effect of external fluctuations on protein dynamics we measure the fluctuations in the bulk solvent and the hydration shell with broadband dielectric spectroscopy and compare them with internal fluctuations measured with the mssbauer effect and neutron scattering the result is clear large scale protein motions are slaved to the fluctuations in the bulk solvent they are controlled by the solvent viscosity and are absent in a solid environment internal protein motions are slaved to the beta fluctuations of the hydration shell are controlled by hydration and are absent in a dehydrated protein the model quantitatively predicts the rapid increase of the mean square displacement above k shows that the external beta fluctuations determine the temperature and time dependence of the passage of carbon monoxide through myoglobin and explains the nonexponential time dependence of the protein relaxation photodissociation
the nematode caenorhabditis elegans is a popular model system in genetics not least because a majority of human disease genes are conserved in c elegans to generate a comprehensive inventory of its expressed proteome we performed extensive shotgun proteomics and identified more than half of all predicted c elegans proteins this allowed us to confirm and extend genome annotations characterize the role of operons in c elegans and semiquantitatively infer abundance levels for thousands of proteins furthermore for the first time to our knowledge we were able to compare two animal proteomes c elegans and drosophila melanogaster we found that the abundances of orthologous proteins in metazoans correlate remarkably well better than protein abundance versus transcript abundance within each organism or transcript abundances across organisms this suggests that changes in transcript abundance may have been partially offset during evolution by opposing changes in abundance
we present a practical and statistically consistent scheme for actively learning binary classifiers under general loss functions our algorithm uses importance weighting to correct sampling bias and by controlling the variance we are able to give rigorous label complexity bounds for the learning process experiments on passively labeled data show that this approach reduces the label complexity required to achieve good predictive performance on many problems
this is the written version of a talk given in memory of gunnar k a ll e n at the departments of theoretical physics physics and astronomy of lund university on february it will be published in a collection of the papers of gunnar k a ll e n edited by c jarlskog and a c t wu i discuss some of k a ll e n s work especially regarding the problem of infinities in quantum field theory and recount my own interactions with him in addition i describe for non specialists the current status of the problem and present my personal view on how it may be resolved in future
this study surveyed valparaiso university freshmen to discover their feelings about librarians using facebook and myspace as outreach tools the vast majority of respondents had online social network profiles most indicated that they would be accepting of library contact through those web sites but a sizable minority reacted negatively to the concept because of the potential to infringe on students sense of personal privacy it is recommended that librarians proceed with caution when implementing online social profiles
bowtie is an ultrafast memory efficient alignment program for aligning short dna sequence reads to large genomes for the human genome burrows wheeler indexing allows bowtie to align more than million reads per cpu hour with a memory footprint of approximately gigabytes bowtie extends previous burrows wheeler techniques with a novel quality aware backtracking algorithm that permits mismatches multiple processor cores can be used simultaneously to achieve even greater alignment speeds bowtie is open source and available at http cbcb bowtie edu
this paper explores the use of cloud computing for scientific workflows focusing on a widely used astronomy application montage the approach is to evaluate from the point of view of a scientific workflow the tradeoffs between running in a local environment if such is available and running in a virtual environment via remote wide area network resource access our results show that for montage a workflow with short job runtimes the virtual environment can provide good compute time performance but it can suffer from resource scheduling delays and communications
p the identification of regulatory sequences in animal genomes remains a significant challenge comparative genomic methods that use patterns of evolutionary conservation to identify non coding sequences with regulatory function have yielded many new vertebrate enhancers however these methods have not contributed significantly to the identification of regulatory sequences in sequenced invertebrate taxa we demonstrate here that this differential success which is often attributed to fundamental differences in the nature of vertebrate and invertebrate regulatory sequences is instead primarily a product of the relatively small size of sequenced invertebrate genomes we sequenced and compared loci involved in early embryonic patterning from four species of true fruit flies family tephritidae that have genomes four to six times larger than those of italic drosophila melanogaster italic unlike in italic drosophila italic where virtually all non coding dna is highly conserved blocks of conserved non coding sequence in tephritids are flanked by large stretches of poorly conserved sequence similar to what is observed in vertebrate genomes we tested the activities of nine conserved non coding sequences flanking the italic even skipped italic gene of the teprhitid italic ceratis capitata italic in transgenic italic d melanogaster italic embryos six of which drove patterns that recapitulate those of known italic d melanogaster italic enhancers in contrast none of the three non conserved tephritid non coding sequences that we tested drove expression in italic d melanogaster italic embryos based on the landscape of non coding conservation in tephritids and our initial success in using conservation in tephritids to identify italic d melanogaster italic regulatory sequences we suggest that comparison of tephritid genomes may provide a systematic means to annotate the non coding portion of the italic d melanogaster italic genome we also propose that large genomes be given more consideration in the selection of species for comparative genomics projects to provide increased power to detect functional non coding dnas and to provide a less biased view of the evolution and function of animal p
in the last few years we have witnessed the emergence primarily in on line communities of new types of social networks that require for their representation more complex graph structures than have been employed in the past one example is the folksonomy a tripartite structure of users resources and tagslabels collaboratively applied by the users to the resources in order to impart meaningful structure on an otherwise undierentiated database here we propose a mathematical model of such tripartite structures which represents them as random hypergraphs we show that it is possible to calculate many properties of this model exactly in the limit of large network size and we compare the results against observations of a real folksonomy that of the on line photography web site flickr we show that in some cases the model matches the properties of the observed network well while in others there are signicant dierences which we nd to be attributable to the practice of multiple tagging i e the application by a single user of many tags to one resource or one tag to resources
a decision view provides a useful complement to the traditional sets of architectural views and viewpoints it gives an explanatory perspective that illuminates the reasoning process itself and not solely its results the decision view documents aspects of the architecture that are hard to reverse engineer from the software itself and that are often left tacit the decision view and the decisions that it captures embody high level architectural knowledge that can be transferred to other practitioners and merged when systems are merged and they offer useful support for maintaining large long lived software intensive systems this article leads readers through a succession of epiphanies from design to architecture then architecture representation to architecture design methods and finally to architectural decisions
background currently a lack of consensus exists on how best to perform and interpret quantitative real time pcr qpcr experiments the problem is exacerbated by a lack of sufficient experimental detail in many publications which impedes a reader s ability to evaluate critically the quality of the results presented or to repeat the experiments content the minimum information for publication of quantitative real time pcr experiments miqe guidelines target the reliability of results to help ensure the integrity of the scientific literature promote consistency between laboratories and increase experimental transparency miqe is a set of guidelines that describe the minimum information necessary for evaluating qpcr experiments included is a checklist to accompany the initial submission of a manuscript to the publisher by providing all relevant experimental conditions and assay characteristics reviewers can assess the validity of the protocols used full disclosure of all reagents sequences and analysis methods is necessary to enable other investigators to reproduce results miqe details should be published either in abbreviated form or as an online supplement summary following these guidelines will encourage better experimental practice allowing more reliable and unequivocal interpretation of results
background pharmacogenomics studies the relationship between genetic variation and the variation in drug response phenotypes the field is rapidly gaining importance it promises drugs targeted to particular subpopulations based on genetic background the pharmacogenomics literature has expanded rapidly but is dispersed in many journals it is challenging therefore to identify important associations between drugs and molecular entities particularly genes and gene variants and thus these critical connections are often lost text mining techniques can allow us to convert the free style text to a computable searchable format in which pharmacogenomic concepts such as genes drugs polymorphisms and diseases are identified and important links between these concepts are recorded availability of full text articles as input into text mining engines is key as literature abstracts often do not contain sufficient information to identify these pharmacogenomic associations results thus building on a tool called textpresso we have created the pharmspresso tool to assist in identifying important pharmacogenomic facts in full text articles pharmspresso parses text to find references to human genes polymorphisms drugs and diseases and their relationships it presents these as a series of marked up text fragments in which key concepts are visually highlighted to evaluate pharmspresso we used a gold standard of human curated articles pharmspresso identified and of target gene polymorphism and drug concepts respectively conclusion pharmspresso is a text analysis tool that extracts pharmacogenomic concepts from the literature automatically and thus captures our current understanding of gene drug interactions in a computable form we have made pharmspresso available at http pharmspresso edu
in this paper we review the extant innovation research from three fieldseconomics organizational sociology and technology managementin order to find points at which the fields approaches and assumptions overlap by comparing research methods and approaches along three dimensions stage of adoption level of analysis and type of innovation we found firstly that studies from the three fields can be re mapped into five more specific groups we then illustrate how research from different groups can be cross fertilized to help management of innovation in organizations the paper suggests that knowing the ways in which different groups of studies differ from each other may lead to a more accurate understanding of the relative value of innovation research from each group for both theorists managers
motivation textual data compression and the associated techniques coming from information theory are often perceived as being of interest for data communication and storage however they are also deeply related to classification and data mining and analysis in recent years a substantial effort has been made for the application of textual data compression techniques to various computational biology tasks ranging from storage and indexing of large datasets to comparison and reverse engineering of biological networks results the main focus of this review is on a systematic presentation of the key areas of bioinformatics and computational biology where compression has been used when possible a unifying organization of the main ideas and techniques is also provided availability it goes without saying that most of the research results reviewed here offer software prototypes to the bioinformatics community the supplementary material provides pointers to software and benchmark datasets for a range of applications of broad interest in addition to provide reference to software the supplementary material also gives a brief presentation of some fundamental results and techniques related to this paper it is at http www math unipa it raffaele suppmaterial compreview contact raffaele math unipa bioinformatics
the brisk discovery of novel inherited disease markers by genome wide association gwa studies has raised expectations for predicting disease risk by analysing multiple common alleles however the statistics used during the discovery phase of research such as odds ratios or p values for association are not the most appropriate measures for evaluating the predictive value of genetic profiles we argue that other measures such as sensitivity specificity and positive and negative predictive values are more useful when proposing a genetic profile for prediction
abstract background within research each experiment is different the focus changes and the data is generated from a continually evolving barrage of technologies there is a continual introduction of new techniques whose usage ranges from in house protocols through to high throughput instrumentation to support these requirements data management systems are needed that can be rapidly built and readily adapted for new usage results the adaptable data management system discussed is designed to support the seamless mining and analysis of biological experiment data that is commonly used in systems biology e g chip chip gene expression proteomics imaging flow cytometry we use different content graphs to represent different views upon the data these views are designed for different roles equipment specific views are used to gather instrumentation information data processing oriented views are provided to enable the rapid development of analysis applications and research project specific views are used to organize information for individual research experiments this management system allows for both the rapid introduction of new types of information and the evolution of the knowledge it represents conclusion data management is an important aspect of any research enterprise it is the foundation on which most applications are built and must be easily extended to serve new functionality for new scientific areas we have found that adopting a three tier architecture for data management built around distributed standardized content repositories allows us to rapidly develop new applications to support a diverse community
pnas although cancer types differ substantially many cancers share common gene expression signatures consistent with this observation we find convergent and representative distributions and correlation vectors that are distinct in cancer and noncancer ensembles these differences originate in many genes but comparatively few genes account for the major differences we identify genes with different combinatorial regulation in cancer and noncancer as indicated by significant differences in their correlation vectors among the identified genes are many established oncogenes and apoptotic genes such as members of the bcl the mapk and the ras families and new candidate oncogenes our findings expand and complement the tumorigenic role of up and down regulation of these genes by emphasizing cancer specific changes in their couplings and correlation patterns at genome wide level that are independent from their mean levels of expression in cancer cells given the central role of these genes in defining the cancerous state it may be worth investigating them and the differences in their combinatorial regulation for developing wide spectrum drugs
the discovery that the cosmic expansion is accelerating has been followed by an intense theoretical and experimental response in physics and astronomy the discovery implies that our most basic notions about how gravity works are violated on cosmological distance scales a simple fix is to introduce a cosmological constant into the field equations for general relativity however the extremely small value of the cosmological constant relative to theoretical expectations has led theorists to explore numerous alternative explanations that involve the introduction of an exotic negative pressure fluid or a modification of general relativity here we review the evidence for cosmic acceleration we then survey some of the theoretical attempts to account for it including the cosmological constant quintessence and its variants mass varying neutrinos and modifications of general relativity we discuss experimental and observational tests that may allow us to distinguish among some of the theoretical ideas that have proposed
variation in gene expression is an essential material for biological diversity among single cells individuals and populations or species here we show that expression variability is an intrinsic property that persists at those different levels each promoter seems to have a unique capacity to respond to external signals that can be environmental genetic or even stochastic our investigation into nucleosome organization of variably responding promoters revealed a commonly positioned nucleosome at a critical regulatory region where most transcription start sites and tata elements are located a deviation from typical nucleosome free status the nucleotide sequences in this region of variable promoters showed a high propensity for dna bending and a periodic distribution of particular dinucleotides encoding preferences for dna nucleosome interaction variable expression is likely to occur during removal of this nucleosome for gene activation this is a unique example of how promoter sequences intrinsically encode regulatory flexibility which is vital for biological processes such as adaptation development evolution
eukaryotic transcription occurs within a chromatin environment whose organization has an important regulatory function and is partly encoded in cis by the dna sequence itself here we examine whether evolutionary changes in gene expression are linked to changes in the dna encoded nucleosome organization of promoters we find that in aerobic yeast species where cellular respiration genes are active under typical growth conditions the promoter sequences of these genes encode a relatively open nucleosome depleted chromatin organization this nucleosome depleted organization requires only dna sequence information is independent of any cofactors and of transcription and is a general property of growth related genes in contrast in anaerobic yeast species where cellular respiration genes are relatively inactive under typical growth conditions respiration gene promoters encode relatively closed nucleosome occupied chromatin organizations our results suggest a previously unidentified genetic mechanism underlying phenotypic diversity consisting of dna sequence changes that directly alter the dna encoded nucleosome organization promoters
motivation the biological community s reliance on computational annotations of protein function makes correct assessment of function prediction methods an issue of great importance the fact that a large fraction of the annotations in current biological databases are based on computational methods can lead to bias in estimating the accuracy of function prediction methods this can happen since predicting an annotation that was derived computationally in the first place is likely easier than predicting annotations that were derived experimentally leading to over optimistic classifier performance estimates results we illustrate this phenomenon in a set of controlled experiments using a nearest neighbor classifier that uses psi blast similarity scores our results demonstrate that the source of gene ontology go annotations used to assess a protein function predictor can have a highly significant influence on classifier accuracy the average accuracy over four species and over go terms in the biological process namespace increased from to when the classifier was given access to annotations that are assigned evidence codes that indicate a possible computational source instead of experimentally determined annotations slightly smaller increases were observed in the other namespaces in these comparisons the total number of annotations and their distribution across go terms were kept the same conclusion in conclusion taking into account go evidence codes is required for reporting accuracy statistics that do not overestimate a model s performance and is of particular importance for a fair comparison of classifiers that rely on different information sources contact rogersma cs colostate edu supplementary information supplementary data are available at bioinformatics bioinformatics
reversible protein phosphorylation is a signaling mechanism involved in all cellular processes to create a systems view of the signaling apparatus in budding yeast we generated an epistatic miniarray profile e map comprised of pairwise quantitative genetic interactions including virtually all protein and small molecule kinases and phosphatases as well as key cellular regulators quantitative genetic interaction mapping reveals factors working in compensatory pathways negative genetic interactions or those operating in linear pathways positive genetic interactions we found an enrichment of positive genetic interactions between kinases phosphatases and their substrates in addition we assembled a higher order map from sets of three genes that display strong interactions with one another triplets enriched for functional connectivity the resulting network view provides insights into signaling pathway regulation and reveals a link between the cell cycle kinase the map kinase and a pathway that regulates chromatin integrity during transcription by rna ii
motivation gene class testing gct or gene set analysis gsa is a statistical approach to determine whether some functionally predefined sets of genes express differently under different experimental conditions shortcomings of the fisher s exact test for the overrepresentation analysis are illustrated by an example most alternative gsa methods are developed for data collected from two experimental conditions and most is based on a univariate gene by gene test statistic or assume independence among genes in the gene set a multivariate analysis of variance manova approach is proposed for studies with two or more experimental conditions results when the number of genes in the gene set is greater than the number of samples the sample covariance matrix is singular and ill condition the use of standard multivariate methods can result in biases in the analysis the proposed manova test uses a shrinkage covariance matrix estimator for the sample covariance matrix the manova test and six other gsa published methods principal component analysis sam gs analysis of covariance global gsea and maxmean are evaluated using simulation the manova test appears to perform the best in terms of control of type i error and power under the models considered in the simulation several publicly available microarray datasets under two and three experimental conditions are analyzed for illustrations of gsa most methods except for gsea and maxmean generally are comparable in terms of power of identification of significant gene sets availability a free r code to perform manova test is available at http mail cmu edu tw catsai research htm supplementary information supplementary data are available at online
libraries all over the world are undergoing fundamental paradigm shifts in the way they see their users and in how they offer their services the thrust is on exploiting the internet and in particular web applications to engage users not only in developing new library services but also building a community this paper investigates the prevalence and use of web applications of websites of libraries from north america europe and asia the findings reveal that all three categories of web applications namely those that support information push pull retrieval and exchange have been adopted in libraries across the three regions with libraries in north america leading their european and asian counterparts the ways in which individual web applications have been used are also detailed springer heidelberg
background dna copy number variation cnv has been recognized as an important source of genetic variation array comparative genomic hybridization acgh is commonly used for cnv detection but the microarray platform has a number of inherent limitations results here we describe a method to detect copy number variation using shotgun sequencing cnv seq the method is based on a robust statistical model that describes the complete analysis procedure and allows the computation of essential confidence values for detection of cnv our results show that the number of reads not the length of the reads is the key factor determining the resolution of detection this favors the next generation sequencing methods that rapidly produce large amount of short reads conclusion simulation of various sequencing methods with coverage between to show overall specificity between and sensitivity between we also show the results for assessment of cnv between two individual genomes
social tagging provides valuable and crucial information for large scale web image retrieval it is ontology free and easy to obtain however irrelevant tags frequently appear and users typically will not tag all semantic objects in the image which is also called semantic loss to avoid noises and compensate for the semantic loss tag recommendation is proposed in literature however current recommendation simply ranks the related tags based on the single modality of tag co occurrence on the whole dataset which ignores other modalities such as visual correlation this paper proposes a multi modality recommendation based on both tag and visual correlation and formulates the tag recommendation as a learning problem each modality is used to generate a ranking feature and rankboost algorithm is applied to learn an optimal combination of these ranking features from different modalities experiments on flickr data demonstrate the effectiveness of this learning based multi modality strategy
background the human microbiome project has ushered in a new era for human metagenomics and high throughput next generation sequencing strategies content this review describes evolving strategies in metagenomics with a special emphasis on the core technology of dna pyrosequencing the challenges of microbial identification in the context of microbial populations are discussed the development of next generation pyrosequencing strategies and the technical hurdles confronting these methodologies are addressed bioinformatics related topics include taxonomic systems sequence databases sequence alignment tools and classifiers dna sequencing based on rrna genes or entire genomes is summarized with respect to potential pyrosequencing applications summary both the approach of rdna amplicon sequencing and the whole genome sequencing approach may be useful for human metagenomics and numerous bioinformatics tools are being deployed to tackle such vast amounts of microbiological sequence diversity metagenomics or genetic studies of microbial communities may ultimately contribute to a more comprehensive understanding of human health disease susceptibilities and the pathophysiology of infectious and immune mediated clinchem
over the period the dutch open universiteit nederland conducted an experiment in which open educational resources oer were offered in an effort to bridge the gap between informal and formal learning and to establish a new style of entry portal to higher education with no barriers at all opener received considerable attention both in terms of visitors and in the media about of the visitors reported that oer influenced their decision to start some formal learning track at academic level lessons learned were both from users and from inside the open universiteit the experiment changed the attitude towards oer within the university itself and led to a growing awareness in the netherlands of the value of oer in general in other educational levels as well as among policy makers and politicians contains tables figure
this review article identifies and discusses some of main issues and potential problems paradoxes and pathologies around the communication of recorded information and points to some possible solutions the article considers the changing contexts of information communication with some caveats about the identification of pathologies of information and analyses the changes over time in the way in which issues of the quantity and quality of information available have been regarded two main classes of problems and issues are discussed the first comprises issues relating to the quantity and diversity of information available information overload information anxiety etc the second comprises issues relating to the changing information environment with the advent of web loss of identity and authority emphasis on micro chunking and shallow novelty and the impermanence of information a final section proposes some means of solution of problems and of improvements to situation
pnas the src family kinases are allosteric enzymes that play a key role in the regulation of cell growth and proliferation in response to cellular signals they undergo large conformational changes to switch between distinct inactive and active states a computational strategy for characterizing the conformational transition pathway is presented to bridge the inactive and active states of the catalytic domain of hck the information from a large number of independent all atom molecular dynamics trajectories with explicit solvent is combined together to assemble a connectivity map of the conformational transition two intermediate states along the activation pathways are identified and their structural features are characterized a coarse free energy landscape is built in terms of the collective motions corresponding to the opening of the activation loop a loop and the rotation of the c helix this landscape shows that the protein can adopt a multitude of conformations in which the a loop is partially open while the c helix remains in the orientation characteristic of the inactive conformation the complete transition leading to the active conformation requires a concerted movement involving further opening of the a loop the relative alignment of n lobe and c lobe and the rotation of the c helix needed to recruit the residues necessary for catalysis in the active site the analysis leads to a dynamic view of the full length kinase activation whereby transitions of the catalytic domain to intermediate configurations with a partially open a loop are permitted even while the clamp remains fully engaged these transitions would render available for the transphosphorylation event that ultimately locks down the active state the results provide a broad framework for picturing the conformational transitions leading to activation
copy number variation cnv is a key source of genetic diversity but a comprehensive understanding of its phenotypic effect is only beginning to emerge we have generated a cnv map in wild mice and classical inbred strains genome wide expression data from six major organs show not only that expression of genes within cnvs tend to correlate with copy number changes but also that cnvs influence the expression of genes in their vicinity an effect that extends up to half a megabase genes within cnvs show lower expression and more specific spatial expression patterns than genes mapping elsewhere our analyses reveal differential constraint on copy number changes of genes expressed in different tissues dosage alterations of brain expressed genes are less frequent than those of other genes and are buffered by tighter transcriptional regulation our study provides initial evidence that cnvs shape tissue transcriptomes on a scale
sec title background title p intricate maps of science have been created from citation data to visualize the structure of scientific activity however most scientific publications are now accessed online scholarly web portals record detailed log data at a scale that exceeds the number of all existing citations combined such log data is recorded immediately upon publication and keeps track of the sequences of user requests clickstreams that are issued by a variety of users across many different domains given these advantages of log datasets over citation data we investigate whether they can produce high resolution more current maps of science p sec sec title methodology title p over the course of and we collected nearly billion user interactions recorded by the scholarly web portals of some of the most significant publishers aggregators and institutional consortia the resulting reference data set covers a significant part of world wide use of scholarly web portals in and provides a balanced coverage of the humanities social sciences and natural sciences a journal clickstream model i e a first order markov chain was extracted from the sequences of user interactions in the logs the clickstream model was validated by comparing it to the getty research institute s architecture and art thesaurus the resulting model was visualized as a journal network that outlines the relationships between various scientific domains and clarifies the connection of the social sciences and humanities to the natural sciences p sec sec title conclusions title p maps of science resulting from large scale clickstream data provide a detailed contemporary view of scientific activity and correct the underrepresentation of the social sciences and humanities that is commonly found in citation data sec
one of the early success stories of computational systems biology was the work done on cell cycle regulation the earliest mathematical descriptions of cell cycle control evolved into very complex detailed computational models that describe the regulation of cell division in many different cell types on the way these models predicted several dynamical properties and unknown components of the system that were later experimentally verified identified still research on this field is far from over we need to understand how the core cell cycle machinery is controlled by internal and external signals also in yeast cells and in the more complex regulatory networks of higher eukaryotes furthermore there are many computational challenges what we face as new types of data appear thanks to continuing advances in experimental techniques we have to deal with cell to cell variations revealed by single cell measurements as well as the tremendous amount of data flowing from high throughput machines we need new computational concepts and tools to handle these data and develop more detailed more precise models of cell cycle regulation in various organisms here we review past and present of computational modeling of cell cycle regulation and discuss possible future directions of the bib
motivation novel high throughput sequencing technologies pose new algorithmic challenges in handling massive amounts of shortread high coverage data a robust and versatile consensus tool is of particular interest for such data since a sound multi read alignment is a prerequisite for variation analyses accurate genome assemblies and insert sequencing results a multi read alignment algorithm for de novo or reference guided genome assembly is presented the program identifies segments shared by multiple reads and then aligns these segments using a consistency enhanced alignment graph on real de novo sequencing data obtained from the newly established ncbi short read archive the program performs similarly in quality to other comparable programs on more challenging simulated data sets for insert sequencing and variation analyses our program outperforms the other tools availability availability the consensus program can be downloaded from http www seqan de projects consensus html it can be used stand alone or in conjunction with the celera assembler both application scenarios as well as the usage of the tool are described in the documentation contact rausch inf fu de
international collaboration as measured by co authorship relations on refereed papers grew linearly from to in terms of the number of papers but exponentially in terms of the number of international addresses this confirms persson et al s persson o glnzel w danell r inflationary bibliometrics values the role of scientific collaboration and the need for relative indicators in evaluative studies scientometrics hypothesis of an inflation in international collaboration patterns in international collaboration in science can be considered as network effects since there is no political institution mediating relationships at that level except for the initiatives of the european commission science at the international level shares features with other complex adaptive systems whose order arises from the interactions of hundreds of agents pursuing self interested strategies during the period the network of global collaborations appears to have reinforced the formation of a core group of fourteen most cooperative countries this core group can be expected to use knowledge from the global network with great efficiency since these countries have strong national systems countries at the periphery may be disadvantaged by the increased strength of core
background finding functional regulatory elements in dna sequences is a very important problem in computational biology and providing a reliable algorithm for this task would be a major step towards understanding regulatory mechanisms on genome wide scale major obstacles in this respect are that the fact that the amount of non coding dna is vast and that the methods for predicting functional transcription factor binding sites tend to produce results with a high percentage of false positives this makes the problem of finding regions significantly enriched in binding sites difficult results we develop a novel method for predicting regulatory regions in dna sequences which is designed to exploit the evolutionary conservation of regulatory elements between species without assuming that the order of motifs is preserved across species we have implemented our method and tested its predictive abilities on various datasets from different organisms conclusion we show that our approach enables us to find a majority of the known crms using only sequence information from different species together with currently publicly available motif data also our method is robust enough to perform well in predicting crms despite differences in tissue specificity and even across species provided that the evolutionary distances between compared species do not change substantially the complexity of the proposed algorithm is polynomial and the observed running times show that it may be applied
complete sequences of myriad eukaryotic genomes including several human genomes are now available and recent dramatic developments in dna sequencing technology are opening the floodgates to vast volumes of sequence data yet despite knowing for several decades that a significant proportion of cytosines in the genomes of plants and animals are present in the form of methylcytosine until very recently the precise locations of these modified bases have never been accurately mapped throughout a eukaryotic genome advanced next generation dna sequencing technologies are now enabling the global mapping of this epigenetic modification at single base resolution providing new insights into the regulation and dynamics of dna methylation genomes
the current nonredundant protein sequence database contains over seven million entries and the number of individual functional domains is significantly larger than this value the vast quantity of data associated with these proteins poses enormous challenges to any attempt at function annotation classification of proteins into sequence and structural groups has been widely used as an approach to simplifying the problem in this article we question such strategies we describe how the multifunctionality and structural diversity of even closely related proteins confounds efforts to assign function on the basis of overall sequence or structural similarity rather we suggest that strategies that avoid classification may offer a more robust approach to protein annotation
genome wide association studies have identified a large number of single nucleotide polymorphisms snps that individually predispose to diseases however many genetic risk factors remain unaccounted for proteins coded by genes interact in the cell and it is most likely that certain variants mainly affect the phenotype in combination with other variants termed epistasis an exhaustive search for epistatic effects is computationally demanding as several billions of snp pairs exist for typical genotyping chips in this study the experimental knowledge on biological networks is used to narrow the search for two locus epistasis we provide evidence that this approach is computationally feasible and statistically powerful by applying this method to the wellcome trust casecontrol consortium data sets we report four significant cases of epistasis between unlinked loci in susceptibility to crohn s disease bipolar disorder hypertension and arthritis
social software applications have contributed to the establishing of personal learning environments people have started to use a number of tools and services for managing their personal social networks to communicate and to interact in a technology enhanced learning environment the move from large centralistic systems to personal technological landscapes allows a more individual approach to learning management in communities of practice the evolution from simple web applications to a large offer of shared services that can be integrated in own members environments is seen simultaneously as an opportunity for enhanced interaction and a risk for adoption at the same time we as educators researchers and learners have become aware of the importance of the interplay between support technologies and social networks for the learning process the aim of this symposium is to report and discuss on the learning impacts of personalization on the adoption of social software and the consolidation of the sense of belonging
although the aetiology of chronic fatigue syndrome cfs is unknown there have been a number of reports of blood flow abnormalities within the cerebral circulation and systemic blood pressure defects manifesting as orthostatic intolerance neither of these phenomena has been explained adequately but recent reports have linked cerebral hypoperfusion to abnormalities in cholinergic metabolism our group has previously reported enhanced skin vasodilatation in response to cumulative doses of transdermally applied acetylcholine ach implying an alteration of peripheral cholinergic function to investigate this further we studied the time course of ach induced vasodilatation following a single dose of ach in patients with cfs and age and gender matched healthy control subjects no differences in peak blood flow was seen between patients and controls but the time taken for the ach response to recover to baseline was significantly longer in the cfs patients than in control subjects the time taken to decay to of the peak response in patients and controls was versus p respectively and time taken to decay to of the peak response was versus p respectively prolongation of ach induced vasodilatation is suggestive of a disturbance to cholinergic pathways perhaps within the vascular endothelium of patients with cfs and might be related to some of the unusual vascular symptoms such as hypotension and orthostatic intolerance which are characteristic of condition
networks in which the formation of connections is governed by a random process often undergo a percolation transition wherein around a critical point the addition of a small number of connections causes a sizable fraction of the network to suddenly become linked together typically such transitions are continuous so that the percentage of the network linked together tends to zero right above the transition point whether percolation transitions could be discontinuous has been an open question here we show that incorporating a limited amount of choice in the classic erds rnyi network formation model causes its percolation transition to discontinuous
memories are thought to be encoded by sparsely distributed groups of neurons however identifying the precise neurons supporting a given memory the memory trace has been a long standing challenge we have shown previously that lateral amygdala la neurons with increased cyclic adenosine monophosphate response element binding protein creb are preferentially activated by fear memory expression which suggests that they are selectively recruited into the memory trace we used an inducible diphtheria toxin strategy to specifically ablate these neurons selectively deleting neurons overexpressing creb but not a similar portion of random la neurons after learning blocked expression of that fear memory the resulting memory loss was robust and persistent which suggests that the memory was permanently erased these results establish a causal link between a specific neuronal subpopulation and memory expression thereby identifying critical neurons within the trace
unravelling regulatory programs governed by transcription factors tfs is fundamental to understanding biological systems tfcat is a catalog of mouse and human tfs based on a reliable core collection of annotations obtained by expert review of the scientific literature the collection including proven and homology based candidate tfs is annotated within a function based taxonomy and dna binding proteins are organized within a classification system all data and user feedback mechanisms are available at the tfcat portal http www ca
in this paper we use a partition of the links of a network in order to uncover its community structure this approach allows for communities to overlap at nodes so that nodes may be in more than one community we do this by making a node partition of the line graph of the original network in this way we show that any algorithm which produces a partition of nodes can be used to produce a partition of links we discuss the role of the degree heterogeneity and propose a weighted version of the line graph in order to account this
the three dimensional molecular structure of dna specifically the shape of the backbone and grooves of genomic dna can be dramatically affected by nucleotide changes which can cause differences in protein binding affinity and phenotype we developed an algorithm to measure constraint on the basis of similarity of dna topography among multiple species using hydroxyl radical cleavage patterns to interrogate the solvent accessible surface area of dna this algorithm found that of bases in the human genome are evolutionarily constrained double the number detected by nucleotide sequence based algorithms topography informed constrained regions correlated with functional noncoding elements including enhancers better than did regions identified solely on the basis of nucleotide sequence these results support the idea that the molecular shape of dna is under selection and can identify evolutionary science
cross species comparison has emerged as a powerful paradigm for predicting cis regulatory modules crms and understanding their evolution the comparison requires reliable sequence alignment which remains a challenging task for less conserved noncoding sequences furthermore the existing models of dna sequence evolution generally do not explicitly treat the special properties of crm sequences to address these limitations we propose a model of crm evolution that captures different modes of evolution of functional transcription factor binding sites tfbss and the background sequences a particularly novel aspect of our work is a probabilistic model of gains and losses of tfbss a process being recognized as an important part of regulatory sequence evolution we present a computational framework that uses this model to solve the problems of crm alignment and prediction our alignment method is similar to existing methods of statistical alignment but uses the conserved binding sites to improve alignment our crm prediction method deals with the inherent uncertainties of binding site annotations and sequence alignment in a probabilistic framework in simulated as well as real data we demonstrate that our program is able to improve both alignment and prediction of crm sequences over several state of the art methods finally we used alignments produced by our program to study binding site conservation in genome wide binding data of key transcription factors in the drosophila blastoderm with two intriguing results i the factor bound sequences are under strong evolutionary constraints even if their neighboring genes are not expressed in the blastoderm and ii binding sites in distal bound sequences relative to transcription start sites tend to be more conserved than those in proximal regions our approach is implemented as software emma evolutionary model based cis regulatory module analysis ready to be applied in a broad context
the analysis of gene regulatory networks grns is a central goal of bioinformatics highly accelerated by the advent of new experimental techniques such as rna interference a battery of reverse engineering methods has been developed in recent years to reconstruct the underlying grns from these and other experimental data however the performance of the individual methods is poorly understood and validation of algorithmic performances is still missing to a large extent to enable such systematic validation we have developed the web application genge gene network generator a controlled framework for the automatic generation of grns the theoretical model for a grn is a non linear differential equation system networks can be user defined or constructed in a modular way with the option to introduce global and local network perturbations resulting data can be used e g as benchmark data for evaluating grn reconstruction methods or for predicting effects of perturbations as theoretical counterparts of biological experiments availability available online at http genge molgen mpg de supplementary information supplementary data are available at online
chromatin immunoprecipitation chip allows specific proteindna interactions to be isolated combining chip with high throughput sequencing reveals the dna sequence involved in these interactions here we describe how to perform chip seq starting with whole tissues or cell lines and ending with millions of short sequencing tags that can be aligned to the reference genome of the species under investigation we also outline additional procedures to recover chip chip libraries for chip seq and discuss contemporary issues in analysis
we present a study of anonymized data capturing a month of high level communication activities within the whole of the microsoft messenger instant messaging system we examine characteristics and patterns that emerge from the collective dynamics of large numbers of people rather than the actions and characteristics of individuals the dataset contains summary properties of billion conversations among million people from the data we construct a communication graph with million nodes and billion undirected edges creating the largest social network constructed and analyzed to date we report on multiple aspects of the dataset and synthesized graph we find that the graph is well connected and robust to node removal we investigate on a planetary scale the oft cited report that people are separated by six degrees of separation and find that the average path length among messenger users is we find that people tend to communicate more with each other when they have similar age language and location and that cross gender conversations are both more frequent and of longer duration than conversations with the gender
translational research the effort to couple the results of basic research to clinical applications depends on the ability to effectively answer questions using information that spans multiple disciplines the semantic web with its emphasis on combining information using standard representation languages access to that information via standard web protocols and technologies to leverage computation such as in the form of inference and distributable query offers a social and technological basis for assembling integrating and making available biomedical knowledge at web scale in this article we discuss the use of semantic web technology for assembling and querying biomedical knowledge from multiple sources and disciplines we present the neurocommons prototype knowledge base a demonstration intended to show the feasibility and benefits of using these technologies the prototype knowledge base can be used to experiment with and assess the scalability of current tools and methods for creating such a resource and to elicit issues that will need to be addressed in order to expand the scope and use of it we demonstrate the utility of the knowledge base by reviewing a few example queries that provide answers to precise questions relevant to the understanding of disease all components of the knowledge base are freely available at http neurocommons org enabling readers to reconstruct the knowledge base and experiment with this technology
next generation dna sequencing technologies can generate unprecedented amounts of genomic data even for non model organisms here we describe how these new technologies have facilitated recent key advances in ecology and evolutionary biology and highlight several outstanding ecological and evolutionary questions that are distinctly suited to the innovations they provide importantly using these technologies to their full potential requires careful experimental design and critical consideration of several caveats associated with them although several significant challenges remain to be resolved before the integration of next generation sequencing technologies into single investigator research programs we argue that they will soon transform ecology and evolution by fundamentally changing the ranges and types of questions that can addressed
global advances in patient safety have been hampered by the lack of a uniform classification of patient safety concepts this is a significant barrier to developing strategies to reduce risk performing evidence based research and evaluating existing healthcare policies relevant to patient safety since the world health organization s world alliance for patient safety has undertaken the project to develop an international classification for patient safety icps to devise a classification which transforms patient safety information collected from disparate systems into a common format to facilitate aggregation analysis and learning across disciplines borders and time a drafting group comprised of experts from the fields of patient safety classification theory health informatics consumer patient advocacy law and medicine identified and defined key patient safety concepts and developed an internationally agreed conceptual framework for the icps based upon existing patient safety classifications the conceptual framework was iteratively improved through technical expert meetings and a two stage web based modified delphi survey of over international experts this work culminated in a conceptual framework consisting of ten high level classes incident type patient outcomes patient characteristics incident characteristics contributing factors hazards organizational outcomes detection mitigating factors ameliorating actions and actions taken to reduce risk while the framework for the icps is in place several challenges remain concepts need to be defined guidance for using the classification needs to be provided and further real world testing needs to occur to progressively refine the icps to ensure it is fit for intqhc
if you want a glimpse of what health care could look like a few years from now consider hello health the brooklyn based primary care practice that is fast becoming an emblem of modern medicine a paperless concierge practice that eschews the limitations of insurance based medicine hello health is popular and successful largely because of the powerful and cost effective communication tools it employs web based social media indeed across the health care industry from large hospital networks to patient support groups new media tools like weblogs instant messaging platforms video chat and social networks are reengineering the way doctors and interact
a practical approach that enables one to calculate the standard free energy of binding from a one dimensional potential of mean force pmf is proposed umbrella sampling and the weighted histogram analysis method are used to generate a pmf along the reaction coordinate of binding at each point a restraint is applied orthogonal to the reaction coordinate to make possible the determination of the volume sampled by the ligand the free energy of binding from an arbitrary unbound volume to the restrained bound form is calculated from the ratio of the pmf integrated over the bound region to that of the unbound adding the free energy changes from the standard state volume to the unbound volume and from the restrained to the unrestrained bound state gives the standard free energy of binding exploration of the best choice of binding paths is also made this approach is first demonstrated on a model binding system and then tested on the benzamidinetrypsin system for which reasonable agreement with experiment is found a comparison is made with other methods to obtain the standard free energy of binding from pmf
the last few years have seen extensive efforts to catalogue human genetic variation and correlate it with phenotypic differences most common snps have now been assessed in genome wide studies for statistical associations with many complex traits including many important common diseases although these studies have provided new biological insights only a limited amount of the heritable component of any complex trait has been identified and it remains a challenge to elucidate the functional link between associated variants and phenotypic traits technological advances such as the ability to detect rare and structural variants and a clear understanding of the challenges in linking different types of variation with phenotype will be essential for progress
transcription factors are key cellular components that control gene expression their activities determine how cells function and respond to the environment currently there is great interest in research into human transcriptional regulation however surprisingly little is known about these regulators themselves for example how many transcription factors does the human genome contain how are they expressed in different tissues are they evolutionarily conserved here we present an analysis of manually curated sequence specific dna binding transcription factors their functions genomic organization and evolutionary conservation much remains to be explored but this study provides a solid foundation for future investigations to elucidate regulatory mechanisms underlying diverse mammalian processes
brains are restless we have long known of the existence of a great deal of uninterrupted brain activity that maintains the body in a stable state from an evolutionary standpoint one of the brain s most ancient tasks but intrinsic ongoing activity is not limited to subcortical life maintaining structures cortex too is remarkably active even in the absence of a sensory stimulus or a specific behavioral task this is evident both in its enormous energy consumption at rest and in the large spontaneous but coherent fluctuations of neural activity that spread across different areas not surprisingly a growing number of electrophysiological and functional magnetic resonance imaging fmri studies are appearing that report on various aspects of the brain s spontaneous activity or default mode of operation one recent study reports results from simultaneously combined electrophysiological and fmri measurements in the monkey visual cortex shmuel a leopold d a neuronal correlates of spontaneous fluctuations in fmri signals in monkey visual cortex implications for functional connectivity at rest hum brain mapp the authors claim to be able to demonstrate correlations between slow fluctuations in blood oxygen level dependent bold signals and concurrent fluctuations in the underlying locally measured neuronal activity they even go on to speculate that the fluctuations display wave like spatiotemporal patterns across cortex in the present report however we re analyze the data presented in that study and demonstrate that the measurements were not actually taken during rest visual cortex was subject to almost imperceptible but physiologically clearly detectable flicker induced by the visual stimulator an examination of the power spectral density of the neural responses and the neurovascular impulse response function shows that such imperceptible flicker strongly suppresses the slow oscillations and changes the degree of covariance between neural and vascular signals in addition a careful analysis of the spatiotemporal patterns demonstrates that no slow waves of activity exist in visual cortex instead the presented wave data reflect differences in signal to noise ratio at various cortical sites due to local differences in vascularization in this report assuming that the term spontaneous activity refers to intrinsic physiological processes at the absence of sensory inputs or motor outputs we discuss the need for careful selection of experimental protocols and of examining the degree to which the activation of sensory areas might influence the cortical or subcortical processes in other regions
induced pluripotent stem cells ipscs derived from somatic cells of patients represent a powerful tool for biomedical research and may provide a source for replacement therapies however the use of viruses encoding the reprogramming factors represents a major limitation of the current technology since even low vector expression may alter the differentiation potential of the ipscs or induce malignant transformation here we show that fibroblasts from five patients with idiopathic parkinson s disease can be efficiently reprogrammed and subsequently differentiated into dopaminergic neurons moreover we derived hipscs free of reprogramming factors using cre recombinase excisable viruses factor free hipscs maintain a pluripotent state and show a global gene expression profile more closely related to hescs than to hipscs carrying the transgenes our results indicate that residual transgene expression in virus carrying hipscs can affect their molecular characteristics and that factor free hipscs therefore represent a more suitable source of cells for modeling of disease
we are teaching a new generation of students cradled in technologies communication and abundance of information the implications are that we need to focus the design of learning technologies to support social learning in context instead of designing technologies that teach the learner the new social learning technologies will perform three main roles support the learner in finding the right content right for the context for the particular learner for the specific purpose of the learner right pedagogically support learners to connect with the right people again right for the context learner purpose educational goal etc and motivate incentivize people to learn in the pursuit of such environments new areas of sciences become relevant as a source of methods and techniques social psychology economic game theory multi agent systems the paper illustrates how social learning technologies can be designed using some existing and emerging technologies ontologies vs social tagging exploratory search collaborative vs self managed social recommendations trust and reputation mechanisms mechanism design and visualization
quantum computers which harness the superposition and entanglement of physical states could outperform their classical counterparts in solving problems with technological impactsuch as factoring large numbers and searching a quantum processor executes algorithms by applying a programmable sequence of gates to an initialized register of qubits which coherently evolves into a final state containing the result of the computation building a quantum processor is challenging because of the need to meet simultaneously requirements that are in conflict state preparation long coherence times universal gate operations and qubit readout processors based on a few qubits have been demonstrated using nuclear magnetic cold ion and systems but a solid state realization has remained an outstanding challenge here we demonstrate a two qubit superconducting processor and the implementation of the grover search and deutschjozsa quantum we use a two qubit interaction tunable in strength by two orders of magnitude on nanosecond timescales which is mediated by a cavity bus in a circuit quantum electrodynamics this interaction allows the generation of highly entangled states with concurrence up to per cent although this processor constitutes an important step in quantum computing with integrated circuits continuing efforts to increase qubit coherence times gate performance and register size will be required to fulfil the promise of a technology
the human body is composed of diverse cell types with distinct functions although it is known that lineage specification depends on cell specific gene expression which in turn is driven by promoters enhancers insulators and other cis regulatory dna sequences for each the relative roles of these regulatory elements in this process are not clear we have previously developed a chromatin immunoprecipitation based microarray method chip chip to locate promoters enhancers and insulators in the human here we use the same approach to identify these elements in multiple cell types and investigate their roles in cell type specific gene expression we observed that the chromatin state at promoters and ctcf binding at insulators is largely invariant across diverse cell types in contrast enhancers are marked with highly cell type specific histone modification patterns strongly correlate to cell type specific gene expression programs on a global scale and are functionally active in a cell type specific manner our results define over potential transcriptional enhancers in the human genome significantly expanding the current catalogue of human enhancers and highlighting the role of these elements in cell type specific expression
background the european randomized study of screening for prostate cancer was initiated in the early to evaluate the effect of screening with prostate specific antigen psa testing on death rates from prostate cancer methods we identified men between the ages of and years through registries in seven european countries for inclusion in our study the men were randomly assigned to a group that was offered psa screening at an average of once every years or to a control group that did not receive such screening the predefined core age group for this study included men between the ages of and years the primary outcome was the rate of death from prostate cancer mortality follow up was identical for the two study groups and ended on december results in the screening group of men accepted at least one offer of screening during a median follow up of years the cumulative incidence of prostate cancer was in the screening group and in the control group the rate ratio for death from prostate cancer in the screening group as compared with the control group was confidence interval ci to adjusted p the absolute risk difference was death per men this means that men would need to be screened and additional cases of prostate cancer would need to be treated to prevent one death from prostate cancer the analysis of men who were actually screened during the first round excluding subjects with noncompliance provided a rate ratio for death from prostate cancer of ci to conclusions psa based screening reduced the rate of death from prostate cancer by but was associated with a high risk of overdiagnosis current controlled number
in many genomic studies one works with genome position dependent data e g chip chip or chip seq scores using conventional tools it can be difficult to get a good feel for the data especially the distribution of features this article argues that the so called hilbert curve visualization can complement genome browsers and help to get further insights into the structure of one s data this is demonstrated with examples from different use cases an open source application called hilbertvis is presented that allows the user to produce and interactively explore such plots availability http www ebi ac uk huber srv hilbert contact sanders fs tum desupplementary information supplementary data are available at online
networks have become a key approach to understanding systems of interacting objects unifying the study of diverse phenomena including biological organisms and human one crucial step when studying the structure and dynamics of networks is to identify groups of related nodes that correspond to functional subunits such as protein or social communities in networks often such that nodes simultaneously belong to several groups meanwhile many networks are known to possess hierarchical organization where communities are recursively grouped into a hierarchical however the fact that many real networks have communities with pervasive overlap where each and every node belongs to more than one group has the consequence that a global hierarchy of nodes cannot capture the relationships between overlapping groups here we reinvent communities as groups of links rather than nodes and show that this unorthodox approach successfully reconciles the antagonistic organizing principles of overlapping communities and hierarchy in contrast to the existing literature which has entirely focused on grouping nodes link communities naturally incorporate overlap while revealing hierarchical organization we find relevant link communities in many networks including major biological networks such as proteinprotein and metabolic and show that a large social contains hierarchically organized community structures spanning inner city to regional scales while maintaining pervasive overlap our results imply that link communities are fundamental building blocks that reveal overlap and hierarchical organization in networks to be two aspects of the phenomenon
two experiments revealed that i people can more accurately predict their affective reactions to a future event when they know how a neighbor in their social network reacted to the event than when they know about the event itself and ii people do not believe this undergraduates made more accurate predictions about their affective reactions to a minute speed date n and to a peer evaluation n when they knew only how another undergraduate had reacted to these events than when they had information about the events themselves both participants and independent judges mistakenly believed that predictions based on information about the event would be more accurate than predictions based on information about how another person had reacted to science
these notes are loosely based on lectures given at the cern winter school on supergravity strings and gauge theories february and at the ipm string school in tehran april i have focused on a few concrete topics and also on addressing questions that have arisen repeatedly background condensed matter physics material is included as motivation and easy reference for the high energy physics community the discussion of holographic techniques progresses from equilibrium to transport and superconductivity
the chapter is organized as follows in section we discuss the hierarchical dirichlet process showing how it can be used to link multiple dirichlet processes we present several examples of real world applications in which such models are natural section shows how the hierarchical dirichlet process can be used to build nonparametric hidden markov models these are hidden markov models in which the cardinality of the state space is unbounded we also discuss extensions to nonparametric hidden markov trees and nonparametric probabilistic context free grammars in section we consider a different nonparametric hierarchy based on the pitman yor model showing that it is natural in domains such as natural language processing in which data often exhibit power law behavior section discusses the beta process an alternative to the dirichlet process which yields sparse featural representations we show that the counterpart of the chinese restaurant process is a distribution on sparse binary matrices known as the indian buffet process we also consider hierarchical models based on the beta process in section we consider some semiparametric models that are based on nonparametric hierarchies finally in section we present an overview of some of the algorithms that have been developed for posterior inference in hierarchical bayesian models
aggregated journal ndash journal citation networks based on the journal citation reports of the science citation index journals and the social science citation index journals are made accessible from the perspective of any of these journals a vector space model is used for normalization and the results are brought online at as input files for the visualization program pajek the user is thus able to analyze the citation environment in terms of links and graphs furthermore the local impact of a journal is defined as its share of the total citations in the specific journal s citation environments the vertical size of the nodes is varied proportionally to this citation impact the horizontal size of each node can be used to provide the same information after correction for within journal self citations in the ldquo citing rdquo environment the equivalents of this measure can be considered as a citation activity index which maps how the relevant journal environment is perceived by the collective of authors of a given journal as a policy application the mechanism of interdisciplinary developments among the sciences is elaborated for the case of nanotechnology journals copy wiley inc
current data integration approaches by bioinformaticians frequently involve extracting data from a wide variety of public and private data repositories each with a unique vocabulary and schema via scripts these separate data sets must then be normalized through the tedious and lengthy process of resolving naming differences and collecting information into a single view attempts to consolidate such diverse data using data warehouses or federated queries add significant complexity and have shown limitations in flexibility the alternative of complete semantic integration of data requires a massive sustained effort in mapping data types and maintaining ontologies we focused instead on creating a data architecture that leverages semantic mapping of experimental metadata to support the rapid prototyping of scientific discovery applications with the twin goals of reducing architectural complexity while still leveraging semantic technologies to provide flexibility efficiency and more fully characterized data relationships a metadata ontology was developed to describe our discovery process a metadata repository was then created by mapping metadata from existing data sources into this ontology generating rdf triples to describe the entities finally an interface to the repository was designed which provided not only search and browse capabilities but complex query templates that aggregate data from both rdf and rdbms sources we describe how this approach i allows scientists to discover and link relevant data across diverse data sources and ii provides a platform for development of integrative applications
the semantic web technology enables integration of heterogeneous data on the world wide web by making the semantics of data explicit through formal ontologies in this article we survey the feasibility and state of the art of utilizing the semantic web technology to represent integrate and analyze the knowledge in various biomedical networks we introduce a new conceptual framework semantic graph mining to enable researchers to integrate graph mining with ontology reasoning in network data analysis through four case studies we demonstrate how semantic graph mining can be applied to the analysis of disease causal genes gene ontology category cross talks drug efficacy analysis and herb drug interactions bib
motivation a new protocol for sequencing the messenger rna in a cell known as rna seq generates millions of short sequence fragments in a single run these fragments or reads can be used to measure levels of gene expression and to identify novel splice variants of genes however current software for aligning rna seq data to a genome relies on known splice junctions and cannot identify novel ones tophat is an efficient read mapping algorithm designed to align reads from an rna seq experiment to a reference genome without relying on known splice sites results we mapped the rna seq reads from a recent mammalian rna seq experiment and recovered more than of the splice junctions reported by the annotation based software from that study along with nearly previously unreported junctions the tophat pipeline is much faster than previous systems mapping nearly million reads per cpu hour which is sufficient to process an entire rna seq experiment in less than a day on a standard desktop computer we describe several challenges unique to ab initio splice site discovery from rna seq reads that will require further algorithm development availability tophat is free open source software available from http tophat cbcb umd educontact cole cs umd edusupplementary information supplementary data are available at online
a new generation of high throughput sequencing strategies will soon lead to the acquisition of high coverage genomic profiles of hundreds to thousands of individuals within species generating unprecedented levels of information on the frequencies of nucleotides segregating at individual sites however because these new technologies are error prone and yield uneven coverage of alleles in diploid individuals they also introduce the need for novel methods for analyzing the raw read data a maximum likelihood method for the estimation of allele frequencies is developed eliminating both the need to arbitrarily discard individuals with low coverage and the requirement for an extrinsic measure of the sequence error rate the resultant estimates are nearly unbiased with asymptotically minimal sampling variance thereby defining the limits to our ability to estimate population genetic parameters and providing a logical basis for the optimal design of population surveys
deep brain stimulation dbs is a therapeutic option for intractable neurological and psychiatric disorders including parkinson s disease and major depression because of the heterogeneity of brain tissues where electrodes are placed it has been challenging to elucidate the relevant target cell types or underlying mechanisms of dbs we employed optogenetics and solid state optics to systematically drive or inhibit an array of distinct circuit elements in freely moving parkinsonian rodents and found that therapeutic effects within the subthalamic nucleus can be accounted for by direct selective stimulation of afferent axons projecting to this region in addition to providing insight into dbs mechanisms these results demonstrate an optical approach for dissection of disease circuitry and define the technological toolbox needed for systematic deconstruction of disease circuits by selectively controlling individual science
social bookmarking systems are becoming increasingly important data sources for bootstrapping and maintaining semantic web applications their emergent information structures have become known as folksonomies a key question for harvesting semantics from these systems is how to extend and adapt traditional notions of similarity to folksonomies and which measures are best suited for applications such as community detection navigation support semantic search user profiling and ontology learning here we build an evaluation framework to compare various general folksonomy based similarity measures which are derived from several established information theoretic statistical and practical measures our framework deals generally and symmetrically with users tags and resources for evaluation purposes we focus on similarity between tags and between resources and consider different methods to aggregate annotations across users after comparing the ability of several tag similarity measures to predict user created tag relations we provide an external grounding by user validated semantic proxies based on wordnet and the open directory project we also investigate the issue of scalability we find that mutual information with distributional micro aggregation across users yields the highest accuracy but is not scalable per user projection with collaborative aggregation provides the best scalable approach via incremental computations the results are consistent across resource and similarity
summary the biopython project is a mature open source international collaboration of volunteer developers providing python libraries for a wide range of bioinformatics problems biopython includes modules for reading and writing different sequence file formats and multiple sequence alignments dealing with macro molecular structures interacting with common tools such as blast clustalw and emboss accessing key online databases as well as providing numerical methods for statistical learning availability biopython is freely available with documentation and source code at www biopython org under the biopython license contact all queries should be directed to the biopython mailing lists see www biopython org wiki cock scri ac bioinformatics
title author summary title p it is of great interest to understand the patterns and mechanisms of scientific knowledge growth but such studies have been hampered by the lack of ideal cases in which the structure of the knowledge is known the knowledge is quantifiable and the process of knowledge discovery is well understood and documented the biological knowledge about a species is in part described by its protein interaction network and genetic interaction network here we conduct a temporal meta analysis of three decades of discoveries of protein interactions and genetic interactions in baker s yeast to reveal the tempo and mode of the growth of yeast biology we show that the growth is exponential over time and that important subjects tend to be studied earlier however expansions of different domains of knowledge are highly heterogeneous and episodic such that the temporal turnover of knowledge hubs is much greater than that expected by chance familiar subjects are preferentially studied over new subjects leading to a reduced pace of innovation while research is increasingly done in teams the number of discoveries per researcher is greater in smaller teams these findings reveal collective human behaviors in scientific research and help design better strategies in future knowledge p
the transcription factor is a key tumor suppressor and a central regulator of the stress response to ensure a robust and precise response to cellular signals gene expression must be tightly regulated from the transcriptional to the post translational levels computational predictions suggest that several micrornas are involved in the post transcriptional regulation of here we demonstrate that mir a brain enriched microrna is a bona fide negative regulator of in both zebrafish and humans mir mediated down regulation of is strictly dependent on the binding of mir to a microrna response element in the untranslated region of mrna overexpression of mir represses the endogenous level of protein and suppresses apoptosis in human neuroblastoma cells and human lung fibroblast cells in contrast knockdown of mir elevates the level of protein and induces apoptosis in human lung fibroblasts and in the zebrafish brain this phenotype can be rescued significantly by either an ablation of endogenous function or ectopic expression of mir in zebrafish interestingly mir is down regulated when zebrafish embryos are treated with gamma irradiation or camptothecin corresponding to the rapid increase in protein in response to dna damage ectopic expression of mir suppresses the increase of and stress induced apoptosis together our study demonstrates that mir is an important negative regulator of and induced apoptosis during development and during the response
next generation high throughput dna sequencing techniques are opening fascinating opportunities in the life sciences novel fields and applications in biology and medicine are becoming a reality beyond the genomic sequencing which was original development goal and application serving as examples are personal genomics with detailed analysis of individual genome stretches precise analysis of rna transcripts for gene expression surpassing and replacing in several respects analysis by various microarray platforms for instance in reliable and precise quantification of transcripts and as a tool for identification and analysis of dna regions interacting with regulatory proteins in functional regulation of gene expression the next generation sequencing technologies offer novel and rapid ways for genome wide characterisation and profiling of mrnas small rnas transcription factor regions structure of chromatin and dna methylation patterns microbiology and metagenomics in this article development of commercial sequencing devices is reviewed and some european contributions to the field are mentioned presently commercially available very high throughput dna sequencing platforms as well as techniques under development are described and their applications in bio medical discussed
understanding the relationship between genetic variation and gene expression is a central question in genetics with the availability of data from high throughput technologies such as chip chip expression and genotyping arrays we can begin to not only identify associations but to understand how genetic variations perturb the underlying transcription regulatory networks to induce differential gene expression in this study we describe a simple model of transcription regulation where the expression of a gene is completely characterized by two properties the concentrations and promoter affinities of active transcription factors we devise a method that extends network component analysis nca to determine how genetic variations in the form of single nucleotide polymorphisms snps perturb these two properties applying our method to a segregating population of saccharomyces cerevisiae we found statistically significant examples of trans acting snps located in regulatory hotspots that perturb transcription factor concentrations and affinities for target promoters to cause global differential expression and cis acting genetic variations that perturb the promoter affinities of transcription factors on a single gene to cause local differential expression although many genetic variations linked to gene expressions have been identified it is not clear how they perturb the underlying regulatory networks that govern gene expression our work begins to fill this void by showing that many genetic variations affect the concentrations of active transcription factors in a cell and their affinities for target promoters understanding the effects of these perturbations can help us to paint a more complete picture of the complex landscape of transcription regulation the software package implementing the algorithms discussed in this work is available as a matlab package request
the discovery of micrornas mirnas almost years ago changed dramatically our perspective on eukaryotic gene expression regulation however the broad and important functions of these regulators are only now becoming apparent the expansion of our catalogue of mirna genes and the identification of the genes they regulate owe much to the development of sophisticated computational tools that have helped either to focus or interpret experimental assays in this article we review the methods for mirna gene finding and target identification that have been proposed in the last few years we identify some problems that current approaches have not yet been able to overcome and we offer some perspectives on the next generation of computational nar
to study the evolution of human micrornas mirnas we examined nucleotide variation in humans sequence divergence between species and genomic clustering patterns for mirnas with different expression levels we found that expression level is a major indicator of the rate of evolution and that approximately of currently annotated human mirna genes are almost free of pressure
genome scale screening studies are gradually accumulating a wealth of data on the putative involvement of hundreds of genes proteins in various cellular responses or functions a fundamental challenge is to chart out the protein pathways that underlie these systems previous approaches to the problem have either employed a local optimization criterion aiming to infer each pathway independently or a global criterion searching for the overall most parsimonious subnetwork here we study the trade off between the two approaches and present a new intermediary scheme that provides explicit control over it we demonstrate its utility in the analysis of the apoptosis network in humans and the telomere length maintenance tlm system in yeast our results show that in the majority of real life cases the intermediary approach provides the most plausible solutions we use a new set of perturbation experiments measuring the role of essential genes in telomere length regulation to further study the tlm network surprisingly we find that the proteasome plays an important role in telomere length regulation through its associations with transcription and dna circuits
over the past decade a class of small rna molecules called micrornas mirnas has been shown to regulate gene expression at the post transcription stage while early work focused on the identification of mirnas using a combination of experimental and computational techniques subsequent studies have focused on identification of mirna target mrna pairs as each mirna can have hundreds of mrna targets the experimental validation of some mirnas as oncogenic has provided further motivation for research in this area in this article we propose an odds ratio or statistic for identification of regulatory mirnas it is based on integrative analysis of matched mirna and mrna time course microarray data the or statistic was used for i identification of mirnas with regulatory potential ii identification of mirna target mrna pairs and iii identification of time lags between changes in mirna expression and those of its target mrnas we applied the or statistic to a cancer data set and identified a small set of mirnas that were negatively correlated to mrnas a literature survey revealed that some of the mirnas that were predicted to be regulatory were indeed oncogenic or tumor suppressors finally some of the predicted mirna targets have been shown to be experimentally nar
key benefit written by a leader in the field of information retrieval this text provides the background and tools needed to evaluate compare and modify search engines key topics coverage of the underlying ir and mathematical models reinforce key concepts numerous programming exercises make extensive use of galago a java based open source search engine market a valuable tool for search engine and information professionals
summary infernal builds consensus rna secondary structure profiles called covariance models cms and uses them to search nucleic acid sequence databases for homologous rnas or to create new sequence and structure based multiple sequence alignments availability source code documentation and benchmark downloadable from http infernal janelia org infernal is freely licensed under the gnu and should be portable to any posix compliant operating system including linux and mac os x contact nawrockie janelia hhmi org kolbed janelia hhmi org eddys janelia org
motivation kegg pathway is a service of kyoto encyclopedia of genes and genomes kegg constructing manually curated pathway maps that represent current knowledge on biological networks in graph models while valuable graph tools have been implemented in r bioconductor to our knowledge there is currently no software package to parse and analyze kegg pathways with graph theory results we introduce the software package kegggraph in r and bioconductor an interface between kegg pathways and graph models as well as a collection of tools for these graphs superior to existing approaches kegggraph captures the pathway topology and allows further analysis or dissection of pathway graphs we demonstrate the use of the package by the case study of analyzing human pancreatic cancer pathway availability kegggraph is freely available at the bioconductor website http www bioconductor org kgml fles can be downloaded from kegg ftp site ftp ftp genome jp pub kegg xml contact j zhang dkfz de
summary bioconductorbuntu is a custom distribution of ubuntu linux that automatically installs a server side microarray processing environment providing a user friendly web based gui to many of the tools developed by the bioconductor project accessible locally or across a network system installation is via booting off a cd image or by using a debian package provided to upgrade an existing ubuntu installation in its current version several microarray analysis pipelines are supported including oligonucleotide dual or single dye experiments including post processing with gene set enrichment analysis bioconductorbuntu is designed to be extensible by server side integration of further relevant bioconductor modules as required facilitated by its straightforward underlying python based infrastructure bioconductorbuntu offers an ideal environment for the development of processing procedures to facilitate the analysis of next generation sequencing datasets availability bioconductorbuntu is available for download under a creative commons license along with additional documentation and a tutorial from http bioinf nuigalway ie contact paul geeleher nuigalway bioinformatics
gr genome wide scans for recent positive selection in humans have yielded insight into the mechanisms underlying the extensive phenotypic diversity in our species but have focused on a limited number of populations here we present an analysis of recent selection in a global sample of populations using genotype data from the human genome diversity ceph panel we refine the geographic distributions of known selective sweeps and find extensive overlap between these distributions for populations in the same continental region but limited overlap between populations outside these groupings we present several examples of previously unrecognized candidate targets of selection including signals at a number of genes in the developmental pathway in non african populations analysis of recently identified genes involved in complex diseases suggests that there has been selection on loci involved in susceptibility to type ii diabetes finally we search for local adaptation between geographically close populations and highlight examples
applications of genomic and proteomic technologies have seen a major increase resulting in an explosion in the amount of highly dimensional and complex data being generated subsequently this has increased the effort by the bioinformatics community to develop novel computational approaches that allow for meaningful information to be extracted this information must be of biological relevance and thus correlate to disease phenotypes of interest artificial neural networks are a form of machine learning from the field of artificial intelligence with proven pattern recognition capabilities and have been utilized in many areas of bioinformatics this is due to their ability to cope with highly dimensional complex datasets such as those developed by protein mass spectrometry and dna microarray experiments as such neural networks have been applied to problems such as disease classification and identification of biomarkers this review introduces and describes the concepts related to neural networks the advantages and caveats to their use examples of their applications in mass spectrometry and microarray research with a particular focus on cancer studies and illustrations from recent literature showing where neural networks have performed well in comparison to other machine learning methods this should form the necessary background knowledge and information enabling researchers with an interest in these methodologies but not necessarily from a machine learning background to apply the concepts to their own datasets thus maximizing the information gain from these complex systems
many cellular proteins are intrinsically disordered and undergo folding in whole or in part upon binding to their physiological targets the past few years have seen an exponential increase in papers describing characterization of intrinsically disordered proteins both free and bound to targets although nmr spectroscopy remains the favored tool a number of new biophysical techniques are proving exceptionally useful in defining the limits of the conformational ensembles advances have been made in prediction of the recognition elements in disordered proteins in elucidating the kinetics and mechanism of the coupled folding and binding process and in understanding the role of post translational modifications in tuning the biological response here we review these and other recent advances that are providing new insights into the conformational propensities and interactions of intrinsically disordered proteins and are beginning to reveal general principles underlying their functions
this article discusses the doctrinal content of the group of ideas known as new public management npm the intellectual provenance of those ideas explanations for their apparent persuasiveness in the s and criticisms which have been made of the new doctrines particular attention is paid to the claim that npm offers an all purpose key to better provision of public services this article argues that nfm has been most commonly criticized in terms of a claimed contradiction between equity and efficiency values but that any critique which is to survive npm s claim to infinite reprogrammability must be couched in terms of possible conflicts between administrative values the conclusion is that the esrc s management in government research initiative has been more valuable in helping to identify rather than to definitively answer the key conceptual questions raised npm
background the optimal target range for blood glucose in critically ill patients remains unclear methods within hours after admission to an intensive care unit icu adults who were expected to require treatment in the icu on or more consecutive days were randomly assigned to undergo either intensive glucose control with a target blood glucose range of to mg per deciliter to mmol per liter or conventional glucose control with a target of mg or less per deciliter mmol or less per liter we defined the primary end point as death from any cause within days after randomization results of the patients who underwent randomization were assigned to undergo intensive control and to undergo conventional control data with regard to the primary outcome at day were available for and patients respectively the two groups had similar characteristics at baseline a total of patients in the intensive control group and in the conventional control group died odds ratio for intensive control confidence interval to p the treatment effect did not differ significantly between operative surgical patients and nonoperative medical patients odds ratio for death in the intensive control group and respectively p severe hypoglycemia blood glucose level or mg per deciliter mmol per liter was reported in of patients in the intensive control group and of in the conventional control group p there was no significant difference between the two treatment groups in the median number of days in the icu p or hospital p or the median number of days of mechanical ventilation p or renal replacement therapy p conclusions in this large international randomized trial we found that intensive glucose control increased mortality among adults in the icu a blood glucose target of mg or less per deciliter resulted in lower mortality than did a target of to mg per deciliter clinicaltrials number
classification of web page content is essential to many tasks in web information retrieval such as maintaining web directories and focused crawling the uncontrolled nature of web content presents additional challenges to web page classification as compared to traditional text classification but the interconnected nature of hypertext also provides features that can assist process
genome wide association studies gwass are regularly used to map genomic regions contributing to common human diseases but they often do not identify the precise causative genes and sequence variants to identify causative type diabetes variants we resequenced exons and splice sites of candidate genes in pools of dna from patients and controls and tested their disease association in over participants we discovered four rare variants that lowered risk independently of each other odds ratio to p x to x in interferon induced with helicase c domain a gene located in a region previously associated with by gwass these variants are predicted to alter the expression and structure of melanoma differentiation associated protein a cytoplasmic helicase that mediates induction of interferon response to viral rna this finding firmly establishes the role of in and demonstrates that resequencing studies can pinpoint disease causing genes in genomic regions initially identified gwass
this study provided a comparative analysis of three social network sites the open to all facebook the professionally oriented linkedin and the exclusive members only asmallworld the analysis focused on the underlying structure or architecture of these sites on the premise that it may set the tone for particular types of interaction through this comparative examination four themes emerged highlighting the private public balance present in each social networking site styles of self presentation in spaces privately public and publicly private cultivation of taste performances as a mode of sociocultural identification and organization and the formation of tight or loose social settings facebook emerged as the architectural equivalent of a glasshouse with a publicly open structure looser behavioral norms and an abundance of tools that members use to leave cues for each other linkedin and asmallworld produced tighter spaces which were consistent with the taste ethos of each network and offered less room for spontaneous interaction and generation
we offer a solution to a long standing problem in the physics of networks the creation of a plausible solvable model of a network that displays clustering or transitivity the propensity for two neighbors of a network node also to be neighbors of one another we show how standard random graph models can be generalized to incorporate clustering and give exact solutions for various properties of the resulting networks including sizes of network components size of the giant component if there is one position of the phase transition at which the giant component forms and position of the phase transition for percolation on network
prions cause transmissible neurodegenerative diseases and replicate by conformational conversion of normal benign forms of prion protein prp c to disease causing prp sc isoforms a systems approach to disease postulates that disease arises from perturbation of biological networks in the relevant organ we tracked global gene expression in the brains of eight distinct mouse strain prion strain combinations throughout the progression of the disease to capture the effects of prion strain host genetics and prp concentration on disease incubation time subtractive analyses exploiting various aspects of prion biology and infection identified a core of differentially expressed genes degs that appeared central to prion disease degs were mapped into functional pathways and networks reflecting defined neuropathological events and prp sc replication and accumulation enabling the identification of novel modules and modules that may be involved in genetic effects on incubation time and in prion strain specificity our systems analysis provides a comprehensive basis for developing models for prion replication and disease and suggests some possible approaches
proteomics has made tremendous progress attaining throughput and comprehensiveness so far only seen in genomics technologies the consequent avalanche of proteome level data poses great analytical challenges for downstream interpretation we review bioinformatic analysis of qualitative and quantitative proteomic data focusing on current and emerging paradigms employed for functional analysis data mining and knowledge discovery from high resolution quantitative mass spectrometric data many bioinformatics tools developed for microarrays can be reused in proteomics however the uniquely quantitative nature of proteomics data also offers entirely novel analysis possibilities which directly suggest and illuminate mechanisms
background in computational biology one often faces the problem of deriving the causal relationship among different elements such as genes proteins metabolites neurons and so on based upon multi dimensional temporal data currently there are two common approaches used to explore the network structure among elements one is the granger causality approach and the other is the dynamic bayesian network inference approach both have at least a few thousand publications reported in the literature a key issue is to choose which approach is used to tackle the data in particular when they give rise to contradictory results results in this paper we provide an answer by focusing on a systematic and computationally intensive comparison between the two approaches on both synthesized and experimental data for synthesized data a critical point of the data length is found the dynamic bayesian network outperforms the granger causality approach when the data length is short and vice versa we then test our results in experimental data of short length which is a common scenario in current biological experiments it is again confirmed that the dynamic bayesian network works better conclusion when the data size is short the dynamic bayesian network inference performs better than the granger causality approach otherwise the granger causality approach better
advances in experimental and computational methods have quietly ushered in a new era in protein function annotation this age of multiplicity is marked by the notion that only the use of multiple tools multiple evidence and considering the multiple aspects of function can give us the broad picture that century biology will need to link and alter micro and macroscopic phenotypes it might also help us to undo past mistakes by removing errors from our databases and prevent us from producing more on the downside multiplicity is often confusing we therefore systematically review methods and resources for automated protein function prediction looking at individual biochemical and contextual network respectively
the advent of high throughput sequencing hts methods has enabled direct approaches to quantitatively profile small rna populations however these methods have been limited by several factors including representational artifacts and lack of established statistical methods of analysis furthermore massive hts data sets present new problems related to data processing and mapping to a reference genome here we show that cluster based sequencing by synthesis technology is highly reproducible as a quantitative profiling tool for several classes of small rna from arabidopsis thaliana we introduce the use of synthetic rna oligoribonucleotide standards to facilitate objective normalization between hts data sets and adapt microarray type methods for statistical analysis of multiple samples these methods were tested successfully using mutants with small rna biogenesis mirna defective mutant and sirna defective triple mutant or effector protein mutant deficiencies computational methods were also developed to rapidly and accurately parse quantify and map small data
at brown university there is excitement of having access to the brown corpus containing one million english words since then we have seen several notable corpora that are about times larger and in google released a trillion word corpus with frequency counts for all sequences up to five words long in some ways this corpus is a step backwards from the brown corpus it s taken from unfiltered web pages and thus contains incomplete sentences spelling errors grammatical errors and all sorts of other errors it s not annotated with carefully hand corrected part of speech tags but the fact that it s a million times larger than the brown corpus outweighs these drawbacks a trillion word corpus along with other web derived corpora of millions billions or trillions of links videos images tables and user interactions captures even very rare aspects of human behavior so this corpus could serve as the basis of a complete model for certain tasks if only we knew how to extract the model from data
the need of tools for design and evaluation of pedestrian areas subways stations entrance hall shopping mall escape routes stadium etc lead to the necessity of a pedestrian model one approach pedestrian model is microscopic pedestrian simulation model to be able to develop and calibrate a microscopic pedestrian simulation model a number of variables need to be considered as the first step of model development some data was collected using video and the coordinate of the head path through image processing were also taken several numbers of variables can be gathered to describe the behavior of pedestrian from a different point of view this paper describes how to obtain variables from video taking and simple image processing that can represent the movement of pedestrians and variables
the effect of loading on the chloride penetration into plain concrete pc and fiber reinforced concrete frc was studied experimentally by using modified nt build non steady state chloride migration test that include the application of loading on the specimen during the test three types of polypropylene fibers with different lengths and shapes were used the concretes were tested for chloride penetration at different stress ratios under static and cyclic loading the results of the static loading showed that there was a slight reduction in the chloride penetration under low level of compressive stress while an increase in the chloride penetration was found at higher stress level there are significance difference in chloride penetration behavior of the plain concrete long fiber frc and short fiber frc chloride penetration increased even more at cyclic loading conditions showing difference behavior of frc and pc at difference number of cycle and level
operators of online social networks are increasingly sharing potentially sensitive information about users and their relationships with advertisers application developers and data mining researchers privacy is typically protected by anonymization i e removing names addresses etc we present a framework for analyzing privacy and anonymity in social networks and develop a new re identification algorithm targeting anonymized social network graphs to demonstrate its effectiveness on real world networks we show that a third of the users who can be verified to have accounts on both twitter a popular microblogging service and flickr an online photo sharing site can be re identified in the anonymous twitter graph with only a error rate our de anonymization algorithm is based purely on the network topology does not require creation of a large number of dummy sybil nodes is robust to noise and all existing defenses and works even when the overlap between the target network and the adversary s auxiliary information small
the majority of existing computational tools rely on sequence homology and or structural similarity to identify novel microrna mirna genes recently supervised algorithms are utilized to address this problem taking into account sequence structure and comparative genomics information in most of these studies mirna gene predictions are rarely supported by experimental evidence and prediction accuracy remains uncertain in this work we present a new computational tool sscprofiler utilizing a probabilistic method based on profile hidden markov models to predict novel mirna precursors via the simultaneous integration of biological features such as sequence structure and conservation sscprofiler achieves a performance accuracy of sensitivity and specificity on a large set of human mirna genes the trained classifier is used to identify novel mirna gene candidates located within cancer associated genomic regions and rank the resulting predictions using expression information from a full genome tiling array finally four of the top scoring predictions are verified experimentally using northern blot analysis our work combines both analytical and experimental techniques to show that sscprofiler is a highly accurate tool which can be used to identify novel mirna gene candidates in the human genome sscprofiler is freely available as a web service at http www imbb forth gr sscprofiler nar
eukaryotic dna replication is highly stratified with different genomic regions shown to replicate at characteristic times during s phase here we observe that mutation rate as reflected in recent evolutionary divergence and human nucleotide diversity is markedly increased in later replicating regions of the human genome all classes of substitutions are affected suggesting a generalized mechanism involving replication time dependent dna damage this correlation between mutation rate and regionally stratified replication timing may have substantial implications
many important proteinprotein interactions are mediated by the binding of a short peptide stretch in one protein to a large globular segment in another recent efforts have provided hundreds of examples of new peptides binding to proteins for which a three dimensional structure is available either known experimentally or readily modeled but where no structure of the proteinpeptide complex is known to address this gap we present an approach that can accurately predict peptide binding sites on protein surfaces for peptides known to bind a particular protein the method predicts binding sites with great accuracy and the specificity of the approach means that it can also be used to predict whether or not a putative or predicted peptide partner will bind we used known proteinpeptide complexes to derive preferences in the form of spatial position specific scoring matrices which describe the binding site environment in globular proteins for each type of amino acid in bound peptides we then scan the surface of a putative binding protein for sites for each of the amino acids present in a peptide partner and search for combinations of high scoring amino acid sites that satisfy constraints deduced from the peptide sequence the method performed well in a benchmark and largely agreed with experimental data mapping binding sites for several recently discovered interactions mediated by peptides including rg rich proteins with smn domains epstein barr virus with tradd domains with and the ago hook with argonaute piwi domain the method and associated statistics is an excellent tool for predicting and studying binding sites for newly discovered peptides mediating critical events biology
abstract background a common method for presenting and studying biological interaction networks is visualization software tools can enhance our ability to explore network visualizations and improve our understanding of biological systems particularly when these tools offer analysis capabilities however most published network visualizations are static representations that do not support user interaction results jnets was designed as a network visualization tool that incorporates annotation to explore the underlying features of interaction networks the software is available as an application and a configurable applet that can provide a flexible and dynamic online interface to many types of network data as a case study we use jnets to investigate approved drug targets present within the hiv human protein interaction network our software highlights the intricate influence that hiv has on the host immune response conclusions jnets is a software tool that allows interaction networks to be visualized and studied remotely from within a standard web page therefore using this free software network data can be presented in an enhanced interactive format more information about jnets is available at http www manchester ac uk jnets
during the advances in the abilities to perform computer simulations of chemical and biomolecular systems and to calculate free energy changes led to the expectation that such methodology would soon show great utility for guiding molecular design important potential applications included design of selective receptors catalysts and regulators of biological function including enzyme inhibitors this time also saw the rise of high throughput screening and combinatorial chemistry along with complementary computational methods for de novo design and virtual screening including docking these technologies appeared poised to deliver diverse lead compounds for any biological target as with many technological advances realization of the expectations required significant additional effort and time however as summarized here striking success has now been achieved for computer aided drug lead generation and optimization de novo design using both molecular growing and docking are illustrated for lead generation and lead optimization features free energy perturbation calculations in conjunction with monte carlo statistical mechanics simulations for proteininhibitor complexes in aqueous solution the specific applications are to the discovery of non nucleoside inhibitors of hiv reverse transcriptase hiv rt and inhibitors of the binding of the proinflammatory cytokine mif to its receptor a standard protocol is presented that includes scans for possible additions of small substituents to a molecular core interchange of heterocycles and focused optimization of substituents at one site initial leads with activities at low micromolar concentrations have been advanced rapidly to low inhibitors
abstract nbsp nbsp this article reviews recent evaluation studies of online learning communities to provide a systematic understanding of how such communities are evaluated forty two representative studies were selected and categorized into a newly developed taxonomy of online learning community evaluations this taxonomy is divided into four components evaluation purpose evaluation approach measures for evaluation and evaluation techniques the findings suggest that it is inappropriate to conceptualize evaluation of such communities as a one size fits all generalizable measure of good or bad instead we recommend a comprehensive on going diagnostic approach to measuring clusters of indicators or syndromes of a particular olc and examining the causal relation assumed by the evaluators between what is measured and the success of olc as an outcome
transcription factors govern gene expression by binding to short dna sequences called cis regulatory elements these sequences are typically located in promoters which are regions of variable length upstream of the open reading frames of genes here we report that promoter length and gene function are related in yeast fungi and plants in particular the promoters for stress responsive genes are in general longer than those of other genes essential genes have on the other hand relatively short promoters we utilize these findings in a novel method for identifying relevant cis regulatory elements in a set of co expressed genes the method is shown to generate more accurate results and fewer false positives compared to other common procedures our results suggest that genes with complex transcriptional regulation tend to have longer promoters than genes responding to few signals this phenomenon is present in all investigated species indicating that evolution adjust promoter length according to gene function identification of cis regulatory elements in saccharomyces cerevisiae can be done with the web service located at http enricher zool gu molbev
an overview is given of various dark matter candidates among the many suggestions given in the literature axions inert higgs doublet sterile neutrinos supersymmetric particles and kaluza klein particles are discussed the situation has recently become very interesting with new results on antimatter in the cosmic rays having dark matter as one of the leading possible explanations problems arising from this explanation and possible solutions are discussed and the importance of new measurements is emphasized if the explanation is indeed dark matter a whole new field of physics with unusual although not impossible mass and interaction properties may soon open itself discovery
capturing defining and modeling the essence of context are challenging compelling and prominent issues for interdisciplinary research and discussion the roots of its emergence lie in the inconsistencies and ambivalent definitions across and within different research specializations e g philosophy psychology pragmatics linguistics computer science and artificial intelligence within the area of computer science the advent of mobile context aware computing has stimulated broad and contrasting interpretations due to the shift from traditional static desktop computing to heterogeneous mobile environments this transition poses many challenging complex and largely unanswered research issues relating to contextual interactions and usability to address those issues many researchers strongly encourage a multidisciplinary approach the primary aim of this article is to review and unify theories of context within linguistics computer science and psychology summary models within each discipline are used to propose an outline and detailed multidisciplinary model of context involving a the differentiation of focal and contextual aspects of the user and application s world b the separation of meaningful and incidental dimensions and c important user and application processes the models provide an important foundation in which complex mobile scenarios can be conceptualized and key human and social issues can be identified the models were then applied to different applications of context aware computing involving user communities and mobile tourist guides the authors future work involves developing a user centered multidisciplinary design framework based on their proposed models this will be used to design a large scale user study investigating the usability issues of a context aware mobile computing navigation aid for visually people
the profusion of high throughput instruments and the explosion of new results in the scientific literature particularly in molecular biomedicine is both a blessing and a curse to the bench researcher even knowledgeable and experienced scientists can benefit from computational tools that help navigate this vast and rapidly evolving terrain in this paper we describe a novel computational approach to this challenge a knowledge based system that combines reading reasoning and reporting methods to facilitate analysis of experimental data reading methods extract information from external resources either by parsing structured data or using biomedical language processing to extract information from unstructured data and track knowledge provenance reasoning methods enrich the knowledge that results from reading by for example noting two genes that are annotated to the same ontology term or database entry reasoning is also used to combine all sources into a knowledge network that represents the integration of all sorts of relationships between a pair of genes and to calculate a combined reliability score reporting methods combine the knowledge network with a congruent network constructed from experimental data and visualize the combined network in a tool that facilitates the knowledge based analysis of that data an implementation of this approach called the hanalyzer is demonstrated on a large scale gene expression array dataset relevant to craniofacial development the use of the tool was critical in the creation of hypotheses regarding the roles of four genes never previously characterized as involved in craniofacial development each of these hypotheses was validated by further work
a key challenge of the post genomic era is the identification of the function s of all the molecules in a given organism here we review the status of sequence and structure based approaches to protein function inference and ligand screening that can provide functional insights for a significant fraction of the of orfs of unassigned function in an average proteome we then describe findsite a recently developed algorithm for ligand binding site prediction ligand screening and molecular function prediction which is based on binding site conservation across evolutionary distant proteins identified by threading importantly findsite gives comparable results when high resolution experimental structures as well as predicted protein models used
abstract background next generation sequencing ngs platforms are currently being utilized for targeted sequencing of candidate genes or genomic intervals to perform sequence based association studies to evaluate these platforms for this application we analyzed human sequence generated by the roche illumina ga and the abi solid technologies for the same kb in four individuals results local sequence characteristics contribute to systematic variability in sequence coverage fold difference in per base coverage resulting in patterns for each ngs technology that are highly correlated between samples a comparison of the base calls to kb of overlapping abi sanger sequence generated for the same samples showed that the ngs platforms all have high sensitivity identifying of variant sites at high coverage depth base calling errors are systematic resulting from local sequence contexts as the coverage is lowered additional random sampling errors in base calling occur conclusions our study provides important insights into systematic biases and data variability that need to be considered when utilizing ngs platforms for population targeted studies
we review the theory of the temperature anisotropy and polarization of the cosmic microwave background cmb radiation and describe what we have learned from current cmb observations in particular we discuss how the cmb is being used to provide precise measurements of the composition and geometry of the observable universe and to constrain the physics of the early universe we also briefly review the physics of the small scale cmb fluctuations generated during and after the epoch of reionization and which are the target of a new breed of arcminute instruments
achieving coherent quantum control over massive mechanical resonators is a current research goal nano and micromechanical devices can be coupled to a variety of systems for example to single electrons by or magnetic and to photons by radiation or optical dipole so far all such experiments have operated in a regime of weak coupling in which reversible energy exchange between the mechanical device and its coupled partner is suppressed by fast decoherence of the individual systems to their local environments controlled quantum experiments are in principle not possible in such a regime but instead require strong coupling so far this has been demonstrated only between microscopic quantum systems such as atoms and photons in the context of cavity quantum or solid state qubits and strong coupling is an essential requirement for the preparation of mechanical quantum states such as squeezed or entangled and also for using mechanical resonators in the context of quantum information processing for example as quantum transducers here we report the observation of optomechanical normal mode which provides unambiguous evidence for strong coupling of cavity photons to a mechanical resonator this paves the way towards full quantum optical control of nano and devices
a principal task in dissecting the genetics of complex traits is to identify causal genes for disease phenotypes we previously developed a method to infer causal relationships among genes through the integration of dna variation gene transcription and phenotypic information here we have validated our method through the characterization of transgenic and knockout mouse models of genes predicted to be causal for abdominal obesity perturbation of eight out of the nine genes with and being newly confirmed resulted in significant changes in obesity related traits liver expression signatures revealed alterations in common metabolic pathways and networks contributing to abdominal obesity and overlapped with a macrophage enriched metabolic network module that is highly associated with metabolic traits in mice and humans integration of gene expression in the design and analysis of traditional intercross studies allows high confidence prediction of causal genes and identification of pathways and involved
we report a targeted cost effective method to quantify rare single nucleotide polymorphisms from pooled human genomic dna using second generation sequencing we pooled dna from individuals and targeted four genes to identify rare germline variants our base calling algorithm snpseeker derived from large deviation theory detected single nucleotide polymorphisms present at frequencies below the raw error rate of the platform
the orchestrated binding of transcriptional activators and repressors to specific dna sequences in the context of chromatin defines the regulatory program of eukaryotic genomes we developed a digital approach to assay regulatory protein occupancy on genomic dna in vivo by dense mapping of individual dnase i cleavages from intact nuclei using massively parallel dna sequencing analysis of million cleavages across the saccharomyces cerevisiae genome revealed thousands of protected regulatory protein footprints enabling de novo derivation of factor binding motifs and the identification of hundreds of new binding sites for major regulators we observed striking correspondence between single nucleotide resolution dnase i cleavage patterns and protein dna interactions determined by crystallography the data also yielded a detailed view of larger chromatin features including positioned nucleosomes flanking factor binding regions digital genomic footprinting should be a powerful approach to delineate the cis regulatory framework of any organism with an available sequence
gr comprehensive understanding of functional elements in the human genome will require thorough interrogation and comparison of individual human genomes and genomic structures such an endeavor will require improvements in the throughputs and costs of dna sequencing next generation sequencing platforms have impressively low costs and high throughputs but are limited by short read lengths an immediate and widely recognized solution to this critical limitation is the paired end tag pet sequencing for various applications collectively called the pet sequencing strategy in which short and paired tags are extracted from the ends of long dna fragments for ultra high throughput sequencing the pet sequences can be accurately mapped to the reference genome thus demarcating the genomic boundaries of pet represented dna fragments and revealing the identities of the target dna elements pet protocols have been developed for the analyses of transcriptomes transcription factor binding sites epigenetic sites such as histone modification sites and genome structures the exclusive advantage of the pet technology is its ability to uncover linkages between the two ends of dna fragments using this unique feature unconventional fusion transcripts genome structural variations and even molecular interactions between distant genomic elements can be unraveled by pet analysis extensive use of pet data could lead to efficient assembly of individual human genomes transcriptomes and interactomes enabling new biological and clinical insights with its versatile and powerful nature for dna analysis the pet sequencing strategy has a bright ahead
micrornas mirnas are known to post transcriptionally regulate target mrnas through the utr which interacts mainly with the end of mirna in animals limited knowledge of the mechanism of translation repression and the challenge of identifying real mirna targets among the many predicted have hindered the use of mirna in biomedical applications here we identify many endogenous motifs within human utrs specific to the ends of mirnas the end of conserved mirnas in particular have significant interaction sites in the human enriched less conserved utr mirna motifs while human specific mirnas have significant interaction sites only in the conserved utr motifs implying both mirna and utr are actively evolving in response to each other additionally many mirnas with their end interaction sites in the utrs turn out to simultaneously contain end interaction sites in the utrs based on these findings we demonstrate combinatory interactions between a single mirna and both end regions of an mrna using model systems by experimentally validating mirna functional dependency on both utrs we further show that genes exhibiting large scale protein changes due to mirna overexpression or deletion contain both utr interaction sites predicted we provide the predicted targets of this new mirna target class mibridge as an efficient way to screen potential targets especially for non conserved mirnas since the target search space is reduced by an order of magnitude compared with considering the utr alone efficacy is confirmed by showing regulation with hsa mir a mirna identified only in primate opening the door to the study of non conserved mirnas finally mirnas and associated proteins involved in this new targeting class may prevent ribosome scanning through the utr and keep it from reaching the start codon association
this exciting and pioneering new overview of multiagent systems which are online systems composed of multiple interacting intelligent agents i e online trading offers a newly seen computer science perspective on multiagent systems while integrating ideas from operations research game theory economics logic and even philosophy and linguistics the authors emphasize foundations to create a broad and rigorous treatment of their subject with thorough presentations of distributed problem solving game theory multiagent communication and learning social choice mechanism design auctions cooperative game theory and modal logics of knowledge and belief for each topic basic concepts are introduced examples are given proofs of key results are offered and algorithmic considerations are examined an appendix covers background material in probability theory classical logic markov decision processes and mathematical programming written by two of the leading researchers of this engaging field this book will surely serve as the reference for researchers in the fastest growing area of computer science and be used as a text for advanced undergraduate or courses
micrornas are short non coding rnas that regulate the stability and translation of mrnas profiling experiments using microarray or deep sequencing technology have identified micrornas that are preferentially expressed in certain tissues specific stages of development or disease states such as cancer deep sequencing utilizes massively parallel sequencing generating millions of small rna sequence reads from a given sample profiling of micrornas by deep sequencing measures absolute abundance and allows for the discovery of novel micrornas that have eluded previous cloning and standard sequencing efforts public databases provide in silico predictions of microrna gene targets by various algorithms to better determine which of these predictions represent true positives microrna expression data can be integrated with gene expression data to identify putative microrna mrna functional pairs here we discuss tools and methodologies for the analysis of microrna expression data from deep bib
the change history of a software project contains a rich collection of code changes that record previous development experience changes that fix bugs are especially interesting since they record both the old buggy code and the new fixed code this paper presents a bug finding algorithm using bug fix memories a project specific bug and fix knowledge base developed by analyzing the history of bug fixes a bug finding tool bugmem implements the algorithm the approach is different from bug finding tools based on theorem proving or static model checking such as bandera esc java findbugs jlint and pmd since these tools use pre defined common bug patterns to find bugs they do not aim to identify project specific bugs bug fix memories use a learning process so the bug patterns are project specific and project specific bugs can be detected the algorithm and tool are assessed by evaluating if real bugs and fixes in project histories can be found in the bug fix memories analysis of five open source projects shows that for these projects of bugs appear repeatedly in the memories and of bug and fix pairs are found in memories the results demonstrate that project specific bug fix patterns occur frequently enough to be useful as a bug detection technique furthermore for the bug and fix pairs it is possible to both detect the bug and provide a strong suggestion for the fix however there is also a high false positive rate with of non bug containing changes also having patterns found in the memories a comparison of bugmem with a bug finding tool pmd shows that the bug sets identified by both tools are mostly exclusive indicating that bugmem complements other bug tools
the large scale generation and integration of genomic proteomic signalling and metabolomic data are increasingly allowing the construction of complex networks that provide a new framework for understanding the molecular basis of physiological or pathophysiological states network based drug discovery aims to harness this knowledge to investigate and understand the impact of interventions such as candidate drugs on the molecular networks that define these states in this article we describe how such an approach offers a novel way to understand biology characterize disease and ultimately develop improved therapies and discuss the challenges to realizing goals
systems biology approaches are extensively used to model and reverse engineer gene regulatory networks from experimental data conversely synthetic biology allows de novo construction of a regulatory network to seed new functions in the cell at present the usefulness and predictive ability of modeling and reverse engineering cannot be assessed and compared rigorously we built in theyeast saccharomyces cerevisiae a synthetic network irma for invivo benchmarking of reverse engineering and modeling approaches the network is composed of five genes regulating each other through a variety of regulatory interactions it is negligibly affected by endogenous genes and it is responsive to small molecules we measured time series and steady state expression data after multiple perturbations these data were used to assess state of the art modeling and reverse engineering techniques a semiquantitative model was able to capture and predict the behavior of the network reverse engineering based on differential equations and bayesian networks correctly inferred regulatory interactions from the data
one of the philosophical problems in neuroscience is seeing the trees before the forest indeed it is essential to know how local events fit into the whole picture but we must also look beyond correlations between stimuli and neural responses in one sensory nucleus a lifelong dialogue between the environment and human brain begins at birth different environments enable different experiences each experience is unique because culture gives it meaning the eye tells the brain about each experience and the context in which it occurs the brain records transmits and recalls all events in their proper sequence and with attendant emotion culture gives behavior meaning it helps us see beyond technique alone to better integrate diverse approaches to answering key questions about how the works
for centuries scientists have attempted to identify and document analytical laws that underlie physical phenomena in nature despite the prevalence of computing power the process of finding natural laws and their corresponding equations has resisted automation a key challenge to finding analytic relations automatically is defining algorithmically what makes a correlation in observed data important and insightful we propose a principle for the identification of nontriviality we demonstrated this approach by automatically searching motion tracking data captured from various physical systems ranging from simple harmonic oscillators to chaotic double pendula without any prior knowledge about physics kinematics or geometry the algorithm discovered hamiltonians lagrangians and other laws of geometric and momentum conservation the discovery rate accelerated as laws found for simpler systems were used to bootstrap explanations for more complex systems gradually uncovering the alphabet used to describe systems
the basis of science is the hypothetico deductive method and the recording of experiments in sufficient detail to enable reproducibility we report the development of robot scientist adam which advances the automation of both adam has autonomously generated functional genomics hypotheses about the yeast saccharomyces cerevisiae and experimentally tested these hypotheses by using laboratory automation we have confirmed adam s conclusions through manual experiments to describe adam s research we have developed an ontology and logical language the resulting formalization involves over different research units in a nested treelike structure levels deep that relates the million biomass measurements to their logical description this formalization describes how a machine contributed to scientific science
high throughput experimental techniques are generating large data collections with the aim of identifying novel entities involved in fundamental cellular processes as well as drawing a systematic picture of the relationships between individual components determining the accuracy of the resulting data and the selection of a subset of targets for more careful characterizations often requires relying on information provided by manually annotated data repositories these repositories are incomplete and cover only a small fraction of the knowledge contained in the literature we propose in this paper the use of text mining technologies to extract organize and present information relevant for a particular biological topic the aims of the resulting approach are to enable topic centric biological literature navigation to assist in the construction of manually revised data repositories to provide prioritization of biological entities for experimental studies and to enable human interpretation of large scale experiments by providing direct links of bio entities to relevant descriptions in literature
the ability to quantitatively survey the global behavior of transcriptomes has been a key milestone in the field of systems biology enabled by the advent of dna microarrays while this approach has literally transformed our vision and approach to cellular physiology microarray technology has always been limited by the requirement to decide a priori what regions of the genome to examine while very high density tiling arrays have reduced this limitation for simpler organisms it remains an obstacle for larger more complex eukaryotic genomes the recent development of next generation massively parallel sequencing mps technologies by companies such as roche gs flx illumina genome analyzer ii and abi ab solid has completely transformed the way in which quantitative transcriptomics can be done these new technologies have reduced both the cost per reaction and time required by orders of magnitude making the use of sequencing a cost effective option for many experimental approaches one such method that has recently been developed uses mps technology to directly survey the rna content of cells without requiring any of the traditional cloning associated with est sequencing this approach called rna seq can generate quantitative expression scores that are comparable to microarrays with the added benefit that the entire transcriptome is surveyed without the requirement of a priori knowledge of transcribed regions the important advantage of this technique is that not only can quantitative expression measures be made but transcript structures including alternatively spliced transcript isoforms can also be identified this article discusses the experimental approach for both sample preparation and data analysis for the technique of seq
a substantial challenge in engineering molecular motors is designing mechanisms to coordinate the motion between multiple domains of the motor so as to bias random thermal motion for bipedal motors this challenge takes the form of coordinating the movement of the biped s legs so that they can move in a synchronized fashion to address this problem we have constructed an autonomous dna bipedal walker that coordinates the action of its two legs by cyclically catalyzing the hybridization of metastable dna fuel strands this process leads to a chemically ratcheted walk along a directionally polar dna track by covalently cross linking aliquots of the walker to its track in successive walking states we demonstrate that this brownian motor can complete a full walking cycle on a track whose length could be extended for longer walks we believe that this study helps to uncover principles behind the design of unidirectional devices that can function without intervention this device should be able to fulfill roles that entail the performance of useful mechanical work on the nanometer science
we modeled the mobility of mobile phone users in order to study the fundamental spreading patterns that characterize a mobile virus outbreak we find that although bluetooth viruses can reach all susceptible handsets with time they spread slowly because of human mobility offering ample opportunities to deploy antiviral software in contrast viruses using multimedia messaging services could infect all users in hours but currently a phase transition on the underlying call graph limits them to only a small fraction of the susceptible users these results explain the lack of a major mobile virus breakout so far and predict that once a mobile operating system s market share reaches the phase transition point viruses will pose a serious threat to communications
pnas natural selection operating in protein coding genes is often studied by examining the ratio of the rates of nonsynonymous to synonymous nucleotide substitution the branch site method bsm based on a likelihood ratio test is one of such tests to detect positive selection for a predetermined branch of a phylogenetic tree however because the number of nucleotide substitutions involved is often very small we conducted a computer simulation to examine the reliability of bsm in comparison with the small sample method ssm based on fisher s exact test the results indicate that bsm often generates false positives compared with ssm when the number of nucleotide substitutions is or smaller because the value is also used for predicting positively selected sites we examined the reliabilities of the site prediction methods using nucleotide sequence data for the dim light and color vision genes in vertebrates the results showed that the site prediction methods have a low probability of identifying functional changes of amino acids experimentally determined and often falsely identify other sites where amino acid substitutions are unlikely to be important this low rate of predictability occurs because most of the current statistical methods are designed to identify codon sites with high values which may not have anything to do with functional changes the codon sites showing functional changes generally do not show a high value to understand adaptive evolution some form of experimental confirmation necessary
protein conformational changes and dynamic behavior are fundamental for such processes as catalysis regulation and substrate recognition although protein dynamics have been successfully explored in computer simulation there is an intermediate scale of motions that has proven difficult to simulatethe motion of individual segments or domains that move independently of the body the protein here we introduce a molecular dynamics perturbation method the rotamerically induced perturbation rip which can generate large coherent motions of structural elements in picoseconds by applying large torsional perturbations to individual sidechains despite the large scale motions secondary structure elements remain intact without the need for applying backbone positional restraints owing to its computational efficiency rip can be applied to every residue in a protein producing a global map of deformability this map is remarkably sparse with the dominant sites of deformation generally found on the protein surface the global map can be used to identify loops and helices that are less tightly bound to the protein and thus are likely sites of dynamic modulation that may have important functional consequences additionally they identify individual residues that have the potential to drive large scale coherent conformational change applying rip to two well studied proteins dihdydrofolate reductase and triosephosphate isomerase which possess functionally relevant mobile loops that fluctuate on the microsecond millisecond timescale the rip deformation map identifies and recapitulates the flexibility of these elements in contrast the rip deformation map of lytic protease a kinetically stable protein results in a map with no significant deformations in the n terminal domain of the rip deformation map clearly identifies the ligand binding lid as a highly flexible region capable of large conformational changes in the estrogen receptor ligand binding domain the rip deformation map is quite sparse except for one large conformational change involving helix which is the structural element that allosterically links ligand binding to receptor activation rip analysis has the potential to discover sites of functional conformational changes and the linchpin residues critical in determining these states
background for the past years the sanger method has been the dominant approach and gold standard for dna sequencing the commercial launch of the first massively parallel pyrosequencing platform in ushered in the new era of high throughput genomic analysis now referred to as next generation sequencing ngs content this review describes fundamental principles of commercially available ngs platforms although the platforms differ in their engineering configurations and sequencing chemistries they share a technical paradigm in that sequencing of spatially separated clonally amplified dna templates or single dna molecules is performed in a flow cell in a massively parallel manner through iterative cycles of polymerase mediated nucleotide extensions or in one approach through successive oligonucleotide ligations sequence outputs in the range of hundreds of megabases to gigabases are now obtained routinely highlighted in this review are the impact of ngs on basic research bioinformatics considerations and translation of this technology into clinical diagnostics also presented is a view into future technologies including real time single molecule dna sequencing and nanopore based sequencing summary in the relatively short time frame since ngs has fundamentally altered genomics research and allowed investigators to conduct experiments that were previously not technically feasible or affordable the various technologies that constitute this new paradigm continue to evolve and further improvements in technology robustness and process streamlining will pave the path for translation into diagnostics
pnas many biological pathways were first uncovered by identifying mutants with visible phenotypes and by scoring every sample in a screen via tedious and subjective visual inspection now automated image analysis can effectively score many phenotypes in practical application customizing an image analysis algorithm or finding a sufficient number of example cells to train a machine learning algorithm can be infeasible particularly when positive control samples are not available and the phenotype of interest is rare here we present a supervised machine learning approach that uses iterative feedback to readily score multiple subtle and complex morphological phenotypes in high throughput image based screens first automated cytological profiling extracts hundreds of numerical descriptors for every cell in every image next the researcher generates a rule i e classifier to recognize cells with a phenotype of interest during a short interactive training session using iterative feedback finally all of the cells in the experiment are automatically classified and each sample is scored based on the presence of cells displaying the phenotype by using this approach we successfully scored images in rna interference screens in organisms for the prevalence of diverse cellular morphologies some of which were intractable
the identification of protein protein interaction sites is an essential intermediate step for mutant design and the prediction of protein networks in recent years a significant number of methods have been developed to predict these interface residues and here we review the current status of the field progress in this area requires a clear view of the methodology applied the data sets used for training and testing the systems and the evaluation procedures we have analysed the impact of a representative set of features and algorithms and highlighted the problems inherent in generating reliable protein data sets and in the posterior analysis of the results although it is clear that there have been some improvements in methods for predicting interacting sites several major bottlenecks remain proteins in complexes are still under represented in the structural databases and in particular many proteins involved in transient complexes are still to be crystallized we provide suggestions for effective feature selection and make it clear that community standards for testing training and performance measures are necessary for progress in the bib
pnas modelers of molecular signaling networks must cope with the combinatorial explosion of protein states generated by posttranslational modifications and complex formation rule based models provide a powerful alternative to approaches that require explicit enumeration of all possible molecular species of a system such models consist of formal rules stipulating the partial contexts wherein specific proteinprotein interactions occur these contexts specify molecular patterns that are usually less detailed than molecular species yet the execution of rule based dynamics requires stochastic simulation which can be very costly it thus appears desirable to convert a rule based model into a reduced system of differential equations by exploiting the granularity at which rules specify interactions we present a formal and automated method for constructing a coarse grained and self consistent dynamical system aimed at molecular patterns that are distinguishable by the dynamics of the original system as posited by the rules the method is formally sound and never requires the execution of the rule based model the coarse grained variables do not depend on the values of the rate constants appearing in the rules and typically form a system of greatly reduced dimension that can be amenable to numerical integration and further model reduction er
we study the ultraviolet complete non relativistic theory recently proposed by horava after introducing a lifshitz scalar for a general background we analyze the cosmology of the model in lorentzian and euclidean signature vacuum solutions are found and it is argued the existence of non singular bouncing profiles we find a general qualitative agreement with both the picture of causal dynamical triangulations and quantum einstein gravity however inflation driven by a lifshitz scalar field on a classical background might not produce a scale invariant spectrum when the principle of detailed balance assumed
pmid small aromatic ring systems are of central importance in the development of novel synthetic protein ligands here we generate a complete list of such ring systems we call this list and associated annotations vehicle which stands for virtual exploratory heterocyclic library searches of literature and compound databases using this list as substructure queries identified only as synthesized using a carefully validated machine learning approach we were able to estimate that the number of unpublished but synthetically tractable vehicle rings could be over however analysis also shows that the rate of publication of novel examples to be as low as per year with this work we aim to provide fresh stimulus to creative organic chemists by highlighting a small set of apparently simple ring systems that are predicted to be tractable but are to the best of our unconquered
bib systems level modelling and simulations of biological processes are proving to be invaluable in obtaining a quantitative and dynamic perspective of various aspects of cellular function in particular constraint based analyses of metabolic networks have gained considerable popularity for simulating cellular metabolism of which flux balance analysis fba is most widely used unlike mechanistic simulations that depend on accurate kinetic data which are scarcely available fba is based on the principle of conservation of mass in a network which utilizes the stoichiometric matrix and a biologically relevant objective function to identify optimal reaction flux distributions fba has been used to analyse genome scale reconstructions of several organisms it has also been used to analyse the effect of perturbations such as gene deletions or drug inhibitions in silico this article reviews the usefulness of fba as a tool for gaining biological insights advances in methodology enabling integration of regulatory information and thermodynamic constraints and finally addresses the challenges that lie ahead various use scenarios and biological insights obtained from fba and applications in fields such metabolic engineering and drug target identification are also discussed genome scale constraint based models have an immense potential for building and testing hypotheses as well as to experimentation
next generation sequencing technology is a powerful tool for transcriptome analysis however under certain conditions only a small amount of material is available which requires more sensitive techniques that can preferably be used at the single cell level here we describe a single cell digital gene expression profiling assay using our mrna seq assay with only a single mouse blastomere we detected the expression of more genes than microarray techniques and identified previously unknown splice junctions called by at least reads moreover of the genes with multiple known transcript isoforms expressed at least two isoforms in the same blastomere or oocyte which unambiguously demonstrated the complexity of the transcript variants at whole genome scale in individual cells finally for and oocytes we found that and genes respectively were abnormally upregulated compared to wild type controls with genes common
at geneva in the presentations of the reports of the international panel on climate change ipcc it was clear that surprisingly little attention had been given to the role of the oceans in climate change few of the general circulation models gcms now operating take adequate account of ocean processes if one is to predict climate as opposed to weather then the oceans must be better understood and key processes incorporated in the models task group concluded that the predicted global warming will have a significant effect on sea level rise and will modify ocean circulation changing marine ecosystems with severe socio economic consequences to follow the task group recommended that a permanent global ocean observing system for the purposes of improving climate change be established and a concerted and coordinated global programme of coastal zone monitoring established
pnas the evolutionary rates of protein coding genes in an organism span approximately orders of magnitude and show a universal approximately log normal distribution in a broad variety of species from prokaryotes to mammals this universal distribution implies a steady state process with identical distributions of evolutionary rates among genes that are gained and genes that are lost a mathematical model of such process is developed under the single assumption of the constancy of the distributions of the propensities for gene loss pgl this model predicts that genes of different ages that is genes with homologs detectable at different phylogenetic depths substantially differ in those variables that correlate with pgl we computationally partition protein coding genes from humans flies and fungus into age classes and show that genes of different ages retain the universal log normal distribution of evolutionary rates with a shift toward higher rates in younger classes but also with a substantial overlap the only exception involves human primate specific genes that show a heavy tail of rapidly evolving genes probably owing to gene annotation artifacts as predicted the gene age classes differ in characteristics correlated with pgl compared with young genes e g mammal specific human ones old genes e g eukaryote specific on average are longer are expressed at a higher level possess a higher intron density evolve slower on the short time scale and are subject to stronger purifying selection thus genome evolution fits a simple model with approximately uniform rates of gene gain and loss without major bursts of innovation
the origin of cooperation is a central challenge to our understanding of evolution the fact that microbial interactions can be manipulated in ways that animal interactions cannot has led to a growing interest in microbial models of cooperation and competition for the budding yeast saccharomyces cerevisiae to grow on sucrose the disaccharide must first be hydrolysed by the enzyme invertase this hydrolysis reaction is performed outside the cytoplasm in the periplasmic space between the plasma membrane and the cell wall here we demonstrate that the vast majority approximately per cent of the monosaccharides created by sucrose hydrolysis diffuse away before they can be imported into the cell serving to make invertase production and secretion a cooperative behaviour a mutant cheater strain that does not produce invertase is able to take advantage of and invade a population of wild type cooperator cells however over a wide range of conditions the wild type cooperator can also invade a population of cheater cells therefore we observe steady state coexistence between the two strains in well mixed culture resulting from the fact that rare strategies outperform common strategies the defining features of what game theorists call the snowdrift game a model of the cooperative interaction incorporating nonlinear benefits explains the origin of this coexistence we are able to alter the outcome of the competition by varying either the cost of cooperation or the glucose concentration in the media finally we note that glucose repression of invertase expression in wild type cells produces a strategy that is optimal for the snowdrift game wild type cells cooperate only when competing against cells
summary we have developed a tool called probematch for matching a large set of oligonucleotide sequences against a genome database using gapped alignments unlike most of the existing tools such as eland which only perform ungapped alignments allowing at most two mismatches probematch generates both ungapped and gapped alignments allowing up to three errors including insertion deletion and mismatch to speedup sequence alignment probematch uses gapped q grams and q grams of various patterns to identify target hits to a query sequence this approach results in fewer initial sequences to examine with no loss in sensitivity probematch has been used to align illumina gaii reads against the human genome which could not be mapped by eland and found alignments for reads of the reads in less than h availability source code is freely available at http www cs wisc edu jignesh probematch contact jignesh cs wisc edu supplementary information supplementary data are available at bioinformatics bioinformatics
all cancers arise as a result of changes that have occurred in the dna sequence of the genomes of cancer cells over the past quarter of a century much has been learnt about these mutations and the abnormal genes that operate in human cancers we are now however moving into an era in which it will be possible to obtain the complete dna sequence of large numbers of cancer genomes these studies will provide us with a detailed and comprehensive perspective on how individual cancers developed
a protein interaction network describes a set of physical associations that can occur between proteins however within any particular cell or tissue only a subset of proteins is expressed and so only a subset of interactions can occur integrating interaction and expression data we analyze here this interplay between protein expression and physical interactions in humans proteins only expressed in restricted cell types like recently evolved proteins make few physical interactions most tissue specific proteins do however bind to universally expressed proteins and so can function by recruiting or modifying core cellular processes conversely most housekeeping proteins that are expressed in all cells also make highly tissue specific protein interactions these results suggest a model for the evolution of tissue specific biology and show that most and possibly all housekeeping proteins actually have important tissue specific interactions
motivation next generation dna sequencing machines are generating an enormous amount of sequence data placing unprecedented demands on traditional single processor read mapping algorithms cloudburst is a new parallel read mapping algorithm optimized for mapping next generation sequence data to the human genome and other reference genomes for use in a variety of biological analyses including snp discovery genotyping and personal genomics it is modeled after the short read mapping program rmap and reports either all alignments or the unambiguous best alignment for each read with any number of mismatches or differences this level of sensitivity could be prohibitively time consuming but cloudburst uses the open source hadoop implementation of mapreduce to parallelize execution using multiple compute nodes results cloudburst s running time scales linearly with the number of reads mapped and with near linear speedup as the number of processors increases in a processor core configuration cloudburst is up to times faster than rmap executing on a single core while computing an identical set of alignments using a larger remote compute cloud with cores cloudburst improved performance by more than fold reducing the running time from hours to mere minutes for typical jobs involving mapping of millions of short reads to the human genome availability cloudburst is available open source as a model for parallelizing algorithms with mapreduce at http cloudburst bio sourceforge net contact mschatz umiacs edu
motivation many biological systems operate in a similar manner across a large number of species or conditions cross species analysis of sequence and interaction data is often applied to determine the function of new genes in contrast to these static measurements microarrays measure the dynamic condition specific response of complex biological systems the recent exponential growth in microarray expression datasets allows researchers to combine expression experiments from multiple species to identify genes that are not only conserved in sequence but also operated in a similar way in the different species studied results in this review we discuss the computational and technical challenges associated with these studies the approaches that have been developed to address these challenges and the advantages of cross species analysis of microarray data we show how successful application of these methods lead to insights that cannot be obtained when analyzing data from a single species we also highlight current open problems and discuss possible ways to them
summary the generic genome browser gbrowse is one of the most widely used tools for visualizing genomic features along a reference sequence however the installation and configuration of gbrowse is not trivial for biologists we have developed a web server webgbrowse that allows users to upload genome annotation in the format configure the display of each genomic feature by simply using a web browser and visualize the configured genomic features with the integrated gbrowse software availability webgbrowse is accessible via http webgbrowse cgb indiana edu and the system is also freely available for local installations contact dongq indiana bioinformatics
pnas a large number of complex systems find a natural abstraction in the form of weighted networks whose nodes represent the elements of the system and the weighted edges identify the presence of an interaction and its relative strength in recent years the study of an increasing number of large scale networks has highlighted the statistical heterogeneity of their interaction pattern with degree and weight distributions that vary over many orders of magnitude these features along with the large number of elements and links make the extraction of the truly relevant connections forming the network s backbone a very challenging problem more specifically coarse graining approaches and filtering techniques come into conflict with the multiscale nature of large scale systems here we define a filtering method that offers a practical procedure to extract the relevant connection backbone in complex multiscale networks preserving the edges that represent statistically significant deviations with respect to a null model for the local assignment of weights to edges an important aspect of the method is that it does not belittle small scale interactions and operates at all scales defined by the weight distribution we apply our method to real world network instances and compare the obtained results with alternative backbone techniques
pnas many complex systems including networks are not static but can display strong fluctuations at various time scales characterizing the dynamics in complex networks is thus of the utmost importance in the understanding of these networks and of the dynamical processes taking place on them in this article we study the example of the us airport network in the time period we show that even if the statistical distributions of most indicators are stationary an intense activity takes place at the local microscopic level with many disappearing appearing connections links between airports we find that connections have a very broad distribution of lifetimes and we introduce a set of metrics to characterize the links dynamics we observe in particular that the links that disappear have essentially the same properties as the ones that appear and that links that connect airports with very different traffic are very volatile motivated by this empirical study we propose a model of dynamical networks inspired from previous studies on firm growth which reproduces most of the empirical observations both for the stationary statistical distributions and for the properties
addressing the sustainable energy crisis in an objective manner this enlightening book analyzes the relevant numbers and organizes a plan for change on both a personal level and an internationalscalefor europe the united states and the world in case study format this informative reference answers questions surrounding nuclear energy the potential of sustainable fossil fuels and the possibilities of sharing renewable power with foreign countries while underlining the difficulty of minimizing consumption the tone remains positive as it debunks misinformation and clearly explains the calculations of expenditure per person to encourage people to make individual changes that will benefit the world large
the traditional view that proteins possess absolute functional specificity and a single fixed structure conflicts with their marked ability to adapt and evolve new functions and structures we consider an alternative avant garde view in which proteins are conformationally dynamic and exhibit functional promiscuity we surmise that these properties are the foundation stones of protein evolvability they facilitate the divergence of new functions within existing folds and the evolution of entirely new folds packing modes of proteins also affect their evolvability and poorly packed disordered and conformationally diverse proteins may exhibit high evolvability this dynamic view of protein structure function and evolvability is extrapolated to describe hypothetical scenarios for the evolution of the early proteins and future research directions in the area of protein dynamism and science
recent studies of cellular networks have revealed modular organizations of genes and proteins for example in interactome networks a module refers to a group of interacting proteins that form molecular complexes and or biochemical pathways and together mediate a biological process however it is still poorly understood how biological information is transmitted between different modules we have developed information flow analysis a new computational approach that identifies proteins central to the transmission of biological information throughout the network in the information flow analysis we represent an interactome network as an electrical circuit where interactions are modeled as resistors and proteins as interconnecting junctions construing the propagation of biological signals as flow of electrical current our method calculates an information flow score for every protein unlike previous metrics of network centrality such as degree or betweenness that only consider topological features our approach incorporates confidence scores of protein protein interactions and automatically considers all possible paths in a network when evaluating the importance of each protein we apply our method to the interactome networks of saccharomyces cerevisiae and caenorhabditis elegans we find that the likelihood of observing lethality and pleiotropy when a protein is eliminated is positively correlated with the protein s information flow score even among proteins of low degree or low betweenness high information scores serve as a strong predictor of loss of function lethality or pleiotropy the correlation between information flow scores and phenotypes supports our hypothesis that the proteins of high information flow reside in central positions in interactome networks we also show that the ranks of information flow scores are more consistent than that of betweenness when a large amount of noisy data is added to an interactome finally we combine gene expression data with interaction data in c elegans and construct an interactome network for muscle specific genes we find that genes that rank high in terms of information flow in the muscle interactome network but not in the entire network tend to play important roles in muscle function this framework for studying tissue specific networks by the information flow model can be applied to other tissues and other organisms well
structure based drug design traditionally uses static protein models as inspirations for focusing on active site targets allosteric regulation of biological macromolecules however is affected by both conformational and dynamic properties of the protein or protein complex and can potentially lead to more avenues for therapeutic development we discuss the advantages of searching for molecules that conformationally trap a macromolecule in its inactive state although multiple methodologies exist to probe protein dynamics and ligand binding our current discussion highlights the use of nuclear magnetic resonance spectroscopy in the drug discovery and design science
synonymous mutations do not alter the encoded protein but they can influence gene expression to investigate how we engineered a synthetic library of genes that varied randomly at synonymous sites but all encoded the same green fluorescent protein gfp when expressed in escherichia coli gfp protein levels varied fold across the library gfp messenger rna mrna levels mrna degradation patterns and bacterial growth rates also varied but codon bias did not correlate with gene expression rather the stability of mrna folding near the ribosomal binding site explained more than half the variation in protein levels in our analysis mrna folding and associated rates of translation initiation play a predominant role in shaping expression levels of individual genes whereas codon bias influences global translation efficiency and fitness
molecular dynamics simulations allow for atomic level characterization of biomolecular processes such as the conformational transitions associated with protein function the computational demands of such simulations however have historically prevented them from reaching the microsecond and greater timescales on which these events often occur recent advances in algorithms software and computer hardware have made microsecond timescale simulations with tens of thousands of atoms practical with millisecond timescale simulations on the horizon this review outlines these advances in high performance molecular dynamics simulation and discusses recent applications to studies of protein dynamics and function as well as experimental validation of the underlying models
pmid we describe a system combining cloud computing and open source software that allows individual laboratories or users to create scalable virtual proteomics analysis clusters and have large scale computational resources at their disposal at a very low cost without the investment in computational hardware or software licensing fees we provide detailed step by step instructions on using these virtual proteomics analysis clusters at the medical college of wisconsin proteomics center web site http proteomics mcw vipdac
author summary the emerging field of metagenomics aims to understand the structure and function of microbial communities solely through dna analysis current metagenomics studies comparing communities resemble large scale clinical trials with multiple subjects from two general populations e g sick and healthy to improve analyses of this type of experimental data we developed a statistical methodology for detecting differentially abundant features between microbial communities that is features that are enriched or depleted in one population versus another we show our methods are applicable to various metagenomic data ranging from taxonomic information to functional annotations we also provide an assessment of taxonomic differences in gut microbiota between lean and obese humans as well as differences between the functional capacities of mature and infant gut microbiomes and those of microbial and viral metagenomes our methods are the first to statistically address differential abundance in comparative metagenomics studies with multiple subjects and we hope will give researchers a more complete picture of how exactly two differ
in this article i propose the classification of the evolutionary stages that a scientific discipline evolves through and the type of scientists that are the most productive at each stage i believe that each scientific discipline evolves sequentially through four stages scientists at stage one introduce new objects and phenomena as subject matter for a new scientific discipline to do this they have to introduce a new language adequately describing the subject matter at stage two scientists develop a toolbox of methods and techniques for the new discipline owing to this advancement in methodology the spectrum of objects and phenomena that fall into the realm of the new science are further understood at this stage most of the specific knowledge is generated at the third stage at which the highest number of original research publications is generated the majority of third stage investigation is based on the initial application of new research methods to objects and or phenomena the purpose of the fourth stage is to maintain and pass on scientific knowledge generated during the first three stages groundbreaking new discoveries are not made at this stage however new ways to present scientific information are generated and crucial revisions are often made of the role of the discipline within the constantly evolving scientific environment the very nature of each stage determines the optimal psychological type and modus operandi of the scientist operating within it thus it is not only the talent and devotion of scientists that determines whether they are capable of contributing substantially but rather whether they have the right type of talent for the chosen scientific discipline at that time understanding the four different evolutionary stages of a scientific discipline might be instrumental for many scientists in optimizing their career path in addition to being useful in assembling scientific teams precluding conflicts and maximizing productivity the proposed model of scientific evolution might also be instrumental for society in organizing and managing the scientific process no public policy aimed at stimulating the scientific process can be equally beneficial for all four stages attempts to apply the same criteria to scientists working on scientific disciplines at different stages of their scientific evolution would be stimulating for one and detrimental for another in addition researchers operating at a certain stage of scientific evolution might not possess the mindset adequate to evaluate and stimulate a discipline that is at a different evolutionary stage this could be the reason for suboptimal implementation of otherwise well conceived policies
the nuclear factor kappab nf kappab transcription factor regulates cellular stress responses and the immune response to infection nf kappab activation results in oscillations in nuclear nf kappab abundance to define the function of these oscillations we treated cells with repeated short pulses of tumor necrosis factor alpha at various intervals to mimic pulsatile inflammatory signals at all pulse intervals that were analyzed we observed synchronous cycles of nf kappab nuclear translocation lower frequency stimulations gave repeated full amplitude translocations whereas higher frequency pulses gave reduced translocation indicating a failure to reset deterministic and stochastic mathematical models predicted how negative feedback loops regulate both the resetting of the system and cellular heterogeneity altering the stimulation intervals gave different patterns of nf kappab dependent gene expression which supports the idea that oscillation frequency has a role
a fundamental problem in genome biology is to elucidate the evolutionary forces responsible for generating nonrandom patterns of genome organization as the first metazoan to benefit from full genome sequencing caenorhabditis elegans has been at the forefront of research in this area studies of genomic patterns and their evolutionary underpinnings continue to be augmented by the recent push to obtain additional full genome sequences of related caenorhabditis taxa in the near future we expect to see major advances with the onset of whole genome resequencing of multiple wild individuals of the same species in this review we synthesize many of the important insights to date in our understanding of genome organization and function that derive from the evolutionary principles made explicit by theoretical population genetics and molecular evolution and highlight fertile areas for future research on unanswered questions in c elegans genome evolution we call attention to the need for c elegans researchers to generate and critically assess nonadaptive hypotheses for genomic and developmental patterns in addition to adaptive scenarios we also emphasize the potential importance of evolution in the gonochoristic female and male ancestors of the androdioecious hermaphrodite and male c elegans as the source for many of its genomic and patterns
libraries private and public offer valuable resources to library patrons as of today the only way to locate information archived exclusively in libraries is through their catalogs library patrons however often find it difficult to formulate a proper query which requires using specific keywords assigned to different fields of desired library catalog records to obtain relevant results these improperly formulated queries often yield irrelevant results or no results at all this negative experience in dealing with existing library systems turns library patrons away from directly querying library catalogs instead they rely on web search engines to perform their searches first and upon obtaining the initial information e g titles subject headings or authors on the desired library materials they query library catalogs this searching strategy is an evidence of failure of today s library systems in solving this problem we propose an enhanced library system which allows partial similarity matching of a tags defined by ordinary users at a folksonomy site that describe the content of books and b unrestricted keywords specified by an ordinary library patron in a query to search for relevant library catalog records the proposed library system allows patrons posting a query q using commonly used words and ranks the retrieved results according to their degrees of resemblance with q while maintaining the query processing time comparable with that achieved by current library search engines copy wiley inc
in microorganisms noise in gene expression gives rise to cell to cell variability in protein in mammalian cells protein levels also and individual cells differ widely in their responsiveness to uniform physiological in the case of apoptosis mediated by trail tumour necrosis factor tnf related apoptosis inducing ligand it is common for some cells in a clonal population to die while others survivea striking divergence in cell fate among cells that die the time between trail exposure and caspase activation is highly variable here we image sister cells expressing reporters of caspase activation and mitochondrial outer membrane permeabilization after exposure to trail we show that naturally occurring differences in the levels or states of proteins regulating receptor mediated apoptosis are the primary causes of cell to cell variability in the timing and probability of death in human cell lines protein state is transmitted from mother to daughter giving rise to transient heritability in fate but protein synthesis promotes rapid divergence so that sister cells soon become no more similar to each other than pairs of cells chosen at random our results have implications for understanding fractional killing of tumour cells after exposure to chemotherapy and for variability in mammalian signal transduction general
there is currently considerable enthusiasm around the mapreduce mr paradigm for large scale data analysis although the basic control flow of this framework has existed in parallel sql database management systems dbms for over years some have called mr a dramatically new computing model in this paper we describe and compare both paradigms furthermore we evaluate both kinds of systems in terms of performance and development complexity to this end we define a benchmark consisting of a collection of tasks that we have run on an open source version of mr as well as on two parallel dbmss for each task we measure each system s performance for various degrees of parallelism on a cluster of nodes our results reveal some interesting trade offs although the process to load data into and tune the execution of parallel dbmss took much longer than the mr system the observed performance of these dbmss was strikingly better we speculate about the causes of the dramatic performance difference and consider implementation concepts that future systems should take from both kinds architectures
for more than years the institute for scientific information isi now part of thomson reuters produced the only available bibliographic databases from which bibliometricians could compile large scale bibliometric indicators isi s citation indexes now regrouped under the web of science wos were the major sources of bibliometric data until when scopus was launched by the publisher reed elsevier for those who perform bibliometric analyses and comparisons of countries or institutions the existence of these two major databases raises the important question of the comparability and stability of statistics obtained from different data sources this paper uses macrolevel bibliometric indicators to compare results obtained from the wos and scopus it shows that the correlations between the measures obtained with both databases for the number of papers and the number of citations received by countries as well as for their ranks are extremely high ap there is also a very high correlation when countries papers are broken down by field the paper thus provides evidence that indicators of scientific production and citations at the country level are stable and largely independent of database
questions for richard thaler and cass sunstein amazon com what do you mean by nudge and why do people sometimes need to be nudged thaler and sunstein by a nudge we mean anything that influences our choices a school cafeteria might try to nudge kids toward good diets by putting the healthiest foods at front we think that it s time for institutions including government to become much more user friendly by enlisting the science of choice to make life easier for people and by gentling nudging them in directions that will make their lives better amazon com what are some of the situations where nudges can make a difference thaler and sunstein well to name just a few better investments for everyone more savings for retirement less obesity more charitable giving a cleaner planet and an improved educational system we could easily make people both wealthier and healthier by devising friendlier choice environments or architectures amazon com can you describe a nudge that is now being used successfully thaler and sunstein one example is the save more tomorrow program firms offer employees who are not saving very much the option of joining a program in which their saving rates are automatically increased whenever the employee gets a raise this plan has more than tripled saving rates in some firms and is now offered by thousands of employers amazon com what is choice architecture and how does it affect the average person s daily life thaler and sunstein choice architecture is the context in which you make your choice suppose you go into a cafeteria what do you see first the salad bar or the burger and fries stand where s the chocolate cake where s the fruit these features influence what you will choose to eat so the person who decides how to display the food is the choice architect of the cafeteria all of our choices are similarly influenced by choice architects the architecture includes rules deciding what happens if you do nothing what s said and what isn t said what you see and what you don t doctors employers credit card companies banks and even parents are choice architects we show that by carefully designing the choice architecture we can make dramatic improvements in the decisions people make without forcing anyone to do anything for example we can help people save more and invest better in their retirement plans make better choices when picking a mortgage save on their utility bills and improve the environment simultaneously good choice architecture can even improve the process of getting a divorce or a happier thought getting married in the first place amazon com you are very adamant about allowing people to have choice even though they may make bad ones but if we know what s best for people why just nudge why not push and shove thaler and sunstein those who are in position to shape our decisions can overreach or make mistakes and freedom of choice is a safeguard to that one of our goals in writing this book is to show that it is possible to help people make better choices and retain or even expand freedom if people have their own ideas about what to eat and drink and how to invest their money they should be allowed to do so amazon com you point out that most people spend more time picking out a new tv or audio device than they do choosing their health plan or retirement investment strategy why do most people go into what you describe as auto pilot mode even when it comes to making important long term decisions thaler and sunstein there are three factors at work first people procrastinate especially when a decision is hard and having too many choices can create an information overload research shows that in many situations people will just delay making a choice altogether if they can say by not joining their k plan or will just take the easy way out by selecting the default option or the one that is being suggested by a pushy salesman second our world has gotten a lot more complicated thirty years ago most mortgages were of the year fixed rate variety making them easy to compare now mortgages come in dozens of varieties and even finance professors can have trouble figuring out which one is best since the cost of figuring out which one is best is so hard an unscrupulous mortgage broker can easily push unsophisticated borrowers into taking a bad deal third although one might think that high stakes would make people pay more attention instead it can just make people tense in such situations some people react by curling into a ball and thinking well err i ll do something else instead like stare at the television or think about baseball so much of our lives is lived on auto pilot just because weighing complicated decisions is not so easy and sometimes not so fun nudges can help ensure that even when we re on auto pilot or unwilling to make a hard choice the deck is stacked in our favor amazon com are we humans just poorly adapted for making sound judgments in an increasingly fast paced and complex world what can we do to position ourselves better thaler and sunstein the human brain is amazing but it evolved for specific purposes such as avoiding predators and finding food those purposes do not include choosing good credit card plans reducing harmful pollution avoiding fatty foods and planning for a decade or so from now fortunately a few nudges can help a lot a few small hints sign up for automatic payment plans so you dont pay late fees stop using your credit cards until you can pay them off on time every month make sure you re enrolled in a k plan a final hint read review how often do you read a book that is both important and amusing both practical and deep this gem of a book presents the best idea that has come out of behavioral economics it is a must read for anyone who wants to see both our minds and our society working better it will improve your decisions and it will make the world a better place daniel kahneman princeton university nobel laureate in economics daniel kahneman in this utterly brilliant book thaler and sunstein teach us how to steer people toward better health sounder investments and cleaner environments without depriving them of their inalienable right to make a mess of things if they want to the inventor of behavioral economics and one of the nation s best legal minds have produced the manifesto for a revolution in practice and policy nudge won t nudge you it will knock you off your feet daniel gilbert professor of psychology harvard university author of stumbling on happiness daniel gilbert this is an engaging informative and thoroughly delightful book thaler and sunstein provide important lessons for structuring social policies so that people still have complete choice over their own actions but are gently nudged to do what is in their own best interests well done don norman northwestern university author of the design of everyday things and the design of future things don norman this book is terrific it will change the way you think not only about the world around you and some of its bigger problems but also about yourself michael lewis author of the blind side evolution of a game and liar s poker michael lewis two university of chicago professors sketch a new approach to public policy that takes into account the odd realities of human behavior like the deep and unthinking tendency to conform even in areas like energy consumption where conformity is irrelevant thaler has documented the ways people act illogically barbara kiviat time barbara kiviat richard thaler and cass sunstein s nudge is a wonderful book more fun than any important book has a right to be and yet it is truly both roger lowenstein author of when genius failed roger lowenstein a manifesto for using the recent behavioral research to help people as well as government agencies companies and charities make better decisions david leonhardt the new york times magazine david leonhardt new york times i love this book it is one of the few books i ve read recently that fundamentally changes the way i think about the world just as surprising it is fun to read drawing on examples as far afield as urinals k plans organ donations and marriage academics aren t supposed to be able to write this well steven levitt alvin baum professor of economics university of chicago graduate school of business and co author of freakonomics a rogue economist explores the hidden side of everything steven levitt http g ecx images amazon com images g books promos jpg http g ecx images amazon com images g books jpg
this report demonstrates that introduction of micrornas mirnas specific to embryonic stem cells enhances the production of mouse induced pluripotent stem ips cells the mirnas mir mir and mir increase the efficiency of reprogramming by and but not by these factors plus cmyc cmyc binds the promoter of the mirnas suggesting that they are downstream effectors of cmyc during reprogramming however unlike cmyc the mirnas induce a homogeneous population of ips colonies
there are several useful guides available for how to review a paper in computer science these are soberly presented carefully reasoned and sensibly argued as a result they are not much fun so as a contrast this note is a checklist of how not to review a paper it details techniques that are unethical unfair or just plain nasty since in computer science we often present arguments about how an adversary would approach a particular problem this note describes the adversary strategy
recent studies have emphasized the importance of pathway specific interpretations for understanding the functional relevance of gene alterations in human cancers although signaling activities are often conceptualized as linear events in reality they reflect the activity of complex functional networks assembled from modules that each respond to input signals to acquire a deeper understanding of this network structure we developed an approach to deconstruct pathways into modules represented by gene expression signatures our studies confirm that they represent units of underlying biological activity linked to known biochemical pathway structures importantly we show that these signaling modules provide tools to dissect the complexity of oncogenic states that define disease outcomes as well as response to pathway specific therapeutics we propose that this model of pathway structure constitutes a framework to study the processes bywhich information propogates through cellular networks and to elucidate the relationships of fundamental modules to cellular and phenotypes
biologists have tended to assume that closely related species will have similar cognitive abilities johan j bolhuis and clive d l wynne put this evolutionarily inspired idea through its paces darwin s theory of evolution by natural selection is broadly accepted among biologists but its implications for the study of cognition are far from clear few within the scientific pale would argue against the proposition that life on earth has evolved and that this general principle can be extended to the process thought
interaction specificity is a required feature of biological networks and a necessary characteristic of protein or small molecule reagents and therapeutics the ability to alter or inhibit protein interactions selectively would advance basic and applied molecular science assessing or modelling interaction specificity requires treating multiple competing complexes which presents computational and experimental challenges here we present a computational framework for designing protein interaction specificity and use it to identify specific peptide partners for human basic region leucine zipper bzip transcription factors protein microarrays were used to characterize designed synthetic ligands for all but one of bzip families the bzip proteins share strong sequence and structural similarities and thus are challenging targets to bind specifically nevertheless many of the designs including examples that bind the oncoproteins c jun c fos and c maf also called jun fos and maf respectively were selective for their targets over all other families collectively the designs exhibit a wide range of interaction profiles and demonstrate that human bzips have only sparsely sampled the possible interaction space accessible to them our computational method provides a way to systematically analyse trade offs between stability and specificity and is suitable for use with many types of structure scoring functions thus it may prove broadly useful as a tool for design
we describe an isothermal single reaction method for assembling multiple overlapping dna molecules by the concerted action of a exonuclease a dna polymerase and a dna ligase first we recessed dna fragments yielding single stranded dna overhangs that specifically annealed and then covalently joined them this assembly method can be used to seamlessly construct synthetic and natural genes genetic pathways and entire genomes and could be a useful molecular tool
traditional personalized search approaches rely solely on individual profiles to construct a user model they are often confronted by two major problems data sparseness and cold start for new individuals data sparseness refers to the fact that most users only visit a small portion of web pages and hence a very sparse user term relationship matrix is generated while cold start for new individuals means that the system cannot conduct any personalization without previous browsing history recently community based approaches were proposed to use the group s social behaviors as a supplement to personalization however these approaches only consider the commonality of a group of users and still cannot satisfy the diverse information needs of different users in this article we present a new approach called collaborative personalized search it considers not only the commonality factor among users for defining group user profiles and global user profiles but also the specialties of individuals then a statistical user language model is proposed to integrate the individual model group user model and global user model together in this way the probability that a user will like a web page is calculated through a two step smoothing mechanism first a global user model is used to smooth the probability of unseen terms in the individual profiles and provide aggregated behavior of global users then in order to precisely describe individual interests by looking at the behaviors of similar users users are clustered into groups and group user models are constructed the group user models are integrated into an overall model through a cluster based language model the behaviors of the group users can be utilized to enhance the performance of personalized search this model can alleviate the two aforementioned problems and provide a more effective personalized search than previous approaches large scale experimental evaluations are conducted to show that the proposed approach substantially improves the relevance of a search over several methods
summary we introduce a new visual analytics tool named mapview to facilitate the representation of large scale short reads alignment data and genetic variation analysis mapview can handle hundreds of millions of short reads on a desktop computer with limited memory it supports a compact alignment view for both single end and paired end short reads multiple navigation and zoom modes and multi thread processing moreover mapview offers automated genetic variation detection mapview has been used in our lab and by over research labs worldwide availability http evolution sysu edu cn mapview contact hotmail com lssssh mail sysu edu cn supplementary information supplementary data are available at http evolution sysu edu cn mapview mvf bioinformatics
despite the rapidly increasing number of sequenced and re sequenced genomes many issues regarding the computational assembly of large scale sequencing data have remain unresolved computational assembly is crucial in large genome projects as well for the evolving high throughput technologies and plays an important role in processing the information generated by these methods here we provide a comprehensive overview of the current publicly available sequence assembly programs we describe the basic principles of computational assembly along with the main concerns such as repetitive sequences in genomic dna highly expressed genes and alternative transcripts in est sequences we summarize existing comparisons of different assemblers and provide a detailed descriptions and directions for download of assembly programs at http genome ku dk resources assembly methods html c elsevier ltd all reserved
many bacterial systems rely on dynamic genetic circuits to control crucial biological processes a major goal of systems biology is to understand these behaviours in terms of individual genes and their interactions however traditional techniques based on population averages wash out crucial dynamics that are either unsynchronized between cells or are driven by fluctuations or noise in cellular components recently the combination of time lapse microscopy quantitative image analysis and fluorescent protein reporters has enabled direct observation of multiple cellular components over time in individual cells in conjunction with mathematical modelling these techniques are now providing powerful insights into genetic circuit behaviour in diverse systems
motivation statistical phylogenetics is computationally intensive resulting in considerable attention meted on techniques for parallelization codon based models allow for independent rates of synonymous and replacement substitutions and have the potential to more adequately model the process of protein coding sequence evolution with a resulting increase in phylogenetic accuracy unfortunately due to the high number of codon states computational burden has largely thwarted phylogenetic reconstruction under codon models particularly at the genomic scale here we describe novel algorithms and methods for evaluating phylogenies under arbitrary molecular evolutionary models on graphics processing units gpus making use of the large number of processing cores to efficiently parallelize calculations even for large state size models results we implement the approach in an existing bayesian framework and apply the algorithms to estimating the phylogeny of complete mitochondrial genomes of carnivores under a state codon model we see a near fold speed increase over an optimized cpu based computation and a fold increase over the currently available implementation making this the first practical use of codon models for phylogenetic inference over whole mitochondrial or microorganism genomes availability and implementation source code provided in beagle broad platform evolutionary analysis general likelihood evaluator a cross platform processor library for phylogenetic likelihood computation http beagle lib googlecode com we employ a beagle implementation using the bayesian phylogenetics framework beast http beast bio ed ac uk contact msuchard ucla edu a rambaut ed ac bioinformatics
interfaces to databases have traditionally been designed as single user systems that hide other users and their activity this paper aims to show that collaboration is an important aspect of searching online information stores that requires explicit computerised support the claim is made that a truly user centred system must acknowledge and support collaborative interactions between users collaborative working implies a need to share information both the search product and the search process searches need not be restricted to inanimate resources but people can also search for other people the ariadne system is introduced as an example of computerised support for collaboration between browsers a number of systems offering varied approaches to supporting collaboration are surveyed and a structure for analysing the various aspects of collaboration is applied support for communication and collaboration is as important as support for informationseeking and
summarythe microrna mir is perfectly conserved from annelids to humans and yet some of the genes that it regulates in drosophila are not regulated in mammals we have explored the role of lineage restricted targets using drosophila in order to betterunderstand the evolutionary significance of microrna target relationships from studies of twowell characterized developmental regulatory networks we find that mir functions in several interlocking feedback and feedforward loops and propose that its role in these networks is to buffer them against perturbation to directly demonstrate this function for mir we subjected the networks to temperature fluctuation and found that mir is essential for the maintenance of regulatory stability under conditions of environmental flux we suggest that some conserved micrornas like mir may enter into novel genetic relationships to buffer developmental programs against variation and impart robustness to diverse regulatory networks introductionbiological systems are imbued with the property of robustness systems are robust in that their response or output is buffered against perturbation and variability to yield uniform behavior numerous examples abound in which robust systems can compensate for remarkably large genetic or environmental perturbations kitano how this occurs is not well understood and is currently the focus of intense study robustness is thought to be attained by a variety of mechanisms hartman etal for example redundancy ensures normal performance in the face of localized failure and it can be achieved through gene duplication or duplication of functional components kitano positive and negative feedback is another means to generate stability within networks of interacting regulatory molecules lee etal milo etal and spirin and mirny robustness is not merely a property of complex systems but it has the potential to evolve in living organisms buffering might play a role in evolution by canalizing or masking genetic variation at the level of phenotypic expression meiklejohn and hartl and siegal and bergman in this study we examine the role of micrornas mirnas in biological robustness these noncoding rnas are transcribed from plant algal and animal genomes where their gene numbers range in the hundreds griffiths jones etal transcription of mirnas is performed by rna polymerase ii and transcripts are capped and polyadenylated reviewed in carthew and sontheimer although some animal mirnas are individually produced from separate transcription units many more mirnas are produced from transcription units that make more than one product after transcription the rna folds into a stem loop that is endonucleolytically processed to generate a duplex rna of approximately base pairs length the mature mirna duplex is a short lived entity it is rapidly unwound when it associates with a member of the ago protein family this mirna bound ago in association with protein is called the mirisc complex carthew and sontheimer the mirna acts as an adaptor for mirisc to specifically recognize and regulate particular mrnas with few exceptions mirna binding sites in animal mrnas lie in the utr and are usually present in multiple copies most animal mirnas bind with mismatches and bulges although a key feature of recognition involves watson crick base pairing of mirna nucleotides representing the seed region the degree of mirna mrna complementarity has been considered a key determinant of the regulatory mechanism perfect complementarity allows ago catalyzed cleavage of the mrna strand whereas central mismatches exclude cleavage and promote repression of mrna translation this latter mechanism is predominant for regulation by animal mirnas and repression increases additively with mirisc occupancy on messages bushati and cohen most targeted genes are only modestly repressed by mirnas which indicates that mirnas primarily tune gene expression baek etal nakahara etal and selbach etal it has been speculated that one of the functions of mirnas is to provide robustness to programs of gene expression hornstein and shomron stark and colleagues stark etal observed anti correlative expression of mirnas and their target mrnas this suggests that transcription primarily controls gene expression while mirnas lend further reinforcement togene regulation by attenuating unwanted transcripts micrornas also may provide robustness by acting in feedback and feedforward loops which impart robustness to complex networks milo etal bioinformatic analysis has indicated that mirnas frequently collaborate with transcription factors infeedback and feedforward loops to regulate their targets martinez etal and tsang etal and there are several experimentally defined examples of these kinds of regulatory relationships hobert despite these speculations about mirnas and robustness to date there has been no direct evidence that a mirna buffers gene expression against perturbation or variability to explore the possible link between mirnas and biological robustness we have focused on one of the most highly conserved animal mirnas called mir the mir gene is found in most sequenced urbilateria species and the sequence of its mature mirna product is perfectly conserved from annelids to humans indicating a strong functional conservation prochnik etal in support of this notion mir is specifically expressed in neurosecretory cells of the vertebrate brain and in homologous cells of the invertebrate nervous system tessmar raible etal and wienholds etal a link with secretory cells is further suggested by the specific expression of mir in the islet cells of the pancreas correa medina etal and joglekar etal although studies of vertebrate mir have not yet clearly defined its normal function human tumor cell studies indicate that mir downregulates signal transduction downstream of the epidermal growth factor egf kefas etal and webster etal its targets include the egf receptor and several kinases in drosophila mir does not inhibit but stimulates egf signal transduction and the molecular target is a transcription repressor downstream of the kinase cascade li and carthew in the present study we find that drosophila mir acts within two complex regulatory networks that determine the fates of photoreceptor cells proprioceptor organs and olfactory organs mir acts within several interlocking feedback and feedforward loops theoretically implicated as network stabilizers thus we provide a mechanistic picture of mir working in networks to buffer gene expression against perturbation to directly demonstrate this function for mir we subjected the networks to temperature fluctuation and show that mir is essential for stable gene expression and cell fate determination in the face of this perturbation thus we have demonstrated that this mirna imparts robustness to diverse regulatory networks results novel functional and target acquisition by mir during evolutionin addition to the compound eye other drosophila sensory organs also express mir including proprioceptor and olfactory organs located on the antenna leg and wing figures strikingly mir is not expressed in the homologous sensory organs of vertebrates implying that mir function has differentially evolved landgraf etal and wienholds etal to examine the issue more closely we focused on genes whose expression is regulated by mir in developing sensory organs of drosophila the yan and e spl genes are direct targets of mir and these factors are essential for development of insect sensory organs li and carthew stark etal and lai etal expression of the yan gene is inhibited by mir in photoreceptor cells due to four mir binding sites in its transcript the e spl gene family are direct targets of mir mediated repression in other sensory organs are their vertebrate orthologs also targets of mir we compared the predicted mir targets from drosophila and humans using six different prediction algorithms based on this meta analysis genes were predicted with high or moderate stringency to be mir targets in drosophila and table available with this article online a total of mir targets were predicted with high or moderate stringency in humans table we then compared the overlap between the two datasets and observed that only targets from both datasets were defined orthologs table strikingly the mammalian orthologs of yan and e spl were not predicted to be targets of mir therefore these mir targets were either differentially acquired or lost in different evolutionary lineages does mir provide robustness to gene expression we asked what function mir played in regulating these nonconserved gene targets in drosophila we were not able to assay e spl protein expression however we had previously found that mir mutants had only minor defects in yan protein expression li and carthew moreover though mir is expressed in developing sensory organs loss of mir had little or no detectable impact on their development under uniform laboratory conditions li and carthew data not shown one possible explanation is that mir is functionally redundant with other mirnas however loss of all mature mirnas within dicer clones had negligible effects on determination of these structures t hayashi and r w c unpublished data these results are consistent with mir providing robustness to gene expression programs in development it was especially intriguing to consider that this function could evolve in some animal lineages and not others if robustness is a mir function we had two predictions first mir would act in gene networks as a stabilizing factor second mir would prevent development from being perturbed when the environment of the animal was perturbed we embarked on a systematic test of these two predictions mir acts within a gene network controlling photoreceptor determinationthe yan gene encodes a transcription repressor voas and rebay that binds to a cluster of sites in dna located upstream of the mir sequence li and carthew and to show that the cluster acts as a mir transcription enhancer we placed it into a transgenic expression reporter and observed strong reporter expression in photoreceptor cells and weak expression in their precursors figures and this pattern resembled the endogenous mir rna expression pattern figures and therefore the cluster behaves as a mir transcription enhancer we next examined enhancer activity in a yan mutant enhancer activity was greatly increased in precursor cells indicating that the enhancer is repressed by yan in these cells figures and yan competes with a transcription activator called pnt for the same dna binding sites in enhancers flores etal and xu etal to determine if pnt activates the mir enhancer we misexpressed pnt in precursor cells and observed a tremendous increase in enhancer activity altogether these data indicate that yan and pnt regulate the mir enhancer in opposing directions yan indirectly regulates two other transcription repressors and yan represses the transcription of phyllopod phyl which encodes an ubiquitin ligase subunit that targets and proteins for degradation li etal tang etal and treier etal thus the presence of yan stabilizes these repressors we wondered if yan might also act through these repressors to inhibit the mir enhancer examination of the enhancer dna sequence revealed two binding sites and misexpression of in photoreceptor cells led to decreased mir rna figures and and enhancer activity figures and misexpression of in photoreceptor cells had no effect on mir rna expression consistent with the absence of binding sites in the enhancer data not shown these data suggest that and not can bind to the mir enhancerand repress its activity yan plays a central role in transducing extracellular signals through the notch and egf receptor egfr that affect cell fates voas and rebay to ascertain how these extracellular signals regulate the mir enhancer we used signaling mutants when enhancer activity was monitored in precursor cells containing constitutively active egfr activity was strongly upregulated figures and conversely activity was greatly reduced in photoreceptor cells carrying a dominant negative egfr mutant figures and egfr signaling activates pnt synthesis and inhibits yan by stimulating degradation of yan protein voas and rebay thus egfr signaling activates the mir enhancer most probably through its effects on pnt and yan we also determined how notch signaling regulates the enhancer we observed an increase in mir expression in precursor cells carrying a temperature sensitive notch mutation figures and enhancer activity was also upregulated figures and indicating that notch signaling represses the mir enhancer notch signals are transduced through the transcription effector su h mumm and kopan it was previously found that su h activates yan transcription rohrbaugh etal thus yan is the most likely mediator of the repressive effect of notch on the mir enhancer consistent with this idea a constitutively active su h mutant repressed enhancer activity and notch mutant cells with greater enhancer activity had reduced yan protein levels our genetic analysis has revealed a network like architecture acting in photoreceptor determination yan represses mir transcription directly and also represses transcription indirectly through this mode of direct and indirect repression is an example of a coherent feed forward loop mir is involved in a second coherent feed forward loop pnt directly activates mir transcription which in turn represses yan pnt also directly represses yan transcription rohrbaugh etal this coherent feed forward loop between pnt and yan interlocks with the other coherent feed forward loop between yan and mir coherent feed forward loops of this type in which x regulates y and both negatively regulate z create stability against fluctuations in x it generates a delay or persistence that rejects fluctuating dips in x and only accepts persistent decreases in x mangan and alon and mangan etal thus we can hypothesize that levels of mir and yan are buffered against fluctuating drops in yan and pnt this buffering would ensure that a cell only switches from one state yan on to the other state yan off when there is a persistent decrease in yan the yan off state would also be buffered against switching back to yan on due to pnt fluctuations this mechanism likely functions in collaboration with degradation of yan protein to promote zero order ultrasensitivity melen etal which ensures that a cell s fate change is not spontaneously induced or reverted mir acts within a gene network controlling proprioceptor determinationmir is expressed in developing proprioceptor and olfactory organs within the antenna leg and wing figures the mir enhancer is also specifically active in these organs figures precursor cells of proprioceptor and olfactory organs transiently express the atonal ato gene in a zone called the proneural cluster pnc artavanis tsakonas etal ato protein activates transcription of genes that enable a subset of pnc cells to adopt a sensory organ precursor sop fate jafar nejad etal sops then proceed to form the sensory organs since ato is present in cells with an activated mir enhancer figures we wondered if this transcription factor might directly regulate the enhancer ato protein binds to dna as a heterodimer with the ubiquitously expressed bhlh protein daughterless da an ato da binding consensus sequence has been deduced powell etal we identified two conserved sequences that matched the ato da consensus in the mir enhancer figures and to determine if ato da activates the enhancer by binding these sequences we misexpressed ato or another proneural protein in the leg antenna and wing and observed ectopic enhancer activation in those cells figures we then constructed a mutant form of the enhancer in which the ato da sequences were mutated the resulting enhancer was completely inactive in the leg antenna and wing figures taken together our results argue that ato directly activates the mir enhancer e spl genes can be directly repressed by mir stark etal and lai etal e spl genes encode proteins that directly repress transcription of the ato gene taken together these data suggest that mir can stimulate ato transcription and it would do so by repressing e spl mediated repression in support of this idea we observed ectopic ato expression in cells that misexpressed mir rna d ff hh to determine if this effect was mediated through e spl we misexpressed mir rna along with mutant e spl mrnas that lacked mir binding sites in their under these circumstances we saw little or no ectopic ato in cells misexpressing both mir rna and e spl proteins ll nn this regulatory pathway should also affect sop fate determination as predicted misexpression of either mir rna or ato protein induced sop determination lai etal and figures and misexpression of mir resistant e spl genes inhibited sop determination figures and when we misexpressed both mir rna with different mir resistant e spl proteins we saw inhibition of sop determination figures and similar effects were observed when external sensory organ formation was assayed in adults and table altogether these data indicate that e spl genes act downstream of mir to mediate its effects on ato expression and sop fate determination since we found that ato activates mir transcription it would suggest the existence of a feedback loop in which ato activates mir which then represses e spl which otherwise represses ato the feedback loop would imply that mir rna positively activates its own transcription as confirmation of this prediction we observed activation of the mir enhancer in cells misexpressing mir rna figures this mechanism is not restricted to proprioceptors and olfactory organs alone it also operates during photoreceptor fate determination at the earliest stages of eye patterning we observed mir rna expression and mir enhancer activity in cells where determination occurs c enhancer activity was not detected in this region when ato da binding sites were mutated this suggests that ato activates the enhancer in the eye and is consistent with our observation that misexpressed ato activates the mir enhancer figures and we also found that mir feeds back onto ato in the eye overexpression of mir rna in the furrow caused a modest increase in the number of cells that maintained ato expression and adopted cell fate figures consistent with previous observations that ato triggers determination of jarman et al our analysis of sop determination has uncovered network like features ato activates mir which in turn represses e spl ato also directly activates transcription of e spl cave etal cooper etal and nellesen etal therefore ato both directly activates and indirectly represses e spl this is an example of an incoherent feed forward loop incoherent feed forward loops of this type impart an accelerated and transient pulse of downstream gene expression mangan and alon in addition e spl feeds back to ato to create a double negative feedback loop that is interconnected with the feed forward loop the overall effect is a network in which fluctuating peaks of ato would result in transient pulses of ato repression by e spl but sustained increase of ato would result in sustained repression of e spl by mir and stabilization of ato mir stabilizes developmental processes against temperature perturbationif mir provides biological robustness then mir should prevent development from being perturbed when the environment of the animal is perturbed environmental fluctuation is one type of perturbation against which gene expression can be remarkably stable freeman we speculated that mir may stabilize gene expression under fluctuating conditions and that this would not be apparent under uniform conditions indeed ato expression is normal in mir loss of function mutants under uniform laboratory conditions figures and and data not shown we then perturbed the environment around developing drosophila larvae by fluctuating the environmental temperature between and every hr when wild type larvae were challenged with such a temperature fluctuation they exhibited no defects in expression of ato and yan figures and in contrast mir mutant eyes exhibited a strong decrease in ato expression under fluctuating temperature conditions yan expression was abnormally strong and irregular in mir mutant eyes the directions of expression change were consistent with the mutant failing to activate ato and repress yan we also examined the capacity of mir to stabilize proprioceptor and olfactory sop determination when perturbed for temperature we subjected wild type and mir mutant animals to temperature fluctuations and then followed the formation of antennal sops groups of sops that constituted the johnston s organ appeared near normal however the arista sop group failed to form in the mir mutant figures the number of sops that form the coeloconic sensillae were reduced and those that did develop were abnormally patterned these defects were correlated with a reduction in ato expression within antennal cells figures altogether our experiments indicate that mir buffers specific gene expression and cell fates against environmental perturbation this function appears dispensable under uniform environmental conditions discussiontwo features of mirnas have suggested that they could potentially play a role in generating biological robustness first they regulate gene expression additively and thus tune rather than switch gene expression graduated output modulation in response to variable input is a mechanism for simple stabilization second bioinformatic analysis suggests that many mirnas act in feedback and feedforward network motifs martinez etal and tsang etal some of these motifs have been theoretically and experimentally implicated to stabilize networks milo etal however direct experimental evidence that a mirna promotes robustness stability against noise or perturbation has been missing here we provide such evidence for mir in drosophila this mirna is required to maintain normal gene expression and sensory organ fate determination under fluctuating temperature conditions we interpret this to mean that mir buffers gene expression against environmental fluctuation the fact that this function of mir is exposed under fluctuating conditions underscores its primary role as a stabilizer for sensory organ development the robustness that mir provided was most apparent for its proximate gene targets yan and ato determination of and sop sensory cells was less dependent upon mir under the fluctuation paradigm although it led to defects in patterning of these in the eye data not shown and the antenna not surprisingly it hints that there are mechanisms in place downstream or in parallel to ensure further robustness when there is fluctuation these likely compensate and normalize the outcome however since certain sop cell types were considerably more sensitive to fluctuation when mir was absent perhaps it underscores the mechanistic diversity that different cell types utilize for generating robustness the conceptual significance of the robustness mirna connection is several fold their dynamic kinetic properties help answer the question of why mirna gene regulation instead of just using more transcription factors their rate of biogenesis is more rapid than proteins and they affect expression with less delay than factors that regulate nuclear events these features enable mirnas to produce rapid responses something that is expected to counteract rapid and variable fluctuations it also explains why mirnas frequently appear dispensable under uniform laboratory conditions bushati and cohen leaman etal and miska etal our analysis of two gene networks explains how mir can buffer gene expression against perturbation the mirna acts in feedforward and feedback loops that are theoretically implicated as network stabilizers stability is experimentally apparent under conditions of temperature fluctuation though there is no reason a priori why stability cannot be expressed under other variable conditions another key point is that tight regulation of mirnas is crucial misexpression of mirnas frequently mimic loss of function phenotypes for their targets bushati and cohen our results with mir hint at how this is normally prevented namely mir has a restricted expression pattern that is strictly controlled by its targets the restricted expression pattern can also explain how off targeting effects are carefully limited mirnas as canalization factorswaddington coined the word canalization to describe how development is buffered against perturbation siegal and bergman and waddington despite considerable genetic or environmental variation organisms develop traits that are remarkably uniform in phenotype indeed the insect compound eye and sensory organs appear to be deeply canalized systems jander and jander meir etal and rendel it has been speculated that mirnas might be important for canalization hornstein and shomron certainly mir has many attributes that suggest it helps canalize development in drosophila there is an evolutionary implication to canalization if canalization masks the phenotypic expression of genetic variation then individuals within a species appear highly uniform waddington this lack of diversity limits the number of traits upon which selection can act resulting in stabilization of a species and reduced evolution conversely lack of canalization results in enhanced phenotypic variation and the possibility of selection to evolve new forms theoretical and experimental studies indicate that canalization itself can evolve that is increase or decrease over evolutionary time gibson and hogness proulx and phillips rendel and sheldon and siegal and bergman in this light it is interesting to consider mir several lines of evidence indicate that mir has acquired a novel role in sensory organ development specifically within insects and not other animals the mirna is expressed in these drosophila organs but not the orthologous organs of vertebrates the enhancer that drives its expression in drosophila sensory organs is not conserved in vertebrates we found strong conservation of the mir enhancer in drosophila species divergent over myrs a cluster of binding sites is also present upstream of the mosquito mir sequence c which implies conserved mir transcription in the eyes of other insects in contrast the human mir gene lacks a cluster of binding sites for the yan ortholog indicating divergent regulation of the human mir ortholog moreover the vertebrate orthologs of e spl and yan are not predicted targets of mir indeed only a few vertebrate drosophilid orthologs have been conserved as mir targets and most of these conserved targets have no known role in sensory organ development we propose that mir was recruited into insect sensory organ development specifically for the purposes of canalization of those systems as such it has helped stabilize the remarkable uniformity of sensory organ form within different insects particularly observed in the compound eye strausfeld and nassel if mir is typical of highly conserved animal mirnas then it would imply that the acquisition of novel targets by these mirnas is not necessarily to generate new traits but to stabilize pre existing traits experimental procedures assaying mir enhancer activitya bp dna fragment located bp upstream of the end of the drosophila pre mir sequence was pcr amplified and inserted into the transgenic expression vector ph stinger barolo etal this contains a minimal promoter driving nuclear gfp the resulting mir e gfp construct was transformed into drosophila to make the reporter with mutated ato da binding sites the two predicted binding sites were mutated from cagctg to ccgcta and from catctg to cctcta the mutated bp enhancer was cloned into ph stinger to make mir e gfp prn and transformed into drosophila enhancer activity was assayed invivo by visualizing gfp fluorescence or gfp protein localization by immunofluorescence geneticswe used drosophila stocks carrying mir gmr dpp ptc uas uas uas uas egfr top uas der uas egfr dn uas ato uas sc uas dsred mir uas mir uas e spl uas e spl uas e spl m flies were grown at and shifted to for hr before dissection flies carrying or uas constructs were grown at or wing notching and ectopic posterior sternopleural bristles were scored twice per animal once for each left and right side whereas any lack of or extra scutellar bristles were scored once temperature perturbationw or cantons wild type and mir stocks were grown in bottles at a uniform temperature of to for several days they were shifted to for hr they were then subjected to two to five rounds of temperature cycles each round consisted of a shift to for hr and then back to for hr bottles were incubated in air circulating incubators for each temperature step at the completion of the final round either wandering third instar larvae or white pre pupae were harvested for analysis in situ hybridization and immunofluorescencein situ hybridization against mir mature rna was performed as described liand carthew using an antisense mir lna probe acaacaaaatcactagtcttcca obtained from exiqon vedbaek denmark to detect rna by fluorescence tsa plus fluorescence systems from nen was used following manufacturer s instructions immunofluorescence of third instar larval and pupal discs was performed as described li and carthew antibodies used were guinea pig anti ato guinea pig anti sens rabbit anti ato rat anti elav mouse anti gfp mouse anti yan and mouse anti acknowledgmentswe thank h bellen s bray c delidakis s cohen t hayashi g mardon and the bloomington stock center for fly strains n baker h bellen t hayashi z c lai and the developmental studies hybridoma bank for antibodies t hayashi for sharing unpublished data r gejman for advice and help with all stages of the informatic analysis carthew lab members and c labonne for discussions the biology imaging facility at northwestern university for imaging this work was supported by the by the chicago biomedical consortium with support from the searle funds at the chicago community trust the nih the northwestern vision training grant c a r the cmdb training grant j j c and the malkin program
background several recent studies have demonstrated the effectiveness of deep sequencing for transcriptome analysis rna seq in mammals as rna seq becomes more affordable whole genome transcriptional profiling is likely to become the platform of choice for species with good genomic sequences as yet a rigorous analysis methodology has not been developed and we are still in the stages of exploring the features of the data results we investigated the effect of transcript length bias in rna seq data using three different published data sets for standard analyses using aggregated tag counts for each gene the ability to call differentially expressed genes between samples is strongly associated with the length of the transcript conclusion transcript length bias for calling differentially expressed genes is a general feature of current protocols for rna seq technology this has implications for the ranking of differentially expressed genes and in particular may introduce bias in gene set testing for pathway analysis and other multi gene systems biology analyses reviewers this article was reviewed by rohan williams nominated by gavin huttley nicole cloonan nominated by mark ragan and james bullard nominated by dudoit
background vertebrates share the same general body plan and organs possess related sets of genes and rely on similar physiological mechanisms yet show great diversity in morphology habitat and behavior alteration of gene regulation is thought to be a major mechanism in phenotypic variation and evolution but relatively little is known about the broad patterns of conservation in gene expression in non mammalian vertebrates results we measured expression of all known and predicted genes across twenty tissues in chicken frog and pufferfish by combining the results with human and mouse data and considering only ten common tissues we have found evidence of conserved expression for more than a third of unique orthologous genes we find that on average transcription factor gene expression is neither more nor less conserved than that of other genes strikingly conservation of expression correlates poorly with the amount of conserved nonexonic sequence even using a sequence alignment technique that accounts for non collinearity in conserved elements many genes show conserved human fish expression despite having almost no nonexonic conserved primary sequence conclusions there are clearly strong evolutionary constraints on tissue specific gene expression a major challenge will be to understand the precise mechanisms by which many gene expression patterns remain similar despite extensive cis restructuring
quantum states are very delicate so it is likely some sort of quantum error correction will be necessary to build reliable quantum computers the theory of quantum error correcting codes has some close ties to and some striking differences from the theory of classical error correcting codes many quantum codes can be described in terms of the stabilizer of the codewords the stabilizer is a finite abelian group and allows a straightforward characterization of the error correcting properties of the code the stabilizer formalism for quantum codes also illustrates the relationships to classical coding theory particularly classical codes over gf the finite field with four elements to build a quantum computer which behaves correctly in the presence of errors we also need a theory of fault tolerant quantum computation instructing us how to perform quantum gates on qubits which are encoded in a quantum error correcting code the threshold theorem states that it is possible to create a quantum computer to perform an arbitrary quantum computation provided the error rate per physical gate or time step is below some constant value
p scientific innovation depends on finding integrating and re using the products of previous research here we explore how recent developments in web technology particularly those related to the publication of data and metadata might assist that process by providing semantic enhancements to journal articles within the mainstream process of scholarly journal publishing we exemplify this by describing semantic enhancements we have made to a recent biomedical research article taken from italic plos neglected tropical diseases italic providing enrichment to its content and increased access to datasets within it these semantic enhancements include provision of live dois and hyperlinks semantic markup of textual terms with links to relevant third party information resources interactive figures a re orderable reference list a document summary containing a study summary a tag cloud and a citation analysis and two novel types of semantic enrichment the first a supporting claims tooltip to permit citations in context and the second tag trees that bring together semantically related terms in addition we have published downloadable spreadsheets containing data from within tables and figures have enriched these with provenance information and have demonstrated various types of data fusion mashups with results from other research articles and with google maps we have also published machine readable rdf metadata both about the article and about the references it cites for which we developed a citation typing ontology cito ext link xmlns xlink http www org xlink ext link type uri xlink href http purl org net cito xlink type simple http purl org net cito ext link the enhanced article which is available at ext link xmlns xlink http www org xlink ext link type uri xlink href http dx doi org journal pntd xlink type simple http dx doi org journal pntd ext link presents a compelling existence proof of the possibilities of semantic publication we hope the showcase of examples and ideas it contains described in this paper will excite the imaginations of researchers and publishers stimulating them to explore the possibilities of semantic publishing for their own research articles and thereby break down present barriers to the discovery and re use of information within traditional modes of scholarly p
this article reviews a diverse set of proposals for dual processing in higher cognition within largely disconnected literatures in cognitive and social psychology all these theories have in common the distinction between cognitive processes that are fast automatic and unconscious and those that are slow deliberative and conscious a number of authors have recently suggested that there may be two architecturally and evolutionarily distinct cognitive systems underlying these dual process accounts however it emerges that a there are multiple kinds of implicit processes described by different theorists and b not all of the proposed attributes of the two kinds of processing can be sensibly mapped on to two systems as currently conceived it is suggested that while some dual process theories are concerned with parallel competing processes involving explicit and implicit knowledge systems others are concerned with the influence of preconscious processes that contextualize and shape deliberative reasoning and making
background microarrays revolutionized biological research by enabling gene expression comparisons on a transcriptome wide scale microarrays however do not estimate absolute expression level accurately at present high throughput sequencing is emerging as an alternative methodology for transcriptome studies although free of many limitations imposed by microarray design its potential to estimate absolute transcript levels is unknown results in this study we evaluate relative accuracy of microarrays and transcriptome sequencing rna seq using third methodology proteomics we find that rna seq provides a better estimate of absolute expression levels conclusion our result shows that in terms of overall technical performance rna seq is the technique of choice for studies that require accurate estimation of absolute levels
studies using genome wide platforms have yielded an unprecedented number of promising signals of association between genomic variants and human traits this review addresses the steps required to validate augment and refine such signals to identify underlying causal variants for well defined phenotypes these steps include large scale exact replication across both similar and diverse populations fine mapping and resequencing determination of the most informative markers and multiple independent informative loci incorporation of functional information and improved phenotype mapping of the implicated genetic effects even in cases for which replication proves that an effect exists confident localization of the causal variant often elusive
knowledge is now being seen as the most important strategic resource in organizations and the management of this knowledge is considered critical to organizational success if organizations have to capitalize on the knowledge they possess they have to understand how knowledge is created shared and used within the organization knowledge exists and is shared at different levels in organizations this article examines knowledge sharing at the most basic level namely between individuals in organizations based on a review of existing literature in this area this article presents a model that identifies factors that most significantly influence knowledge sharing at level
genes are not simply turned on or off but instead their expression is fine tuned to meet the needs of a cell how genes are modulated so precisely is not well understood the glucocorticoid receptor gr regulates target genes by associating with specific dna binding sites the sequences of which differ between genes traditionally these binding sites have been viewed only as docking sites using structural biochemical and cell based assays we show that gr binding sequences differing by as little as a single base pair differentially affect gr conformation and regulatory activity we therefore propose that dna is a sequence specific allosteric ligand of gr that tailors the activity of the receptor toward specific target science
with the development of high throughput sequencing and genotyping technologies the number of markers collected in genetic association studies is growing rapidly increasing the importance of methods for correcting for multiple hypothesis testing the permutation test is widely considered the gold standard for accurate multiple testing correction but it is often computationally impractical for these large datasets recently several studies proposed efficient alternative approaches to the permutation test based on the multivariate normal distribution mvn however they cannot accurately correct for multiple testing in genome wide association studies for two reasons first these methods require partitioning of the genome into many disjoint blocks and ignore all correlations between markers from different blocks second the true null distribution of the test statistic often fails to follow the asymptotic distribution at the tails of the distribution we propose an accurate and efficient method for multiple testing correction in genome wide association studies slide our method accounts for all correlation within a sliding window and corrects for the departure of the true null distribution of the statistic from the asymptotic distribution in simulations using the wellcome trust case control consortium data the error rate of slide s corrected p values is more than times smaller than the error rate of the previous mvn based methods corrected p values while slide is orders of magnitude faster than the permutation test and other competing methods we also extend the mvn framework to the problem of estimating the statistical power of an association study with correlated markers and propose an efficient and accurate power estimation method slip slip and slide are available at http slide cs edu
we show that the nuclear architecture of rod photoreceptor cells differs fundamentally in nocturnal and diurnal mammals the rods of diurnal retinas possess the conventional architecture found in nearly all eukaryotic cells with most heterochromatin situated at the nuclear periphery and euchromatin residing toward the nuclear interior the rods of nocturnal retinas have a unique inverted pattern where heterochromatin localizes in the nuclear center whereas euchromatin as well as nascent transcripts and splicing machinery line the nuclear border the inverted pattern forms by remodeling of the conventional one during terminal differentiation of rods the inverted rod nuclei act as collecting lenses and computer simulations indicate that columns of such nuclei channel light efficiently toward the light sensing rod outer segments comparison of the two patterns suggests that the conventional architecture prevails in eukaryotic nuclei because it results in more flexible chromosome arrangements facilitating positional regulation of functions
both dna methylation and histone modification are involved in establishing patterns of gene repression during development certain forms of histone methylation cause local formation of heterochromatin which is readily reversible whereas dna methylation leads to stable long term repression it has recently become apparent that dna methylation and histone modification pathways can be dependent on one another and that this crosstalk can be mediated by biochemical interactions between set domain histone methyltransferases and dna methyltransferases relationships between dna methylation and histone modification have implications for understanding normal development as well as somatic cell reprogramming tumorigenesis
despite the importance of epigenetic regulation in neurological disorders little is known about neuronal chromatin cerebellar purkinje neurons have large and euchromatic nuclei whereas granule cell nuclei are small and have a more typical heterochromatin distribution while comparing the abundance of methylcytosine in purkinje and granule cell nuclei we detected the presence of an unusual dna nucleotide using thin layer chromatography high pressure liquid chromatography and mass spectrometry we identified the nucleotide as hydroxymethyl deoxycytidine hmdc hmdc constitutes of total nucleotides in purkinje cells in granule cells and is not present in cancer cell lines hmdc is a constituent of nuclear dna that is highly abundant in the brain suggesting a role in epigenetic control of neuronal science
abstract parking garages that stow and retrieve cars automatically are becoming viable solutions for parking shortages however these are complex systems and a number of severe incidents involving such garages have been reported many of these are related to safety issues in software we apply verification techniques to develop a software design for an automated parking garage this design meets a number of safety requirements we provide a software architecture that allows one to split implementation safety and algorithmic aspects of the software consequently we give a high level description of the safety aspects and verify a number of safety requirements on this model also we briefly discuss how this analysis is simplified by using a custom tool
the activity of spiking neurons is frequently described by renewal point process models that assume the statistical independence and identical distribution of the intervals between action potentials however the assumption of independent intervals must be questioned for many different types of neurons we review experimental studies that reported the feature of a negative serial correlation of neighboring intervals commonly observed in neurons in the sensory periphery as well as in central neurons notably in the mammalian cortex in our experiments we observed the same short lived negative serial dependence of intervals in the spontaneous activity of mushroom body extrinsic neurons in the honeybee to model serial interval correlations of arbitrary lags we suggest a family of autoregressive point processes its marginal interval distribution is described by the generalized gamma model which includes as special cases the log normal and gamma distributions which have been widely used to characterize regular spiking neurons in numeric simulations we investigated how serial correlation affects the variance of the neural spike count we show that the experimentally confirmed negative correlation reduces single neuron variability as quantified by the fano factor by up to which favors the transmission of a rate code we argue that the feature of a negative serial correlation is likely to be common to the class of spike frequency adapting neurons and that it might have been largely overlooked in extracellular single unit recordings due to spike sorting errors the american society
abstract edgeexpressdb is a novel database and set of interfaces for interpreting biological networks and comparing large high throughput expression datasets that requires minimal development for new data types and search patterns the edgeexpress database http fantom gsc riken jp edgeexpress summarizes gene expression patterns in the context of alternative promoter structures and regulatory transcription factors and micrornas using intuitive gene centric and sub network views this is an important resource for gene regulation in acute myeloid leukemia monocyte macrophage differentiation and human networks
abstract in an international collaborative research project we collected a wide range of genome scale data including million mrna reads cage tags and microarray expression profiles along a differentiation time course of the human thp cell line and under systematic sirna perturbations in addition data regarding chromatin status derived from chip chip to elucidate the transcriptional regulatory interactions are included here we present these data to the research community as an integrated resource
we derive a standard quantum limit for probing mechanical energy quantization in a class of systems with mechanical modes parametrically coupled to external degrees of freedom to resolve a single mechanical quantum it requires a strong coupling regimethe decay rate of external degrees of freedom is smaller than the parametric coupling rate in the case for cavity assisted optomechanical systems e g the one proposed by thompson etal nature london zero point motion of the mechanical oscillator needs to be comparable to the linear dynamical range of the optical system which is characterized by the optical wavelength divided by the finesse
motivation protein protein interaction ppi extraction from published biological articles has attracted much attention because of the importance of protein interactions in biological processes despite significant progress mining ppis from literatures still rely heavily on time and resource consuming manual annotations results in this study we developed a novel methodology based on bayesian networks bns for extracting ppi triplets a ppi triplet consists of two protein names and the corresponding interaction word from unstructured text the method achieved an overall accuracy of on a cross validation test using manually annotated dataset we also showed through extracting ppi triplets from a large number of pubmed abstracts that our method was able to complement human annotations to extract large number of new ppis from literature availability programs scripts we developed used in the study are available at http stat fsu edu jinfeng datasets bio si programs bayesian chowdhary zhang liu zip contact jliu stat harvard edu supplementary information supplementary data are available at bioinformatics bioinformatics
large scale systematic resequencing has been proposed as the key future strategy for the discovery of rare disease causing sequence variants across the spectrum of human complex disease we have sequenced the coding exons of the x chromosome in families with x linked mental retardation xlmr the largest direct screen for constitutional disease causing mutations thus far reported the screen has discovered nine genes implicated in xlmr including syp and cask reported here confirming the power of this strategy the study has however also highlighted issues confronting whole genome sequencing screens including the observation that loss of function of or more of x chromosome genes is compatible with apparently existence
summary animal micrornas mirnas guide proteins to repress the translation of target mrnas via imperfect base pairing between the mirna and the target computational analyses suggest that each mirna regulates tens or hundreds of targets and yet genetic studies usually show that the repression of a few targets plays a physiological role and the extent of mirna mediated repression which rarely exceeds fold and is also surprisingly lower than most well tolerated intrinsic variations in gene expression and although mirna targets are well conserved among closely related species they differ greatly between more distant animals the prevailing view is that mirnas tune expression of most of their targets and here i propose an alternative hypothesis that could resolve these three paradoxes many computationally identified mirna targets may actually be competitive inhibitors of mirna function preventing mirnas from binding their authentic targets by sequestering them depending on the prevalence of this type of mirna mrna interaction this new conception of mirna regulation could have profound implications on our assumptions about function
various observations argue for a role of adaptation in recent human evolution including results from genome wide studies and analyses of selection signals at candidate genes here we use genome wide snp data from the hapmap and ceph human genome diversity panel samples to study the geographic distributions of putatively selected alleles at a range of geographic scales we find that the average allele frequency divergence is highly predictive of the most extreme f st values across the whole genome on a broad scale the geographic distribution of putatively selected alleles almost invariably conforms to population clusters identified using randomly chosen genetic markers given this structure there are surprisingly few fixed or nearly fixed differences between human populations among the nearly fixed differences that do exist nearly all are due to fixation events that occurred outside of africa and most appear in east asia these patterns suggest that selection is often weak enough that neutral processes especially population history migration and drift exert powerful influences over the fate and geographic distribution of alleles
using deep sequencing deepcage the study measured the genome wide dynamics of transcription start site usage in the human monocytic cell line thp throughout a time course of growth arrest and differentiation modeling the expression dynamics in terms of predicted cis regulatory sites we identified the key transcription regulators their time dependent activities and target genes systematic sirna knockdown of transcription factors confirmed the roles of individual factors in the regulatory network our results indicate that cellular states are constrained by complex networks involving both positive and negative regulatory interactions among substantial numbers of transcription factors and that no single transcription factor is both necessary and sufficient to drive the process
although repetitive elements pervade mammalian genomes their overall contribution to transcriptional activity is poorly defined here as part of the project we report that of cap selected mouse and human rna transcripts initiate within repetitive elements analysis of approximately retrotransposon derived transcription start sites shows that the associated transcripts are generally tissue specific coincide with gene dense regions and form pronounced clusters when aligned to full length retrotransposon sequences retrotransposons located immediately of protein coding loci frequently function as alternative promoters and or express noncoding rnas more than a quarter of refseqs possess a retrotransposon in their utr with strong evidence for the reduced expression of these transcripts relative to retrotransposon free transcripts finally a genome wide screen identifies candidate regulatory regions derived from retrotransposons in addition to more than examples of bidirectional transcription we conclude that retrotransposon transcription has a key influence upon the transcriptional output of the genome
background to identify differentially expressed genes degs from microarray data users of the affymetrix genechip system need to select both a preprocessing algorithm to obtain expression level measurements and a way of ranking genes to obtain the most plausible candidates we recently recommended suitable combinations of a preprocessing algorithm and gene ranking method that can be used to identify degs with a higher level of sensitivity and specificity however in addition to these recommendations researchers also want to know which combinations enhance reproducibility results we compared eight conventional methods for ranking genes weighted average difference wad average difference ad fold change fc rank products rp moderated t statistic modt significance analysis of microarrays samt shrinkage t statistic shrinkt and intensity based moderated t statistic ibmt with six preprocessing algorithms plier vsn farms multi mgmos mmgmos mbei and gcrma a total of real experimental datasets was evaluated on the basis of the area under the receiver operating characteristic curve auc as a measure for both sensitivity and specificity we found that the rp method performed well for vsn farms mbei and gcrma preprocessed data and the wad method performed well for mmgmos preprocessed data our analysis of the microarray quality control maqc project s datasets showed that the fc based gene ranking methods wad ad fc and rp had a higher level of reproducibility the percentages of overlapping genes pogs across different sites for the fc based methods were higher overall than those for the t statistic based methods modt samt shrinkt and ibmt in particular pog values for wad were the highest overall among the fc based methods irrespective of the choice of preprocessing algorithm conclusion our results demonstrate that to increase sensitivity specificity and reproducibility in microarray analyses we need to select suitable combinations of preprocessing algorithms and gene ranking methods we recommend the use of fc based methods in particular rp wad
motivation controlled vocabularies such as the medical subject headings mesh thesaurus and the gene ontology go provide an efficient way of accessing and organizing biomedical information by reducing the ambiguity inherent to free text data different methods of automating the assignment of mesh concepts have been proposed to replace manual annotation but they are either limited to a small subset of mesh or have only been compared with a limited number of other systems results we compare the performance of six mesh classification systems metamap eagl a language and a vector space model based approach a k nearest neighbor knn approach and mti in terms of reproducing and complementing manual mesh annotations a knn system clearly outperforms the other published approaches and scales well with large amounts of text using the full mesh thesaurus our measurements demonstrate to what extent manual mesh annotations can be reproduced and how they can be complemented by automatic annotations we also show that a statistically significant improvement can be obtained in information retrieval ir when the text of a user s query is automatically annotated with mesh concepts compared to using the original textual query alone conclusions the annotation of biomedical texts using controlled vocabularies such as mesh can be automated to improve text only ir furthermore the automatic mesh annotation system we propose is highly scalable and it generates improvements in ir comparable with those observed for manual annotations contact trieschn ewi utwente nl supplementary information supplementary data are available at bioinformatics bioinformatics
community question answering cqa has emerged as a popular forum for users to pose questions for other users to answer over the last few years cqa portals such as naver and yahoo answers have exploded in popularity and now provide a viable alternative to general purpose web search at the same time the answers to past questions submitted in cqa sites comprise a valuable knowledge repository which could be a gold mine for information retrieval and automatic question answering unfortunately the quality of the submitted questions and answers varies widely increasingly so that a large fraction of the content is not usable for answering queries previous approaches for retrieving relevant and high quality content have been proposed but they require large amounts of manually labeled data which limits the applicability of the supervised approaches to new sites and domains in this paper we address this problem by developing a semi supervised coupled mutual reinforcement framework for simultaneously calculating content quality and user reputation that requires relatively few labeled examples to initialize the training process results of a large scale evaluation demonstrate that our methods are more effective than previous approaches for finding high quality answers questions and users more importantly our quality estimation significantly improves the accuracy of search over cqa archives over the state of the methods
users of social networking services can connect with each other by forming communities for online interaction yet as the number of communities hosted by such websites grows over time users have even greater need for effective commu nity recommendations in order to meet more users in this paper we investigate two algorithms from very different do mains and evaluate their effectiveness for personalized com munity recommendation first is association rule mining arm which discovers associations between sets of com munities that are shared across many users second is latent dirichlet allocation lda which models user community co occurrences using latent aspects in comparing lda with arm we are interested in discovering whether modeling low rank latent structure is more effective for recommen dations than directly mining rules from the observed data we experiment on an orkut data set consisting of users and communities our empirical comparisons using the top k recommendations metric show that lda performs consistently better than arm for the community recommendation task when recommending a list of or more communities however for recommendation lists of up to communities arm is still a bit better we analyze exam ples of the latent information learned by lda to explain this finding to efficiently handle the large scale data set we parallelize lda on distributed computers and demon strate our parallel implementation s scalability with varying numbers machines
online social networking sites like myspace facebook and flickr have become a popular way to share and disseminate content their massive popularity has led to viral marketing techniques that attempt to spread content products and ideas on these sites however there is little data publicly available on viral propagation in the real world and few studies have characterized how information spreads over current online networks
it has been reported that relatively short rnas of heterogeneous sizes are derived from sequences near the promoters of eukaryotic genes in conjunction with the project we have identified tiny rnas with a modal length of nt that map within to nt of transcription start sites tsss in human chicken and drosophila these transcription initiation rnas tirnas are derived from sequences on the same strand as the tss and are preferentially associated with g c rich promoters the ends of tirnas show peak density nt downstream of tsss indicating that they are processed tirnas are generally although not exclusively associated with highly expressed transcripts and sites of rna polymerase ii binding we suggest that tirnas may be a general feature of transcription in metazoa and possibly eukaryotes
title author summary title p to help the understanding of physiological failures diseases are defined as specific sets of phenotypes affecting one or several physiological systems yet the complexity of biological systems implies that our working definitions of diseases are careful discretizations of a complex phenotypic space to reconcile the discrete nature of diseases with the complexity of biological organisms we need to understand how diseases are connected as connections between these different discrete categories can be informative about the mechanisms causing physiological failures here we introduce the phenotypic disease network pdn as a map summarizing phenotypic connections between diseases and show that diseases progress preferentially along the links of this map furthermore we show that this progression is different for patients with different genders and racial backgrounds and that patients affected by diseases that are connected to many other diseases in the pdn tend to die sooner than those affected by less connected diseases additionally we have created a queryable online database ext link xmlns xlink http www org xlink ext link type uri xlink href http hudine neu edu xlink type simple http hudine neu edu ext link of the different datasets generated from the more than million patients in this study the disease associations can be explored online or downloaded in p
the main theories of biodiversity either neglect species interactions or assume that species interact randomly with each other however recent empirical work has revealed that ecological networks are highly structured and the lack of a theory that takes into account the structure of interactions precludes further assessment of the implications of such network patterns for biodiversity here we use a combination of analytical and empirical approaches to quantify the influence of network architecture on the number of coexisting species as a case study we consider mutualistic networks between plants and their animal pollinators or seed dispersers these networks have been found to be highly nested with the more specialist species interacting only with proper subsets of the species that interact with the more generalist we show that nestedness reduces effective interspecific competition and enhances the number of coexisting species furthermore we show that a nested network will naturally emerge if new species are more likely to enter the community where they have minimal competitive load nested networks seem to occur in many biological and social contexts suggesting that our results are relevant in a wide range fields
molecular graphics provides an intuitive way for representation modeling and analysis of complex chemical and biological systems it is now widely used in the theoretical chemistry structural biology molecular modeling and drug design communities traditional molecular graphics techniques mainly dedicate to showing molecular architectures at three dimensional level however in some occasions the two dimensional representation of molecular configurations profiles behaviors and interactions may be more readily acceptable for audiences especially when we need to describe abstract information in a straightforward way or to present numerous data in schematic diagrams in recent years representation methods tools have been developed rapidly for various purposes ranging from the aesthetic depiction of atomic arrangement for small organic molecules to schematic layout of complicated nonbonding network across the biomolecular binding interfaces and have received considerable interest in the fields of chemistry biology and medicine in this article we first propose the term of molecular graphics to cover the spectrum of representing chemical and biological systems we also give a comprehensive review on the methods tools and applications of molecular bib
the difficulty associated with the cultivation of most microorganisms and the complexity of natural microbial assemblages such as marine plankton or human microbiome hinder genome reconstruction of representative taxa using cultivation or metagenomic approaches here we used an alternative single cell sequencing approach to obtain high quality genome assemblies of two uncultured numerically significant marine microorganisms we employed fluorescence activated cell sorting and multiple displacement amplification to obtain hundreds of micrograms of genomic dna from individual uncultured cells of two marine flavobacteria from the gulf of maine that were phylogenetically distant from existing cultured strains shotgun sequencing and genome finishing yielded mbp in contigs and mbp in contigs for the two flavobacteria with estimated genome recoveries of about and respectively only of the assembling sequences were contaminants and were removed from further analysis using rigorous quality control in contrast to all cultured strains of marine flavobacteria the two single cell genomes were excellent global ocean sampling gos metagenome fragment recruiters demonstrating their numerical significance in the ocean the geographic distribution of gos recruits along the northwest atlantic coast coincided with ocean surface currents metabolic reconstruction indicated diverse potential energy sources including biopolymer degradation proteorhodopsin photometabolism and hydrogen oxidation compared to cultured relatives the two uncultured flavobacteria have small genome sizes few non coding nucleotides and few paralogous genes suggesting adaptations to narrow ecological niches these features may have contributed to the abundance of the two taxa in specific regions of the ocean and may have hindered their cultivation we demonstrate the power of single cell dna sequencing to generate reference genomes of uncultured taxa from a complex microbial community of marine bacterioplankton a combination of single cell genomics and metagenomics enabled us to analyze the genome content metabolic adaptations and biogeography of taxa
high throughput sequencing studies and new software tools are revolutionizing microbial community analyses yet the variety of experimental and computational methods can be daunting in this review we discuss some of the different approaches to community profiling highlighting strengths and weaknesses of various experimental approaches sequencing methodologies and analytical methods we also address one key question emerging from various human microbiome projects is there a substantial core of abundant organisms or lineages that we all share it appears that in some human body habitats such as the hand and the gut the diversity among individuals is so great that we can rule out the possibility that any species is at high abundance in all individuals it is possible that the focus should instead be on higher level taxa or on functional instead
engineering artificial gene networks from modular components is a major goal of synthetic biology however the construction of gene networks with predictable functions remains hampered by a lack of suitable components and the fact that assembled networks often require extensive iterative retrofitting to work as intended here we present an approach that couples libraries of diversified components synthesized with randomized nonessential sequence with in silico modeling to guide predictable gene network construction without the need for post hoc tweaking we demonstrate our approach in saccharomyces cerevisiae by synthesizing regulatory promoter libraries and using them to construct feed forward loop networks with different predicted input output characteristics we then expand our method to produce a synthetic gene network acting as a predictable timer modifiable by component choice we use this network to control the timing of yeast sedimentation illustrating how the plug and play nature of our design can be readily applied biotechnology
pnas community network analysis derived from molecular dynamics simulations is used to identify and compare the signaling pathways in a bacterial glutamyl trna synthetase glurs trna and an archaeal leucyl trna synthetase leurs trna complex although the class i synthetases have remarkably different interactions with their cognate trnas the allosteric networks for charging trna with the correct amino acid display considerable similarities a dynamic contact map defines the edges connecting nodes amino acids and nucleotides in the physical network whose overall topology is presented as a network of communities local substructures that are highly intraconnected but loosely interconnected whereas nodes within a single community can communicate through many alternate pathways the communication between monomers in different communities has to take place through a smaller number of critical edges or interactions consistent with this analysis there are a large number of suboptimal paths that can be used for communication between the identity elements on the trnas and the catalytic site in the aars trna complexes residues and nucleotides in the majority of pathways for intercommunity signal transmission are evolutionarily conserved and are predicted to be important for allosteric signaling the same monomers are also found in a majority of the suboptimal paths modifying these residues or nucleotides has a large effect on the communication pathways in the protein rna complex consistent with data
there are many on line settings in which users publicly express opinions a number of these offer mechanisms for other users to evaluate these opinions a canonical example is amazon com where reviews come with annotations like of people found the following review helpful opinion evaluation appears in many off line settings as well including market research and political campaigns reasoning about the evaluation of an opinion is fundamentally different from reasoning about the opinion itself rather than asking what did y think of x we are asking what did z think of y s opinion of x here we develop a framework for analyzing and modeling opinion evaluation using a large scale collection of amazon book reviews as a dataset we find that the perceived helpfulness of a review depends not just on its content but also but also in subtle ways on how the expressed evaluation relates to other evaluations of the same product as part of our approach we develop novel methods that take advantage of the phenomenon of review plagiarism to control for the effects of text in opinion evaluation and we provide a simple and natural mathematical model consistent with our findings our analysis also allows us to distinguish among the predictions of competing theories from sociology and social psychology and to discover unexpected differences in the collective opinion evaluation behavior of user populations from countries
to understand the biology and evolution of ruminants the cattle genome was sequenced to about sevenfold coverage the cattle genome contains a minimum of genes with a core set of orthologs shared among seven mammalian species of which are absent or undetected in noneutherian marsupial or monotreme genomes cattle specific evolutionary breakpoint regions in chromosomes have a higher density of segmental duplications enrichment of repetitive elements and species specific variations in genes associated with lactation and immune responsiveness genes involved in metabolism are generally highly conserved although five metabolic genes are deleted or extensively diverged from their human orthologs the cattle genome sequence thus provides a resource for understanding mammalian evolution and accelerating livestock genetic improvement for milk and meat science
in this paper we give models and algorithms to describe and analyze the collaboration among authors of wikipedia from a network analytical perspective the edit network encodes who interacts how with whom when editing an article it significantly extends previous network models that code author communities in wikipedia several characteristics summarizing some aspects of the organization process and allowing the analyst to identify certain types of authors can be obtained from the edit network moreover we propose several indicators characterizing the global network structure and methods to visualize edit networks it is shown that the structural network indicators are correlated with quality labels of the associated articles
natural rewards and drugs of abuse can alter dopamine signaling and ventral tegmental area vta dopaminergic neurons are known to fire action potentials tonically or phasically under different behavioral conditions however without technology to control specific neurons with appropriate temporal precision in freely behaving mammals the causal role of these action potential patterns in driving behavioral changes has been unclear we used optogenetic tools to selectively stimulate vta dopaminergic neuron action potential firing in freely behaving mammals we found that phasic activation of these neurons was sufficient to drive behavioral conditioning and elicited dopamine transients with magnitudes not achieved by longer lower frequency spiking these results demonstrate that phasic dopaminergic activity is sufficient to mediate mammalian conditioning
the majority of the genome in animals and plants is transcribed in a developmentally regulated manner to produce large numbers of non protein coding rnas ncrnas whose incidence increases with developmental complexity there is growing evidence that these transcripts are functional particularly in the regulation of epigenetic processes leading to the suggestion that they compose a hitherto hidden layer of genomic programming in humans and other complex organisms however to date very few have been identified in genetic screens here i show that this is explicable by an historic emphasis both phenotypically and technically on mutations in protein coding sequences and by presumptions about the nature of regulatory mutations most variations in regulatory sequences produce relatively subtle phenotypic changes in contrast to mutations in protein coding sequences that frequently cause catastrophic component failure until recently most mapping projects have focused on protein coding sequences and the limited number of identified regulatory mutations have been interpreted as affecting conventional cis acting promoter and enhancer elements although these regions are often themselves transcribed moreover ncrna directed regulatory circuits underpin most if not all complex genetic phenomena in eukaryotes including rna interference related processes such as transcriptional and post transcriptional gene silencing position effect variegation hybrid dysgenesis chromosome dosage compensation parental imprinting and allelic exclusion paramutation and possibly transvection and transinduction the next frontier is the identification and functional characterization of the myriad sequence variations that influence quantitative traits disease susceptibility and other complex characteristics which are being shown by genome wide association studies to lie mostly in noncoding presumably regulatory regions there is every possibility that many of these variations will alter the interactions between regulatory rnas and their targets a prospect that should be borne in mind in future analyses
homologous genes are classified into orthologs and paralogs depending on whether they arose by speciation or duplication it is widely assumed that orthologs share similar functions whereas paralogs are expected to diverge more from each other but does this assumption hold up on further examination we present evidence that orthologs and paralogs are not so different in either their evolutionary rates or their mechanisms of divergence we emphasize the importance of appropriately designed studies to test models of gene evolution between orthologs and between paralogs thus functional change between orthologs might be as common as between paralogs and future studies should be designed to test the impact of duplication against this model
biomedical text mining biotm is providing valuable approaches to the automated curation of scientific literature however most efforts have addressed the benchmarking of new algorithms rather than user operational needs bridging the gap between biotm researchers and biologists needs is crucial to solve real world problems and promote further research we present note a platform for biotm that aims at the effective translation of the advances between three distinct classes of users biologists text miners and software developers its main functional contributions are the ability to process abstracts and full texts an information retrieval module enabling pubmed search and journal crawling a pre processing module with pdf to text conversion tokenisation and stopword removal a semantic annotation schema a lexicon based annotator a user friendly annotation view that allows to correct annotations and a text mining module supporting dataset preparation and algorithm evaluation note improves the interoperability modularity and flexibility when integrating in home and open source third party components its component based architecture allows the rapid development of new applications emphasizing the principles of transparency and simplicity of use although it is still on going it has already allowed the development of applications that are currently used
cortical gamma oscillations hz predict increases in focused attention and failure in gamma regulation is a hallmark of neurological and psychiatric disease current theory predicts that gamma oscillations are generated by synchronous activity of fast spiking inhibitory interneurons with the resulting rhythmic inhibition producing neural ensemble synchrony by generating a narrow window for effective excitation we causally tested these hypotheses in barrel cortex in vivo by targeting optogenetic manipulation selectively to fast spiking interneurons here we show that light driven activation of fast spiking interneurons at varied frequencies hz selectively amplifies gamma oscillations in contrast pyramidal neuron activation amplifies only lower frequency oscillations a cell type specific double dissociation we found that the timing of a sensory input relative to a gamma cycle determined the amplitude and precision of evoked responses our data directly support the fast spiking gamma hypothesis and provide the first causal evidence that distinct network activity states can be induced in vivo by cell type activation
ultrafast optics has undergone a revolution in the past two decades driven by new methods of pulse generation amplification manipulation and measurement we review the advances made in the latter field over this period indicating the general principles involved how these have been implemented in various experimental approaches and how the most popular methods encode the temporal electric field of a short optical pulse in the measured signal and extract the field from data
objective to determine the significance of the english wikipedia as a source of online health information design the authors measured wikipedias ranking on general internet search engines by entering keywords from medlineplus nhs direct online and the national organization of rare diseases as queries into search engine optimization software we assessed whether article quality influenced this ranking the authors tested whether traffic to wikipedia coincided with epidemiological trends and news of emerging health concerns and how it compares to medlineplus measurements cumulative incidence and average position of wikipedia compared to other web sites among the first results on general internet search engines google google uk yahoo and msn and page view statistics for selected wikipedia articles and medlineplus pages results wikipedia ranked among the first ten results in of search engines and keywords tested wikipedia surpassed medlineplus and nhs direct online except for queries from the latter on google uk and ranked higher with quality articles wikipedia ranked highest for rare diseases although its incidence in several categories decreased page views increased parallel to the occurrence of seasonal disorders and news of three emerging health concerns wikipedia articles were viewed more often than medlineplus topic p but for medlineplus encyclopedia pages the trend was not significant p conclusions based on its search engine ranking and page view statistics the english wikipedia is a prominent source of online health information compared to the other online health information studied
background micrornas mirnas are short non coding rna regulators of protein coding genes mirnas play a very important role in diverse biological processes and various diseases many algorithms are able to predict mirna genes and their targets but their transcription regulation is still under investigation it is generally believed that intragenic mirnas located in introns or exons of protein coding genes are co transcribed with their host genes and most intergenic mirnas transcribed from their own rna polymerase ii pol ii promoter however the length of the primary transcripts and promoter organization is currently unknown methodology we performed pol ii chromatin immunoprecipitation chip chip using a custom array surrounding regions of known mirna genes to identify the true core transcription start sites of the mirna genes we developed a new tool cppp we showed that mirna genes can be transcribed from promoters located several kilobases away and that their promoters share the same general features as those of protein coding genes finally we found evidence that as many as of the intragenic mirnas may be transcribed from their own unique promoters conclusion mirna promoters have similar features to those of protein coding genes but mirna transcript organization is complex
abstract background a wide variety of ontologies relevant to the biological and medical domains are available through the obo foundry portal and their number is growing rapidly integration of these ontologies while requiring considerable effort is extremely desirable however heterogeneities in format and style pose serious obstacles to such integration in particular inconsistencies in naming conventions can impair the readability and navigability of ontology class hierarchies and hinder their alignment and integration while other sources of diversity are tremendously complex and challenging agreeing a set of common naming conventions is an achievable goal particularly if those conventions are based on lessons drawn from pooled practical experience and surveys of community opinion results we summarize a review of existing naming conventions and highlight certain disadvantages with respect to general applicability in the biological domain we also present the results of a survey carried out to establish which naming conventions are currently employed by obo foundry ontologies and to determine what their special requirements regarding the naming of entities might be lastly we propose an initial set of typographic syntactic and semantic conventions for labelling classes in obo foundry ontologies conclusions adherence to common naming conventions is more than just a matter of aesthetics such conventions provide guidance to ontology creators help developers avoid flaws and inaccuracies when editing and especially when interlinking ontologies common naming conventions will also assist consumers of ontologies to more readily understand what meanings were intended by the authors of ontologies used in annotating bodies data
a neuroscientific experiment typically generates a large amount of data of which only a small fraction is analyzed in detail and presented in a publication however selection among noisy measurements can render circular an otherwise appropriate analysis and invalidate results here we argue that systems neuroscience needs to adjust some widespread practices to avoid the circularity that can arise from selection in particular double dipping the use of the same dataset for selection and selective analysis will give distorted descriptive statistics and invalid statistical inference whenever the results statistics are not inherently independent of the selection criteria under the null hypothesis to demonstrate the problem we apply widely used analyses to noise data known to not contain the experimental effects in question spurious effects can appear in the context of both univariate activation analysis and multivariate pattern information analysis we suggest a policy for circularity
summary support vector machine svms classification is a widely used and one of the most powerful classification techniques however a major limitation is that svm cannot perform automatic gene selection to overcome this restriction a number of penalized feature selection methods have been proposed in the r package penalizedsvm implemented penalization functions l norm and smoothly clipped absolute deviation scad provide automatic feature selection for svm classification tasks availability the r package penalizedsvm is available from the comprehensive r archive network http cran r project org under gpl later
in this the anniversary of charles darwin s birth and the anniversary of the publication of the origin of species it is fitting to revisit the classification of protein structures from an evolutionary perspective existing classifications use homologous sequence relationships but knowing that structure is much more conserved that sequence creates an iterative loop from which structures can be further classified beyond that of the domain thereby teasing out distant evolutionary relationships the desired classification scheme is then one in which a fold is merely semantics and structure can be classified as either ancestral derived
cavity enhanced radiation pressure coupling between optical and mechanical degrees of freedom allows quantum limited position measurements and gives rise to dynamical backaction enabling amplification and cooling of mechanical motion here we demonstrate purely dispersive coupling of high q nanomechanical oscillators to an ultrahigh finesse optical microresonator via its evanescent field extending cavity optomechanics to nanomechanical oscillators dynamical backaction mediated by the optical dipole force is observed leading to laser like coherent nanomechanical oscillations solely due to radiation pressure moreover sub displacement sensitivity is achieved with a measurement imprecision equal to the standard quantum limit sql which coincides with the nanomechanical oscillators zero point fluctuations the achievement of an imprecision at the sql and radiation pressure dynamical backaction for nanomechanical oscillators may have implications not only for detecting quantum phenomena in mechanical systems but also for a variety of other precision experiments owing to the flexibility of the near field coupling platform it can be readily extended to a diverse set of nanomechanical oscillators in addition the approach provides a route to experiments where radiation pressure quantum backaction dominates at room temperature enabling ponderomotive squeezing or quantum non measurements
software architecture design is a critical aspect of developing large scale software systems however the practice of architecture design reasoning is immature partly because of a lack of practical methodology support the authors discuss why capturing design rationalethe elements of design reasoningis useful and how developers use it in architecture design they demonstrate architecture design reasoning s application with a uml based modeling method called architecture rationale and linkage
the core methods in today s econometric toolkit are linear regression for statistical control instrumental variables methods for the analysis of natural experiments and differences in differences methods that exploit policy changes in the modern experimentalist paradigm these techniques address clear causal questions such as do smaller classes increase learning should wife batterers be arrested how much does education raise wages harmless shows how the basic tools of applied econometrics allow the data to speak in addition to econometric essentials harmless covers important new extensions regression discontinuity designs and quantile regression as well as how to get standard errors right joshua angrist and j rn steffen pischke explain why fancier econometric techniques are typically unnecessary and even dangerous the applied econometric methods emphasized in this book are easy to use and relevant for many areas of contemporary social science an irreverent review of econometric essentials a focus on tools that applied researchers use most chapters on regression discontinuity designs quantile regression and standard errors many empirical examples a clear and concise resource with applications
genetic changes that help explain the differences between two individuals might create or disrupt sites complementary to micrornas mirnas but the extent to which such polymorphic sites influence mirna mediated repression is unknown here we describe a method to measure mrna allelic imbalances associated with a regulatory site found in mrna transcribed from one allele but not found in that transcribed from the other applying this method called allelic imbalance sequencing to sites for three mirnas mir mir and mir provided quantitative measurements of repression in vivo without altering either the mirnas or their targets a substantial fraction of polymorphic sites mediated repression in tissues that expressed the cognate mirna and downregulation was correlated with site type and site context extrapolating these results to the other broadly conserved mirnas suggests that when comparing two mouse strains or two human individuals polymorphic mirna sites cause expression of many genes often hundreds differ
pnas much of neuroscience has to do with relating neural activity and behavior or environment one common measure of this relationship is the firing rates of neurons as functions of behavioral or environmental parameters often called tuning functions and receptive fields firing rates are estimated from the spike trains of neurons recorded by electrodes implanted in the brain individual neurons spike trains are not typically readily available because the signal collected at an electrode is often a mixture of activities from different neurons and noise extracting individual neurons spike trains from voltage signals which is known as spike sorting is one of the most important data analysis problems in neuroscience because it has to be undertaken prior to any analysis of neurophysiological data in which more than one neuron is believed to be recorded on a single electrode all current spike sorting methods consist of clustering the characteristic spike waveforms of neurons the sequence of first spike sorting based on waveforms then estimating tuning functions has long been the accepted way to proceed here we argue that the covariates that modulate tuning functions also contain information about spike identities and that if tuning information is ignored for spike sorting the resulting tuning function estimates are biased and inconsistent unless spikes can be classified with perfect accuracy this means for example that the commonly used peristimulus time histogram is a biased estimate of the firing rate of a neuron that is not perfectly isolated we further argue that the correct conceptual way to view the problem out is to note that spike sorting provides information about rate estimation and vice versa so that the two relationships should be considered simultaneously rather than sequentially indeed we show that when spike sorting and tuning curve estimation are performed in parallel unbiased estimates of tuning curves can be recovered even from imperfectly neurons
ultrafast real time optical imaging is an indispensable tool for studying dynamical events such as shock chemical dynamics in living neural laser and however conventional ccds charge coupled devices and their complementary metaloxidesemiconductor cmos counterparts are incapable of capturing fast dynamical processes with high sensitivity and resolution this is due in part to a technological limitationit takes time to read out the data from sensor arrays also there is the fundamental compromise between sensitivity and frame rate at high frame rates fewer photons are collected during each framea problem that affects nearly all optical imaging systems here we report an imaging method that overcomes these limitations and offers frame rates that are at least times faster than those of conventional ccds our technique maps a two dimensional image into a serial time domain data stream and simultaneously amplifies the image in the optical domain we capture an entire image using a single pixel photodetector and achieve a net image amplification of a factor of this overcomes the compromise between sensitivity and frame rate without resorting to cooling and high intensity illumination as a proof of concept we perform continuous real time imaging at a frame speed of a frame rate of and a shutter speed of we also demonstrate real time imaging of microfluidic flow and phase explosion effects that occur during ablation
autism spectrum disorders asds are childhood neurodevelopmental disorders with complex genetic origins previous studies focusing on candidate genes or genomic regions have identified several copy number variations cnvs that are associated with an increased risk of asds here we present the results from a whole genome cnv study on a cohort of asd cases and healthy children of european ancestry who were genotyped with approximately single nucleotide polymorphism markers in an attempt to comprehensively identify cnvs conferring susceptibility to asds positive findings were evaluated in an independent cohort of asd cases and controls of european ancestry besides previously reported asd candidate genes such as ref and refs several new susceptibility genes encoding neuronal cell adhesion molecules including and were enriched with cnvs in asd cases compared to controls p x furthermore cnvs within or surrounding genes involved in the ubiquitin pathways including and were affected by cnvs not observed in controls p x we also identified duplications kilobases upstream of complementary dna p x although these variants may be individually rare they target genes involved in neuronal cell adhesion or ubiquitin degradation indicating that these two important gene networks expressed within the central nervous system may contribute to the genetic susceptibility asd
autism spectrum disorders asds represent a group of childhood neurodevelopmental and neuropsychiatric disorders characterized by deficits in verbal communication impairment of social interaction and restricted and repetitive patterns of interests and behaviour to identify common genetic risk factors underlying asds here we present the results of genome wide association studies on a cohort of families subjects with affected children and a second cohort of affected subjects and control subjects all of whom were of european ancestry six single nucleotide polymorphisms between cadherin and cadherin two genes encoding neuronal cell adhesion moleculesrevealed strong association signals with the most significant snp being p odds ratio these signals were replicated in two independent cohorts with combined pvalues ranging from to our results implicate neuronal cell adhesion molecules in the pathogenesis of asds and represent to our knowledge the first demonstration of genome wide significant association of common variants with susceptibility asds
enveloped viruses that rely on a low ph dependent step for entry initiate infection by fusing with acidic endosomes whereas the entry sites for ph independent viruses such as hiv have not been defined these viruses have long been assumed to fuse directly with the plasma membrane here we used population based measurements of the viral content delivery into the cytosol and time resolved imaging of single viruses to demonstrate that complete hiv fusion occurred in endosomes in contrast viral fusion with the plasma membrane did not progress beyond the lipid mixing step hiv underwent receptor mediated internalization long before endosomal fusion thus minimizing the surface exposure of conserved viral epitopes during fusion and reducing the efficacy of inhibitors targeting these epitopes we also show that strikingly endosomal fusion is sensitive to a dynamin inhibitor dynasore these findings imply that hiv infects cells via endocytosis and envelope glycoprotein and dynamin dependent fusion with compartments
africa is the source of all modern humans but characterization of genetic variation and of relationships among populations across the continent has been enigmatic we studied african populations four african american populations and non african populations for patterns of variation at nuclear microsatellite and insertion deletion markers we identified ancestral population clusters in africa that correlate with self described ethnicity and shared cultural and or linguistic properties we observed high levels of mixed ancestry in most populations reflecting historical migration events across the continent our data also provide evidence for shared ancestry among geographically diverse hunter gatherer populations khoesan speakers and pygmies the ancestry of african americans is predominantly from niger kordofanian approximately european approximately and other african approximately populations although admixture levels varied considerably among individuals this study helps tease apart the complex evolutionary history of africans and african americans aiding both anthropological and genetic studies
during evolution novel phenotypes emerge through changes in gene expression but the genetic basis is poorly understood we compared the allele specific expression of two yeast species and their hybrid which allowed us to distinguish changes in regulatory sequences of the gene itself cis from changes in upstream regulatory factors trans expression divergence between species was generally due to changes in cis divergence in trans reflected a differential response to the environment and explained the tendency of certain genes to diverge rapidly hybrid specific expression deviating from the parental range occurred through novel cis trans interactions or more often through modified trans regulation associated with environmental sensing these results provide insights on the regulatory changes in cis and trans during the divergence of species and hybridization
as civil engineering enters the century the demands on the profession will move toward complex interdisciplinary tasks such as infrastructure rehabilitation environmental cleanup and the delivery of high technology facilities e g hospitals r d laboratories and advanced manufacturing plants the current structural design paradigm is a top down process that includes a nonhomogeneous approach to decision making there is an apparent lack of basic principles to formalize and evaluate conceptual design decisions while preliminary and detailed design decisions reflect increasing formalization and reliance on computational methods this nonhomogeneous approach to decision making limits how well the practicing engineer can meet the impending design challenges particularly since conceptual design decisions determine a significant portion of a project s total cost axiomatic design is presented as a systematic framework for structural design because it aids the designer in satisfying multiple design objectives in a homogeneous manner throughout the design process it is also an effective framework for formalizing and evaluating conceptual design decisions the design of a structural frame for an innovative mechanical parking system is presented as an illustrative case study this paper represents an initial effort to apply the principles of axiomatic design to the domain of civil engineering structures springer verlag new inc
transcription factor tf regulation is often post translational tf modifications such as reversible phosphorylation and missense mutations which can act independent of tf expression level are overlooked by differential expression analysis using bovine piedmontese myostatin mutants as proof of concept we propose a new algorithm that correctly identifies the gene containing the causal mutation from microarray data alone the myostatin mutation releases the brakes on piedmontese muscle growth by translating a dysfunctional protein compared to a less muscular non mutant breed we find that myostatin is not differentially expressed at any of ten developmental time points despite this challenge the algorithm identifies the myostatin smoking gun through a coordinated simultaneous weighted integration of three sources of microarray information transcript abundance differential expression and differential wiring by asking the novel question which regulator is cumulatively most differentially wired to the abundant most differentially expressed genes it yields the correct answer myostatin our new approach identifies causal regulatory changes by globally contrasting co expression network dynamics the entirely data driven weighting procedure emphasises regulatory movement relative to the phenotypically relevant part of the network in contrast to other published methods that compare co expression networks significance testing is not used to connections
gr identifying targets of positive selection in humans has until recently been frustratingly slow relying on the analysis of individual candidate genes genomics however has provided the necessary resources to systematically interrogate the entire genome for signatures of natural selection to date genome wide scans for recent or ongoing positive selection have been performed in humans a key challenge is to begin synthesizing these newly constructed maps of positive selection into a coherent narrative of human evolutionary history and derive a deeper mechanistic understanding of how natural populations evolve here i chronicle the recent history of the burgeoning field of human population genomics critically assess genome wide scans for positive selection in humans identify important gaps in knowledge and discuss both short and long term strategies for traversing the path from the low resolution incomplete and error prone maps of selection today to the ultimate goal of a detailed molecular mechanistic phenotypic and population genetics characterization of alleles
several studies have found evidence for more positive selection on the chimpanzee lineage compared with the human lineage since the two species split a potential concern however is that these findings may simply reflect artifacts of the data inaccuracies in the underlying chimpanzee genome sequence which is of lower quality than human to test this hypothesis we generated de novo genome assemblies of chimpanzee and macaque and aligned them with human we also implemented a novel bioinformatic procedure for producing alignments of closely related species that uses synteny information to remove misassembled and misaligned regions and sequence quality scores to remove nucleotides that are less reliable we applied this procedure to re examine genes recently identified as candidates for positive selection in chimpanzees the great majority of these signals disappear after application of our new bioinformatic procedure we also carried out laboratory based resequencing of of the regions in multiple chimpanzees and humans and found that our alignments were correct wherever there was a conflict with the published results these findings throw into question previous findings that there has been more positive selection in chimpanzees than in humans since the two species diverged our study also highlights the challenges of searching the extreme tails of distributions for signals of natural selection inaccuracies in the genome sequence at even a tiny fraction of genes can produce false positive signals which make it difficult to identify loci that have genuinely been targets selection
since its introduction at chi the esp game has inspired many similar games that share the goal of gathering data from players this paper introduces a new mechanism for collecting labeled data using games with a purpose in this mechanism players are provided with either the same or a different object and asked to describe that object to each other based on each other s descriptions players must decide whether they have the same object or not we explain why this new mechanism is superior for input data with certain characteristics introduce an enjoyable new game called tagatune that collects tags for music clips via this mechanism and present findings on the data that is collected by game
computational microrna mirna target prediction is one of the key means for deciphering the role of mirnas in development and disease here we present the diana microt web server as the user interface to the diana microt mirna target prediction algorithm the web server provides extensive information for predicted mirna target gene interactions with a user friendly interface providing extensive connectivity to online biological resources target gene and mirna functions may be elucidated through automated bibliographic searches and functional information is accessible through kyoto encyclopedia of genes and genomes kegg pathways the web server offers links to nomenclature sequence and protein databases and users are facilitated by being able to search for targeted genes using different nomenclatures or functional features such as the genes possible involvement in biological pathways the target prediction algorithm supports parameters calculated individually for each mirna target gene interaction and provides a signal to noise ratio and a precision score that helps in the evaluation of the significance of the predicted results using a set of mirna targets recently identified through the psilac method the performance of several computational target prediction programs was assessed diana microt achieved there with the highest ratio of correctly predicted targets over all predicted targets the diana microt web server is freely available at www microrna microt
binder drainage occurs with mixes of small aggregate surface area particularly porous asphalt the binder drainage test developed by the transport research laboratory uk is commonly used to set an upper limit on the acceptable binder content for a porous mix this paper presents the results of a laboratory investigation to determine the effects of different binder types on the binder drainage characteristics of porous mix made of various maximum aggregate sizes and mm two types of binder were used conventional pen bitumen and styrene butadiene styrene sbs modified bitumen the amount of binder lost through drainage after three hours at the maximum mixing temperature were measured in duplicate for mixes of different maximum sizes and binder contents the maximum mixing temperature adopted depends on the types of binder used the retained binder is plotted against the initial mixed binder content together with the line of equality where the retained binder equals the mixed binder content the results indicate the significant contribution of using sbs modified bitumen to increase the target bitumen binder content their significance is discussed in terms of target binder content the critical binder content the maximum mixed binder content and the maximum retained binder content values obtained from the binder drainage test it was concluded that increasing maximum aggregate sizes decrease the maximum retained binder content critical binder content target binder content maximum mixed binder content and mixed content for both binders but however for all mixtures sbs is highest
leading is one of the most important facets in managing construction projects and behaving as an effective leader is a vital project managers responsibility to ensure that work efforts of other persons are directed toward the accomplishment of organizational objectives this paper aims to determine basic and actual leadership styles of construction project managers in surabaya the effectiveness of the actual leadership style is also examined to accomplish the objective the paper first briefly reviews the ways in which leadership is approached data were then collected through an empirical survey to project managers taking fiedler and hersey blanchards models as the point of departure the results indicate that the basic leadership of project managers in surabaya falls slightly on task oriented behavior meanwhile selling is the most common style used as actual leadership in practice the paper discusses the effectiveness of the styles adopted and situational affecting
perencanaan campuran beton mix design adalah suatu langkah yang sangat penting dalam pengendalian mutu beton campuran yang salah akan mempengaruhi kemudahan pelaksanaan maupun performa beton dalam pemakaian makalah yang menarik ini mengungkapkan pengalaman dan praktek yang dilakukan di romania dalam merencanakan campuran beton untuk kepentingan
this paper presents a research project which investigates the use of virtual reality and computer communication technology to facilitate building design coordination in distributed environments the emphasis of the system called vr based design coordination vrdeco is providing a communication tool that can be used by remote designers for settling ideas before they fully engage in concurrent engineering environments vrdeco provides the necessary design tools library of building elements and communication procedures for designers from remote places to perform and coordinate their initial tasks it has been implemented using available commercial software packages and is used in designing a simple house vrdeco facilitates the creation a preliminary design and simple communication with the client there are however some difficulties in the development of the full version of vrdeco i e creating an adequate number of building elements building specification database with a sufficient number of choices and establishing a systematic rule to determine the parts of a building that updateable
some shop houses were built late and others are on time or faster than the schedule that had been planned the questions are how it could happen and what kind of factors that made it happened this research has an objective to investigate time performance of shop house constructions in surabaya by representing factors that influence it it first assembles potential influencing factors through literature review and second conducts an empirical study by collecting data from finished thirty two shop house projects throughout the city results of analyses using one way analysis of variance anova indicate eight factors to be statistically significant influencing time performance they are construction design change schedule of work that will be done workers discipline material availability owners payment quality control workers availability and material delivery the paper discusses the factors and proposes possible solutions to improve performance
the using of roller compacted concrete rcc is one of many alternatives that can be used to decrease dam construction cost many roller compacted concrete rcc composition has been developed to achieve maximum compressive strength due to the economical consideration and the possibility of the execution drop hammer system has been used for this research compression test is done after the age of the sample reaches seven and days the result shows that composition of gravel sand has higher average compressive strength on all age of sample the highest compressive strength the achieve is mpa for days sample br hr abstract in bahasa indonesia br penggunaan roller compacted concrete rcc merupakan salah satu alternatif yang dapat digunakan untuk mengurangi biaya pembuatan konstruksi bendungan berbagai komposisi benda uji roller compacted concrete rcc dibuat untuk mengetahui kuat tekan yang paling maksimal ditinjau dari segi ekonomis dan kemudahan pelaksanaan maka digunakan sistem alat pemadat drop hammer dilakukan tes kuat tekan setelah umur benda uji masing masing mencapai tujuh dan hari hasil penelitian menunjukkan bahwa komposisi kerikil pasir sebesar selalu memiliki kuat tekan rata rata yang lebih tinggi pada semua umur benda uji kuat tekan terbesar pada benda uji umur hari mpa
the slake durability test is regarded as a simple test for assessing weathering of rocks this simple test has been accepted as a standard test by the rock mechanics society however mechanisms into slaking processes have not been fully understood yet as many factors involved in the processes the current research explored mechanisms performed by the test by conducting a series of slake durability tests for four types of soft rocks taken from coober pedy south australia results show that the slake durability index of weathered soft rocks was influenced by the degree of weathering distinctly weathered rocks had lower indeces compared to partly weathered rocks shapes also influenced the of these soft rocks different shapes displayed different mechanisms in the slaking processes samples that had irregular shapes tended to have a lower compared to samples that had rounded shapes thus the slake durability test might have simple procedures but it could have complicated mechanisms in slaking processes that contribute to the result of test
understanding characteristics of soil mixtures lead to increasing the confidence level before applying such materials in the field the outcomes of this study can provide insight into the swelling and the compressibility behavior of soil bentonite mixtures between non swelling materials and swelling materials a simple swell and compression laboratory test has been conducted for the purposes of this study the result of this study indicated that the existence of bentonite in the soil mixtures influence the swelling behavior which follows a hyperbolic curve model amount and size of nonswelling fraction affected the swelling compressibility
issues related to pricing strategy in the indonesian construction industry are covered including problems of current pricing strategy in construction exploration of pricing strategies with a market based approach and survey findings of the top indonesian contractors regarding their current pricing practices and the applicability of market based pricing strategy models developed by mochtar and arditi comparisons with similar survey findings of the top u s contractors are conducted whenever possible in conclusion the belief that current pricing strategy in construction is predominantly cost based is confirmed by the survey findings indeed in setting the markup most contractors rely on subjective assessment of the competition using simulated bidding scenarios it is discovered that indonesian contractors tend to be more market based as they know more about the owner characteristics competitors characteristics and market demand consequently the implementation of bidding procedure proposed by mochtar and arditi is supported to maximize the benefits of market based pricing strategies the bidding procedure change should be explored by all parties involved in the indonesian construction industry br hr abstract in bahasa indonesia br br cost based pricing market based pricing pricing variables procedure
the intestinal microflora is a positive health asset that crucially influences the normal structural and functional development of the mucosal immune system mucosal immune responses to resident intestinal microflora require precise control and an immunosensory capacity for distinguishing commensal from pathogenic bacteria in genetically susceptible individuals some components of the flora can become a liability and contribute to the pathogenesis of various intestinal disorders including inflammatory bowel diseases it follows that manipulation of the flora to enhance the beneficial components represents a promising therapeutic strategy the flora has a collective metabolic activity equal to a virtual organ within an organ and the mechanisms underlying the conditioning influence of the bacteria on mucosal homeostasis and immune responses are beginning to be unravelled an improved understanding of this hidden organ will reveal secrets that are relevant to human health and to several infectious inflammatory and neoplastic processes
summary due to the increasing number of text mining resources tools and corpora available to biologists interoperability issues between these resources are becoming significant obstacles to using them effectively uima the unstructured information management architecture is an open framework designed to aid in the construction of more interoperable tools u compare is built on top of the uima framework and provides both a concrete framework for out of the box text mining and a sophisticated evaluation platform allowing users to run specific tools on any target text generating both detailed statistics and instance based visualizations of outputs u compare is a joint project providing the world s largest and still growing collection of uima compatible resources these resources originally developed by different groups for a variety of domains include many famous tools and corpora u compare can be launched straight from the web without needing to be manually installed all u compare components are provided ready to use and can be combined easily via a drag and drop interface without any programming external uima components can also simply be mixed with u compare components without distinguishing between locally and remotely deployed resources availability http u compare org contact kano is s u tokyo ac bioinformatics
gene families are growing rapidly but standard methods for inferring phylogenies do not scale to alignments with over sequences we present fasttree a method for constructing large phylogenies and for estimating their reliability instead of storing a distance matrix fasttree stores sequence profiles of internal nodes in the tree fasttree uses these profiles to implement neighbor joining and uses heuristics to quickly identify candidate joins fasttree then uses nearest neighbor interchanges to reduce the length of the tree for an alignment with n sequences l sites and a different characters a distance matrix requires o space and o time but fasttree requires just o nla n img medium gif alt formula memory and o n img medium gif alt formula log n la time to estimate the tree s reliability fasttree uses local bootstrapping which gives another fold speedup over a distance matrix for example fasttree computed a tree and support values for distinct ribosomal rnas in h and gb of memory just computing pairwise jukes cantor distances and storing them without inferring a tree or bootstrapping would require h and gb of memory in simulations fasttree was slightly more accurate than neighbor joining bionj or fastme on genuine alignments fasttree s topologies had higher likelihoods fasttree is available at http microbesonline org molbev
the classic view of molecular oncology indicates that cancer is a genetic disease involving tumor suppressor and oncogenic proteins however in the recent years it has been demonstrated that small regulatory non coding rnas ncrnas named micrornas mirnas are involved in human tumorigenesis thus revealing a new layer in the molecular architecture of human cancer gene expression studies revealed that hundreds of mirnas are deregulated in cancer cells and functional studies clarified that mirnas are involved in all the molecular and biological processes that drive tumorigenesis here we summarize the recent advances in mirna involvement in human cancer and illustrate the benefits of using these knowledge for medical practice new diagnostic classifiers based on mirnas will soon be available for medical practitioners and even more importantly mirnas may become novel anti tools
recently a number of analytic prescriptions for computing the non linear matter power spectrum have appeared in the literature these typically involve resummation or closure prescriptions which do not have a rigorous error control thus they must be compared with numerical simulations to assess their range of validity we present a direct side by side comparison of several of these analytic approaches using a suite of high resolution n body simulations as a reference and discuss some general trends all of the analytic results correctly predict the behavior of the power spectrum at the onset of non linearity and improve upon a pure linear theory description at very large scales all of these theories fail at sufficiently small scales at low redshift the dynamic range in scale where perturbation theory is both relevant and reliable can be quite small we also compute for the first time the loop contribution to standard perturbation theory for cdm models finding improved agreement with simulations at large redshift at low redshifts however the loop term is larger than the loop term on quasi linear scales indicating a breakdown of the perturbation expansion finally we comment on possible implications of our results for studies
the unique structural motifs and self recognition properties of dna can be exploited to generate self assembling dna nanostructures of specific shapes using a bottom up several assembly strategies have been developed for building complex three dimensional dna recently the dna origami method was used to build two dimensional addressable dna structures of arbitrary that can be used as platforms to arrange nanomaterials with high precision and a long term goal of this field has been to construct fully addressable dna here we extend the dna origami method into three dimensions by creating an addressable dna box in size that can be opened in the presence of externally supplied dna keys we thoroughly characterize the structure of this dna box using cryogenic transmission electron microscopy small angle x ray scattering and atomic force microscopy and use fluorescence resonance energy transfer to optically monitor the opening of the lid controlled access to the interior compartment of this dna nanocontainer could yield several interesting applications for example as a logic sensor for multiple sequence signals or for the controlled release nanocargos
we analyze the anonymous communication patterns of million customers of a belgian mobile phone operator grouping customers by billing address we build a social network of cities that consists of communications between cities in belgium we show that inter city communication intensity is characterized by a gravity model the communication intensity between two cities is proportional to the product of their sizes divided by the square of distance
a topography map is a map which have an information about surface of the land height at a place from the surface of the sea that drawn with topography s lines topography information in topography map can be used to create three dimensions model of this map s surface with this three dimension model an object at the map can be seen more live as in the real world and because of that to analyze a topography map could be done easier abstract in bahasa indonesia peta topografi adalah peta yang memiliki informasi tentang ketinggian permukaan tanah pada suatu tempat terhadap permukaan laut yang digambarkan dengan garis garis kontur informasi topografi yang terdapat pada peta topografi dapat digunakan untuk membuat model tiga dimensi dari permukaan tanah pada peta tersebut dengan model tiga dimensi maka objek pada peta dilihat lebih hidup seperti pada keadaan sesungguhnya di alam sehingga untuk menganalisa suatu peta topografi dapat lebih mudah dilakukan kata kunci peta topografi geografi dimensi
tracking and recognizing human face becomes one of the important research subjects nowadays where it is applicable in security system like room access surveillance as well as searching for person identity in police database because of applying in security case it is necessary to have robust system for certain conditions such as background influence non frontal face pose of male or female in different age and race the aim of this research is to develop software which combines human face tracking using camshift algorithm and face recognition system using embedded hidden markov models the software uses video camera webcam for real time input video avi for dynamic input and image file for static input the software uses object oriented programming oop coding style with c programming language microsoft visual c compiler and assisted by some libraries of intel image processing library ipl and intel open source computer vision opencv system testing shows that object tracking based on skin complexion using camshift algorithm comes out well for tracking of single or even two face objects at once human face recognition system using embedded hidden markov models method has reach accuracy percentage of using human faces in database that consists of individuals with poses and human face testers abstract in bahasa indonesia pelacakan dan pengenalan wajah manusia merupakan salah satu bidang yang cukup berkembang dewasa ini dimana aplikasi dapat diterapkan dalam bidang keamanan security system seperti ijin akses masuk ruangan pengawasan lokasi surveillance maupun pencarian identitas individu pada database kepolisian karena diterapkan dalam kasus keamanan dibutuhkan sistem yang handal terhadap beberapa kondisi seperti pengaruh latar belakang pose wajah non frontal terhadap pria maupun wanita dalam perbedaan usia dan ras tujuan penelitiam ini adalah untuk membuat perangkat lunak yang menggabungkan sistem pelacakan wajah manusia dengan menggunakan algoritma camshift dan sistem pengenalan wajah dengan menggunakan algoritma embedded hidden markov models sebagai input sistem digunakan video kamera webcam untuk input bersifat real time video avi untuk input bersifat dinamis dan file image untuk input statis pemrograman perangkat lunak menggunakan prinsip pemrograman berorientasi objek oop dengan menggunakan bahasa pemrograman c kompiler microsoft visual c dan dibantu dengan library dari intel image processing library ipl dan intel open source computer vision opencv hasil pengujian sistem menunjukkan bahwa pelacakan berdasarkan warna kulit manusia dengan menggunakan algoritma camshift cukup baik dalam melakukan pelacakan terhadap satu maupun dua objek wajah sekaligus sistem pengenalan wajah manusia menggunakan metode embedded hidden markov models mencapai tingkat akurasi pengenalan sebesar dengan database citra wajah sebanyak citra yang terdiri dari individu dengan pose dan jumlah citra penguji sebanyak citra wajah kata kunci computer vision pelacakan objek camshift pengenalan wajah hidden model
indonesia has several laws and rules one of them is kitab undang undang hukum pidana kuhp or criminal code the most frequent to happened in criminal case is criminal case of wealth law domain is very complex so that it s difficult for common people to understand and categorize a criminal case based on the criminal code it will be very helpful if there is a computer program that could help people to understand and categorize a criminal case based on the criminal code the main topic in this research is the designing and making of a rule based expert system for criminal case of wealth problems this expert system development is using forward chaining inference method which is a data driven inference process for finding a conclusion law contents of this expert system program are adopted from the criminal code criminal cases discussed in this expert system program are thievery extortion and threat embezzling cheating act of damaging and passing goods from a criminal case the purpose of this software is to make an expert system program to select chapters of the criminal code which are involved in a criminal case this expert system development are based on this following steps analyze the law topic designing block diagrams dependency diagrams and decision tables implement design into a computer program and testing the program this expert system development is using borland delphi as programming language and micorosoft access as database based on testing this expert system program shows that this program still need more improvement on its law content with broaden law topic abstract in bahasa indonesia indonesia sebagai negara hukum memiliki bermacam macam peraturan hukum salah satunya adalah kitab undang undang hukum pidana kuhp yang digunakan untuk mengatur berbagai macam tindak pidana adapun jenis tindak pidana yang sering terjadi adalah tindak pidana terhadap harta kekayaan masalah hukum pidana sangat kompleks sehingga sulit bagi orang awam untuk mengerti dan memilah milah pasal pasal yang mengatur suatu kasus hukum hal ini sering membingungkan bagi orang awam saat terlibat dalam suatu kasus hukum sehingga perlu ada sebuah program komputer untuk membantu memahami dan memilah milah pasal pasal yang terlibat dalam suatu kasus hukum pembahasan utama dalam penelitian ini adalah perancangan dan pembuatan sistem pakar rule based untuk permasalahan hukum pidana terhadap harta kekayaan pengembangan sistem pakar ini menggunakan metode inferensi forward chaining yaitu proses inferensi yang memulai pencarian dari premis atau data menuju pada konklusi materi hukum untuk program sistem pakar ini diadopsi dari kitab undang undang hukum pidana kuhp permasalahan hukum yang dibahas meliputi pencurian pemerasan dan pengancaman penggelapan kecurangan perusakan dan penadahan tujuan dari software ini adalah membuat sistem pakar yang digunakan untuk menyeleksi pasal pasal kuhp yang terlibat dalam sebuah kasus pidana pembuatan sistem pakar ini dilakukan dengan tahapan sebagai berikut menganalisa permasalahan hukum dengan melibatkan praktisi hukum membuat desain sistem pakar mengimplementasikan desain dalam program komputer dan melakukan uji coba dengan melibatkan praktisi hukum dan orang awam pembuatan sistem pakar ini menggunakan bahasa pemrograman borland delphi dengan basis data microsoft access hasil pengujian menunjukkan bahwa program masih membutuhkan pengembangan pada sisi materi hukumnya dengan pengembangan program sejenis dengan domain permasalahan hukum yang lebih luas kata kunci forward chaining sistem pakar rule based pidana
an intersection is a critical area of conflicting traffic caused by the conflict problems of traffic movement this conflicting traffic leads to driver delay accident and traffic jam a high traffic flow increases problems due to vehicle conflict traffic light should be set up in order to separate and merge vehicle movement within traffic flow a coordination of undersaturated intersections could reduce delay travel time and travel cost a computer program called transyt was used to coordinate two undersaturated intersection discussed in this project research variables included were traffic volume saturation flow speed traffic light setting and geometrical data of the intersections the output of transyt showed that the initial performance index was rp and could be reduced to rp after coordination of the two intersections total delay decreased from the initial delay and the travel time was faster the optimum offset value of both intersection coordinated was found and the minimum performance index was attained using second cycle time br hr abstract in bahasa indonesia br br persimpangan merupakan salah satu lokasi yang rawan terhadap kemacetan akibat konflik pergerakan kendaraan konflik pergerakan ini menyebabkan tundaan kecelakaan serta kemacetan arus lalu lintas yang terlalu tinggi menimbulkan masalah karena adanya konflik yang meningkat maka pemasangan lampu lalu lintas perlu dilakukan pengkoordinasian dua simpang dibawah jenuh yang dianalisa dimaksudkan untuk mengurangi tundaan waktu perjalanan dan biaya perjalanan yang terjadi dari hasil analisa dengan menggunakan program transyt diperoleh kinerja simpang seperti nilai indek kinerja sebelum koordinasi sebesar rp dan rp setelah dikoordinasi tundaan total yang terjadi berkurang dari tundaan sebelum dikoordinasikan dan waktu perjalanan yang lebih cepat dari waktu perjalanan semula di samping itu diperoleh juga nilai offset optimum pada kedua simpang yang dikoordinasikan serta nilai indek kinerja paling minimum yang didapat dengan waktu siklus detik conflicting traffic delay saturation flow index
consolidation settlement is commonly computed using one point method where a clay deposit is assumed as one layer and the stress increase from foundation is taken at the middle of the layer this method is not accurate for determining consolidation settlement of a thick clay deposit this paper presents sub layer method to compute consolidation settlement which assumes that a clay layer is composed of several thinner layers the results of both methods were compared to the settlement observed from the three experiments using a model of square footing with dimension of cm by cm which was laid on a layer of clay the thickness of the clay layer were cm cm and cm or b b b respectively where b is the width of the foundation the result shows that the consolidation settlement calculated using sub layer method is always greater than that computed using the one point method and it has a better agreement to the settlement of the models minimum number of layers that gives sufficient accuracy of settlement is br hr abstract in bahasa indonesia br penurunan akibat konsolidasi pada umumnya dihitung dengan memakai metode one point dimana lapisan tanah liat dianggap satu lapisan dan penambahan tegangan akibat beban dari pondasi hanya ditinjau pada tengah tengah lapisan cara ini kurang teliti terutama untuk menghitung penurunan lapisan tanah liat yang tebal makalah ini mengemukakan metode sub layer untuk menghitung penurunan akibat konsolidasi metode ini mengasumsikan bahwa suatu lapisan tanah liat terdiri dari beberapa lapisan tipis sub layer dan perhitungan penurunannya dilakukan pada setiap lapisan tersebut hasil perhitungan dari kedua metode dibandingkan dengan penurunan yang diperoleh dari tiga percobaan pada model pondasi dengan ukuran panjang cm dan lebar cm yang diletakkan di atas lapisan tanah liat ketebalan lapisan tanah yang dipakai adalah cm cm dan cm atau masing masing sebesar b b dan b dimana b adalah lebar pondasi hasil penelitian menunjukkan bahwa penurunan yang dihitung dengan metode sub layer selalu lebih besar dari penurunan yang dihitung dengan metode one point dan lebih mendekati penurunan hasil percobaan jumlah layer minimal untuk mendapatkan penurunan yang cukup akurat lapisan
abstract background very frequently the same biological system is described by several sometimes competing mathematical models this usually creates a confusion around their validity i e which one is correct however this is unnecessary since validity of a model cannot be established model validation is actually a misnomer in principle the only statement that one can make about a system model is that it is incorrect i e invalid a fact which can be established given appropriate experimental data nonlinear models of high dimension and with many parameters are impossible to invalidate through simulation and as such the invalidation process is often overlooked or ignored results we develop different approaches for showing how competing ordinary differential equation ode based models of the same biological phenomenon containing nonlinearities and parametric uncertainty can be invalidated using experimental data we first emphasize the strong interplay between system identification and model invalidation and we describe a method for obtaining a lower bound on the error between candidate model predictions and data we then turn to model invalidation and formulate a methodology for discrete time and continuous time model invalidation the methodology is algorithmic and uses semidefinite programming as the computational tool it is emphasized that trying to invalidate complex nonlinear models through exhaustive simulation is not only computationally intractable but also inconclusive conclusions biological models derived from experimental data can never be validated in fact in order to understand biological function one should try to invalidate models that are incompatible with available data this work describes a framework for invalidating both continuous and discrete time ode models based on convex optimization techniques the methodology does not require any simulation of the candidate models the algorithms presented in this paper have a worst case polynomial time complexity and can provide an exact answer to the problem
the importance of contextual information has been recognized by researchers and practitioners in many disciplines including e commerce personalization information retrieval ubiquitous and mobile computing data mining marketing and management while a substantial amount of research has already been performed in the area of recommender systems most existing approaches focus on recommending the most relevant items to users without taking into account any additional contextual information such as time location or the company of other people e g for watching movies or dining out in this chapter we argue that relevant contextual information does matter in recommender systems and that it is important to take this information into account when providing recommendations we discuss the general notion of context and how it can be modeled in recommender systems furthermore we introduce three different algorithmic paradigms contextual pre filtering post filtering and modeling for incorporating contextual information into the recommendation process discuss the possibilities of combining several context aware recommendation techniques into a single unifying approach and provide a case study of one such combined approach finally we present additional capabilities for context aware recommenders and discuss important and promising directions for research
the next generation sequencing technology coupled with the growing number of genome sequences opens the opportunity to redesign genotyping strategies for more effective genetic mapping and genome analysis we have developed a high throughput method for genotyping recombinant populations utilizing whole genome resequencing data generated by the illumina genome analyzer a sliding window approach is designed to collectively examine genome wide single nucleotide polymorphisms for genotype calling and recombination breakpoint determination using this method we constructed a genetic map for rice recombinant inbred lines with an expected genotype calling accuracy of and a resolution of recombination breakpoints within an average of kb in comparison to the genetic map constructed with pcr based markers for the rice population the sequencing based method was faster in data collection and more precise in recombination breakpoint determination using the sequencing based genetic map we located a quantitative trait locus of large effect on plant height in a kb region containing the rice green revolution gene through computer simulation we demonstrate that the method is robust for different types of mapping populations derived from organisms with variable quality of genome sequences and is feasible for organisms with large genome sizes and low polymorphisms with continuous advances in sequencing technologies this genome based method may replace the conventional marker based genotyping approach to provide a powerful tool for large scale gene discovery and for addressing a wide range of questions
biomart central portal www biomart org offers a one stop shop solution to access a wide array of biological databases these include major biomolecular sequence pathway and annotation databases such as ensembl uniprot reactome hgnc wormbase and pride for a complete list visit http www biomart org biomart martview moreover the web server features seamless data federation making cross querying of these data sources in a user friendly and unified way the web server not only provides access through a web interface martview it also supports programmatic access through a perl api as well as restful and soap oriented web services the website is free and open to all users and there is no requirement
parietal and premotor cortex regions are serious contenders for bringing motor intentions and motor responses into awareness we used electrical stimulation in seven patients undergoing awake brain surgery stimulating the right inferior parietal regions triggered a strong intention and desire to move the contralateral hand arm or foot whereas stimulating the left inferior parietal region provoked the intention to move the lips and to talk when stimulation intensity was increased in parietal areas participants believed they had really performed these movements although no electromyographic activity was detected stimulation of the premotor region triggered overt mouth and contralateral limb movements yet patients firmly denied that they had moved conscious intention and motor awareness thus arise from increased parietal activity before movement science
the degree of confidence in a decision provides a graded and probabilistic assessment of expected outcome although neural mechanisms of perceptual decisions have been studied extensively in primates little is known about the mechanisms underlying choice certainty we have shown that the same neurons that represent formation of a decision encode certainty about the decision rhesus monkeys made decisions about the direction of moving random dots spanning a range of difficulties they were rewarded for correct decisions on some trials after viewing the stimulus the monkeys could opt out of the direction decision for a small but certain reward monkeys exercised this option in a manner that revealed their degree of certainty neurons in parietal cortex represented formation of the direction decision and the degree of certainty underlying the decision to out
we propose and analyze a setup to achieve strong coupling between a single trapped atom and a mechanical oscillator the interaction between the motion of the atom and the mechanical oscillator is mediated by a quantized light field in a laser driven high finesse cavity in particular we show that high fidelity transfer of quantum states between the atom and the mechanical oscillator is in reach for existing or near future experimental parameters our setup provides the basic toolbox from atomic physics for coherent manipulation preparation and measurement of micromechanical and oscillators
social media sharing web sites like flickr allow users to annotate images with free tags which significantly facilitate web image search and organization however the tags associated with an image generally are in a random order without any importance or relevance information which limits the effectiveness of these tags in search and other applications in this paper we propose a tag ranking scheme aiming to automatically rank the tags associated with a given image according to their relevance to the image content we first estimate initial relevance scores for the tags based on probability density estimation and then perform a random walk over a tag similarity graph to refine the relevance scores experimental results on a flickr photo collection show that the proposed tag ranking method is both effective and efficient we also apply tag ranking into three applications tag based image search tag recommendation and group recommendation which demonstrates that the proposed tag ranking approach really boosts the performances of social tagging applications
we have developed a set of web based snp selection tools freely available at http www niehs nih gov snpinfo where investigators can specify genes or linkage regions and select snps based on gwas results linkage disequilibrium ld and predicted functional characteristics of both coding and non coding snps the algorithm uses gwas snp p value data and finds all snps in high ld with gwas snps so that selection is from a much larger set of snps than the gwas itself the program can also identify and choose tag snps for snps not in high ld with any gwas snp we incorporate functional predictions of protein structure gene regulation splicing and mirna binding and consider whether the alternative alleles of a snp are likely to have differential effects on function users can assign weights for different functional categories of snps to further tailor snp selection the program accounts for ld structure of different populations so that a gwas study from one ethnic group can be used to choose snps for one or more other ethnic groups finally we provide an example using prostate cancer and demonstrate that this algorithm can select a small panel of snps that include many of the recently validated prostate cancer nar
micrornas mirnas are small nt nonprotein coding nucleic acids that regulate specific target gene products via hybridization to mrna transcripts resulting in translational blockade or transcript degradation although mirnas have been implicated in numerous developmental and adult diseases their specific impact on biological pathways and cellular phenotypes in addition to mirna gene promoter regulation remain largely unknown to improve and facilitate research of mirna functions and regulation we have developed mmia microrna and mrna integrated analysis a versatile and user friendly web server by incorporating three commonly used and accurate mirna prediction algorithms targetscan pita and pictar mmia integrates mirna and mrna expression data with predicted mirna target information for analyzing mirna associated phenotypes and biological functions by gene set analysis in addition to analysis of mirna primary transcript gene promoters to assign biological relevance to the integrated mirna mrna profiles mmia uses exhaustive human genome coverage including classification into various disease associated genes as well as conventional canonical pathways and gene ontology in summary this novel web server cancer informatics indiana edu mmia will provide life science researchers with a valuable tool for the study of the biological and pathological causes and effects of the expression of this class of interesting regulators
dna data bank of japan ddbj provides web based systems for biological analysis called web apis for biology wabi so far we have developed over soap services and several workflows that consist of a series of method invocations in this article we present newly developed services of wabi that is rest based web services additional workflows and a workflow navigation system each web service and workflow can be used as a complete service or a building block for programmers to construct more complex information processing systems the workflow navigation system aims to help non programming biologists perform analysis tasks by providing next applicable services on web browsers according to the output of a previously selected service with this function users can apply multiple services consecutively only by following links without any programming or manual copy and paste operations on web browsers the listed services are determined automatically by the system referring to the dictionaries of service categories the input output types of services and html tags wabi and the workflow navigation system are freely accessible at http www xml nig ac jp index html and http cyclamen ddbj nig ac respectively
program comprehension is an important activity in software maintenance as software must be sufficiently understood before it can be properly modified the study of a program textquoteright s execution known as dynamic analysis has become a common technique in this respect and has received substantial attention from the research community particularly over the last decade these efforts have resulted in a large research body of which currently there exists no comprehensive overview this paper reports on a systematic literature survey aimed at the identification and structuring of research on program comprehension through dynamic analysis from a research body consisting of articles published in relevant venues between july and june and the references therein we have systematically selected articles and characterized them in terms of four main facets activity target method and evaluation the resulting overview offers insight in what constitutes the main contributions of the field supports the task of identifying gaps and opportunities and has motivated our discussion of several important research directions that merit additional consideration in the future
background the uniprot consortium was formed in by groups from the swiss institute of bioinformatics sib the european bioinformatics institute ebi and the protein information resource pir at georgetown university and soon afterwards the website http www uniprot org was set up as a central entry point to uniprot resources requests to this address were redirected to one of the three organisations websites while these sites shared a set of static pages with general information about uniprot their pages for searching and viewing data were different to provide users with a consistent view and to cut the cost of maintaining three separate sites the consortium decided to develop a common website for uniprot following several years of intense development and a year of public beta testing the http www uniprot org domain was switched to the newly developed site described in this paper in july description the uniprot consortium is the main provider of protein sequence and annotation data for much of the life sciences community the http www uniprot org website is the primary access point to this data and to documentation and basic tools for the data these tools include full text and field based text search similarity search multiple sequence alignment batch retrieval and database identifier mapping this paper discusses the design and implementation of the new website which was released in july and shows how it improves data access for users with different levels of experience as well as to machines for programmatic access http www uniprot org is open for both academic and commercial use the site was built with open source tools and libraries feedback is very welcome and should be sent to help uniprot org conclusion the new uniprot website makes accessing and understanding uniprot easier than ever the two main lessons learned are that getting the basics right for such a data provider website has huge benefits but is not trivial and easy to underestimate and that there is no substitute for using empirical data throughout the development process to decide on what is and what is not working for users
title author summary title p biological systems are exceedingly complicated they consist of a large number of elements those elements interact in nonlinear and highly unpredictable ways and collective interactions typically play a critical role it would seem surprising then that one could build a quantitative description of biological systems based only on knowledge of how pairs of elements interact yet that is what a number of studies have found those studies however focused on relatively small systems here we ask the question do their conclusions extend to large systems we show that the answer depends on the size of the system relative to a crossover point below the crossover point the results on the small system have no predictive power for large systems above the crossover point the results on the small system may have predictive power moreover the crossover point can be computed analytically this work thus provides a general framework for determining the extent to which pairwise models can be used to predict the behavior of large biological systems it also provides a useful heuristic for designing experiments if one is interested in understanding truly large systems via pairwise interactions then make sure that the system one studies is above the crossover p
jbc the structure stability solubility and function of proteins depend on their net charge and on the ionization state of the individual residues consequently biochemists are interested in the p values of the ionizable groups in proteins and how these p values depend on their environment we review what has been learned about p values of ionizable groups in proteins from experimental studies and discuss the important contributions they make to protein stability solubility
gr next generation massively parallel sequencing technologies provide ultrahigh throughput at two orders of magnitude lower unit cost than capillary sanger sequencing technology one of the key applications of next generation sequencing is studying genetic variation between individuals using whole genome or target region resequencing here we have developed a consensus calling and snp detection method for sequencing by synthesis illumina genome analyzer technology we designed this method by carefully considering the data quality alignment and experimental errors common to this technology all of this information was integrated into a single quality score for each base under bayesian theory to measure the accuracy of consensus calling we tested this methodology using a large scale human resequencing data set of coverage and assembled a high quality nonrepetitive consensus sequence for of the diploid autosomes and of the haploid x chromosome comparison of the consensus sequence with illumina human beadchip genotyped alleles from the same dna sample showed that of the genotyped alleles on the x chromosome and of genotyped alleles on autosomes were covered at and consistency respectively at a low sequencing depth we used prior probability of dbsnp alleles and were able to improve coverage of the dbsnp sites significantly as compared to that obtained using a nonimputation model our analyses demonstrate that our method has a very low false call rate at any sequencing depth and excellent genome coverage at a high depth
anyone who regularly reads life science literature often comes across names of genes proteins or small molecules that they would like to know more about to make this process easier we have developed a new free service called reflect http reflect ws that can be installed as a plug in to firefox or internet explorer reflect tags gene protein and small molecule names in any web page typically within a few seconds and without affecting document layout clicking on a tagged gene or protein name opens a popup showing a concise summary that includes synonyms database identifiers sequence domains structure interaction partners subcellular location and related literature clicking on a tagged small molecule name opens a popup showing structure and interaction partners the popups also allow navigation to commonly used databases in the future we plan to add further entity types to reflect including outside the sciences
selection acting on genomic functional elements can be detected by its indirect effects on population diversity at linked neutral sites to illuminate the selective forces that shaped hominid evolution we analyzed the genomic distributions of human polymorphisms and sequence differences among five primate species relative to the locations of conserved sequence features neutral sequence diversity in human and ancestral hominid populations is substantially reduced near such features resulting in a surprisingly large genome average diversity reduction due to selection of on the autosomes and on the x chromosome the overall trends are broadly consistent with background selection or hitchhiking in ancestral populations acting to remove deleterious variants average selection is much stronger on exonic both protein coding and untranslated conserved features than non exonic features long term selection rather than complex speciation scenarios explains the large intragenomic variation in human chimpanzee divergence our analyses reveal a dominant role for selection in shaping genomic diversity and divergence patterns clarify hominid evolution and provide a baseline for investigating specific events
like all scientific disciplines drug discovery chemistry is rife with terminology and methodology that can seem intractable to those outside the sphere of synthetic chemistry derived from a successful in house workshop this foundation review aims to demystify some of this inherent terminology providing the non specialist with a general insight into the nomenclature terminology and workflow of medicinal chemists within the industry
this paper presents sofie a system for automated ontology extension sofie can parse natural language documents extract ontological facts from them and link the facts into an ontology sofie uses logical reasoning on the existing knowledge and on the new knowledge in order to disambiguate words to their most probable meaning to reason on the meaning of text patterns and to take into account world knowledge axioms this allows sofie to check the plausibility of hypotheses and to avoid inconsistencies with the ontology the framework of sofie unites the paradigms of pattern matching word sense disambiguation and ontological reasoning in one unified model our experiments show that sofie delivers high quality output even from unstructured documents
chemical synthesis of custom dna made to order calls for software streamlining the design of synthetic dna sequences genocadtm www genocad org is a free web based application to design protein expression vectors artificial gene networks and other genetic constructs composed of multiple functional blocks called genetic parts by capturing design strategies in grammatical models of dna sequences genocad guides the user through the design process by successively clicking on icons representing structural features or actual genetic parts complex constructs composed of dozens of functional blocks can be designed in a matter of minutes genocad automatically derives the construct sequence from its comprehensive libraries of genetic parts upon completion of the design process users can download the sequence for synthesis or further analysis users who elect to create a personal account on the system can customize their workspace by creating their own parts libraries adding new parts to the libraries or reusing designs to quickly generate sets of related nar
graphene has been attracting great interest because of its distinctive band structure and physical properties today graphene is limited to small sizes because it is produced mostly by exfoliating graphite we grew large area graphene films of the order of centimeters on copper substrates by chemical vapor deposition using methane the films are predominantly single layer graphene with a small percentage less than of the area having few layers and are continuous across copper surface steps and grain boundaries the low solubility of carbon in copper appears to help make this growth process self limiting we also developed graphene film transfer processes to arbitrary substrates and dual gated field effect transistors fabricated on silicon silicon dioxide substrates showed electron mobilities as high as square centimeters per volt per second at temperature
exercise promotes longevity and ameliorates type diabetes mellitus and insulin resistance however exercise also increases mitochondrial formation of presumably harmful reactive oxygen species ros antioxidants are widely used as supplements but whether they affect the health promoting effects of exercise is unknown we evaluated the effects of a combination of vitamin c mg day and vitamin e iu day on insulin sensitivity as measured by glucose infusion rates gir during a hyperinsulinemic euglycemic clamp in previously untrained n and pretrained n healthy young men before and after a week intervention of physical exercise gir was determined and muscle biopsies for gene expression analyses as well as plasma samples were obtained to compare changes over baseline and potential influences of vitamins on exercise effects exercise increased parameters of insulin sensitivity gir and plasma adiponectin only in the absence of antioxidants in both previously untrained p and pretrained p individuals this was paralleled by increased expression of ros sensitive transcriptional regulators of insulin sensitivity and ros defense capacity peroxisome proliferator activated receptor gamma ppargamma and ppargamma coactivators and only in the absence of antioxidants p for all molecular mediators of endogenous ros defense superoxide dismutases and glutathione peroxidase were also induced by exercise and this effect too was blocked by antioxidant supplementation consistent with the concept of mitohormesis exercise induced oxidative stress ameliorates insulin resistance and causes an adaptive response promoting endogenous antioxidant defense capacity supplementation with antioxidants may preclude these health promoting effects of exercise humans
modern drug discovery is characterized by the production of vast quantities of compounds and the need to examine these huge libraries in short periods of time the need to store manage and analyze these rapidly increasing resources has given rise to the field known as computer aided drug design cadd cadd represents computational methods and resources that are used to facilitate the design and discovery of new therapeutic solutions digital repositories containing detailed information on drugs and other useful compounds are goldmines for the study of chemical reactions capabilities design libraries with the potential to generate molecular variants in their entirety allow the selection and sampling of chemical compounds with diverse characteristics fold recognition for studying sequence structure homology between protein sequences and structures are helpful for inferring binding sites and molecular functions virtual screening the in silico analog of high throughput screening offers great promise for systematic evaluation of huge chemical libraries to identify potential lead candidates that can be synthesized and tested in this article we present an overview of the most important data sources and computational methods for the discovery of new molecular entities the workflow of the entire virtual screening campaign is discussed from data collection through to post analysis
motivation the use of prior knowledge to improve gene regulatory network modelling has often been proposed in this article we present the first research on the massive incorporation of prior knowledge from literature for bayesian network learning of gene networks as the publication rate of scientific papers grows updating online databases which have been proposed as potential prior knowledge in past research becomes increasingly challenging the novelty of our approach lies in the use of gene pair association scores that describe the overlap in the contexts in which the genes are mentioned generated from a large database of scientific literature harnessing the information contained in a huge number of documents into a simple clear format results we present a method to transform such literature based gene association scores to network prior probabilities and apply it to learn gene sub networks for yeast escherichia coli and human organisms we also investigate the effect of weighting the influence of the prior knowledge our findings show that literature based priors can improve both the number of true regulatory interactions present in the network and the accuracy of expression value prediction on genes in comparison to a network learnt solely from expression data networks learnt with priors also show an improved biological interpretation with identified subnetworks that coincide with known biological pathways contact emma steele brunel ac uk supplementary information supplementary data are available at bioinformatics bioinformatics
background micrornas mirnas are small non coding rnas affecting the expression of target genes via translational repression or mrna degradation mechanisms with the increasing availability of mrna and mirna expression data it might be possible to assess functional targets using the fact that a mirna might down regulate its target mrnas in this work we computed the correlation of expression profiles between mirnas and target mrnas using the nci expression data the aim is to investigate whether the correlations between mirna and mrna expression profiles either positive or negative can be used to assist the identification of functional mirna mrna relationships results predicted mirna mrna interactions were taken from targetscan and mirbase release pearson correlation coefficients between the mirna and the mrna expression profiles were computed using nci data the correlation coefficients were then subject to the benjamini and hochberg correction our results show that the percentage of targetscan predicted mirna mrna interactions having negative correlation in expression profiles is higher than that of mirbase predicted pairs using the experimentally validated mirna targets listed in tarbase genes involved in mrna degradation show more negative correlations between mirna and mrna expression profiles comparing with genes involved in translational repression furthermore correlation analysis for mirnas and mrnas transcribed from the same genes shows that correlations of expression profiles between intronic mirnas and host genes tend to be positive finally we found that a target gene might be down regulated by more than one mirnas sharing the same seed region conclusion our results suggest that expression profiles can be used in the computational identification of functional mirna target associations one can expect a higher chance of finding negatively correlated expression profiles for targetscan predicted interactions than for mirbase predicted ones with limited experimentally validated mirna target interactions expression profiles can only serve as a supplementary role in finding interactions between mirnas mrnas
summary interactions between genetic loci might reduce the power to detect genetic effects in genetic association studies if these interactions are not allowed for statistical interaction corresponds to a departure from the additive effects of two or more variables in a linear model describing the relationship between an outcome and predictor variables a variety of methods can be used to test for statistical interaction between predictor variables that encode the genotype and an outcome variable corresponding to the disease phenotype logistic regression is one method that can be used either to test for interaction or to test for association while allowing for interaction given genome wide data an exhaustive search is feasible for investigating two way interactions that is all pairwise combinations of loci but not for investigation of higher order interactions filtering approaches allow one to reduce the number of loci considered and thus the number of interaction tests performed data mining or machine learning methods such as random forests and multifactor dimensionality reduction mdr can allow one to search through the space of possible interactions bayesian model selection approaches offer an alternative approach for searching through the space of possible interactions the biological interpretation of statistical interactions is complex the degree to which statistical interaction implies interaction or synergism in a causal sense might be limited
recent years have witnessed the emergence of a new class of social networks that require us to move beyond previously employed representations of complex graph structures a notable example is that of the folksonomy an online process where users collaboratively employ tags to resources to impart structure to an otherwise undifferentiated database in a recent paper we proposed a mathematical model that represents these structures as tripartite hypergraphs and defined basic topological quantities of interest in this paper we extend our model by defining additional quantities such as edge distributions vertex similarity and correlations as well as clustering we then empirically measure these quantities on two real life folksonomies the popular online photo sharing site flickr and the bookmarking site citeulike we find that these systems share similar qualitative features with the majority of complex networks that have been previously studied we propose that the quantities and methodology described here can be used as a standard tool in measuring the structure of networks
background new methods are needed for genomic scale analysis of emerging model organisms that exemplify important biological questions but lack fully sequenced genomes for example there is an urgent need to understand the potential for corals to adapt to climate change but few molecular resources are available for studying these processes in reef building corals to facilitate genomics studies in corals and other non model systems we describe methods for transcriptome sequencing using as well as strategies for assembling a useful catalog of genes from the output we have applied these methods to sequence the transcriptome of planulae larvae from the coral acropora millepora results more than reads produced in a single sequencing run were assembled into approximately contigs with five fold average sequencing coverage based on sequence similarity with known proteins these analyses identified approximately different genes expressed in a range of conditions including thermal stress and settlement induction assembled sequences were annotated with gene names conserved domains and gene ontology terms targeted searches using these annotations identified the majority of genes associated with essential metabolic pathways and conserved signaling pathways as well as novel candidate genes for stress related processes comparisons with the genome of the anemone nematostella vectensis revealed approximately pairs of orthologs and approximately candidate coral specific genes more than snps were detected in the coral sequences and a subset of these validated by re sequencing conclusion the methods described here for deep sequencing of the transcriptome should be widely applicable to generate catalogs of genes and genetic markers in emerging model organisms our data provide the most comprehensive sequence resource currently available for reef building corals and include an extensive collection of potential genetic markers for association and population connectivity studies the characterization of the larval transcriptome for this widely studied coral will enable research into the biological processes underlying stress responses in corals and evolutionary adaptation to global change
background high throughput cdna synthesis and sequencing of poly a enriched rna is rapidly emerging as a technology competing to replace microarrays as a quantitative platform for measuring gene expression results consequently we compared full length cdna sequencing to channel gene expression microarrays in the context of measuring differential gene expression because of its comparable cost to a gene expression microarray our study focused on the data obtainable from a single lane of an illumina g sequencer we compared sequencing data to a highly replicated microarray experiment profiling two divergent strains of s cerevisiae conclusion using a large number of quantitative pcr qpcr assays more than previous studies we found that neither technology is decisively better at measuring differential gene expression further we report sequencing results from a diploid hybrid of two strains of s cerevisiae that indicate full length cdna sequencing can discover heterozygosity and measure quantitative allele specific simultaneously
next generation sequencing allows now the sequencing of small rna molecules and the estimation of their expression levels consequently there will be a high demand of bioinformatics tools to cope with the several gigabytes of sequence data generated in each single deep sequencing experiment given this scene we developed miranalyzer a web server tool for the analysis of deep sequencing experiments for small rnas the web server tool requires a simple input file containing a list of unique reads and its copy numbers expression levels using these data miranalyzer i detects all known microrna sequences annotated in mirbase ii finds all perfect matches against other libraries of transcribed sequences and iii predicts new micrornas the prediction of new micrornas is an especially important point as there are many species with very few known micrornas therefore we implemented a highly accurate machine learning algorithm for the prediction of new micrornas that reaches auc values of and recall values of up to on unseen data the web tool summarizes all the described steps in a single output page which provides a comprehensive overview of the analysis adding links to more detailed output pages for each analysis module miranalyzer is available at http web bioinformatics cicbiogune es nar
many methods exist for reconstructing phylogenies from molecular sequence data but few phylogenies are known and can be used to check their efficacy simulation remains the most important approach to testing the accuracy and robustness of phylogenetic inference methods however current simulation programs are limited especially concerning realistic models for simulating insertions and deletions we implement a portable and flexible application named indelible for generating nucleotide amino acid and codon sequence data by simulating insertions and deletions indels as well as substitutions indels are simulated under several models of indel length distribution the program implements a rich repertoire of substitution models including the general unrestricted model and nonstationary nonhomogeneous models of nucleotide substitution mixture and partition models that account for heterogeneity among sites and codon models that allow the nonsynonymous synonymous substitution rate ratio to vary among sites and branches with its many unique features indelible should be useful for evaluating the performance of many inference methods including those for multiple sequence alignment phylogenetic tree inference and ancestral sequence or reconstruction
microbial gene expression in the environment has recently been assessed via pyrosequencing of total rna extracted directly from natural microbial assemblages several such metatranscriptomic have reported that many complementary dna sequences shared no significant homology with known peptide sequences and so might represent transcripts from uncharacterized proteins here we report that a large fraction of cdna sequences detected in microbial metatranscriptomic data sets are comprised of well known small rnas srnas as well as new groups of previously unrecognized putative srnas psrnas these psrnas mapped specifically to intergenic regions of microbial genomes recovered from similar habitats displayed characteristic conserved secondary structures and were frequently flanked by genes that indicated potential regulatory functions depth dependent variation of psrnas generally reflected known depth distributions of broad taxonomic but fine scale differences in the psrnas within closely related populations indicated potential roles in niche adaptation genome specific mapping of a subset of psrnas derived from predominant planktonic species such as pelagibacter revealed recently discovered as well as potentially new regulatory elements our analyses show that metatranscriptomic data sets can reveal new information about the diversity taxonomic distribution and abundance of srnas in naturally occurring microbial communities and indicate their involvement in environmentally relevant processes including carbon metabolism and acquisition
numerically microbial species dominate the oceans yet their population dynamics metabolic complexity and synergistic interactions remain largely uncharted a full understanding of life in the ocean requires more than knowledge of marine microbial taxa and their genome sequences the latest experimental techniques and analytical approaches can provide a fresh perspective on the biological interactions within marine ecosystems aiding in the construction of predictive models that can interrelate microbial dynamics with the biogeochemical matter and energy fluxes that make up the ecosystem
rna interference pathways use small rnas to mediate gene silencing in eukaryotes in addition to small interfering rnas sirnas and micrornas several types of endogenously produced small rnas have important roles in gene regulation germ cell maintenance and transposon the production of some of these rnas requires the synthesis of aberrant rnas arnas or pre sirnas which are specifically recognized by rna dependent rna polymerases to make double stranded rna the mechanism for arna synthesis and recognition is largely unknown here we show that dna damage induces the expression of the argonaute protein qde and a new class of small rnas in the filamentous fungus neurospora crassa this class of small rnas known as qirnas because of their interaction with qde are about nucleotides long several nucleotides shorter than neurospora sirnas with a strong preference for uridine at the and originate mostly from the ribosomal dna locus the production of qirnas requires the rna dependent rna polymerase qde the werner and bloom recq dna helicase homologue qde and dicers qirna biogenesis also requires dna damage induced arnas as precursors a process that is dependent on both qde and qde notably our results suggest that qde is the dna dependent rna polymerase that produces arnas furthermore the neurospora rna interference mutants show increased sensitivity to dna damage suggesting a role for qirnas in the dna damage response by inhibiting translation
marine viruses affect bacteria archaea and eukaryotic organisms and are major components of the marine food web most studies have focused on their role as predators and parasites but many of the interactions between marine viruses and their hosts are much more complicated a series of recent studies has shown that viruses have the ability to manipulate the life histories and evolution of their hosts in remarkable ways challenging our understanding of this almost world
microblogging is a new form of communication in which users describe their current status in short posts distributed by instant messages mobile phones email or the web we present our observations of the microblogging phenomena by studying the topological and geographical properties of the social network in twitter one of the most popular microblogging systems we find that people use microblogging primarily to talk about their daily activities and to seek or share information we present a taxonomy characterizing the the underlying intentions users have in making microblogging posts by aggregating the apparent intentions of users in implicit communities extracted from the data we show that users with similar intentions connect with other
the knowledge of the free energy landscape topology is the essential key to understand many biochemical processes the determination of the conformers of a protein and their basins of attraction takes a central role for studying molecular isomerization reactions in this work we present a novel framework to unveil the features of a free energy landscape answering questions such as how many meta stable conformers are how the hierarchical relationship among them is or what the structure and kinetics of the transition paths are exploring the landscape by molecular dynamics simulations the microscopic data of the trajectory are encoded into a conformational markov network the structure of this graph reveals the regions of the conformational space corresponding to the basins of attraction in addition handling the conformational markov network relevant kinetic magnitudes as dwell times or rate constants and the hierarchical relationship among basins complete the global picture of the landscape we show the power of the analysis studying a toy model of a funnel like potential and computing efficiently the conformers of a short peptide the dialanine paving the way to a systematic study of the free energy landscape in peptides
abstract background next generation sequencing technologies hold great potential for many biological questions while mainly used for genomic sequencing they are also very promising for gene expression profiling sequencing of cdna does not only provide an estimate of the absolute expression level it can also be used for the identification of allele specific gene expression results we developed pangea a tool which enables a fast and user friendly analysis of allele specific gene expression using the technology pangea allows mapping of ests to genes or whole genomes displaying gene expression profiles identification of snps and the quantification of allele specific gene expression the intuitive gui of pangea facilitates a flexible and interactive analysis of the data pangea additionally implements a modification of the smith waterman algorithm which deals with incorrect estimates of homopolymer length as occuring in the technology conclusions to our knowledge pangea is the first tool which facilitates the identification of allele specific gene expression pangea is distributed under the mozilla public license and available at www kofler or at pangea
there is strong evidence that rare variants are involved in complex disease etiology the first step in implicating rare variants in disease etiology is their identification through sequencing in both randomly ascertained samples e g the genomes project and samples ascertained according to disease status we investigated to what extent rare variants will be observed across the genome and in candidate genes in randomly ascertained samples the magnitude of variant enrichment in diseased individuals and biases that can occur due to how variants are discovered although sequencing cases can enrich for casual variants when a gene or genes are not involved in disease etiology limiting variant discovery to cases can lead to association studies with dramatically inflated false rates
the dynamic back action caused by electromagnetic forces radiation pressure in and cavities is of growing back action cooling for example is being pursued as a means of achieving the quantum ground state of macroscopic mechanical oscillators work in the optical domain has revolved around millimetre or micrometre scale structures using the radiation pressure force by comparison in microwave devices low loss superconducting structures have been used for gradient force mediated coupling to a nanomechanical oscillator of picogram here we describe measurements of an optical system consisting of a pair of specially patterned nanoscale beams in which optical and mechanical energies are simultaneously localized to a cubic micron scale volume and for which large per photon optical gradient forces are realized the resulting scale of the per photon force and the mass of the structure enable the exploration of cavity optomechanical regimes in which for example the mechanical rigidity of the structure is dominantly provided by the internal light field itself in addition to precision measurement and sensitive force nano optomechanics may find application in reconfigurable and tunable photonic light based radio frequency and the generation of giant optical nonlinearities for wavelength conversion optical
the complexity of mammalian transcriptomes is compounded by alternative splicing which allows one gene to produce multiple transcript isoforms however transcriptome comparison has been limited to differential analysis at the gene level instead of the individual transcript isoform level high throughput sequencing technologies and high resolution tiling arrays provide an unprecedented opportunity to compare transcriptomes at the level of individual splice variants however sequence read coverage or probe intensity at each position may represent a family of splice variants instead of one single isoform here we propose a hierarchical bayesian model basis bayesian analysis of splicing isoforms to infer the differential expression level of each transcript isoform in response to two conditions a latent variable was introduced to perform direct statistical selection of differentially expressed isoforms model parameters were inferred based on an ergodic markov chain generated by our gibbs sampler basis has the ability to borrow information across different probes or positions from the same genes and different genes basis can handle the heteroskedasticity of probe intensity or sequence read coverage we applied basis to a human tiling array data set and a mouse rna seq data set some of the predictions were validated by quantitative real time rt pcr nar
despite comprising much of the eukaryotic genome few transposons are active and they usually confer no benefit to the host through an exaggerated process of genome rearrangement oxytricha trifallax destroys of its germline genome during development this includes the elimination of all transposon dna we show that germline limited transposase genes play key roles in this process of genome wide dna excision which suggests that transposases function in large eukaryotic genomes containing thousands of active transposons we show that transposase gene expression occurs during germline soma differentiation and that silencing of transposase by rna interference leads to abnormal dna rearrangement in the offspring this study suggests a new important role in oxytricha for this large portion of genomic dna that was previously thought of as science
sequence preferences of dna binding proteins are a primary mechanism by which cells interpret the genome despite the central importance of these proteins in physiology development and evolution comprehensive dna binding specificities have been determined experimentally for only a few proteins here we used microarrays containing all pair sequences to examine the binding specificities of distinct mouse dna binding proteins representing structural classes our results reveal a complex landscape of binding with virtually every protein analyzed possessing unique preferences roughly half of the proteins each recognized multiple distinctly different sequence motifs challenging our molecular understanding of how proteins interact with their dna binding sites this complexity in dna recognition may be important in gene regulation and in the evolution of transcriptional networks
quantum physics has remarkable distinguishing characteristics for example it gives only probabilistic predictions non determinism and does not allow copying of unknown states no quantum correlations may be stronger than any classical but information cannot be transmitted faster than light no signalling however these features do not uniquely define quantum physics a broad class of theories exist that share such traits and allow even stronger than quantum here we introduce the principle of information causality and show that it is respected by classical and quantum physics but violated by all no signalling theories with stronger than the strongest quantum correlations the principle relates to the amount of information that an observer bob can gain about a data set belonging to another observer alice the contents of which are completely unknown to him using all his local resources which may be correlated with her resources and allowing classical communication from her the amount of information that bob can recover is bounded by the information volume m of the communication namely if alice communicates m bits to bob the total information obtainable by bob cannot be greater than m for m information causality reduces to the standard no signalling principle however no signalling theories with maximally strong correlations would allow bob access to all the data in any m bit subset of the whole data set held by alice if only one bit is sent by alice m this is tantamount to bob s being able to access the value of any single bit of alice s data but not all of them information causality may therefore help to distinguish physical theories from non physical ones we suggest that information causalitya generalization of the no signalling conditionmight be one of the foundational properties nature
ontology development and the annotation of biological data using ontologies are time consuming exercises that currently requires input from expert curators open collaborative platforms for biological data annotation enable the wider scientific community to become involved in developing and maintaining such resources however this openness raises concerns regarding the quality and correctness of the information added to these knowledge bases the combination of a collaborative web based platform with logic based approaches and semantic web technology can be used to address some of these challenges and concerns we have developed the bowiki a web based system that includes a biological core ontology the core ontology provides background knowledge about biological types and relations against this background an automated reasoner assesses the consistency of new information added to the knowledge base the system provides a platform for research communities such as wikis for the description discussion and annotation of the functions of genes and gene products wang hoehndorf et al giles however an open approach like wikis frequently raises concerns regarding the quality of the information captured the information represented in the wiki should adhere to particular quality criteria such as internal consistency the wiki content does not contain contradictory information and consistency with biological background knowledge the wiki content should be semantically correct to address some of these concerns logic based tools can be employed we have developed the bowiki a wiki system that uses a core ontology together with an automated reasoner to maintain a consistent knowledge base it is specifically targeted at small to medium sized communities to collaboratively integrate information and annotate data the bowiki and supplementary material is available at http www bowiki net the source code is available under the gnu gpl from http onto eva mpg de trac bowiki contact bowiki users lists informatik uni de
motivation figures from biomedical articles contain valuable information difficult to reach without specialized tools currently there is no search engine that can retrieve specific figure types results this study describes a retrieval method that takes advantage of principles in image understanding text mining and optical character recognition ocr to retrieve figure types defined conceptually a search engine was developed to retrieve tables and figure types to aid computational and experimental research availability http iossifovlab cshl edu figuromecontact raul rodriguez esteban com
background despite the growing interest by leaders policy makers and others the terminology of health information technology as well as biomedical and health informatics is poorly understood and not even agreed upon by academics and professionals in the field discussion the paper presented as a debate to encourage further discussion and disagreement provides definitions of the major terminology used in biomedical and health informatics and health information technology for informatics it focuses on the words that modify the term as well as individuals who practice the discipline other categories of related terms are covered as well from the associated disciplines of computer science information technolog and health information management to the major application categories of applications used the discussion closes with a classification of individuals who work in the largest segment of the field namely clinical informatics summary the goal of presenting in debate format is to provide a starting point for discussion to reach a documented consensus on the definition and use of terms
abstract jcc abs charmm chemistry at harvard molecular mechanics is a highly versatile and widely used molecular simulation program it has been developed over the last three decades with a primary focus on molecules of biological interest including proteins peptides lipids nucleic acids carbohydrates and small molecule ligands as they occur in solution crystals and membrane environments for the study of such systems the program provides a large suite of computational tools that include numerous conformational and path sampling methods free energy estimators molecular minimization dynamics and analysis techniques and model building capabilities the charmm program is applicable to problems involving a much broader class of many particle systems calculations with charmm can be performed using a number of different energy functions and models from mixed quantum mechanical molecular mechanical force fields to all atom classical potential energy functions with explicit solvent and various boundary conditions to implicit solvent and membrane models the program has been ported to numerous platforms in both serial and parallel architectures this article provides an overview of the program as it exists today with an emphasis on developments since the publication of the original charmm article in wiley periodicals inc j chem
summary diana mirpath is a web based computational tool developed to identify molecular pathways potentially altered by the expression of single or multiple micrornas the software performs an enrichment analysis of multiple microrna target genes comparing each set of microrna targets to all known kegg pathways the combinatorial effect of co expressed micrornas in the modulation of a given pathway is taken into account by the simultaneous analysis of multiple micrornas the graphical output of the program provides an overview of the parts of the pathway modulated by micrornas facilitating the interpretation and presentation of the analysis results availability the software is available at http microrna gr mirpath and is free for all users with no login or download requirement contact papadopoulos fleming gr hatzigeorgiou fleming bioinformatics
motivation the continuing exponential accumulation of full genome data including full diploid human genomes creates new challenges not only for understanding genomic structure function and evolution but also for the storage navigation and privacy of genomic data here we develop data structures and algorithms for the efficient storage of genomic and other sequence data that may also facilitate querying and protecting the data results the general idea is to encode only the differences between a genome sequence and a reference sequence using absolute or relative coordinates for the location of the differences these locations and the corresponding differential variants can be encoded into binary strings using various entropy coding methods from fixed codes such as golomb and elias codes to variables codes such as huffman codes we demonstrate the approach and various tradeoffs using highly variables human mitochondrial genome sequences as a testbed with only a partial level of optimization genome sequences occupying mb in genbank are compressed down to only kb achieving a fold compression rate using the revised cambridge reference sequence as the reference sequence using the consensus sequence as the reference sequence the data can be stored using only kb corresponding to a fold level of compression roughly a improvement extensions to nuclear genomes and high throughput sequencing data are discussed availability data are publicly available from genbank the hapmap web site and the mitomap database supplementary materials with additional results statistics and software implementations are available from http mammag web uci edu bin view mitowiki projectdnacompression contact pfbaldi ics uci bioinformatics
in this insightful book you ll learn from the best data practitioners in the field just how wide ranging and beautiful working with data can be join contributors as they explain how they developed simple and elegant solutions on projects ranging from the mars lander to a radiohead video with beautiful data you will explore the opportunities and challenges involved in working with the vast number of datasets made available by the web learn how to visualize trends in urban crime using maps and data mashups discover the challenges of designing a data processing system that works within the constraints of space travel learn how crowdsourcing and transparency have combined to advance the state of drug research understand how new data can automatically trigger alerts when it matches or overlaps pre existing data learn about the massive infrastructure required to create capture and process dna data that s only small sample of what you ll find in beautiful data for anyone who handles data this is a truly fascinating book contributors include nathan yau jonathan follett and matt holm j m hughes raghu ramakrishnan brian cooper and utkarsh srivastava jeff hammerbacher jason dykes and jo wood jeff jonas and lisa sokol jud valeski alon halevy and jayant madhavan aaron koblin and valdean klump michal migurski jeff heer coco krumme peter norvig matt wood and ben blackburne jean claude bradley rajarshi guha andrew lang pierre lindenbaum cameron neylon antony williams and egon willighagen lukas biewald and brendan o connor hadley wickham deborah swayne and david poole andrew gelman jonathan p kastellec and yair ghitza segaran
title author summary title p next generation sequencing ngs technologies are revolutionizing the way biologists acquire and analyze genomic data ngs machines such as illumina solexa and ab solid are able to sequence genomes more cheaply by fold than previous methods one of the main application areas of ngs technologies is the discovery of genomic variation within a given species the first step in discovering this variation is the mapping of reads sequenced from a donor individual to a known reference genome differences between the reference and the reads are indicative either of polymorphisms or of sequencing errors since the introduction of ngs technologies many methods have been devised for mapping reads to reference genomes however these algorithms often sacrifice sensitivity for fast running time while they are successful at mapping reads from organisms that exhibit low polymorphism rates they do not perform well at mapping reads from highly polymorphic organisms we present a novel read mapping method shrimp that can handle much greater amounts of polymorphism using italic ciona savignyi italic as our target organism we demonstrate that our method discovers significantly more variation than other methods additionally we develop color space extensions to classical alignment algorithms allowing us to map color space or dibase reads generated by ab solid p
the field of phylogenetics is entering a new era in which trees of historical relationships between species are increasingly inferred from multilocus and genomic data a major challenge for incorporating such large amounts of data into inference of species trees is that conflicting genealogical histories often exist in different genes throughout the genome recent advances in genealogical modeling suggest that resolving close species relationships is not quite as simple as applying more data to the problem here we discuss the complexities of genealogical discordance and review the issues that new methods for multilocus species tree inference will need to address to account successfully for naturally occurring genomic variability in histories
midbrain dopamine neurons are activated by reward or sensory stimuli predicting reward these excitatory responses increase as the reward value increases this response property has led to a hypothesis that dopamine neurons encode value related signals and are inhibited by aversive events here we show that this is true only for a subset of dopamine neurons we recorded the activity of dopamine neurons in monkeys macaca mulatta during a pavlovian procedure with appetitive and aversive outcomes liquid rewards and airpuffs directed at the face respectively we found that some dopamine neurons were excited by reward predicting stimuli and inhibited by airpuff predicting stimuli as the value hypothesis predicts however a greater number of dopamine neurons were excited by both of these stimuli inconsistent with the hypothesis some dopamine neurons were also excited by both rewards and airpuffs themselves especially when they were unpredictable neurons excited by the airpuff predicting stimuli were located more dorsolaterally in the substantia nigra pars compacta whereas neurons inhibited by the stimuli were located more ventromedially some in the ventral tegmental area a similar anatomical difference was observed for their responses to actual airpuffs these findings suggest that different groups of dopamine neurons convey motivational signals in manners
theta oscillations clock hippocampal activity during awake behaviour and rapid eye movement rem sleep these oscillations are prominent in the local field potential and they also reflect the subthreshold membrane potential and strongly modulate the spiking of hippocampal neurons the prevailing view is that theta oscillations are synchronized throughout the hippocampus despite the lack of conclusive experimental evidence in contrast here we show that in freely behaving rats theta oscillations in area are travelling waves that propagate roughly along the septotemporal axis of the hippocampus furthermore we find that spiking in the pyramidal cell layer is modulated in a consistent travelling wave pattern our results demonstrate that theta oscillations pattern hippocampal activity not only in time but also across anatomical space the presence of travelling waves indicates that the instantaneous output of the hippocampus is topographically organized and represents a segment rather than a point of space
background ontology construction for any domain is a labour intensive and complex process any methodology that can reduce the cost and increase efficiency has the potential to make a major impact in the life sciences this paper describes an experiment in ontology construction from text for the animal behaviour domain our objective was to see how much could be done in a simple and relatively rapid manner using a corpus of journal papers we used a sequence of pre existing text processing steps and here describe the different choices made to clean the input to derive a set of terms and to structure those terms in a number of hierarchies we describe some of the challenges especially that of focusing the ontology appropriately given a starting point of a heterogeneous corpus results using mainly automated techniques we were able to construct an term ontology like structure with recall of animal behaviour terms but a precision of only we were able to clean unwanted terms from the nascent ontology using lexico syntactic patterns that tested the validity of term inclusion within the ontology we used the same technique to test for subsumption relationships between the remaining terms to add structure to the initially broad and shallow structure we generated all outputs are available at http thirlmere aston ac uk kiffer animalbehaviour conclusion we present a systematic method for the initial steps of ontology or structured vocabulary construction for scientific domains that requires limited human effort and can make a contribution both to ontology learning and maintenance the method is useful both for the exploration of a scientific domain and as a stepping stone towards formally rigourous ontologies the filtering of recognised terms from a heterogeneous corpus to focus upon those that are the topic of the ontology is identified to be one of the main challenges for research in learning
large scale annotation efforts typically involve several experts who may disagree with each other we propose an approach for modeling disagreements among experts that allows providing each annotation with a confidence value i e the posterior probability that it is correct our approach allows computing certainty level for individual annotations given annotator specific parameters estimated from data we developed two probabilistic models for performing this analysis compared these models using computer simulation and tested each model s actual performance based on a large data set generated by human annotators specifically for this study we show that even in the worst case scenario when all annotators disagree our approach allows us to significantly increase the probability of choosing the correct annotation along with this publication we make publicly available a corpus of sentences annotated according to several cardinal dimensions that we have introduced in earlier work the sentences were all fold annotated by a group of eight experts while a sentence subset was further fold annotated by five new experts while the presented data represent a specialized curation task our modeling approach is general most data annotation studies could benefit from methodology
abstract the functional genomics experiment data model fuge has been developed to increase the consistency and efficiency of experimental data modeling in the life sciences and it has been adopted by a number of high profile standardization organizations fuge can be used directly whereby generic modeling constructs are used to represent concepts from specific experimental activities or as a framework within which method specific models can be developed fuge is both rich and flexible providing a considerable number of modeling constructs which can be used in a range of different ways however such richness and flexibility also mean that modelers and application developers have choices to make when applying fuge in a given context this paper captures emerging best practice in the use of fuge in the light of the experience of several groups by proposing guidelines for the use and extension of the fuge data model presenting design patterns that reflect recurring requirements in experimental data modeling and describing a community software tool kit stk that supports application development using fuge we anticipate that these guidelines will encourage consistent usage of fuge and as such will contribute to the development of convergent data standards in research
systematic identification of protein drug interaction networks is crucial to correlate complex modes of drug action to clinical indications we introduce a novel computational strategy to identify protein ligand binding profiles on a genome wide scale and apply it to elucidating the molecular mechanisms associated with the adverse drug effects of cholesteryl ester transfer protein cetp inhibitors cetp inhibitors are a new class of preventive therapies for the treatment of cardiovascular disease however clinical studies indicated that one cetp inhibitor torcetrapib has deadly off target effects as a result of hypertension and hence it has been withdrawn from phase iii clinical trials we have identified a panel of off targets for torcetrapib and other cetp inhibitors from the human structural genome and map those targets to biological pathways via the literature the predicted protein ligand network is consistent with experimental results from multiple sources and reveals that the side effect of cetp inhibitors is modulated through the combinatorial control of multiple interconnected pathways given that combinatorial control is a common phenomenon observed in many biological processes our findings suggest that adverse drug effects might be minimized by fine tuning multiple off target interactions using single or multiple therapies this work extends the scope of chemogenomics approaches and exemplifies the role that systems biology has in the future of discovery
abstract background dna sequence integrity mrna concentrations and protein dna interactions have been subject to genome wide analyses based on microarrays with ever increasing efficiency and reliability over the past fifteen years however very recently novel technologies for ultra high throughput dna sequencing uhts have been harnessed to study these phenomena with unprecedented precision as a consequence the extensive bioinformatics environment available for array data management analysis interpretation and publication must be extended to include these novel sequencing data types description mimas was originally conceived as a simple convenient and local microarray information management and annotation system focused on genechips for expression profiling studies mimas enables users to manage data from high density oligonucleotide snp chips expression arrays both utr and tiling and promoter arrays beadarrays as well as uhts data using miame compliant standardized vocabulary importantly researchers can export data in mage tab format and upload them to the ebi s arrayexpress certified data repository using a one step procedure conclusion we have vastly extended the capability of the system such that it processes the data output of six types of genechips affymetrix two different beadarrays for mrna and mirna illumina and the genome analyzer a popular ultra high throughput dna sequencer illumina without compromising on its flexibility and user friendliness the system appropriately renamed into multiomics information management and annotation system is currently used by scientists working in approximately academic laboratories and genomics platforms in switzerland and france mimas is freely available via http multiomics net
motivation chromatin immunoprecipitation chip experiments followed by array hybridization or chip chip is a powerful approach for identifying transcription factor binding sites tfbs and has been widely used recently massively parallel sequencing coupled with chip experiments chip seq has been increasingly used as an alternative to chip chip offering cost effective genome wide coverage and resolution up to a single base pair for many well studied tfs both chip seq and chip chip experiments have been applied and their data are publicly available previous analyses have revealed substantial technology specific binding signals despite strong correlation between the two sets of results therefore it is of interest to see whether the two data sources can be combined to enhance the detection of tfbs results in this work hierarchical hidden markov model hhmm is proposed for combining data from chip seq and chip chip in hhmm inference results from individual hmms in chip seq and chip chip experiments are summarized by a higher level hmm simulation studies show the advantage of hhmm when data from both technologies co exist analysis of two well studied tfs nrsf and ccctc binding factor ctcf also suggests that hhmm yields improved tfbs identification in comparison to analyses using individual data sources or a simple merger of the two availability source code for the software chipmeta is freely available for download at http www umich edu hwchoi hhmmsoftware zip implemented in c and supported on linux contact ghoshd psu edu qin umich edu supplementary information supplementary data are available at bioinformatics bioinformatics
motivation the enormous amount of short reads generated by the new dna sequencing technologies call for the development of fast and accurate read alignment programs a first generation of hash table based methods has been developed including maq which is accurate feature rich and fast enough to align short reads from a single individual however maq does not support gapped alignment for single end reads which makes it unsuitable for alignment of longer reads where indels may occur frequently the speed of maq is also a concern when the alignment is scaled up to the resequencing of hundreds of individuals results we implemented burrows wheeler alignment tool bwa a new read alignment package that is based on backward search with burrows wheeler transform bwt to efficiently align short sequencing reads against a large reference sequence such as the human genome allowing mismatches and gaps bwa supports both base space reads e g from illumina sequencing machines and color space reads from ab solid machines evaluations on both simulated and real data suggest that bwa is faster than maq while achieving similar accuracy in addition bwa outputs alignment in the new standard sam sequence alignment map format variant calling and other downstream analyses after the alignment can be achieved with the open source samtools software package availability http maq sourceforge net contact rd sanger uk
structural variants svs are common in the human genome because approximately half of the human genome consists of repetitive transposable dna sequences it is plausible that these elements play an important role in generating svs in humans sequencing of the diploid genome of one individual human huref affords us the opportunity to assess for the first time the impact of mobile elements on svs in an individual in a thorough and unbiased fashion in this study we systematically evaluated more than svs to identify mobile element associated svs as small as bp and specific to the huref genome combining computational and experimental analyses we identified and validated mobile element insertion events including alu sva elements and nonclassical insertions which added more than kb of new dna sequence to the huref genome compared with the human genome project hgp reference sequence we also identified mobile element associated deletions which removed approximately kb of sequence from the huref genome overall approximately of the huref specific indels larger than bp are caused by mobile element associated events more than one third of the insertion deletion events occurred in genic regions and new alu insertions occurred in exons of three human genes based on the number of insertions and the estimated time to the most recent common ancestor of huref and the hgp reference genome we estimated the alu and sva retrotransposition rates to be one in births births and births respectively this study presents the first comprehensive analysis of mobile element related structural variants in the complete dna sequence of an individual and demonstrates that mobile elements play an important role in generating inter individual variation
quantum error correction and fault tolerant quantum computation represent arguably the most vital theoretical aspect of quantum information processing it was well known from the early developments of this exciting field that the fragility of coherent quantum systems would be a catastrophic obstacle to the development of large scale quantum computers the introduction of quantum error correction in showed that active techniques could be employed to mitigate this fatal problem however quantum error correction and fault tolerant computation is now a much more mature field and many new codes techniques and methodologies have been developed to implement error correction for large scale quantum algorithms this development has been so pronounced that many in the field of quantum information specifically those new to quantum information or those focused on the many other important issues in quantum computation have not been able to keep up with the general formalisms and methodologies employed in this area in response we have attempted to summarize the basic aspects of quantum error correction and fault tolerance not as a detailed guide but rather as a basic introduction rather than introducing these concepts from a rigorous mathematical and computer science framework we instead examine error correction and fault tolerance largely through detailed examples progressing from basic examples of the qubit code through to the stabilizer formalism which is now extremely important when understanding large scale concatenated code structures quantum circuit synthesis and the more recent developments in subsystem and codes
resequencing genomic dna from pools of individuals is an effective strategy to detect new variants in targeted regions and compare them between cases and controls there are numerous ways to assign individuals to the pools on which they are to be sequenced the nave disjoint pooling scheme many individuals to one pool in predominant use today offers insight into allele frequencies but does not offer the identity of an allele carrier we present a framework for overlapping pool design where each individual sample is resequenced in several pools many individuals to many pools upon discovering a variant the set of pools where this variant is observed reveals the identity of its carrier we formalize the mathematical framework for such pool designs and list the requirements from such designs we specifically address three practical concerns for pooled resequencing designs false positives due to errors introduced during amplification and sequencing false negatives due to undersampling particular alleles aggravated by nonuniform coverage and consequently ambiguous identification of individual carriers in the presence of errors we build on theory of error correcting codes to design pools that overcome these pitfalls we show that in practical parameters of resequencing studies our designs guarantee high probability of unambiguous singleton carrier identification while maintaining the features of nave pools in terms of sensitivity specificity and the ability to estimate allele frequencies we demonstrate the ability of our designs in extracting rare variations using short read data from the genomes project
we performed a test sample study to try to identify errors leading to irreproducibility including incompleteness of peptide sampling in liquid chromatographymass spectrometrybased proteomics we distributed an equimolar test sample comprising highly purified recombinant human proteins to laboratories each protein contained one or more unique tryptic peptides of da to test for ion selection and sampling in the mass spectrometer of the labs members of only labs initially reported all proteins correctly and members of only lab reported all tryptic peptides of da centralized analysis of the raw data however revealed that all proteins and most of the da peptides had been detected in all labs our centralized analysis determined missed identifications false negatives environmental contamination database matching and curation of protein identifications as sources of problems improved search engines and databases are needed for mass proteomics
this paper studies people recommendations designed to help users find known offline contacts and discover new friends on social networking sites we evaluated four recommender algorithms in an enterprise social networking site using a personalized survey of users and a field study of users we found all algorithms effective in expanding users friend lists algorithms based on social network information were able to produce better received recommendations and find more known contacts for users while algorithms using similarity of user created content were stronger in discovering new friends we also collected qualitative feedback from our survey users and draw several meaningful implications
gr recent studies show that along with single nucleotide polymorphisms and small indels larger structural variants among human individuals are common the human genome structural variation project aims to identify and classify deletions insertions and inversions kbp in a small number of normal individuals with a fosmid based paired end sequencing approach using traditional sequencing technologies the realization of new ultra high throughput sequencing platforms now makes it feasible to detect the full spectrum of genomic variation among many individual genomes including cancer patients and others suffering from diseases of genomic origin unfortunately existing algorithms for identifying structural variation sv among individuals have not been designed to handle the short read lengths and the errors implied by the next gen sequencing ngs technologies in this paper we give combinatorial formulations for the sv detection between a reference genome sequence and a next gen based paired end whole genome shotgun sequenced individual we describe efficient algorithms for each of the formulations we give which all turn out to be fast and quite reliable they are also applicable to all next gen sequencing methods illumina life sciences roche abi solid etc and traditional capillary sequencing technology we apply our algorithms to identify sv among individual genomes very recently sequenced by technology
next generation sequencers have sufficient power to analyze simultaneously dnas from many different specimens a practice known as multiplexing such schemes rely on the ability to associate each sequence read with the specimen from which it was derived the current practice of appending molecular barcodes prior to pooling is practical for parallel analysis of up to many dozen samples here we report a strategy that permits simultaneous analysis of tens of thousands of specimens our approach relies on the use of combinatorial pooling strategies in which pools rather than individual specimens are assigned barcodes thus the identity of each specimen is encoded within the pooling pattern rather than by its association with a particular sequence tag decoding the pattern allows the sequence of an original specimen to be inferred with high confidence we verified the ability of our encoding and decoding strategies to accurately report the sequence of individual samples within a large number of mixed specimens in two ways first we simulated data both from a clone library and from a human population in which a sequence variant associated with cystic fibrosis was present second we actually pooled sequenced and decoded identities within two sets of bacterial clones comprising approximately different artificial micrornas targeting arabidopsis or human genes we achieved greater than accuracy in these trials the strategies reported here can be applied to a wide variety of biological problems including the determination of genotypic variation within large populations individuals
bib new knowledge is produced at a continuously increasing speed and the list of papers databases and other knowledge sources that a researcher in the life sciences needs to cope with is actually turning into a problem rather than an asset the adequate management of knowledge is therefore becoming fundamentally important for life scientists especially if they work with approaches that thoroughly depend on knowledge integration such as systems biology several initiatives to organize biological knowledge sources into a readily exploitable resourceome are presently being carried out ontologies and semantic web technologies revolutionize these efforts here we review the benefits trends current possibilities and the potential this holds for biosciences
gr novel massively parallel sequencing technologies provide highly detailed structures of transcriptomes and genomes by yielding deep coverage of short reads but their utility is limited by inadequate sequencing quality and short read lengths sequencing error trimming in short reads is therefore a vital process that could improve the rate of successful reference mapping and polymorphism detection toward this aim we herein report a frequency based de novo short read clustering method that organizes erroneous short sequences originating in a single abundant sequence into a tree structure in this structure each child sequence is considered to be stochastically derived from its more abundant parent sequence with one mutation through sequencing errors the root node is the most frequently observed sequence that represents all erroneous reads in the entire tree allowing the alignment of the reliable representative read to the genome without the risk of mapping erroneous reads to false positive positions this method complements base calling and the error correction of making direct alignments with the reference genome and is able to improve the overall accuracy of short read alignment by consulting the inherent relationships among the entire set of reads the algorithm runs efficiently with a linear time complexity in addition an error rate evaluation model can be derived from bacterial artificial chromosome sequencing data obtained in the same run as a control in two clustering experiments using small rna and end mrna reads data sets we confirmed a remarkable increase in the percentage of short reads aligned to the sequence
social media treats all users the same trusted friend or total stranger with little or nothing in between in reality relationships fall everywhere along this spectrum a topic social science has investigated for decades under the theme of tie strength our work bridges this gap between theory and practice in this paper we present a predictive model that maps social media data to tie strength the model builds on a dataset of over social media ties and performs quite well distinguishing between strong and weak ties with over accuracy we complement these quantitative findings with interviews that unpack the relationships we could not predict the paper concludes by illustrating how modeling tie strength can improve social media design elements including privacy controls message routing friend introductions and information prioritization author keywords social media social networks relationship ties
studies of membrane proteins have revealed a direct link between the lipid environment and the structure and function of some of these proteins although some of these effects involve specific chemical interactions between lipids and protein residues many can be understood in terms of protein induced perturbations to the membrane shape the free energy cost of such perturbations can be estimated quantitatively and measurements of channel gating in model systems of membrane proteins with their lipid partners are now confirming predictions of models
the objective of this study is to evaluate the usability of electronic books e books an experiment was designed to compare the differences between reading an e book and a conventional book c book with objective measures twenty junior college students ages sixteen to eighteen participated in the study response measures included reading performance and critical flicker fusion cff the results indicate that reading an e book causes significantly higher eye fatigue than reading a c book reading a c book generated a higher level of reading performance than reading an e book in addition females demonstrated better reading performance than males in reading book
molecular self assembly offers a bottom up route to fabrication with subnanometre precision of complex structures from simple dna has proved to be a versatile building for programmable construction of such objects including two dimensional and three dimensional wireframe templated self assembly of into custom two dimensional shapes on the megadalton scale has been demonstrated previously with a multiple kilobase scaffold strand that is folded into a flat array of antiparallel helices by interactions with hundreds of oligonucleotide staple here we extend this method to building custom three dimensional shapes formed as pleated layers of helices constrained to a honeycomb lattice we demonstrate the design and assembly of nanostructures approximating six shapesmonolith square nut railed bridge genie bottle stacked cross slotted crosswith precisely controlled dimensions ranging from to we also show hierarchical assembly of structures such as homomultimeric linear tracks and heterotrimeric wireframe icosahedra proper assembly requires week long folding times and calibrated monovalent and divalent cation concentrations we anticipate that our strategy for self assembling custom three dimensional shapes will provide a general route to the manufacture of sophisticated devices bearing features on the scale
understanding the structure and the dynamics of the complex intercellular network of interactions that contributes to the structure and function of a living cell is one of the main challenges of today s biology snow inputs a collection of protein or gene identifiers and by using the interactome as scaffold draws the connections among them calculates several relevant network parameters and as a novelty among the rest of tools of its class it estimates their statistical significance the parameters calculated for each node are connectivity betweenness and clustering coefficient it also calculates the number of components number of bicomponents and articulation points an interactive network viewer is also available to explore the resulting network snow is available at http snow bioinfo cipf nar
g protein coupled receptors gpcrs mediate most of our physiological responses to hormones neurotransmitters and environmental stimulants and so have great potential as therapeutic targets for a broad spectrum of diseases they are also fascinating molecules from the perspective of membrane protein structure and biology great progress has been made over the past three decades in understanding diverse gpcrs from pharmacology to functional characterization in vivo recent high resolution structural studies have provided insights into the molecular mechanisms of gpcr activation and activity
summary the embrace registry is a web portal that collects and monitors web services according to test scripts provided by the their administrators users are able to search for rank and annotate services enabling them to select the most appropriate working service for inclusion in their bioinformatics analysis tasks availability and implementation web site implemented with php python mysql and apache with all major browsers supported www net
jclust is a user friendly application which provides access to a set of widely used clustering and clique finding algorithms the toolbox allows a range of filtering procedures to be applied and is combined with an advanced implementation of the medusa interactive visualization module these implemented algorithms are k means affinity propagation bronkerbosch mulic restricted neighborhood search cluster algorithm markov clustering and spectral clustering while the supported filtering procedures are haircut outsideinside best neighbors and density control operations the combination of a simple input file format a set of clustering and filtering algorithms linked together with the visualization tool provides a powerful tool for data analysis and information extraction availability http jclust embl de contact pavlopou embl de rschneid embl de skossida bioacademy grsupplementary information supplementary data are available at online
noncoding rnas that are like mrnas spliced capped and polyadenylated have important functions in cellular processes the inventory of these mrna like noncoding rnas mlncrnas however is incomplete even in well studied organisms and so far no computational methods exist to predict such rnas from genomic sequences only the subclass of these transcripts that is evolutionarily conserved usually has conserved intron positions we demonstrate here that a genome wide comparative genomics approach searching for short conserved introns is capable of identifying conserved transcripts with a high specificity our approach requires neither an open reading frame nor substantial sequence or secondary structure conservation in the surrounding exons thus it identifies spliced transcripts in an unbiased way after applying our approach to insect genomes we predict introns outside annotated coding transcripts of which are confirmed by expressed sequence tags ests and or noncoding flybase transcripts of the remaining novel introns about half are associated with protein coding genes either extending coding or untranslated regions or likely belonging to unannotated coding genes the remaining introns belong to novel mlncrnas that are largely unstructured using rt pcr we verified seven of tested introns in novel mlncrnas and of introns in novel coding genes the expression level of all verified mlncrna transcripts is low but varies during development which suggests regulation as conserved introns indicate both purifying selection on the exon intron structure and conserved expression of the transcript in related species the novel mlncrnas are good candidates for transcripts
synthetic biology is a research field that combines the investigative nature of biology with the constructive nature of engineering efforts in synthetic biology have largely focused on the creation and perfection of genetic devices and small modules that are constructed from these devices but to view cells as true programmable entities it is now essential to develop effective strategies for assembling devices and modules into intricate customizable larger scale systems the ability to create such systems will result in innovative approaches to a wide range of applications such as bioremediation sustainable energy production and therapies
background obesity is a particularly complex disease that at least partially involves genetic and environmental perturbations to gene networks connecting the hypothalamus and several metabolic tissues resulting in an energy imbalance at the systems level results to provide an inter tissue view of obesity with respect to molecular states that are associated with physiological states we developed a framework for constructing tissue to tissue coexpression networks between genes in the hypothalamus liver or adipose tissue these networks have a scale free architecture and are strikingly independent of gene gene coexpression networks that are constructed from more standard analyses of single tissues this is the first systematic effort to study inter tissue relationships and highlights genes in the hypothalamus that act as information relays in the control of peripheral tissues in obese mice the subnetworks identified as specific to tissue to tissue interactions are enriched in genes that have obesity relevant biological functions such as circadian rhythm energy balance stress response or immune response conclusions tissue to tissue networks enable the identification of disease specific genes that respond to changes induced by different tissues and they also provide unique details regarding candidate genes for obesity that are identified in genome wide association studies identifying such genes from single tissue analyses would be difficult impossible
in contrast to normal differentiated cells which rely primarily on mitochondrial oxidative phosphorylation to generate the energy needed for cellular processes most cancer cells instead rely on aerobic glycolysis a phenomenon termed the warburg effect aerobic glycolysis is an inefficient way to generate adenosine triphosphate atp however and the advantage it confers to cancer cells has been unclear here we propose that the metabolism of cancer cells and indeed all proliferating cells is adapted to facilitate the uptake and incorporation of nutrients into the biomass e g nucleotides amino acids and lipids needed to produce a new cell supporting this idea are recent studies showing that i several signaling pathways implicated in cell proliferation also regulate metabolic pathways that incorporate nutrients into biomass and that ii certain cancer associated mutations enable cancer cells to acquire and metabolize nutrients in a manner conducive to proliferation rather than efficient atp production a better understanding of the mechanistic links between cellular metabolism and growth control may ultimately lead to better treatments for cancer
with the rapid progress of biological research great demands are proposed for integrative knowledge sharing systems to efficiently support collaboration of biological researchers from various fields to fulfill such requirements we have developed a data centric knowledge sharing platform weblab for biologists to fetch analyze manipulate and share data under an intuitive web interface dedicated space is provided for users to store their input data and analysis results users can upload local data or fetch public data from remote databases and then perform analysis using more than integrated bioinformatic tools these tools can be further organized as customized analysis workflows to accomplish complex tasks automatically in addition to conventional biological data weblab also provides rich supports for scientific literatures such as searching against full text of uploaded literatures and exporting citations into various well known citation managers such as endnote and bibtex to facilitate team work among colleagues weblab provides a powerful and flexible sharing mechanism which allows users to share input data analysis results scientific literatures and customized workflows to specified users or groups with sophisticated privilege settings weblab is publicly available at http weblab cbi pku edu cn with all source code released as software
a new possible scenario for the origin of the molecular collective behaviour associated with the emergence of living matter is presented we propose that the transition from a non living to a living cell could be mapped to a quantum transition to a coherent entanglement of condensates like in a multigap bcs superconductor here the decoherence evading qualities at high temperature are based on the feshbach resonance that has been recently proposed as the driving mechanism for high tc superconductors finally we discuss how the proximity to a particular critical point is relevant to the emergence of coherence in the cell
as computation continues to move into the cloud the computing platform of interest no longer resembles a pizza box or a refrigerator but a warehouse full of computers these new large datacenters are quite different from traditional hosting facilities of earlier times and cannot be viewed simply as a collection of co located servers large portions of the hardware and software resources in these facilities must work in concert to efficiently deliver good levels of internet service performance something that can only be achieved by a holistic approach to their design and deployment in other words we must treat the datacenter itself as one massive warehouse scale computer wsc we describe the architecture of wscs the main factors influencing their design operation and cost structure and the characteristics of their software base we hope it will be useful to architects and programmers of today s wscs as well as those of future many core platforms which may one day implement the equivalent of today s wscs on a board
micrornas mirs are small noncoding rnas that regulate gene expression by binding to target messenger rnas mrnas leading to translational repression or degradation here we show that the mir cluster is highly expressed in human endothelial cells and that mir a component of this cluster controls the growth of new blood vessels angiogenesis forced overexpression of mir in endothelial cells blocked angiogenesis in vitro and in vivo in mouse models of limb ischemia and myocardial infarction systemic administration of an antagomir designed to inhibit mir led to enhanced blood vessel growth and functional recovery of damaged tissue mir appears to target mrnas corresponding to several proangiogenic proteins including the integrin subunit thus mir may serve as a valuable therapeutic target in the setting of disease
the meme suite web server provides a unified portal for online discovery and analysis of sequence motifs representing features such as dna binding sites and protein interaction domains the popular meme motif discovery algorithm is now complemented by the algorithm which allows discovery of motifs containing gaps three sequence scanning algorithms mast fimo and allow scanning numerous dna and protein sequence databases for motifs discovered by meme and transcription factor motifs including those discovered using meme can be compared with motifs in many popular motif databases using the motif database scanning algorithm tomtom transcription factor motifs can be further analyzed for putative function by association with gene ontology go terms using the motif go term association tool gomo meme output now contains sequence logos for each discovered motif as well as buttons to allow motifs to be conveniently submitted to the sequence and motif database scanning algorithms mast fimo and tomtom or to gomo for further analysis output similarly contains buttons for further analysis using and for rerunning with different parameters all of the motif based tools are now implemented as web services via opal source code binaries and a web server are freely available for noncommercial use at http meme net
summary the rtracklayer package supports the integration of existing genome browsers with experimental data analyses performed in r the user may transfer annotation tracks to and from a genome browser and create and manipulate browser views to focus on a particular set of annotations in a specific genomic region currently the ucsc genome browser is supported availability freely available from http www bioconductor org contact mflawren fhcrc org supplementary information a quick start vignette is included with package
in this paper axiomatic design ad theory was adopted for the design analysis of an underactuated remotely operated vehicle rov system and its subcomponents the system design issues of the propulsion and control system of the rov ii are analyzed and addressed based on the independence axiom methodology the top level functional requirements frs for the thruster design and configuration are identified and its corresponding design parameters dps are also presented world scientific company
on the basis of axiomatic design ad theory several key techniques for generating conceptual design schemes of mechanical products are studied customer driven issue of conceptual design is discussed it proposes a strategy for establishing the mapping from customer domain to functional domain the concept of domains structure template which is directed against the defects of ad is proposed to extend ad and concept space and concept reasoning network are constructed effective concept space is filtered out by restraint reasoning from concept reasoning network and effective concept space resolving algorithm is symbolically described the functional relationships between design entities involved in individual scheme are analyzed based on conceptual graphs a high speed chainstitch sewing machine is used as a case study to show the effectiveness of the proposed method and algorithms
in bacterial genomes gene order is not random this is most evident when looking at operons these often encoding enzymes involved in the same metabolic pathway or proteins from the same complex is gene order within operons nonrandom however and if so why we examine this issue using metabolic operons as a case study using the metabolic network of escherichia coli we define the temporal order of reactions we find a pronounced trend for genes to appear in operons in the same order as they are needed in metabolism colinearity this is paradoxical as at steady state enzymes abundance should be independent of order within the operon we consider three extensions of the steady state model that could potentially account for colinearity increased productivity associated with higher expression levels of the most genes a faster metabolic processing immediately after up regulation and metabolic stalling owing to stochastic protein loss we establish the validity of these hypotheses by employing deterministic and stochastic models of enzyme kinetics the stochastic stalling hypothesis correctly and uniquely predicts that colinearity is more pronounced both for lowly expressed operons and for genes that are not physically adjacent the alternative models fail to find any support these results support the view that stochasticity is a pervasive problem to a cell and that gene order evolution can be driven by the selective consequences of fluctuations in levels
abstract background gene set analysis gsa is a widely used strategy for gene expression data analysis based on pathway knowledge gsa focuses on sets of related genes and has established major advantages over individual gene analyses including greater robustness sensitivity and biological relevance however previous gsa methods have limited usage as they cannot handle datasets of different sample sizes or experimental designs results to address these limitations we present a new gsa method called generally applicable gene set enrichment gage we successfully apply gage to multiple microarray datasets with different sample sizes experimental designs and profiling techniques gage shows significantly better results when compared to two other commonly used gsa methods of gsea and page we demonstrate this improvement in the following three aspects consistency across repeated studies experiments sensitivity and specificity biological relevance of the regulatory mechanisms inferred gage reveals novel and relevant regulatory mechanisms from both published and previously unpublished microarray studies from two published lung cancer data sets gage derived a more cohesive and predictive mechanistic scheme underlying lung cancer progress and metastasis for a previously unpublished study gage predicted novel regulatory mechanisms for induced osteoblast differentiation including the canonical bmp tgf beta signaling jak stat signaling wnt signaling and estrogen signaling pathways all of which are strongly supported by the experimental literature conclusion gage is generally applicable to gene expression datasets with different sample sizes and experimental designs gage consistently outperformed two most frequently used gsa methods and inferred statistically and biologically more relevant regulatory pathways the gage method is implemented in r in the gage package available under the gnu gpl from http sysbio engin umich edu luow php
micrornas mirnas are short rnas that act as guides for the degradation and translational repression of protein coding mrnas a large body of work showed that mirnas are involved in the regulation of a broad range of biological functions from development to cardiac and immune system function to metabolism to cancer for most of the over mirnas that are encoded in the human genome the functions still remain to be uncovered identifying mirnas whose expression changes between cell types or between normal and pathological conditions is an important step towards characterizing their function as is the prediction of mrnas that could be targeted by these mirnas to provide the community the possibility of exploring interactively mirna expression patterns and the candidate targets of mirnas in an integrated environment we developed the mirz web server which is accessible at www mirz unibas ch the server provides experimental and computational biologists with statistical analysis and data mining tools operating on up to date databases of sequencing based mirna expression profiles and of predicted mirna target sites in species ranging from caenorhabditis elegans to homo nar
background current microrna mirna research in progress has engendered rapid accumulation of expression data evolving from microarray experiments such experiments are generally performed over different tissues belonging to a specific species of metazoan for disease diagnosis microarray probes are also prepared with tissues taken from similar organs of different candidates of an organism expression data of mirnas are frequently mapped to co expression networks to study the functions of mirnas their regulation on genes and to explore the complex regulatory network that might exist between transcription factors tfs genes and mirnas these directions of research relating mirnas are still not fully explored and therefore construction of reliable and compatible methods for mining mirna co expression networks has become an emerging area this paper introduces a novel method for mining the mirna co expression networks in order to obtain co expressed mirnas under the hypothesis that these might be regulated by common tfs results three co expression networks configured from one patient specific one tissue specific and a stem cell based mirna expression data are studied for analyzing the proposed methodology a novel compactness measure is introduced the results establish the statistical significance of the sets of mirnas evolved and the efficacy of the self pruning phase employed by the proposed method all these datasets yield similar network patterns and produce coherent groups of mirnas the existence of common tfs regulating these groups of mirnas is empirically tested the results found are very promising a novel visual validation method is also proposed that reflects the homogeneity as well as statistical properties of the grouped mirnas this visual validation method provides a promising and statistically significant graphical tool for expression analysis conclusion a heuristic mining methodology that resembles a clustering motivation is proposed in this paper however there remains a basic difference between the mining method and a clustering approach the heuristic approach can produce priority modules pm from an mirna co expression network by employing a self pruning phase which are analyzed for statistical and biological significance the mining algorithm minimizes the space time complexity of the analysis and also handles noise in the data in addition the mining method reveals promising results in the unsupervised analysis of tf regulation
motivation the motif discovery problem consists of finding over represented patterns in a collection of biosequences it is one of the classical sequence analysis problems but still has not been satisfactorily solved in an exact and efficient manner this is partly due to the large number of possibilities of defining the motif search space and the notion of over representation even for well defined formalizations the problem is frequently solved in an ad hoc manner with heuristics that do not guarantee to find the best motif results we show how to solve the motif discovery problem almost exactly on a practically relevant space of iupac generalized string patterns using the p value with respect to an i i d model or a markov model as the measure of over representation in particular i we use a highly accurate compound poisson approximation for the null distribution of the number of motif occurrences we show how to compute the exact clump size distribution using a recently introduced device called probabilistic arithmetic automaton paa ii we define two p value scores for over representation the first one based on the total number of motif occurrences the second one based on the number of sequences in a collection with at least one occurrence iii we describe an algorithm to discover the optimal pattern with respect to either of the scores the method exploits monotonicity properties of the compound poisson approximation and is by orders of magnitude faster than exhaustive enumeration of iupac strings h compared with an extrapolated runtime of years iv we justify the use of the proposed scores for motif discovery by showing our method to outperform other motif discovery algorithms e g meme weeder on benchmark datasets we also propose new motifs on mycobacterium tuberculosis availability and implementation the method has been implemented in java it can be obtained from http www cs tu dortmund de marschal
motivation a variety of algorithms have been developed to predict transcription factor binding sites tfbss within the genome by exploiting the evolutionary information implicit in multiple alignments of the genomes of related species one such approach uses an extension of the standard position specific motif model that incorporates phylogenetic information via a phylogenetic tree and a model of evolution however these phylogenetic motif models pmms have never been rigorously benchmarked in order to determine whether they lead to better prediction of tfbss than obtained using simple position weight matrix scanning results we evaluate three pmm based prediction algorithms each of which uses a different treatment of gapped alignments and we compare their prediction accuracy with that of a non phylogenetic motif scanning approach surprisingly all of these algorithms appear to be inferior to simple motif scanning when accuracy is measured using a gold standard of validated yeast tfbss however the pmm scanners perform much better than simple motif scanning when we abandon the gold standard and consider the number of statistically significant sites predicted using column shuffled random motifs to measure significance these results suggest that the common practice of measuring the accuracy of binding site predictors using collections of known sites may be dangerously misleading since such collections may be missing weak sites which are exactly the type of sites needed to discriminate among predictors we then extend our previous theoretical model of the statistical power of pmm based prediction algorithms to allow for loss of binding sites during evolution and show that it gives a more accurate upper bound on scanner accuracy finally utilizing our theoretical model we introduce a new method for predicting the number of real binding sites in a genome the results suggest that the number of true sites for a yeast tf is in general several times greater than the number of known sites listed in the saccharomyces cerevisiae database scpd among the three scanning algorithms that we test the monkey algorithm has the highest accuracy for predicting yeast tfbss contact j hawkins imb uq edu bioinformatics
motivation permutation tests have become a standard tool to assess the statistical significance of an event under investigation the statistical significance as expressed in a p value is calculated as the fraction of permutation values that are at least as extreme as the original statistic which was derived from non permuted data this empirical method directly couples both the minimal obtainable p value and the resolution of the p value to the number of permutations thereby it imposes upon itself the need for a very large number of permutations when small p values are to be accurately estimated this is computationally expensive and often infeasible results a method of computing p values based on tail approximation is presented the tail of the distribution of permutation values is approximated by a generalized pareto distribution a good fit and thus accurate p value estimates can be obtained with a drastically reduced number of permutations when compared with the standard empirical way of computing p values availability the matlab code can be obtained from the corresponding author on request contact tknijnenburg systemsbiology orgsupplementary information supplementary data are available at online
subjective methods have been reported to adapt a general purpose ontology for a specific application for example gene ontology go slim was created from go to generate a highly aggregated report of the human genome annotation we propose statistical methods to adapt the general purpose obo foundry disease ontology do for the identification of gene disease associations thus we need a simplified definition of disease categories derived from implicated genes on the basis of the assumption that the do terms having similar associated genes are closely related we group the do terms based on the similarity of gene to do mapping profiles two types of binary distance metrics are defined to measure the overall and subset similarity between do terms a compactness scalable fuzzy clustering method is then applied to group similar do terms to reduce false clustering the semantic similarities between do terms are also used to constrain clustering results as such the do terms are aggregated and the redundant do terms are largely removed using these methods we constructed a simplified vocabulary list from the do called disease ontology lite dolite we demonstrated that dolite results in more interpretable results than do for gene disease association tests the resultant dolite has been used in the functional disease ontology fundo web application at http www projects bioinformatics northwestern edu fundo contact s northwestern bioinformatics
summary it has been proposed that two amino acid substitutions in the transcription factor have been positively selected during human evolution due to effects on aspects of speech and language here we introduce these substitutions into the endogenous gene of mice although these mice are generally healthy they have qualitatively different ultrasonic vocalizations decreased exploratory behavior and decreased dopamine concentrations in the brain suggesting that the humanized allele affects basal ganglia in the striatum a part of the basal ganglia affected in humans with a speech deficit due to a nonfunctional allele we find that medium spiny neurons have increased dendrite lengths and increased synaptic plasticity since mice carrying one nonfunctional allele show opposite effects this suggests that alterations in cortico basal ganglia circuits might have been important for the evolution of speech and language in humans for a video summary of this article see the paperflick file available with the online data
motivation genome wide association studies are commonly used to identify possible associations between genetic variations and diseases these studies mainly focus on identifying individual single nucleotide polymorphisms snps potentially linked with one disease of interest in this work we introduce a novel methodology that identifies similarities between diseases using information from a large number of snps we separate the diseases for which we have individual genotype data into one reference disease and several query diseases we train a classifier that distinguishes between individuals that have the reference disease and a set of control individuals this classifier is then used to classify the individuals that have the query diseases we can then rank query diseases according to the average classification of the individuals in each disease set and identify which of the query diseases are more similar to the reference disease we repeat these classification and comparison steps so that each disease is used once as reference disease results we apply this approach using a decision tree classifier to the genotype data of seven common diseases and two shared control sets provided by the wellcome trust case control consortium we show that this approach identifies the known genetic similarity between type diabetes and rheumatoid arthritis and identifies a new putative similarity between bipolar disease and hypertension contact serafim cs stanford bioinformatics
motivation promoter prediction is an important task in genome annotation projects and during the past years many new promoter prediction programs ppps have emerged however many of these programs are compared inadequately to other programs in most cases only a small portion of the genome is used to evaluate the program which is not a realistic setting for whole genome annotation projects in addition a common evaluation design to properly compare ppps is still lacking results we present a large scale benchmarking study of state of the art ppps a multi faceted evaluation strategy is proposed that can be used as a gold standard for promoter prediction evaluation allowing authors of promoter prediction software to compare their method to existing methods in a proper way this evaluation strategy is subsequently used to compare the chosen promoter predictors and an in depth analysis on predictive performance promoter class specificity overlap between predictors and positional bias of the predictions is conducted availability we provide the implementations of the four protocols as well as the datasets required to perform the benchmarks to the academic community free of charge on request contact yves vandepeer psb ugent besupplementary information supplementary data are available at online
motivation gene regulatory networks underlying temporal processes such as the cell cycle or the life cycle of an organism can exhibit significant topological changes to facilitate the underlying dynamic regulatory functions thus it is essential to develop methods that capture the temporal evolution of the regulatory networks these methods will be an enabling first step for studying the driving forces underlying the dynamic gene regulation circuitry and predicting the future network structures in response to internal and external stimuli results we introduce a kernel reweighted logistic regression method keller for reverse engineering the dynamic interactions between genes based on their time series of expression values we apply the proposed method to estimate the latent sequence of temporal rewiring networks of genes involved in the developmental process during the life cycle of drosophila melanogaster our results offer the first glimpse into the temporal evolution of gene networks in a living organism during its full developmental course our results also show that many genes exhibit distinctive functions at different stages along the developmental cycle availability source codes and relevant data will be made available at http www sailing cs cmu keller
relative to most regions of the genome tandemly repeated dna sequences display a greater propensity to mutate a search for tandem repeats in the saccharomyces cerevisiae genome revealed that the nucleosome free region directly upstream of genes the promoter region is enriched in repeats as many as of all gene promoters contain tandem repeat sequences genes driven by these repeat containing promoters show significantly higher rates of transcriptional divergence variations in repeat length result in changes in expression and local nucleosome positioning tandem repeats are variable elements in promoters that may facilitate evolutionary tuning of gene expression by affecting local chromatin science
we present the first korean individual genome sequence sjk and analysis results the diploid genome of a korean male was sequenced to fold redundancy using the illumina paired end sequencing method sjk covered of the ncbi human reference genome we identified novel single nucleotide polymorphisms snps that are not in the dbsnp database despite a close similarity significant differences were observed between the chinese genome yh the only other asian genome available and sjk out of snps were sjk specific against venter s against watson s and against the yoruba genomes out of of short indels bp discovered on the same loci had the same size and type as yh and out of deletion structural variants were sjk specific even after attempting to map unmapped reads of sjk to unanchored ncbi scaffolds hgsv and available personal genomes there were still sjk reads that could not be mapped all these findings indicate that the overall genetic differences among individuals from closely related ethnic groups may be significant hence constructing reference genomes for minor socio ethnic groups will be useful for massive individual sequencing
adenosine to inosine a to i rna editing leads to transcriptome diversity and is important for normal brain function to date only a handful of functional sites have been identified in mammals we developed an unbiased assay to screen more than computationally predicted nonrepetitive a to i sites using massively parallel target capture and dna sequencing a comprehensive set of several hundred human rna editing sites was detected by comparing genomic dna with rnas from seven tissues of a single individual specificity of our profiling was supported by observations of enrichment with known features of targets of adenosine deaminases acting on rna adar and validation by means of capillary sequencing this efficient approach greatly expands the repertoire of rna editing targets and can be applied to studies involving rna editing related human science
neuroscientists are beginning to advance explanations of social behavior in terms of underlying brain mechanisms two distinct networks of brain regions have come to the fore the first involves brain regions that are concerned with learning about reward and reinforcement these same reward related brain areas also mediate preferences that are social in nature even when no direct reward is expected the second network focuses on regions active when a person must make estimates of another person s intentions however it has been difficult to determine the precise roles of individual brain regions within these networks or how activities in the two networks relate to one another some recent studies of reward guided behavior have described brain activity in terms of formal mathematical models these models can be extended to describe mechanisms that underlie complex social exchange such a mathematical formalism defines explicit mechanistic hypotheses about internal computations underlying regional brain activity provides a framework in which to relate different types of activity and understand their contributions to behavior and prescribes strategies for performing experiments under strong science
we describe a new program for the alignment of multiple biological sequences that is both statistically motivated and fast enough for problem sizes that arise in practice our fast statistical alignment program is based on pair hidden markov models which approximate an insertion deletion process on a tree and uses a sequence annealing algorithm to combine the posterior probabilities estimated from these models into a multiple alignment fsa uses its explicit statistical model to produce multiple alignments which are accompanied by estimates of the alignment accuracy and uncertainty for every column and character of the alignment previously available only with alignment programs which use computationally expensive markov chain monte carlo approaches yet can align thousands of long sequences moreover fsa utilizes an unsupervised query specific learning procedure for parameter estimation which leads to improved accuracy on benchmark reference alignments in comparison to existing programs the centroid alignment approach taken by fsa in combination with its learning procedure drastically reduces the amount of false positive alignment on biological data in comparison to that given by other methods the fsa program and a companion visualization tool for exploring uncertainty in alignments can be used via a web interface at http orangutan math berkeley edu fsa and the source code is available at http fsa net
micro blogs a relatively new phenomenon provide a new communication channel for people to broadcast information that they likely would not share otherwise using existing channels e g email phone im or weblogs micro blogging has become popu lar quite quickly raising its potential for serving as a new informal communication medium at work providing a variety of impacts on collaborative work e g enhancing information sharing building common ground and sustaining a feeling of connectedness among colleagues this exploratory research project is aimed at gaining an in depth understanding of how and why people use twitter a popular micro blogging tool and exploring micro blog s poten tial impacts on informal communication work
synthetic gene networks can be constructed to emulate digital circuits and devices giving one the ability to program and design cells with some of the principles of modern computing such as counting a cellular counter would enable complex synthetic programming and a variety of biotechnology applications here we report two complementary synthetic genetic counters in escherichia coli that can count up to three induction events the first a riboregulated transcriptional cascade and the second a recombinase based cascade of memory units these modular devices permit counting of varied user defined inputs over a range of frequencies and can be expanded to count higher science
stochastic resonance is said to be observed when increases in levels of unpredictable fluctuations e g random noise cause an increase in a metric of the quality of signal transmission or detection performance rather than a decrease this counterintuitive effect relies on system nonlinearities and on some parameter ranges being suboptimal stochastic resonance has been observed quantified and described in a plethora of physical and biological systems including neurons being a topic of widespread multidisciplinary interest the definition of stochastic resonance has evolved significantly over the last decade or so leading to a number of debates misunderstandings and controversies perhaps the most important debate is whether the brain has evolved to utilize random noise in vivo as part of the neural code surprisingly this debate has been for the most part ignored by neuroscientists despite much indirect evidence of a positive role for noise in the brain we explore some of the reasons for this and argue why it would be more surprising if the brain did not exploit randomness provided by noise via stochastic resonance or otherwise than if it did we also challenge neuroscientists and biologists both computational and experimental to embrace a very broad definition of stochastic resonance in terms of signal processing noise benefits and to devise experiments aimed at verifying that random variability can play a functional role in the brain nervous system or other areas biology
the cell cycle ontology http www cellcycleontology org webcite is an application ontology that automatically captures and integrates detailed knowledge on the cell cycle process cell cycle ontology is enabled by semantic web technologies and is accessible via the web for browsing visualizing advanced querying and computational reasoning cell cycle ontology facilitates a detailed analysis of cell cycle related molecular network components through querying and automated reasoning it may provide new hypotheses to help steer a systems biology approach to biological building
motivation as arrayexpress and other repositories of genome wide experiments are reaching a mature size it is becoming more meaningful to search for related experiments given a particular study we introduce methods that allow for the search to be based upon measurement data instead of the more customary annotation data the goal is to retrieve experiments in which the same biological processes are activated this can be due either to experiments targeting the same biological question or to as yet unknown relationships results we use a combination of existing and new probabilistic machine learning techniques to extract information about the biological processes differentially activated in each experiment to retrieve earlier experiments where the same processes are activated and to visualize and interpret the retrieval results case studies on a subset of arrayexpress show that with a sufficient amount of data our method indeed finds experiments relevant to particular biological questions results can be interpreted in terms of biological processes using the visualization techniques availability the code is available from http www cis hut fi projects mi software contact jose caldas fi
electrical recordings in humans and monkeys show attentional enhancement of evoked responses and gamma synchrony in ventral stream cortical areas does this synchrony result from intrinsic activity in visual cortex or from inputs from other structures using paired recordings in the frontal eye field fef and area we found that attention to a stimulus in their joint receptive field leads to enhanced oscillatory coupling between the two areas particularly at gamma frequencies this coupling appeared to be initiated by fef and was time shifted by about to milliseconds across a range of frequencies considering the expected conduction and synaptic delays between the areas this time shifted coupling at gamma frequencies may optimize the postsynaptic impact of spikes from one area upon the other improving cross area communication attention
intuitively higher intelligence might be assumed to correspond to more efficient information transfer in the brain but no direct evidence has been reported from the perspective of brain networks in this study we performed extensive analyses to test the hypothesis that individual differences in intelligence are associated with brain structural organization and in particular that higher scores on intelligence tests are related to greater global efficiency of the brain anatomical network we constructed binary and weighted brain anatomical networks in each of healthy young adults utilizing diffusion tensor tractography and calculated topological properties of the networks using a graph theoretical method based on their iq test scores all subjects were divided into general and high intelligence groups and significantly higher global efficiencies were found in the networks of the latter group moreover we showed significant correlations between iq scores and network properties across all subjects while controlling for age and gender specifically higher intelligence scores corresponded to a shorter characteristic path length and a higher global efficiency of the networks indicating a more efficient parallel information transfer in the brain the results were consistently observed not only in the binary but also in the weighted networks which together provide convergent evidence for our hypothesis our findings suggest that the efficiency of brain structural organization may be an important biological basis intelligence
research into genome assembly algorithms has experienced a resurgence due to new challenges created by the development of next generation sequencing technologies several genome assemblers have been published in recent years specifically targeted at the new sequence data however the ever changing technological landscape leads to the need for continued research in addition the low cost of next generation sequencing data has led to an increased use of sequencing in new settings for example the new field of metagenomics relies on large scale sequencing of entire microbial communities instead of isolate genomes leading to new computational challenges in this article we outline the major algorithmic approaches for genome assembly and describe recent developments in this bib
adaptation is conventionally regarded as occurring at the level of the individual organism where it functions to maximize the individuals inclusive however it has recently been argued that empirical studies on the evolution of parasite virulence in spatial populations show in particular it has been claimed that the evolution of lower virulence in response to limited parasite provides proof of wynne idea of adaptation at the group level although previous theoretical work has shown that limited dispersal can favour lower virulence it has not clarified why with five different suggestions having been here we show that the effect of dispersal on parasite virulence can be understood entirely within the framework of inclusive fitness theory limited parasite dispersal favours lower parasite growth rates and hence reduced virulence because it decreases the direct benefit of producing offspring dispersers are worth more than non dispersers because they can go to patches with no or fewer parasites and increases the competition for hosts experienced by both the focal individual self shading and their relatives kin shading this demonstrates that reduced virulence can be understood as an individual level adaptation by the parasite to maximize its inclusive fitness and clarifies the links with virulence more
human skin is a large heterogeneous organ that protects the body from pathogens while sustaining microorganisms that influence human health and disease our analysis of ribosomal rna gene sequences obtained from distinct skin sites of healthy humans revealed that physiologically comparable sites harbor similar bacterial communities the complexity and stability of the microbial community are dependent on the specific characteristics of the skin site this topographical and temporal survey provides a baseline for studies that examine the role of bacterial communities in disease states and the microbial interdependencies required to maintain healthy science
the frequency with which scientists fabricate and falsify data or commit other forms of scientific misconduct is a matter of controversy many surveys have asked scientists directly whether they have committed or know of a colleague who committed research misconduct but their results appeared difficult to compare and synthesize this is the first meta analysis of these surveys to standardize outcomes the number of respondents who recalled at least one incident of misconduct was calculated for each question and the analysis was limited to behaviours that distort scientific knowledge fabrication falsification cooking of data etc survey questions on plagiarism and other forms of professional misconduct were excluded the final sample consisted of surveys that were included in the systematic review and in the meta analysis a pooled weighted average of n ci of scientists admitted to have fabricated falsified or modified data or results at least once a serious form of misconduct by any standard and up to admitted other questionable research practices in surveys asking about the behaviour of colleagues admission rates were n ci for falsification and up to for other questionable research practices meta regression showed that self reports surveys surveys using the words falsification or fabrication and mailed surveys yielded lower percentages of misconduct when these factors were controlled for misconduct was reported more frequently by medical pharmacological researchers than others considering that these surveys ask sensitive questions and have other limitations it appears likely that this is a conservative estimate of the true prevalence of misconduct
summarybackground low dose aspirin is of definite and substantial net benefit for many people who already have occlusive vascular disease we have assessed the benefits and risks in primary prevention methods we undertook meta analyses of serious vascular events myocardial infarction stroke or vascular death and major bleeds in six primary prevention trials punctuation space individuals at low average risk punctuation space person years serious vascular events and secondary prevention trials punctuation space individuals at high average risk punctuation space person years serious vascular events that compared long term aspirin versus control we report intention to treat analyses of first events during the scheduled treatment period findings in the primary prevention trials aspirin allocation yielded a proportional reduction in serious vascular events aspirin vs control per year p due mainly to a reduction of about a fifth in non fatal myocardial infarction vs per year p textless the net effect on stroke was not significant vs per year p haemorrhagic stroke vs p other stroke vs per year p vascular mortality did not differ significantly vs per year p aspirin allocation increased major gastrointestinal and extracranial bleeds vs per year p textless and the main risk factors for coronary disease were also risk factors for bleeding in the secondary prevention trials aspirin allocation yielded a greater absolute reduction in serious vascular events vs per year p textless with a non significant increase in haemorrhagic stroke but reductions of about a fifth in total stroke vs per year p and in coronary events vs per year p textless in both primary and secondary prevention trials the proportional reductions in the aggregate of all serious vascular events seemed similar for men and women interpretation in primary prevention without previous disease aspirin is of uncertain net value as the reduction in occlusive events needs to be weighed against any increase in major bleeds further trials are in progress funding uk medical research council british heart foundation cancer research uk and the european community programme
motivation structural variants including duplications insertions deletions and inversions of large blocks of dna sequence are an important contributor to human genome variation measuring structural variants in a genome sequence is typically more challenging than measuring single nucleotide changes current approaches for structural variant identification including paired end dna sequencing mapping and array comparative genomic hybridization acgh do not identify the boundaries of variants precisely consequently most reported human structural variants are poorly defined and not readily compared across different studies and measurement techniques results we introduce geometric analysis of structural variants gasv a geometric approach for identification classification and comparison of structural variants this approach represents the uncertainty in measurement of a structural variant as a polygon in the plane and identifies measurements supporting the same variant by computing intersections of polygons we derive a computational geometry algorithm to efficiently identify all such intersections we apply gasv to sequencing data from nine individual human genomes and several cancer genomes we obtain better localization of the boundaries of structural variants distinguish genetic from putative somatic structural variants in cancer genomes and integrate acgh and paired end sequencing measurements of structural variants this work presents the first general framework for comparing structural variants across multiple samples and measurement techniques and will be useful for studies of both genetic structural variants and somatic rearrangements in cancer availability http cs brown edu people braphael software htmlcontact braphael edu
gr transposable elements tes are ubiquitous genomic parasites the deleterious consequences of the presence and activity of tes have fueled debate about the evolutionary forces countering their expansion purifying selection is thought to purge te insertions from the genome and te sequences are targeted by hosts for epigenetic silencing however the interplay between epigenetic and evolutionary forces countering te expansion remains unexplored here we analyze genomic epigenetic and population genetic data from to yield three observations first gene expression is negatively correlated with the density of methylated tes second the signature of purifying selection is detectable for methylated tes near genes but not for unmethylated tes or for tes far from genes third te insertions are distributed by age and methylation status such that older methylated tes are farther from genes based on these observations we present a model in which host silencing of tes near genes has deleterious effects on neighboring gene expression resulting in the preferential loss of methylated tes from gene rich chromosomal regions this mechanism implies an evolutionary tradeoff in which the benefit of te silencing imposes a fitness cost via deleterious effects on the expression of genes
structure comparison opens a window into the distant past of protein evolution which has been unreachable by sequence comparison alone with entries in the protein data bank and about new structures added each week automated processing comparison and classification are necessary a variety of methods use different representations scoring functions and optimization algorithms and they generate contradictory results even for moderately distant structures sequence mutations insertions and deletions are accommodated by plastic deformations of the common core retaining the precise geometry of the active site and peripheral regions may refold completely therefore structure comparison methods that allow for flexibility and plasticity generate the most biologically meaningful alignments active research directions include both the search for fold invariant features and the modeling of structural transitions in evolution advances have been made in algorithmic robustness multiple alignment and speeding up searches
recently the nature of protein structure space has been widely discussed in the literature the traditional discrete view of protein universe as a set of separate folds has been criticized in the light of growing evidence that almost any arrangement of secondary structures is possible and the whole protein space can be traversed through a path of similar structures here we argue that the discrete and continuous descriptions are not mutually exclusive but complementary the space is largely discrete in evolutionary sense but continuous geometrically when purely structural similarities are quantified evolutionary connections are mainly confined to separate structural prototypes corresponding to folds as islands of structural stability with few remaining traceable links between the islands however for a geometric similarity measure it is usually possible to find a reasonable cutoff that yields paths connecting any two structures intermediates
biomedical ontologies provide essential domain knowledge to drive data integration information retrieval data annotation natural language processing and decision support bioportal http bioportal bioontology org is an open repository of biomedical ontologies that provides access via web services and web browsers to ontologies developed in owl rdf obo format and protg frames bioportal functionality includes the ability to browse search and visualize ontologies the web interface also facilitates community based participation in the evaluation and evolution of ontology content by providing features to add notes to ontology terms mappings between terms and ontology reviews based on criteria such as usability domain coverage quality of content and documentation and support bioportal also enables integrated search of biomedical data resources such as the gene expression omnibus geo clinicaltrials gov and arrayexpress through the annotation and indexing of these resources with ontologies in bioportal thus bioportal not only provides investigators clinicians and developers one stop shopping to programmatically access biomedical ontologies but also provides support to integrate data from a variety of resources
motivation approximately of human genes have no publications documenting their function and for those that are published the number of publications per gene is highly skewed furthermore for reasons not clear the entry of new gene names into the literature has slowed in recent years if we are to better understand human mammalian biology and complete the catalog of human gene function it is important to finish predicting putative functions for these genes based upon existing experimental evidence results a global meta analysis gma of all publicly available geo two channel human microarray datasets experiments total was conducted to identify genes with recurrent reproducible patterns of co regulation across different conditions patterns of co expression were divided into parallel i e genes are up and down regulated together and anti parallel several ranking methods to predict a gene s function based on its top co expressed gene pairs were compared in the best method of predicted gene ontology go categories matched exactly with the known go categories for genes analyzed versus only for random gene sets only of co expressed gene pairs were found as co occurring gene pairs in medline conclusions via a go enrichment analysis genes co expressed in parallel with the query gene were frequently associated with the same go categories whereas anti parallel genes were not combining parallel and anti parallel genes for analysis resulted in fewer significant go categories suggesting they are best analyzed separately expression databases contain much unexpected genetic knowledge that has not yet been reported in the literature a total of human genes with unknown function were differentially expressed in at least experiments availability data matrix available upon request contact jdwren gmail comsupplementary information supplementary data are available at online
we study a two class classification problem with a large number of features out of which many are useless and only a few are useful but we do not know which ones they are the number of features is large compared with the number of training observations calibrating the model with key parametersthe number of features the size of the training sample the fraction and strength of useful featureswe identify a region in parameter space where no trained classifier can reliably separate the two classes on fresh data the complement of this regionwhere successful classification is possibleis also discussed
we report an improved method for the estimation of shock outcome prediction based on novel wavelet transform based time frequency methods wavelet based peak frequency energy mean frequency spectral flatness and a new entropy measure were studied to predict shock outcome of these the entropy measure provided optimal results with specificity at sensitivity achieved for the prediction of return of spontaneous circulation rosc these results represent a major improvement in shock prediction in human fibrillation
we review insights from computational studies of affinities of ligands binding to proteins the power of structural biology is in translating knowledge of protein structures into insights about their forces binding and mechanisms however the complementary power of computer modeling is in showing the rest of the story i e how motions and ensembles and alternative conformers and the entropies and forces that cannot be seen in single molecular structures also contribute to binding affinities upon binding to a protein a ligand can bind in multiple orientations the protein or ligand can be deformed by the binding event waters ions or cofactors can have unexpected involvement and conformational or solvation entropies can sometimes play large and otherwise unpredictable roles computer modeling is helping to elucidate these factors main text introduction computer modeling is an important tool for understanding ligand binding to proteinsstructure based computer modeling of ligand protein interactions is now a core component of modern drug discovery charifson and kuntz it is now difficult to imagine the drug discovery process without computation jorgensen computational methods have played a key role in the drug discovery process for a growing number of marketed drugs including hiv protease inhibitors charifson and kuntz greer etal and jorgensen and zanamivir an antiviral neuraminidase inhibitor von itzstein etal and in the development of new drug candidates such as hiv integrase inhibitors hazuda etal and schames etal hepatitis c protease inhibitors liverton etal and thomson and perni and beta secretase inhibitors bace stauffer etal an early step in this field was the invention of the dock method in kuntz etal there are now at least four classes of physical computer methods listed from fastest to slowest and least physical to most physical very fast molecular docking methods including dock glide autodock flexx icn pmf and gold approximate free energy methods in which the solvent and protein motions are taken into account with fewer approximations relative binding free energy rbfe methods which include full solvent and protein motions but which require prior knowledge of the structure of a complex of the protein with a ligand that is similar to the one of interest and absolute binding free energy abfe methods which are the most expensive computationally but which include the physics in the most rigorous way that is currently practical see abfe methods start from an unbound ligand and potentially the unbound structure of the protein to attempt to predict the structures affinities and thermal properties of the complexes of interest mining minima is another method that is very nearly in this last category and has provided insight into binding chang and gilson gilson and zhou and head etal different computer methods trade off speed versus physical accuracyfirst we define some terms a lead compound is a molecule typically in early stage drug discovery that can be further chemically modified to improve its properties as a possible drug candidate a complex is a receptor and ligand bound together a pose is one conformation of a ligand in a complex and specifies both the ligand conformation and its position relative to the receptor a pose can refer either to a conformation that is known from an experimental structure of a complex or to a hypothetical conformation generated in a computer model the apo form refers to the structure of the protein that has no ligand bound to it the holo form refers to the structure of the protein when it is complexed with ligand the binding free energy go is the free energy of the complex minus the free energies of the ligand and apo protein separately in aqueous solution the binding free energy is related to the equilibrium association constant ka in units of by go rt ln co ka where r is the gas constant t is the absolute temperature and co is the standard concentration m the binding affinity or dissociation constant equals ka the binding free has two components g h ts where h is the enthalpy and s is the entropy here are some of the key approaches used for studying binding dockingdocking methods start with a known protein structure and a known ligand structure and aim to rapidly generate an optimal protein ligand bound conformation docking was designed to be very rapid seconds or less per compound which is desirable for screening large libraries in the short times required for modern pharmaceutical lead discovery docking explores many ligand conformations and orientations and in some cases even different potential binding sites the different poses are rank ordered by a score a quantity that ideally would correlate with the free energy of binding and is obtained either from a physical or knowledge based potential often docking approaches treat the protein as completely rigid having a single fixed receptor conformation other docking methods treat protein motions by moving only certain atoms out of the way though some modern docking approaches can allow for some motions of side chains or backbone corbeil etal cozzini etal leach meiler and baker sherman etal and wei etal treating these degrees of freedom slows down the computations considerably docking is an appealing way to generate leads shoichet etal because of its speed and ability to screen large libraries of potential leads huang and jacobson and babaoglu etal but because docking trades off physical accuracy for speed it is seldom accurate enough to predict binding affinities or rank order compounds its power to discriminate binders from nonbinders varies widely depending on the target protein graves etal and warren etal but because of its speed docking approaches are the method of choice for filtering out compounds that are likely nonbinders and for identifying native like poses mm pbsa gbsamm pbsa gbsa is more physically rigorous than docking theacronym stands for molecular mechanics with poisson boltzmann surface area or mm gbsa gb stands for generalized born and the method was originated by the kollman and case labs in the late cheatham etal kollman etal and srinivasan etal with parallel work by others vorobjev and hermans it involves greater computational cost than docking at least several hours per compound but also is more physical in its more extensive conformational sampling mm pbsa aims to estimate the binding free energies or relative binding free energies of related compounds here a computer generates representative bound and unbound structures by molecular mechanics simulations or by energy minimization of a protein ligand complex usually in explicit solvent the goal is to estimate the change in enthalpy on binding by comparing the average enthalpy of bound and unbound states but this would be a small difference of two large noisy energies so after the all atom simulations the water is removed and the enthalpies and binding free energies are estimated using an implicit poisson boltzmann or generalized born representation of water the binding free energy estimate includes the enthalpy change plus the change in salvation free energy from the implicit solvent model in many cases an approximate value of the entropy is also estimated from these simulations because mm pbsa gbsa invests more effort in sampling and entropies it is closer to a true free energy calculation however often because of limitations in the approximations for estimating entropy gilson and zhou entropic contributions are omitted when estimating relative binding strengths in the hope that these contributions will cancel when comparing similar ligands gilson and zhou and shirts etal early results with the mm pbsa method were quite promising typically with mean squared errors under kcal mol for the first several years huo etal kuhn and kollman mardis etal rizzo etal schwarzl etal and shirts etal but more recent studies have seen larger errors in some cases shirts etal applications have typically been limited to single targets so it is difficult to evaluate how well the method does generally the drawbacks of mm pbsa gbsa are that it too is sometimes not predictive pearlman shirts etal and steinbrecher etal and it requires prior knowledge of a likely bound complex as a starting point although such starting conformations can be taken from prior docking steinbrecher etal relative binding free energiesa still more rigorous approach uses the energetics of a physical force field and extensive conformational sampling from molecular dynamics simulations to actually compute differences in binding free energies between similar ligands this can be done using computational alchemy to obtain the difference in binding free energies gab this is the free energy of changing ligand a into ligand b in the receptor minus the free energy of changing a into b in solution to compute this free energy difference for just one pair of ligands binding to the same protein can cost several hundred cpu days these relative free energies can be computed preciselygiven sufficiently long molecular dynamics simulationsusing one of several different analysis techniques shirts etal though the accuracy of the binding free energies obtained from this method depends on the accuracy of the underlying molecular mechanics force field it does treat fully at least in principle free energies associated with conformational change as well as entropies the first alchemical calculations were performed in the in the mccammon lab tembe and mccammon and wong and mccammon and then by others hermans and subramaniam warshel etal bash etal and shirts etal limitations of these methods are the high computational costs and the need to know at least one bound structure of a similar ligand in the protein as a starting point accuracies are generally better than for mm pbsa pearlman and steinbrecher etal and docking mobley etal and pearlman and charifson but few systematic comparisons have been done these methods are only useful for comparing related ligands or receptors absolute binding free energiesthe most powerful approach in principle is the method of absolute binding free energies abfe boresch etal hermans and wang and roux etal like rbfe methods abfe methods also use full molecular dynamics simulations with fully detailed atomic force fields and also involve separate sets of simulations for the solvated ligand the solvated protein and the complex but abfe methods do not require prior knowledge of the binding affinity of a related ligand hence the term absolute there have been two groups of abfe approaches the first begins with the structure of the ligand of interest bound to the protein however the ultimate goal is to begin with no prior knowledge of either the structure or affinity of the ligand complex a second more recent group replaces starting knowledge of the structure with one or more docking poses mobley etal jayachandran et al various studies suggest that abfe methods are fairly accurate with good correlations to experimental binding affinities and with rms errors often less than kcal mol deng and roux fujitani etal jayachandran etal mobley etal shirts etal and wang etal and sometimes much better ligand binding is described by energy landscapes not just single structuresthe enterprise of structural biology has given us powerful eyes to see single structuresspecific native structures and specific bound complexesand some of the driving forces that hold them together hydrogen bonds hydrophobic interactions ion pairing and van der waals packing however what you see is not always what you get other equally important forces namely the entropies are not visible in native structures to capture both the observable and nonobservable contributions to the energetics it is important to note that binding takes place on an energy landscape exploring energy landscapes often requires modeling and computer simulations for binding the energy landscape is the free energy of the system as a function of its degrees of freedom which are many and include translational rotational conformational and solvation degrees of freedom upon binding a ligand loses translational and rotational entropyrelative to a receptor a ligand has three translational degrees of freedom x y and z directions and three orientational degrees of freedom when bound motion in these degrees of freedom becomes restricted this loss of freedom results in an entropic and free energy cost opposing binding and favoring the dissociated state chang etal chang and gilson chen etal deng and roux lee and olson and wang etal the loss of freedom depends on the mobility remaining in the binding site so that in a series of increasingly tightly bound structures there will be increasing losses in translational and rotational entropies resulting in a contribution opposing binding upon binding a ligand can lose internal freedom and entropysome simple models assume that the ligand entropy lost on binding correlates with the number of rotatable bonds in the ligand bhm gilson and zhou huey etal laederach and reilly taylor etal and gohlke and klebe the ligand is envisioned to start in its unbound state having access to all possible conformers and to end in its bound state having a single conformer however interestingly more rigorous recent computational studies in host guest systems indicate that losses in ligand conformational entropy on binding are not strongly correlated with the number of rotatable bonds chang and gilson chen etal and guimaraes and cardozo recent work on salvation free energies of small molecules has led to similar conclusions mobley etal not all small molecule conformers are populated equally in solution thus computing accurate ligand affinities and entropy losses requires more accurate treatments of the different ligand populations in solution entropic contributions can also vary between different conformations of the same ligand in a particular receptor chen etal and gilson and zhou which may be important even for docking ruvinsky and kozintsev a ligand can bind to a receptor in different posesa ligand can sometimes adopt multiple different conformations or orientations upon binding see these different poses can be separated by energetic barriers in some cases the different poses are due to ligand or protein symmetries hiv protease is a dimer with a nearly symmetric active site as a result many hiv protease inhibitors have two nearly identically binding modes e g see protein data bank pdb codes and ligand symmetries can lead to trivial cases of multiple binding modes which have significant entropic implications multiple binding modes are observed also when symmetries do not play a role computational studies show multiple distinct ligand binding modes in binding sites in lysozyme mobley etal and mobley etal neutrophil elastase steinbrecher etal estrogen receptor inhibitors oostenbrink and van gunsteren fkbp inhibitors jayachandran etal biotin and streptavidin lazaridis etal and cytochrome paulsen and ornstein do experimental studies support these predictions of multiple ligand conformations the challenge is that multiple conformers are difficult to determine experimentally but there is at least some direct crystallographic evidence suggesting multiple relevant orientations in lysozyme graves etal mobley etal and mobley etal influenza neuraminidase stoll etal and possibly in trypsin stubbs etal where the binding mode is affected by ph multiple binding modes of fragment like kinase inhibitors have also been observed constantine etal multiple orientations or binding modes have also been seen in thymidylate synthase montfort etal in the binding of an hiv cell entry inhibitor to the core of hiv zhou etal the binding of a transition state analog to an ampc beta lactamase mutant chen etal the binding of thiocamphor to cytochrome raag and poulos the binding of flavin to para hydroxybenzoate hydrolase gatti etal and in the binding of some hiv protease inhibitors murthy etal there is additional evidence for multiple orientations in several other cases birdsall etal bhm and klebe lazaridis etal mewshaw etal orville etal and uytterhoeven etal spectroscopic data deng etal and studies of drastically different binding modes of related inhibitors stout etal badger etal bhm and klebe kim kim pei etal reich etal and stoll etal some of which may have multiple binding modes montfort etal suggest that multiple binding modes may be relatively common proteins wiggle and may have multiple conformers in both the bound and unbound statesit is not only ligands that can have multiple binding modes proteins can too mobley etal and mobley etal we refer here not to induced fit where the ligand binding event causes a change in protein conformation rather we focus on internal motions or freedom of the protein that occur either in the apo structure of the protein itself or in the complex itself comparisons of different apo structures of the same proteins show that there are some rotamer changes near binding sites even in the absence of the ligand najmanovich etal suggesting that multiple rotameric states may be relevant this is also supported by nmr data chou and bax structural data in the apolar lysozyme binding cavity suggests that helix f which borders on the binding cavity can undergo substantial motions of with little free energy cost morton and matthews various other motions occur in lysozyme as well zhang etal dhfr appears to have multiple relevant conformations both in isolation and when binding ligands and the populations are modulated by ph birdsall etal each state in the catalytic cycle appears to have at least partially occupied conformations that resemble those before or after it in the cycle boehr etal crystallographic evidence suggests multiple protein conformations due to domain motion in some cases ma etal multiple conformers are also seen in host guest binding chang and gilson and chen etal and can be critical for protein mechanisms such as enzyme catalytic motions eisenmesser etal arora and brooks henzler wildman etal and henzler wildman and kern several other studies also have provided evidence for multiple protein conformations eisenmesser etal gerstein etal min etal min etal and fragai etal strain a measure of the free energy of deformationligand binding to a protein may induce strain strain refers to an energy cost usually associated with a deformation of some sort to achieve the lowest free energy of binding in the complex the protein and or ligand may become deformed relative to its unbound state in solvent which costs energy strain computational studies in the apolar lysozyme model binding site have found that protein strain energies for a valine side chain rotamer change can be kcal mol deng and roux mobley etal and mobley etal when such strain energies are not taken into account it leads to errors in predicted binding free energies mobley etal and mobley etal computational modeling suggests that ligand strain free energies can be significant in a survey of crystallographic protein ligand complexes perola and charifson used molecular mechanics scoring functions to assess strain energies and found that roughly of ligands with rotatable bonds had strain energies more than kcal mol and overall of ligands had strain energies more than kcal mol another study computed quantum mechanical torsional potentials for a variety of pdb ligands and found that typical strain energies could be on the order of kcal mol per torsion motif amounting to roughly kcal mol for a ligand with five torsion motifs hao etal another study found for several ligands that free energy costs of restricting the ligand to the bound conformation could be a few kcal mol tirado rives and jorgensen more recent work suggests that these values could be overestimates of thetrue strain as crystal structures from which strain is estimated may be refined with a different force field than is used in estimating the strain introducing artifacts nevertheless strain energies often appear to be greater than several kt huang etal see also warren and perola s warren and perola presentation on the topic from openeye s cup meeting http www eyesopen com about events cups pdfs cup field of extremes pdf apparently binding interactions can be strong enough to pay a substantial strain price for deforming one or both partners hence the true bound structure of a complex will not be the one that maximizes the interaction energy between the receptor and ligand but rather the one that best balances the tradeoff between gaining additional favorable interactions while also inducing strain sharp some experiments support this contention that strain free energies can be substantial in an nmr study on maltose binding protein tang etal found that the unbound protein was predominantly roughly in the open apo conformation and had a smaller roughly population in a minor apo conformation that was more like the holo conformation but with no evidence that it populated the holo conformation at all in the absence of the ligand thus the minor apo conformation is roughly kcal mol less favorable than the major apo conformation and the holo conformation is probably still more unfavorable in another instance in ntrcr a conformational switch in bacteria that undergoes a conformational change upon phosphorylation the active conformation has been shown to be partially populated even when the protein is unphosphorylated volkman etal but with a smaller population based on the populations this active like conformation is about kcal mol less favorable than the norm in active conformation so functional protein conformational changes can make significant contributions to the thermodynamics ligand binding can cause conformational change in protein structureswhen a ligand binds to a protein it causes conformational changes in the protein this may or may not be accompanied by strain in the protein as strain is an invisible energy cost ligand induced protein conformational changes are not rare events comparisons of apo and holo structures from the pdb show that backbone conformational motions on ligand binding are relatively common of binding residues gutteridge and thornton and of binding sites najmanovich etal across a variety of proteins have backbone c motions more than and of binding site residues have side chain motions of more than gutteridge and thornton whereas only of binding sites have been shown to undergo no side chain rotamer changes najmanovich etal more anecdotal reports of conformational changes on ligand binding are available for a wide range of systems kinases for example are notoriously flexible noble etal vajpai etal and weisberg etal as are many other proteins bhm and klebe kim meiler and baker and teague an extreme example may be natively disordered proteins in which large parts of the protein may become ordered upon interacting with binding partners hilser and thompson radivojac etal and wright and dyson in addition a given protein can adopt different conformations for different ligands a pdb study of binding site pairs each pair consisting of two structures of the same protein with different similar ligands in the binding site showed that in of the cases there were significant conformational changes in the binding sites between pair members bostrom etal changes were judged significant if the rmsd for all side chain atoms if at least one amino acid residue within of the ligand is greater than the most frequent differences were changes in water architecture and side chain conformation both occurring in over of the pairs significant backbone conformational changes occurred in only of the set changes were judged significant if the rmsd of at least one backbone heavy atom in three or more consecutive amino acids is more than a smaller study found examples of substantial conformational changes on binding similar ligands for a variety of systems as well kim it is even possible for a single ligand to bind to different protein conformations under different solution conditions miller and dill thus changes in binding site architecture at least at the side chain and water level should be regarded as the rule rather than the exception small changes in conformation can cause big changes in binding affinitiessome computational studies predict that even when a binding site structure is not perturbed very much its energetics can change substantially for example simulations show that neglecting even small protein motions can lead to large errors rms errors relative to experiment of nearly kcal mol when protein motions are not allowed relative to much smaller errors kcal mol rms when protein motion is allowed even small relaxations of the protein reduced the rms errors to kcal mol mobley etal this is important for both conceptual and practical reasons conceptually it means the strength or quality of binding interactions is sensitive to minute details of the bound structure and is not easily assessed by simple metrics like hydrogen bond counts or apparent fit practically it means that free energy methods that include these protein conformational changes can potentially have much higher accuracy than docking methods that neglect them the conclusion that these small changes can make big differences in the energetics is supported by a variety of docking and re scoring studies that have looked at the effects of introducing small amounts of protein flexibility though scores do not necessarily improve for all ligands they do typically change substantially showing some improvement graves etal huang etal meiler and baker sousa etal and wei etal but introducing protein flexibility without accounting for protein strain energies can potentially increase false positive rates by making binding sites too permissive graves etal sousa etal and wei etal this likely highlights the role of conformational change and strain differences in solvation can contribute to binding affinitiesseveral detailed binding free energy studies have suggested that differences in solvation may play an important role in differences in binding free energy between relatively similar compounds jiao etal and reddy and erion two molecules might have similar interactions with a protein similar strain energies etc but have different solvation properties in water leading tosolvation driven differences in binding free energies these differences may not always be intuitive for example the n methylacetamide amine problem rizzo and jorgensen suggests that adding a hydrophobic methyl group to acetamide or ammonia increases the affinity for water whereas subsequent methylations decrease the affinity the importance of solvation and desolvation is supported by an emerging trend toward including approximate estimates of solvation desolvation energies in approximate docking methods for scoring protein ligand binding including such estimates appears to result in improved scoring ferrara etal gilson and zhou and shoichet etal without these contributions charged ligands can wrongly appear to bind better than polar ligands in a polar binding site a charged ligand may make favorable electrostatic interactions in a polar binding site but it also costs a huge amount of energy to remove it from water brenk etal gilson and zhou and shoichet etal in other cases a small modification to a ligand can potentially lead to affinity gains due to a change in the desolvation cost kangas and tidor bound waters usually contribute favorably to ligand binding but not always and their contributions are highly variablecomputer simulations have been used to study the role of crystallographic waters in binding thermodynamics barillari etal hamelberg and mccammon lu etal olano and rick and zhang and hermans see also helms and wade helms and wade and helms and wade for desolvation of a buried binding cavity in many cases binding or ordering of waters occurs concurrently with ligand binding so it can be extremely difficult to experimentally assess the contribution of water binding to overall binding thermodynamics computational methods can directly compute the free energy of inserting or removing a water molecule from a binding site providing key insight that is hard to obtain experimentally these computational studies indicate that bound waters contribute substantially to binding free energies contributing as much as kcal mol for some waters barillari etal but smaller values between and kcal mol are more typical barillari etal hamelberg and mccammon lu etal olano and rick and zhang and hermans in some cases crystallographic waters appear substantially unfavorable relative to bulk raising the possibility of problems with refinement or force fields barillari etal and olano and rick perhaps ligands can be designed with improved affinities by recognizing nearby sites where waters can be easily displaced abel etal and pan etal sometimes ligand binding can involve concerted reordering of many water molecules in some hydrophobic sites in proteins that bind fatty acids or lipids whole networks of more than a half dozen water molecules shift their structures to form a hydrophobic interface with the ligand lalonde etal and sulsky etal protonation states can change on binding influencing affinitybinding free energies can also be affected by other unseen and unexpected factors for example protonation states can change on binding czodrowski etal dullweber etal gohlke and klebe and steuber etal as can tautomeric states pospisil etal and other factors in some cases multiple protonation or tautomeric states can be relevant as observed crystallographically for one inhibitor furet etal and hypothesized in another instance lee etal similar ligands may also adopt different protonation states on binding dullweber etal perspectivewe have reviewed some recent computational studies of ligand binding to proteins ultimately to predict accurate binding affinities it will be necessary to go beyond predicting a single dominant conformation of the ligand complexed with the protein binding free energy is not driven by a single conformation but rather by the free energy landscape it is the shape of the energy landscape that is crucial the shape and width of the minima influences entropies entropies are key contributors to binding thermodynamics and are not observable in single bound structures other factors about the full landscape also play key roles such as multiple ligand poses protein conformations strain energies changes in water structure and solvation and protonation all play roles and none of these are observable in single structures computational tools can help provide insight into the unseen landscape so those doing crystallographic studies may want to complement their work by using computational tools to explore this landscape and those relying on crystallographic data e g in a drug design context should be aware that there are various binding possibilities that might not be captured in a single crystal structure acknowledgmentswe thank scott p brown abbott laboratories sarah boyce university of california san francisco john van drie and john d chodera stanford university for help with references sarah boyce eric manas glaxosmithkline and gabe rocklin university of california san francisco for comments on the manuscript and sarina bromberg for preparing figures we acknowledge financial support of the national institutes of health grant to k d
the first step in gene expression transcription is modulated by the interaction of transcription factors with their corresponding binding sites on the dna sequence pscan is a software tool that scans a set of sequences e g promoters from co regulated or co expressed genes with motifs describing the binding specificity of known transcription factors and assesses which motifs are significantly over or under represented providing thus hints on which transcription factors could be common regulators of the genes studied together with the location of their candidate binding sites in the sequences pscan does not resort to comparisons with orthologous sequences and experimental results show that it compares favorably to other tools for the same task in terms of false positive predictions and computation time the website is free and open to all users and there is no login requirement address http www beaconlab pscan
the modern science of networks has brought significant advances to our understanding of complex systems one of the most relevant features of graphs representing real systems is community structure or clustering i e the organization of vertices in clusters with many edges joining vertices of the same cluster and comparatively few edges joining vertices of different clusters such clusters or communities can be considered as fairly independent compartments of a graph playing a similar role like e g the tissues or the organs in the human body detecting communities is of great importance in sociology biology and computer science disciplines where systems are often represented as graphs this problem is very hard and not yet satisfactorily solved despite the huge effort of a large interdisciplinary community of scientists working on it over the past few years we will attempt a thorough exposition of the topic from the definition of the main elements of the problem to the presentation of most methods developed with a special focus on techniques designed by statistical physicists from the discussion of crucial issues like the significance of clustering and how methods should be tested and compared against each other to the description of applications to networks
motivation there is a growing interest in improving the cluster analysis of expression data by incorporating into it prior knowledge such as the gene ontology go annotations of genes in order to improve the biological relevance of the clusters that are subjected to subsequent scrutiny the structure of the go is another source of background knowledge that can be exploited through the use of semantic similarity results we propose here a novel algorithm that integrates semantic similarities derived from the ontology structure into the procedure of deriving clusters from the dendrogram constructed during expression based hierarchical clustering our approach can handle the multiple annotations from different levels of the go hierarchy which most genes have moreover it treats annotated and unannotated genes in a uniform manner consequently the clusters obtained by our algorithm are characterized by significantly enriched annotations in both cross validation tests and when using an external index such as protein protein interactions our algorithm performs better than previous approaches when applied to human cancer expression data our algorithm identifies among others clusters of genes related to immune response and glucose metabolism these clusters are also supported by protein protein interaction data contact dotna cs bgu ac il supplementary information supplementary data are available at online
since darwin intergroup hostilities have figured prominently in explanations of the evolution of human social behavior yet whether ancestral humans were largely peaceful or warlike remains controversial i ask a more precise question if more cooperative groups were more likely to prevail in conflicts with other groups was the level of intergroup violence sufficient to influence the evolution of human social behavior using a model of the evolutionary impact of between group competition and a new data set that combines archaeological evidence on causes of death during the late pleistocene and early holocene with ethnographic and historical reports on hunter gatherer populations i find that the estimated level of mortality in intergroup conflicts would have had substantial effects allowing the proliferation of group beneficial behaviors that were quite costly to the individual science
monte carlo is a versatile and frequently used tool in statistical physics and beyond correspondingly the number of algorithms and variants reported in the literature is vast and an overview is not easy to achieve in this pedagogical review we start by presenting the probabilistic concepts which are at the basis of the monte carlo method from these concepts the relevant free parameterswhich still may be adjustedare identified having identified these parameters most of the tangled mass of methods and algorithms in statistical physics monte carlo can be regarded as realizations of merely a handful of basic strategies which are employed in order to improve convergence of a monte carlo computation once the notations introduced are available many of the most widely used monte carlo methods and algorithms can be formulated in a few lines in such a formulation the core ideas are exposed and possible generalizations of the methods are less obscured by the details of a algorithm
the origins of modern human behavior are marked by increased symbolic and technological complexity in the archaeological record in western eurasia this transition the upper paleolithic occurred about years ago but many of its features appear transiently in southern africa about years earlier we show that demography is a major determinant in the maintenance of cultural complexity and that variation in regional subpopulation density and or migratory activity results in spatial structuring of cultural skill accumulation genetic estimates of regional population size over time show that densities in early upper paleolithic europe were similar to those in sub saharan africa when modern behavior first appeared demographic factors can thus explain geographic variation in the timing of the first appearance of modern behavior without invoking increased cognitive science
over the past four decades the predominant view of molecular evolution saw little connection between natural selection and genome evolution assuming that the functionally constrained fraction of the genome is relatively small and that adaptation is sufficiently infrequent to play little role in shaping patterns of variation within and even between species recent evidence from drosophila reviewed here suggests that this view may be invalid analyses of genetic variation within and between species reveal that much of the drosophila genome is under purifying selection and thus of functional importance and that a large fraction of coding and noncoding differences between species are adaptive the findings further indicate that in drosophila adaptations may be both common and strong enough that the fate of neutral mutations depends on their chance linkage to adaptive mutations as much as on the vagaries of genetic drift the emerging evidence has implications for a wide variety of fields from conservation genetics to bioinformatics and presents challenges to modelers and alike
background the growth rate of an organism is an important phenotypic trait directly affecting its ability to survive in a given environment here we present the first large scale computational study of the association between ecological strategies and growth rate across bacterial species occupying a variety of metabolic habitats genomic data are used to reconstruct the species metabolic networks and habitable metabolic environments these reconstructions are then used to investigate the typical ecological strategies taken by organisms in terms of two basic species specific measures metabolic variability the ability of a species to survive in a variety of different environments and co habitation score vector the distribution of other species that co inhabit each environment results we find that growth rate is significantly correlated with metabolic variability and the level of co habitation that is competition encountered by an organism most bacterial organisms adopt one of two main ecological strategies a specialized niche with little co habitation associated with a typically slow rate of growth or ecological diversity with intense co habitation associated with a typically fast rate of growth conclusions the pattern observed suggests a universal principle where metabolic flexibility is associated with a need to grow fast possibly in the face of competition this new ability to produce a quantitative description of the growth rate metabolism community relationship lays a computational foundation for the study of a variety of aspects of the communal life
summary due to the availability of new sequencing technologies we are now increasingly interested in sequencing closely related strains of existing finished genomes recently a number of de novo and mapping based assemblers have been developed to produce high quality draft genomes from new sequencing technology reads new tools are necessary to take contigs from a draft assembly through to a fully contiguated genome sequence abacas is intended as a tool to rapidly contiguate align order orientate visualize and design primers to close gaps on shotgun assembled contigs based on a reference sequence the input to abacas is a set of contigs which will be aligned to the reference genome ordered and orientated visualized in the act comparative browser and optimal primer sequences are automatically generated availability and implementation abacas is implemented in perl and is freely available for download from http abacas net
summary here we present a method for estimating the frequencies of snp alleles present within pooled samples of dna using high throughput short read sequencing the method was tested on real data from six strains of the highly monomorphic pathogen salmonella paratyphi a sequenced individually and in a pool a variety of read mapping and quality weighting procedures were tested to determine the optimal parameters which afforded ge sensitivity of snp detection and strong correlation with true snp frequency at poolwide read depth of declining only slightly at read depths availability the method was implemented in perl and relies on the opensource software maq for read mapping and snp calling the perl script is freely available from ftp ftp sanger ac uk pub pathogens pools contact sanger ac uk supplementary information supplementary data are available at bioinformatics bioinformatics
summary is a significantly improved version of the short oligonucleotide alignment program that both reduces computer memory usage and increases alignment speed at an unprecedented rate we used a burrows wheeler transformation bwt compression index to substitute the seed strategy for indexing the reference sequence in the main memory we tested it on the whole human genome and found that this new algorithm reduced memory usage from to gb and improved alignment speed by times is compatible with both single and paired end reads additionally this tool now supports multiple text and compressed file formats a consensus builder has also been developed for consensus assembly and snp detection from alignment of short reads on a reference genome availability http soap genomics org cncontact soap genomics cn
designing and conducting experiments are routine practices for modern biologists the real challenge especially in the post genome era usually comes not from acquiring data but from subsequent activities such as data processing analysis knowledge generation and gaining insight into the research question of interest the approach of inferring gene regulatory networks grns has been flourishing for many years and new methods from mathematics information science engineering and social sciences have been applied we review different kinds of computational methods biologists use to infer networks of varying levels of accuracy and complexity the primary concern of biologists is how to translate the inferred network into hypotheses that can be tested with real life experiments taking the biologists viewpoint we scrutinized several methods for predicting grns in mammalian cells and more importantly show how the power of different knowledge databases of different types can be used to identify modules and subnetworks thereby reducing complexity and facilitating the generation of testable bib
metazoan transcription regulation occurs through the concerted action of multiple transcription factors that bind co operatively to cis regulatory modules crms the annotation of these key regulators of transcription is lagging far behind the annotation of the transcriptome itself here we give an overview of existing computational methods to detect these crms in metazoan genomes we subdivide these methods into three classes crm scanners screen sequences for crms based on predefined models that often consist of multiple position weight matrices pwms crm builders construct models of similar crms controlling a set of co regulated or co expressed genes crm genome screeners screen sequences or complete genomes for crms as homotypic or heterotypic clusters of binding sites for any combination of transcription factors we believe that crm scanners are currently the most advanced methods although their applicability is limited finally we argue that crm builders that make use of pwm libraries will benefit greatly from future advances and will prove to be most instrumental for the annotation of regulatory regions in metazoan bib
motivation clustering medline documents is usually conducted by the vector space model which computes the content similarity between two documents by basically using the inner product of their word vectors recently the semantic information of mesh medical subject headings thesaurus is being applied to clustering medline documents by mapping documents into mesh concept vectors to be clustered however current approaches of using mesh thesaurus have two serious limitations first important semantic information may be lost when generating mesh concept vectors and second the content information of the original text has been discarded methods our new strategy includes three key points first we develop a sound method for measuring the semantic similarity between two documents over the mesh thesaurus second we combine both the semantic and content similarities to generate the integrated similarity matrix between documents third we apply a spectral approach to clustering documents over the integrated similarity matrix results using various datasets of medline records we conduct extensive experiments with changing alternative measures and parameters experimental results show that integrating the semantic and content similarities outperforms the case of using only one of the two similarities being statistically significant we further find the best parameter setting that is consistent over all experimental conditions conducted we finally show a typical example of resultant clusters confirming the effectiveness of our strategy in improving medline document clustering contact zhushanfeng gmail com supplementary information supplementary data are available at bioinformatics bioinformatics
as the number of neuroscience databases increases the need for neuroscience data integration grows this paper reviews and compares several approaches including the neuroscience database gateway ndg neuroscience information framework nif and entrez neuron which enable neuroscience database annotation and integration these approaches cover a range of activities spanning from registry discovery and integration of a wide variety of neuroscience data sources they also provide different user interfaces for browsing querying and displaying query results in entrez neuron for example four different facets or tree views neuron neuronal property gene and drug are used to hierarchically organize concepts that can be used for querying a collection of ontologies the facets are also used to define the structure of the query bib
summary the sequence alignment map sam format is a generic alignment format for storing read alignments against reference sequences supporting short and long reads up to mbp produced by different sequencing platforms it is flexible in style compact in size efficient in random access and is the format in which alignments from the genomes project are released samtools implements various utilities for post processing alignments in the sam format such as indexing variant caller and alignment viewer and thus provides universal tools for processing read alignments availability http samtools sourceforge netcontact rd sanger uk
proper development and functioning of an organism depends on precise spatial and temporal expression of all its genes these coordinated expression patterns are maintained primarily through the process of transcriptional regulation transcriptional regulation is mediated by proteins binding to regulatory elements on the dna in a combinatorial manner where particular combinations of transcription factor binding sites establish specific regulatory codes in this review we survey experimental and computational approaches geared towards the identification of proximal and distal gene regulatory elements in the genomes of complex eukaryotes available approaches that decipher the genetic structure and function of regulatory elements by exploiting various sources of information like gene expression data chromatin structure dna binding specificities of transcription factors cooperativity of transcription factors etc are highlighted we also discuss the relevance of regulatory elements in the context of human health through examples of mutations in some of these regions having serious implications in misregulation of genes and being strongly associated with human bfgp
the advent of high throughput dna sequencers has increased the pace of collecting enormous amounts of genomic information yielding billions of nucleotides on a weekly basis this advance represents an improvement of two orders of magnitude over traditional sanger sequencers in terms of the number of nucleotides per unit time allowing even small groups of researchers to obtain huge volumes of genomic data over fairly short period consequently a pressing need exists for the development of personalized genome browsers for analyzing these immense amounts of locally stored data the utgb university of tokyo genome browser toolkit is designed to meet three major requirements for personalization of genome browsers easy installation of the system with minimum efforts browsing locally stored data and rapid interactive design of web interfaces tailored to individual needs the utgb toolkit is licensed under an open source license availability the software is freely available at http org
summary arrayexpress is one of the largest public repositories of microarray datasets r bioconductor provides a comprehensive suite of microarray analysis and integrative bioinformatics software however easy ways for importing datasets from arrayexpress into r bioconductor have been lacking here we present such a tool that is suitable for both interactive and automated use availability the arrayexpress package is available from the bioconductor project at http www bioconductor org a users guide and examples are provided with the package contact audrey ebi ac uksupplementary information supplementary data are available online
macrophages are versatile immune cells that can detect a variety of pathogen associated molecular patterns through their toll like receptors tlrs in response to microbial challenge the tlr stimulated macrophage undergoes an activation program controlled by a dynamically inducible transcriptional regulatory network mapping a complex mammalian transcriptional network poses significant challenges and requires the integration of multiple experimental data types in this work we inferred a transcriptional network underlying tlr stimulated murine macrophage activation microarray based expression profiling and transcription factor binding site motif scanning were used to infer a network of associations between transcription factor genes and clusters of co expressed target genes the time lagged correlation was used to analyze temporal expression data in order to identify potential causal influences in the network a novel statistical test was developed to assess the significance of the time lagged correlation several associations in the resulting inferred network were validated using targeted chip on chip experiments the network incorporates known regulators and gives insight into the transcriptional control of macrophage activation our analysis identified a novel regulator that may have a role in activation
since windschitl first outlined a research agenda for the world wide web and classroom research significant shifts have occurred in the nature of the web and the conceptualization of classrooms such shifts have affected constructs of learning and instruction and paths for future research this article discusses the characteristics of web that differentiate it from the web of the describes the contextual conditions in which students use the web today and examines how web unique capabilities and youths proclivities in using it influence learning and teaching two important themes learner participation and creativity and online identity formation emerged from this analysis and support a new wave of research questions a stronger research focus on students everyday use of web technologies and their learning with web both in and outside of classrooms is needed finally insights on how educational scholarship might be transformed with web in light of these themes discussed
background web internet tools and methods have attracted considerable attention as a means to improve health care delivery despite evidence demonstrating their use by medical professionals there is no detailed research describing how web influences physicians daily clinical practice hence this study examines web use by junior physicians in clinical settings to further understand their impact on medical practice method diaries and interviews encompassing days of internet use or search incidents analyzed via thematic analysis results results indicate that of internet visits employed user generated or web content with google and wikipedia used by and of physicians respectively despite awareness of information credibility risks with web content it has a role in information seeking for both clinical decisions and medical education this is enabled by the ability to cross check information and the diverse needs for background and non verified information conclusion web use represents a profound departure from previous learning and decision processes which were normally controlled by senior medical staff or medical schools there is widespread concern with the risk of poor quality information with web use and the manner in which physicians are using it suggest effective use derives from the mitigating actions by the individual physician three alternative policy options are identified to manage this risk and improve efficiency in web s use copyright elsevier ireland ltd all reserved
periodicity in materials yields interesting and useful phenomena applied to the propagation of light periodicity gives rise to photonic which can be precisely engineered for such applications as guiding and dispersing optical tightly confining and trapping light and enhancing nonlinear optical photonic crystals can also be formed into planar lightwave circuits for the integration of optical and electrical in a photonic crystal the periodicity of the host medium is used to manipulate the properties of light whereas a phononic crystal uses periodicity to manipulate mechanical as has been demonstrated in studies of raman like scattering in epitaxially grown vertical cavity and photonic crystal the simultaneous confinement of mechanical and optical modes in periodic structures can lead to greatly enhanced lightmatter interactions a logical next step is thus to create planar circuits that act as both photonic and phononic optomechanical crystals here we describe the design fabrication and characterization of a planar silicon chip based optomechanical crystal capable of co localizing and strongly coupling terahertz photons and gigahertz phonons these planar optomechanical crystals bring the powerful techniques of optics and photonic crystals to bear on phononic crystals providing exquisitely sensitive near quantum limited optical measurements of mechanical vibrations while simultaneously providing strong nonlinear interactions for optics in a large and technologically relevant range frequencies
across the world energy planners and transmission system operators are faced with decisions on how to deal with challenges associated with high penetration levels of intermittent energy resources and combined heat and power chp at the same time distributed plant operators are eager to reduce uncertainties related to fuel and electricity price fluctuations these interests meet up for options in distributed supply that introduces the principle of storage and relocation typically by integrating heat pumps hp or electric boilers ebs into the operational strategies of existing chp plants this paper introduces the principle of storage and relocation by energy system design and proposes for the storage and relocation potential of a technology option to be found by comparing options by their storage and relocation coefficient r c defined as the statistical correlation between net electricity exchange between plant and grid and the electricity demand minus intermittent renewable electricity production detailed operational analyses made for various chp options within the west danish energy system point to the concepts of chp hp and chp hp cold storage for effectively increasing energy system flexibility for chp hp cold storage r c increases from to while the plant s fuel efficiency increases from to these findings are discussed within frameworks of renewable energy systems suggesting principles for and generation designs
abstract background the enteropathogen resource integration center eric www ericbrc org has a goal of providing bioinformatics support for the scientific community researching enteropathogenic bacteria such as escherichia coli and salmonella spp rapid and accurate identification of experimental conclusions from the scientific literature is critical to support research in this field natural language processing nlp and in particular information extraction ie technology can be a significant aid to this process description eric has trained a powerful state of the art ie technology on a corpus of abstracts from the microbial literature in pubmed to automatically identify and categorize biologically relevant entities and predicative relations these relations include genes gene products and their roles gene mutations and the resulting phenotypes and organisms and their associated pathogenicity evaluations on blind datasets show an f measure average of greater than for entities genes operons etc and over for relations gene gene product to role etc this ie capability combined with text indexing and relational database technologies constitute the core of eric s recently deployed text mining application conclusions the eric text mining application was recently launched online on the eric website http www ericbrc org portal eric articles the information retrieval interface displays a list of recently published enteropathogen literature abstracts and also provides a search interface to execute custom queries by keyword date range etc upon selection processed abstracts and the entities and relations extracted from them are retrieved from a relational database and marked up to highlight the entities and relations the abstract also provides links from extracted genes and gene products to the eric annotations database thus providing access to comprehensive genomic annotations and adding value to both the text mining and systems
we have developed an online catalog of snp trait associations from published genome wide association studies for use in investigating genomic characteristics of trait disease associated snps tass reported tass were common median risk allele frequency interquartile range iqr and were associated with modest effect sizes median odds ratio or iqr among genomic annotation sets reported tass were significantly overrepresented only in nonsynonymous sites or p x and promoter regions or p x compared to snps randomly selected from genotyping arrays although of tass were intronic or intergenic tass were not overrepresented in introns and were significantly depleted in intergenic regions or p x only slightly more tass than expected by chance were predicted to be in regions under positive selection or p this new online resource together with bioinformatic predictions of the underlying functionality at trait disease associated loci is well suited to guide future investigations of the role of common variants in complex etiology
background metagenomics is the study of the genomic content of an environmental sample of microbes advances in the through put and cost efficiency of sequencing technology is fueling a rapid increase in the number and size of metagenomic datasets being generated bioinformatics is faced with the problem of how to handle and analyze these datasets in an efficient and useful way one goal of these metagenomic studies is to get a basic understanding of the microbial world both surrounding us and within us one major challenge is how to compare multiple datasets furthermore there is a need for bioinformatics tools that can process many large datasets and are easy to use results this article describes two new and helpful techniques for comparing multiple metagenomic datasets the first is a visualization technique for multiple datasets and the second is a new statistical method for highlighting the differences in a pairwise comparison we have developed implementations of both methods that are suitable for very large datasets and provide these in version of our standalone metagenome analysis tool megan conclusion these new methods are suitable for the visual comparison of many large metagenomes and the statistical comparison of two metagenomes at a time nevertheless more work needs to be done to support the comparative analysis of multiple metagenome datasets availability version of megan which implements all ideas presented in this article can be obtained from our web site at www ab informatik uni tuebingen de software megan contact mitra informatik uni tuebingen de supplementary information supplementary data are available at bioinformatics bioinformatics
motivation chromatin states are the key to gene regulation and cell identity chromatin immunoprecipitation chip coupled with high throughput sequencing chip seq is increasingly being used to map epigenetic states across genomes of diverse species chromatin modification profiles are frequently noisy and diffuse spanning regions ranging from several nucleosomes to large domains of multiple genes much of the early work on the identification of chip enriched regions for chip seq data has focused on identifying localized regions such as transcription factor binding sites bioinformatic tools to identify diffuse domains of chip enriched regions have been lacking results based on the biological observation that histone modifications tend to cluster to form domains we present a method that identifies spatial clusters of signals unlikely to appear by chance this method pools together enrichment information from neighboring nucleosomes to increase sensitivity and specificity by using genomic scale analysis as well as the examination of loci with validated epigenetic states we demonstrate that this method outperforms existing methods in the identification of chip enriched signals for histone modification profiles we demonstrate the application of this unbiased method in important issues in chip seq data analysis such as data normalization for quantitative comparison of levels of epigenetic modifications across cell types and growth conditions availability http home gwu edu approximately wpeng software htm supplementary information supplementary data are available at online
we propose a fully homomorphic encryption scheme i e a scheme that allows one to evaluate circuits over encrypted data without being able to decrypt our solution comes in three steps first we provide a general result that to construct an encryption scheme that permits evaluation of arbitrary circuits it suffices to construct an encryption scheme that can evaluate slightly augmented versions of its own decryption circuit we call a scheme that can evaluate its augmented decryption bootstrappable
networks of relationships help determine the careers that people choose the jobs they obtain the products they buy and how they vote the many aspects of our lives that are governed by social networks make it critical to understand how they impact behavior which network structures are likely to emerge in a society and why we organize ourselves as we do in and economic matthew jackson offers a comprehensive introduction to social and economic networks drawing on the latest findings in economics sociology computer science physics and mathematics he provides empirical background on networks and the regularities that they exhibit and discusses random graph based models and strategic models of network formation he helps readers to understand behavior in networked societies with a detailed analysis of learning and diffusion in networks decision making by individuals who are influenced by their social neighbors game theory and markets on networks and a host of related subjects jackson also describes the varied statistical and modeling techniques used to analyze social networks each chapter includes exercises to aid students in their analysis of how networks function this book is an indispensable resource for students and researchers in economics mathematics physics sociology business
background mirnas play important roles in cellular control and in various disease states such as cancers where they may serve as markers or possibly even therapeutics identifying the whole repertoire of mirnas and understanding their expression patterns is therefore an important goal methods here we describe the analysis of pyrosequencing of small rna from four different tissues breast cancer normal adjacent breast and two teratoma cell lines we developed a pipeline for identifying new mirnas emphasizing extracting and retaining as much data as possible from even noisy sequencing data we investigated differential expression of mirnas in the breast cancer and normal adjacent breast samples and systematically examined the mature sequence end variability of mirna compared to non mirna loci results we identified five novel mirnas as well as two putative alternative precursors for known mirnas several mirnas were differentially expressed between the breast cancer and normal breast samples the end variability was shown to be significantly different between mirna and non mirna loci conclusion pyrosequencing of small rnas together with a computational pipeline can be used to identify mirnas in tumor and other tissues measures of mirna end variability may in the future be incorporated into the discovery pipeline as a discriminatory feature breast cancer samples show a distinct mirna expression profile compared to normal breast
rates of evolution differ widely among proteins but the causes and consequences of such differences remain under debate with the advent of high throughput functional genomics it is now possible to rigorously assess the genomic correlates of protein evolutionary rate however dissecting the correlations among evolutionary rate and these genomic features remains a major challenge here we use an integrated probabilistic modeling approach to study genomic correlates of protein evolutionary rate in saccharomyces cerevisiae we measure and rank degrees of association between i an approximate measure of protein evolutionary rate with high genome coverage and ii a diverse list of protein properties sequence structural functional network and phenotypic we observe among many statistically significant correlations that slowly evolving proteins tend to be regulated by more transcription factors deficient in predicted structural disorder involved in characteristic biological functions such as translation biased in amino acid composition and are generally more abundant more essential and enriched for interaction partners many of these results are in agreement with recent studies in addition we assess information contribution of different subsets of these protein properties in the task of predicting slowly evolving proteins we employ a logistic regression model on binned data that is able to account for intercorrelation non linearity and heterogeneity within features our model considers features both individually and in natural ensembles meta features in order to assess joint information contribution and degree of contribution independence meta features based on protein abundance and amino acid composition make strong partially independent contributions to the task of predicting slowly evolving proteins other meta features make additional minor contributions the combination of all meta features yields predictions comparable to those based on paired species comparisons and approaching the predictive limit of optimal lineage insensitive features our integrated assessment framework can be readily extended to other correlational analyses at the scale
pnas mathematical models are an important tool to explain and comprehend complex phenomena and unparalleled computational advances enable us to easily explore them without any or little understanding of their global properties in fact the likelihood of the data under complex stochastic models is often analytically or numerically intractable in many areas of sciences this makes it even more important to simultaneously investigate the adequacy of these modelsin absolute terms against the data rather than relative to the performance of other modelsbut no such procedure has been formally discussed when the likelihood is intractable we provide a statistical interpretation to current developments in likelihood free bayesian inference that explicitly accounts for discrepancies between the model and the data termed pproximate ayesian omputation under odel ncertainty abc we augment the likelihood of the data with unknown error terms that correspond to freely chosen checking functions and provide monte carlo strategies for sampling from the associated joint posterior distribution without the need of evaluating the likelihood we discuss the benefit of incorporating model diagnostics within an abc framework and demonstrate how this method diagnoses model mismatch and guides model refinement by contrasting three qualitative models of protein network evolution to the protein interaction datasets of and our results make a number of model deficiencies explicit and suggest that the network topology is inconsistent with evolution dominated by link turnover or lateral gene alone
background cellular metabolism is a fundamental biological system consisting of myriads of enzymatic reactions that together fulfill the basic requirements of life the recent availability of vast amounts of sequence data from diverse sets of organisms provides an opportunity to systematically examine metabolism from a comparative perspective here we supplement existing genome and protein resources with partial genome datasets derived from eukaryotes to present a comprehensive survey of the conservation of metabolism across taxa representing the three domains of life results in general metabolic enzymes are highly conserved however organizing these enzymes within the context of functional pathways revealed a spectrum of conservation from those that are highly conserved for example carbohydrate energy amino acid and nucleotide metabolism enzymes to those specific to individual taxa for example those involved in glycan metabolism and secondary metabolite pathways applying a novel co conservation analysis kegg defined pathways did not generally display evolutionary coherence instead such modularity appears restricted to smaller subsets of enzymes expanding analyses to a global metabolic network revealed a highly conserved but nonetheless flexible core of enzymes largely involved in multiple reactions across different pathways enzymes and pathways associated with the periphery of this network were less well conserved and associated with taxon specific innovations conclusions these findings point to an emerging picture in which a core of enzyme activities involving amino acid energy carbohydrate and lipid metabolism have evolved to provide the basic functions required for life however the precise complement of enzymes associated within this core for each species flexible
motivation whole transcriptome shotgun sequencing data from non normalized samples offer unique opportunities to study the metabolic states of organisms one can deduce gene expression levels using sequence coverage as a surrogate identify coding changes or discover novel isoforms or transcripts especially for discovery of novel events de novo assembly of transcriptomes is desirable results transcriptome from tumor tissue of a patient with follicular lymphoma was sequenced with base pair bp single and paired end reads on the illumina genome analyzer ii platform we assembled approximately million reads using abyss into contigs or longer with a maximum contig length of representing over million base pairs of unique transcriptome sequence or roughly of the genome availability and implementation source code and binaries of abyss are freely available for download at http www bcgsc ca platform bioinfo software abyss assembler tool is implemented in c the parallel version uses open mpi explorer tool is implemented in java using the java universal network graph framework contact software help abyss bcgsc ca authors ibirol sjackman cydneyn jqian sjones ca
abstract background recent years have seen an increased amount of natural language processing nlp work on full text biomedical journal publications much of this work is done with open access journal articles such work assumes that open access articles are representative of biomedical publications in general and that methods developed for analysis of open access full text publications will generalize to the biomedical literature as a whole if this assumption is wrong the cost to the community will be large including not just wasted resources but also flawed science this paper examines that assumption results we collected two sets of documents one consisting only of open access publications and the other consisting only of traditional journal publications we examined them for differences in surface linguistic structures that have obvious consequences for the ease or difficulty of natural language processing and for differences in semantic content as reflected in lexical items regarding surface linguistic structures we examined the incidence of conjunctions negation passives and pronominal anaphora and found that the two collections did not differ we also examined the distribution of sentence lengths and found that both collections were characterized by the same mode regarding lexical items we found that the kullback leibler divergence between the two collections was low and was lower than the divergence between either collection and a reference corpus where small differences did exist log likelihood analysis showed that they were primarily in the area of formatting and in specific named entities conclusions we did not find structural or semantic differences between the open access and traditional journal collections research on open access full text articles should generalize to the biomedical literature as whole
gene expression analysis of microrna molecules is becoming increasingly important in this study we assess the use of the mean expression value of all expressed micrornas in a given sample as a normalization factor for microrna real time quantitative pcr data and compare its performance to the currently adopted approach we demonstrate that the mean expression value outperforms the current normalization strategy in terms of better reduction of technical variation and more accurate appreciation of changes
the recent availability of proteinprotein interaction networks for several species makes it possible to study protein complexes in an evolutionary context in this article we present a novel network based framework for reconstructing the evolutionary history of protein complexes our analysis is based on generalizing evolutionary measures for single proteins to the level of whole subnetworks comprehensively considering a broad set of computationally derived complexes and accounting for both sequence and interaction changes specifically we compute sets of orthologous complexes across species and use these to derive evolutionary rate and age measures for protein complexes we observe significant correlations between the evolutionary properties of a complex and those of its member proteins suggesting that protein complexes form early in evolution and evolve as coherent units additionally our approach enables us to directly quantify the extent to which gene duplication has played a role in the evolution of complexes we find that about one quarter of the sets of orthologous complexes have originated from evolutionary cores of homodimers that underwent duplication and divergence testifying to the important role of gene duplication in protein evolution
background transcriptome sequencing using next generation sequencing platforms will soon be competing with dna microarray technologies for global gene expression analysis as a preliminary evaluation of these promising technologies we performed deep sequencing of cdna synthesized from the microarray quality control maqc reference rna samples using roche s genome sequencer flx results we generated more that million sequence reads of average length bp for the maqc a and b samples and introduced a data analysis pipeline for translating cdna read counts into gene expression levels using blast of the reads mapped to the human genome and of the reads mapped to the refseq database of well annotated genes with e values less than or equal to we measured gene expression levels in the a and b samples by counting the numbers of reads that mapped to individual refseq genes in multiple sequencing runs to evaluate the maqc quality metrics for reproducibility sensitivity specificity and accuracy and compared the results with dna microarrays and quantitative rt pcr qrtpcr from the maqc studies in addition of the reads were successfully aligned directly to the human genome using the aceview alignment programs with an average sequence similarity to identify unique exon junctions including new exon junctions not yet contained in the refseq database conclusion using the maqc metrics for evaluating the performance of gene expression platforms the expressseq results for gene expression levels showed excellent reproducibility sensitivity and specificity that improved systematically with increasing shotgun sequencing depth and quantitative accuracy that was comparable to dna microarrays and qrtpcr in addition a careful mapping of the reads to the genome using the aceview alignment programs shed new light on the complexity of the human transcriptome including the discovery of thousands of new variants
we model a close knit community of friends and enemies as a fully connected network with positive and negative signs on its edges theories from social psychology suggest that certain sign patterns are more stable than others this notion of social balance allows us to define an energy landscape for such networks its structure is complex numerical experiments reveal a landscape dimpled with local minima of widely varying energy levels we derive rigorous bounds on the energies of these local minima and prove that they have a modular structure that can be used to them
hadoop the definitive guide helps you harness the power of your data ideal for processing large datasets the apache hadoop framework is an open source implementation of the mapreduce algorithm on which google built its empire this comprehensive resource demonstrates how to use hadoop to build reliable scalable distributed systems programmers will find details for analyzing large datasets and administrators will learn how to set up and run hadoop clusters complete with case studies that illustrate how hadoop solves specific problems this book helps you use the hadoop distributed file system hdfs for storing large datasets and run distributed computations over those datasets using mapreduce become familiar with hadoop s data and i o building blocks for compression data integrity serialization and persistence discover common pitfalls and advanced features for writing real world mapreduce programs design build and administer a dedicated hadoop cluster or run hadoop in the cloud use pig a high level query language for large scale data processing take advantage of hbase hadoop s database for structured and semi structured data learn zookeeper a toolkit of coordination primitives for building distributed systems if you have lots of data whether it s gigabytes or petabytes hadoop is the perfect solution hadoop the definitive guide is the most thorough book available on the subject now you have the opportunity to learn about hadoop from a master not only of the technology but also of common sense and plain talk doug cutting hadoop yahoo
the recent boom of large scale online social networks osns both enables and necessitates the use of parallelizable and scalable computational techniques for their analysis we examine the problem of real time community detection and a recently proposed linear timeo m on a network with m edgeslabel propagation or epidemic community detection algorithm we identify characteristics and drawbacks of the algorithm and extend it by incorporating different heuristics to facilitate reliable and multifunctional real time community detection with limited computational resources we employ the algorithm on osn data with nodes and about directed edges experiments and benchmarks reveal that the extended algorithm is not only faster but its community detection accuracy compares favorably over popular modularity gain optimization algorithms known to suffer from their limits
abstract cpe abs web approaches are revolutionizing the internet blurring lines between developers and users and enabling collaboration and social networks that scale into the millions of users as discussed in our previous work the core technologies of web effectively define a comprehensive distributed computing environment that parallels many of the more complicated service oriented systems such as web service and grid service architectures in this paper we build upon this previous work to discuss the applications of web approaches to four different scenarios client side javascript libraries for building and composing grid services integrating server side portlets with rich client ajax tools and web services for analyzing global positioning system data building and analyzing folksonomies of scientific user communities through social bookmarking and applying microformats and georss to problems in scientific metadata description and delivery copyright john wiley ltd
whole genome resequencing is still a costly method to detect genetic mutations that lead to altered forms of proteins and may be associated with disease development since the majority of disease related single nucleotide variations snvs are found in protein coding regions we propose to identify snvs in expressed exons of the human genome using the recently developed rna seq technique we identify and snvs respectively in jurkat t cells and t cells from a healthy donor interestingly our data show that one copy of the tal proto oncogene has a point mutation in utr and only the mutant allele is expressed in jurkat cells we provide a comprehensive dataset for further understanding the cancer biology of jurkat cells our results indicate that this is a cost effective and efficient strategy to systematically identify snvs in the expressed regions of the human nar
motivation a variety of probabilistic models describing the evolution of dna or protein sequences have been proposed for phylogenetic reconstruction or for molecular dating however there still lacks a common implementation allowing one to freely combine these independent features so as to test their ability to jointly improve phylogenetic and dating accuracy results we propose a software package phylobayes that can be used for conducting bayesian phylogenetic reconstruction and molecular dating analyses using a large variety of amino acid replacement and nucleotide substitution models including empirical mixtures or non parametric models as well as alternative clock relaxation processes availability phylobayes is freely available from our website http www phylobayes org it works under linux mac osx and windows operating systems contact nicolas lartillot ca
despite the successes of genomics little is known about how genetic information produces complex organisms a look at the crucial functional elements of fly and worm genomes could change that the primary objective of the human genome project was to produce high quality sequences not just for the human genome but also for those of the chief model organisms escherichia coli yeast saccharomyces cerevisiae worm caenorhabditis elegans fly drosophila melanogaster and mouse mus musculus free access to the resultant data has prompted much biological research including development of a map of common human genetic variants the international hapmap project expression profiling of healthy and diseased and in depth studies of many genes
errors in dynamic random access memory dram are a common form of hardware failure in modern compute clusters failures are costly both in terms of hardware replacement costs and service disruption while a large body of work exists on dram in laboratory conditions little has been reported on real dram failures in large production clusters in this paper we analyze measurements of memory errors in a large fleet of commodity servers over a period of years the collected data covers multiple vendors dram capacities and technologies and comprises many millions of days
autodock vina a new program for molecular docking and virtual screening is presented autodock vina achieves an approximately two orders of magnitude speed up compared with the molecular docking software previously developed in our lab autodock while also significantly improving the accuracy of the binding mode predictions judging by our tests on the training set used in autodock development further speed up is achieved from parallelism by using multithreading on multicore machines autodock vina automatically calculates the grid maps and clusters the results in a way transparent to the user wiley periodicals inc j chem
the complementarity of gene expression and protein dna interaction data led to several successful models of biological systems however recent studies in multiple species raise doubts about the relationship between these two datasets these studies show that the overwhelming majority of genes bound by a particular transcription factor tf are not affected when that factor is knocked out here we show that this surprising result can be partially explained by considering the broader cellular context in which tfs operate factors whose functions are not backed up by redundant paralogs show a fourfold increase in the agreement between their bound targets and the expression levels of those targets in addition we show that incorporating protein interaction networks provides physical explanations for knockout effects new double knockout experiments support our conclusions our results highlight the robustness provided by redundant tfs and indicate that in the context of diverse cellular systems binding is still functional
complex transcriptional behaviours are encoded in the dna sequences of gene regulatory regions advances in our understanding of these behaviours have been recently gained through quantitative models that describe how molecules such as transcription factors and nucleosomes interact with genomic sequences an emerging view is that every regulatory sequence is associated with a unique binding affinity landscape for each molecule and consequently with a unique set of molecule binding configurations and transcriptional outputs we present a quantitative framework based on existing methods that unifies these ideas this framework explains many experimental observations regarding the binding patterns of factors and nucleosomes and the dynamics of transcriptional activation it can also be used to model more complex phenomena such as transcriptional noise and the evolution of regulation
inferring an accurate evolutionary tree of life requires high quality alignments of molecular sequence data sets from large numbers of species however this task is often difficult slow and idiosyncratic especially when the sequences are highly diverged or include high rates of insertions and deletions collectively known as indels we present sate simultaneous alignment and tree estimation an automated method to quickly and accurately estimate both dna alignments and trees with the maximum likelihood criterion in our study it improved tree and alignment accuracy compared to the best two phase methods currently available for data sets of up to sequences showing that coestimation can be both rapid and accurate in studies
pnas the protein universe is the set of all proteins of all organisms here all currently known sequences are analyzed in terms of families that have single domain or multidomain architectures and whether they have a known three dimensional structure growth of new single domain families is very slow almost all growth comes from new multidomain architectures that are combinations of domains characterized by sequence profiles single domain families are mostly shared by the major groups of organisms whereas multidomain architectures are specific and account for species diversity there are known structures for a quarter of the single domain families and of all sequences can be partially modeled thanks to their membership in families
graphene is a wonder material with many superlatives to its name it is the thinnest material in the universe and the strongest ever measured its charge carriers exhibit giant intrinsic mobility have the smallest effective mass it is zero and can travel micrometer long distances without scattering at room temperature graphene can sustain current densities orders higher than copper shows record thermal conductivity and stiffness is impermeable to gases and reconciles such conflicting qualities as brittleness and ductility electron transport in graphene is described by a dirac like equation which allows the investigation of relativistic quantum phenomena in a bench top experiment what are other surprises that graphene keeps in store for us this review analyses recent trends in graphene research and applications and attempts to identify future directions in which the field is likely develop
genotype imputation methods are now being widely used in the analysis of genome wide association studies most imputation analyses to date have used the hapmap as a reference dataset but new reference panels such as controls genotyped on multiple snp chips and densely typed samples from the genomes project will soon allow a broader range of snps to be imputed with higher accuracy thereby increasing power we describe a genotype imputation method impute version that is designed to address the challenges presented by these new datasets the main innovation of our approach is a flexible modelling framework that increases accuracy and combines information across multiple reference panels while remaining computationally feasible we find that impute attains higher accuracy than other methods when the hapmap provides the sole reference panel but that the size of the panel constrains the improvements that can be made we also find that imputation accuracy can be greatly enhanced by expanding the reference panel to contain thousands of chromosomes and that impute outperforms other methods in this setting at both rare and common snps with overall error rates that are lower than those of the closest competing method one particularly challenging aspect of next generation association studies is to integrate information across multiple reference panels genotyped on different sets of snps we show that our approach to this problem has practical advantages over other solutions
micrornas mirnas have critical roles in the regulation of gene expression however as mirna activity requires base pairing with only nucleotides of messenger rna predicting target mrnas is a major challenge recently high throughput sequencing of rnas isolated by crosslinking immunoprecipitation hits clip has identified functional protein rna interaction sites here we use hits clip to covalently crosslink native argonaute ago also called protein rna complexes in mouse brain this produced two simultaneous data sets ago mirna and ago mrna binding sites that were combined with bioinformatic analysis to identify interaction sites between mirna and target mrna we validated genome wide interaction maps for mir and generated additional maps for the most abundant mirnas present in mouse brain ago hits clip provides a general platform for exploring the specificity and range of mirna action in vivo and identifies precise sequences for targeting clinically relevant mirna interactions
e research is a rapidly growing research area both in terms of publications and in terms of funding in this article we argue that it is necessary to reconceptualize the ways in which we seek to measure and understand e research by developing a sociology of knowledge based on our understanding of how science has been transformed historically and shifted into online forms next we report data which allows the examination of e research through a variety of traces in order to begin to understand how knowledge in the realm of e research has been and is being constructed these data indicate that e research has had a variable impact in different fields of research we argue that only an overall account of the scale and scope of e research within and between different fields makes it possible to identify the organizational coherence and diffuseness of e research in terms of its socio technical networks and thus to identify the contributions of e research to various research fronts in the online production of knowledge elsevier ltd all reserved
summary the shorter and vastly more numerous reads produced by second generation sequencing technologies require new tools that can assemble massive numbers of reads in reasonable time existing short read assembly tools can be classified into two categories greedy extension based and graph based while the graph based approaches are generally superior in terms of assembly quality the computer resources required for building and storing a huge graph are very high in this article we present taipan an assembly algorithm which can be viewed as a hybrid of these two approaches taipan uses greedy extensions for contig construction but at each step realizes enough of the corresponding read graph to make better decisions as to how assembly should continue we show that this approach can achieve an assembly quality at least as good as the graph based approaches used in the popular edena and velvet assembly tools using a moderate amount of computing resources availability and implementation source code in c running on linux is freely available at http taipan sourceforge net contact asbschmidt ntu edu bioinformatics
we created a visualization tool called circos to facilitate the identification and analysis of similarities and differences arising from comparisons of genomes our tool is effective in displaying variation in genome structure and generally any other kind of positional relationships between genomic intervals such data are routinely produced by sequence alignments hybridization arrays genome mapping and genotyping studies circos uses a circular ideogram layout to facilitate the display of relationships between pairs of positions by the use of ribbons which encode the position size and orientation of related genomic elements circos is capable of displaying data as scatter line and histogram plots heat maps tiles connectors and text bitmap or vector images can be created from gff style data inputs and hierarchical configuration files which can be easily generated by automated tools making circos suitable for rapid deployment in data analysis and pipelines
studies have revealed that people organize and think of their work in terms of activities that are carried out in pursuit of some overall objective often in collaboration with others nevertheless modern computer systems are typically single user oriented that is designed to support individual tasks such as word processing while sitting at a desk this article presents the concept of activity based computing abc which seeks to create computational support for human activities the abc approach has been designed to address activity based computing support for clinical work in hospitals in a hospital the challenges arising from the management of parallel activities and interruptions are amplified because multitasking is now combined with a high degree of mobility collaboration and urgency the article presents the empirical and theoretical background for activity based computing its principles the java based implementation of the abc framework and an experimental evaluation together with a group of hospital clinicians the article contributes to the growing research on support for human activities mobility collaboration and context aware computing the abc framework presents a unifying perspective on activity based support for human interaction
youtube is one of the most well known and widely discussed sites of participatory media in the contemporary online environment and it is the first genuinely mass popular platform for user created video in this timely and comprehensive introduction to how youtube is being used and why it matters burgess and green discuss the ways that it relates to wider transformations in culture society and the economy the book critically examines the public debates surrounding the site demonstrating how it is central to struggles for authority and control in the new media environment drawing on a range of theoretical sources and empirical research the authors discuss how youtube is being used by the media industries by audiences and amateur producers and by particular communities of interest and the ways in which these uses challenge existing ideas about cultural production and consumption rich with both concrete examples and featuring specially commissioned chapters by henry jenkins and john hartley the book is essential reading for anyone interested in the contemporary and future implications of online media it will be particularly valuable for students and scholars in media communication and studies
summary massively parallel sequencing technologies hold incredible promise for the study of dna sequence variation particularly the identification of variants affecting human disease the unprecedented throughput and relatively short read lengths of roche illumina solexa and other platforms have spurred development of a new generation of sequence alignment algorithms yet detection of sequence variants based on short read alignments remains challenging and most currently available tools are limited to a single platform or aligner type we present varscan an open source tool for variant detection that is compatible with several short read aligners we demonstrate varscan s ability to detect snps and indels with high sensitivity and specificity in both roche sequencing of individuals and deep illumina solexa sequencing of pooled samples availability and implementation source code and documentation freely available at http genome wustl edu tools cancer genomics implemented as a perl package and supported on linux unix ms windows and mac osx contact dkoboldt genome wustl edu supplementary information supplementary data are available at bioinformatics bioinformatics
natural habitats of some microorganisms may fluctuate erratically whereas others which are more predictable offer the opportunity to prepare in advance for the next environmental change in analogy to classical pavlovian conditioning microorganisms may have evolved to anticipate environmental stimuli by adapting to their temporal order of appearance here we present evidence for environmental change anticipation in two model microorganisms escherichia coli and saccharomyces cerevisiae we show that anticipation is an adaptive trait because pre exposure to the stimulus that typically appears early in the ecology improves the organism s fitness when encountered with a second stimulus additionally we observe loss of the conditioned response in e coli strains that were repeatedly exposed in a laboratory evolution experiment only to the first stimulus focusing on the molecular level reveals that the natural temporal order of stimuli is embedded in the wiring of the regulatory networkearly stimuli pre induce genes that would be needed for later ones yet later stimuli only induce genes needed to cope with them our work indicates that environmental anticipation is an adaptive trait that was repeatedly selected for during evolution and thus may be ubiquitous biology
abstract background the recent availability of an expanding collection of genome sequences driven by technological advances has facilitated comparative genomics and in particular the identification of synteny among multiple genomes however the development of effective and easy to use methods for identifying such conserved gene clusters among multiple genomes synteny blocks as well as databases which host synteny blocks from various groups of species especially eukaryotes and also allow users to run synteny identification programs lags behind descriptions orthoclusterdb is a new online platform for the identification and visualization of synteny blocks orthoclusterdb consists of two key web pages run orthocluster and view synteny the run orthocluster page serves as web front end to orthocluster a recently developed program for synteny block detection run orthocluster offers full control over the functionalities of orthocluster such as specifying synteny block size considering order and strandedness of genes within synteny blocks including or excluding nested synteny blocks handling one to many orthologous relationships and comparing multiple genomes in contrast the view synteny page gives access to perfect and imperfect synteny blocks precomputed for a large number of genomes without the need for users to retrieve and format input data additionally genes are cross linked with public databases for effective browsing for both run orthocluster and view synteny identified synteny blocks can be browsed at the whole genome chromosome and individual gene level orthoclusterdb is freely accessible conclusions we have developed an online system for the identification and visualization of synteny blocks among multiple genomes the system is freely available at http genome sfu orthoclusterdb
published estimates of the proportion of positively selected genes psgs in human vary over three orders of magnitude in mammals estimates of the proportion of psgs cover an even wider range of values we used orthologous protein coding genes from human chimpanzee macaque dog cow rat and mouse as well as an established phylogenetic topology to infer the fraction of psgs in all seven terminal branches the inferred fraction of psgs ranged from in human through in macaque to in dog we found three factors that influence the fraction of genes that exhibit telltale signs of positive selection the quality of the sequence the degree of misannotation and ambiguities in the multiple sequence alignment the inferred fraction of psgs in sequences that are deficient in all three criteria of coverage annotation and alignment is times higher than that in genes with high trace sequencing coverage known annotation status and perfect alignment scores we conclude that some estimates on the prevalence of positive darwinian selection in the literature may be inflated and should be treated caution
the extent by which different cellular components generate phenotypic diversity is an ongoing debate in evolutionary biology that is yet to be addressed by quantitative comparative studies we conducted an in vivo mass spectrometry study of the phosphoproteomes of three yeast species saccharomyces cerevisiae candida albicans and schizosaccharomyces pombe in order to quantify the evolutionary rate of change of phosphorylation we estimate that kinase substrate interactions change at most two orders of magnitude more slowly than transcription factor tf promoter interactions our computational analysis linking kinases to putative substrates recapitulates known phosphoregulation events and provides putative evolutionary histories for the kinase regulation of protein complexes across yeast species to validate these trends we used the e map approach to analyze over quantitative genetic interactions in s cerevisiae and sc pombe which demonstrated that protein kinases and to a greater extent tfs show lower than average conservation of genetic interactions we propose therefore that protein kinases are an important source of diversity
genome wide association studies gwas have led to a rapid increase in available data on common genetic variants and phenotypes and numerous discoveries of new loci associated with susceptibility to common complex diseases integrating the evidence from gwas and candidate gene studies depends on concerted efforts in data production online publication database development and continuously updated data synthesis here the authors summarize current experience and challenges on these fronts which were discussed at a multidisciplinary workshop sponsored by the human genome epidemiology network comprehensive field synopses that integrate many reported gene disease associations have been systematically developed for several fields including alzheimer s disease schizophrenia bladder cancer coronary heart disease preterm birth and dna repair genes in various cancers the authors summarize insights from these field synopses and discuss remaining unresolved issues especially in the light of evidence from gwas for which they summarize empirical p value and effect size data on discovered associations for binary outcomes with p they also present a vision of collaboration that builds reliable cumulative evidence for genetic associations with common complex diseases and a transparent distributed authoritative knowledge base on genetic variation and human health as a next step in the evolution of human genome epidemiology reviews the authors invite investigators to submit field synopses for possible publication in the american journal epidemiology
motivation primary data analysis methods are of critical importance in second generation dna sequencing improved methods have the potential to increase yield and reduce the error rates openly documented analysis tools enable the user to understand the primary data this is important for the optimization and validity of their scientific work results in this article we describe swift a new tool for performing primary data analysis on the illumina solexa sequencing platform swift is the first tool outside of the vendors own software which completes the full analysis process from raw images through to base calls as such it provides an alternative to and independent validation of the vendor supplied tool our results show that swift is able to increase yield by at comparable error rate availability and implementation swift is implemented in c and supported under linux it is supplied under an open source license allowing researchers to build upon the platform swift is available from http swiftng sourceforge net contact new sgenomics org nava whiteford nanoporetech com supplementary information supplementary data are available at bioinformatics bioinformatics
abstract background prior to cluster analysis or genetic network analysis it is customary to filter or remove genes considered to be irrelevant from the set of genes to be analyzed often genes which exhibit less variation across samples than some threshold value are deleted this can improve interpretability and reduce bias results this paper introduces modular models for representing network structure in order to study the relative effects of different filtering methods we show that cluster analysis and principal components are strongly affected by filtering filtering methods intended specifically for cluster and network analysis are introduced and compared by simulating modular networks with known statistical properties to study more realistic situations we analyze simulated real data based on well characterized e coli and s cerevisiae regulatory networks conclusions the methods introduced apply very generally to any similarity matrix describing gene expression one of the proposed methods sumcov performed well for all simulated
manual evaluation of translation quality is generally thought to be excessively time consuming and expensive we explore a fast and inexpensive way of doing it using amazon s mechanical turk to pay small sums to a large number of non expert annotators for we redundantly recreate judgments from a translation task we find that when combined non expert judgments have a high level of agreement with the existing gold standard judgments of machine translation quality and correlate more strongly with expert judgments than bleu does we go on to show that mechanical turk can be used to calculate human mediated translation edit rate hter to conduct reading comprehension experiments with machine translation and to create high quality translations
recent papers have described the first application of high throughput sequencing hts technologies to the characterization of transcriptomes these studies emphasize the tremendous power of this new technology in terms of both profiling coverage and quantitative accuracy initial discoveries include the detection of substantial new transcript complexity the elucidation of binding maps and regulatory properties of rna binding proteins and new insights into the links between different steps in pre mrna processing we review these findings focusing on results from profiling mammalian transcriptomes the strengths and limitations of hts relative to microarray profiling are discussed we also consider how future advances in hts technology are likely to transform our understanding of integrated cellular networks operating at the level
chip seq technology which combines chromatin immunoprecipitation chip with massively parallel sequencing is rapidly replacing chip on chip for the genome wide identification of transcription factor binding events identifying bound regions from the large number of sequence tags produced by chip seq is a challenging task here we present glitr global identifier of target regions which accurately identifies enriched regions in target data by calculating a fold change based on random samples of control input chromatin data glitr uses a classification method to identify regions in chip data that have a peak height and fold change which do not resemble regions in an input sample we compare glitr to several recent methods and show that glitr has improved sensitivity for identifying bound regions closely matching the consensus sequence of a given transcription factor and can detect bona fide transcription factor targets missed by other programs we also use glitr to address the issue of sequencing depth and show that sequencing biological replicates identifies far more binding regions than re sequencing the same nar
motivation second generation sequencing technologies produce a massive amount of short reads in a single experiment however sequencing errors can cause major problems when using this approach for de novo sequencing applications moreover existing error correction methods have been designed and optimized for shotgun sequencing therefore there is an urgent need for the design of fast and accurate computational methods and tools for error correction of large amounts of short read data results we present shrec a new algorithm for correcting errors in short read data that uses a generalized suffix trie on the read data as the underlying data structure our results show that the method can identify erroneous reads with sensitivity and specificity of over and for simulated data with error rates of up to as well as for real data furthermore it achieves an error correction accuracy of over for simulated data and over for real data these results are clearly superior to previously published approaches shrec is available as an efficient open source java implementation that allows processing of million of short reads on a standard workstation availability shrec source code in java is freely available at http www informatik uni kiel de jasc shrec contact jasc informatik uni kiel bioinformatics
motivation most microbial species can not be cultured in the laboratory metagenomic sequencing may still yield a complete genome if the sequenced community is enriched and the sequencing coverage is high however the complexity in a natural population may cause the enrichment culture to contain multiple related strains this diversity can confound existing strict assembly programs and lead to a fragmented assembly which is unnecessary if we have a related reference genome available that can function as a scaffold results here we map short metagenomic sequencing reads from a population of strains to a related reference genome and compose a genome that captures the consensus of the population s sequences we show that by iteration of the mapping and assembly procedure the coverage increases while the similarity with the reference genome decreases this indicates that the assembly becomes less dependent on the reference genome and approaches the consensus genome of the multi strain population contact dutilh cmbi ru nl supplementary information supplementary data are available at online
the production environment for analytical data management applications is rapidly changing many enterprises are shifting away from deploying their analytical databases on high end proprietary machines and moving towards cheaper lower end commodity hardware typically arranged in a shared nothing mpp architecture often in a virtualized environment inside public or private clouds at the same time the amount of data that needs to be analyzed is exploding requiring hundreds to thousands of machines to work in parallel to perform the analysis there tend to be two schools of thought regarding what technology to use for data analysis in such an environment proponents of parallel databases argue that the strong emphasis on performance and efficiency of parallel databases makes them wellsuited to perform such analysis on the other hand others argue that mapreduce based systems are better suited due to their superior scalability fault tolerance and flexibility to handle unstructured data in this paper we explore the feasibility of building a hybrid system that takes the best features from both technologies the prototype we built approaches parallel databases in performance and efficiency yet still yields the scalability fault tolerance and flexibility of mapreduce systems
this book focuses on the human users of search engines and the tool they use to interact with them the search user interface the truly worldwide reach of the web has brought with it a new realization among computer scientists and laypeople of the enormous importance of usability and user interface design in the last ten years much has become understood about what works in search interfaces from a usability perspective and what does not researchers and practitioners have developed a wide range of innovative interface ideas but only the most broadly acceptable make their way into major web search engines this book summarizes these developments presenting the state of the art of search interface design both in academic research and in deployment in commercial systems many books describe the algorithms behind search engines and information retrieval systems but the unique focus of this book is specifically on the user interface it will be welcomed by industry professionals who design systems that use search interfaces as well as graduate students and academic researchers who investigate systems
translating a set of disease regions into insight about pathogenic mechanisms requires not only the ability to identify the key disease genes within them but also the biological relationships among those key genes here we describe a statistical method gene relationships among implicated loci grail that takes a list of disease regions and automatically assesses the degree of relatedness of implicated genes using pubmed abstracts we first evaluated grail by assessing its ability to identify subsets of highly related genes in common pathways from validated lipid and height snp associations from recent genome wide studies we then tested grail by assessing its ability to separate true disease regions from many false positive disease regions in two separate practical applications in human genetics first we took nominally associated crohn s disease snps and applied grail to identify a subset of snps with highly related genes of these ten convincingly validated in follow up genotyping genotyping results for the remaining three were inconclusive next we applied grail to rare deletion events seen in schizophrenia cases less than one third of which are contributing to disease risk we demonstrate that grail is able to identify a subset of deletions containing highly related genes many of these genes are expressed in the central nervous system and play a role in neuronal synapses grail offers a statistically robust approach to identifying functionally related genes from across multiple disease regionsthat likely represent key disease pathways an online version of this method is available for public use http www broad mit edu grail
with the significant advances in information and communications technology ict over the last half century there is an increasingly perceived vision that computing will one day be the utility after water electricity gas and telephony this computing utility like all other four existing utilities will provide the basic level of computing service that is considered essential to meet the everyday needs of the general community to deliver this vision a number of computing paradigms have been proposed of which the latest one is known as cloud computing hence in this paper we define cloud computing and provide the architecture for creating clouds with market oriented resource allocation by leveraging technologies such as virtual machines vms we also provide insights on market based resource management strategies that encompass both customer driven service management and computational risk management to sustain service level agreement sla oriented resource allocation in addition we reveal our early thoughts on interconnecting clouds for dynamically creating global cloud exchanges and markets then we present some representative cloud platforms especially those developed in industries along with our current work towards realizing market oriented resource allocation of clouds as realized in aneka enterprise cloud technology furthermore we highlight the difference between high performance computing hpc workload and internet based services workload we also describe a meta negotiation infrastructure to establish global cloud exchanges and markets and illustrate a case study of harnessing storage clouds for high performance content delivery finally we conclude with the need for convergence of competing it paradigms to deliver our vision
the popularity of online social networks osns has given rise to a number of measurements studies that provide a first step towards their understanding so far such studies have been based either on complete data sets provided directly by the osn itself or on breadth first search bfs crawling of the social graph which does not guarantee good statistical properties of the collected sample in this paper we crawl the publicly available social graph and present the first unbiased sampling of facebook fb users using a metropolis hastings random walk with multiple chains we study the convergence properties of the walk and demonstrate the uniformity of the collected sample with respect to multiple metrics of interest we provide a comparison of our crawling technique to baseline algorithms namely bfs and simple random walk as well as to the ground truth obtained through truly uniform sampling of userids our contributions lie both in the measurement methodology and in the collected sample with regards to the methodology our measurement technique i applies and combines known results from random walk sampling specifically in the osn context and ii addresses system implementation aspects that have made the measurement of facebook challenging so far with respect to the collected sample i it is the first representative sample of fb users and we plan to make it publicly available ii we perform a characterization of several key properties of the data set and find that some of them are substantially different from what was previously believed based on non representative samples
abstract the fabric of science is changing driven by a revolution in digital technologies that facilitate the acquisition and communication of massive amounts of data this is changing the nature of collaboration and expanding opportunities to participate in science if digital technologies are the engine of this revolution digital data are its fuel but for many scientific disciplines this fuel is in short supply the publication of primary data is not a universal or mandatory part of science and despite policies and proclamations to the contrary calls to make data publicly available have largely gone unheeded in this short essay i consider why and explore some of the challenges that lie ahead as we work toward a database everything
despite policies and calls for scientists to make data available this is not happening for most environmental and biodiversity related data because scientists concerns about these efforts have not been answered and initiatives to motivate scientists to comply have been inadequate many of the issues regarding data availability can be addressed if the principles of publication rather than sharing are applied however online data publication systems also need to develop mechanisms for data citation and indices of data access comparable to those for citation systems in print journals despite policies and calls for scientists to make data available this is not happening for most environmental and biodiversity related data because scientists concerns about these efforts have not been answered and initiatives to motivate scientists to comply have been inadequate many of the issues regarding data availability can be addressed if the principles of publication rather than sharing are applied however online data publication systems also need to develop mechanisms for data citation and indices of data access comparable to those for citation systems in journals
although the operation of natural selection requires that genotypes differ in fitness some geneticists may find it easier to understand natural selection than fitness partly this reflects the fact that the word fitness has been used to mean subtly different things in this review i distinguish among these meanings for example individual fitness absolute fitness and relative fitness and explain how evolutionary geneticists use fitness to predict changes in the genetic composition of populations through time i also review the empirical study of fitness emphasizing approaches that take advantage of recent genetic and genomic data and i highlight important unresolved problems in fitness
like ecological communities which vary in species composition eukaryote genomes differ in the amount and diversity of transposable elements tes that they harbor given that tes have a considerable impact on the biology of their host species we need to better understand whether their dynamics reflects some form of organization or is primarily driven by stochastic processes here we borrow ecological concepts on species diversity to explore how interactions between tes can contribute to structure te communities within their genomic ecosystem whereas the niche theory predicts a stable diversity of tes because of their divergent characteristics the neutral theory of biodiversity predicts the assembly of te communities from stochastic processes acting at the level of the individual te contrary to ecological communities however te communities are shaped by selection at the level of their ecosystem i e the host individual developing ecological models specific to the genome will thus be a prerequisite for modeling the dynamics tes
motivation in the biological sciences the need to analyse vast amounts of information has become commonplace such large scale analyses often involve drawing together data from a variety of different databases held remotely on the internet or locally on in house servers supporting these tasks are ad hoc collections of data manipulation tools scripting languages and visualisation software which are often combined in arcane ways to create cumbersome systems that have been customised for a particular purpose and are consequently not readily adaptable to other uses for many day to day bioinformatics tasks the sizes of current databases and the scale of the analyses necessary now demand increasing levels of automation nevertheless the unique experience and intuition of human researchers is still required to interpret the end results in any meaningful biological way putting humans in the loop requires tools to support real time interaction with these vast and complex data sets numerous tools do exist for this purpose but many do not have optimal interfaces most are effectively isolated from other tools and databases owing to incompatible data formats and many have limited real time performance when applied to realistically large data sets much of the user s cognitive capacity is therefore focused on controlling the software and manipulating esoteric file formats rather than on performing the research methods to confront these issues harnessing expertise in human computer interaction hci high performance rendering and distributed systems and guided by bioinformaticians and end user biologists we are building reusable software components that together create a toolkit that is both architecturally sound from a computing point of view and addresses both user and developer requirements key to the system s usability is its direct exploitation of semantics which crucially gives individual components knowledge of their own functionality and allows them to interoperate seamlessly removing many of the existing barriers and bottlenecks from standard bioinformatics tasks results the toolkit named utopia is freely available from http utopia cs man uk
the term linked data refers to a set of best practices for publishing and connecting structured data on the web these best practices have been adopted by an increasing number of data providers over the last three years leading to the creation of a global data space containing billions of assertions the web of data in this article the authors present the concept and technical principles of linked data and situate these within the broader context of related technological developments they describe progress to date in publishing linked data on the web review applications that have been developed to exploit the web of data and map out a research agenda for the linked data community as it forward
background large scale statistical analyses have become hallmarks of post genomic era biological research due to advances in high throughput assays and the integration of large biological databases one accompanying issue is the simultaneous estimation of p values for a large number of hypothesis tests in many applications a parametric assumption in the null distribution such as normality may be unreasonable and resampling based p values are the preferred procedure for establishing statistical significance using resampling based procedures for multiple testing is computationally intensive and typically requires large numbers of resamples results we present a new approach to more efficiently assign resamples such as bootstrap samples or permutations within a nonparametric multiple testing framework we formulated a bayesian inspired approach to this problem and devised an algorithm that adapts the assignment of resamples iteratively with negligible space and running time overhead in two experimental studies a breast cancer microarray dataset and a genome wide association study dataset for parkinson s disease we demonstrated that our differential allocation procedure is substantially more accurate compared to the traditional uniform resample allocation conclusion our experiments demonstrate that using a more sophisticated allocation strategy can improve our inference for hypothesis testing without a drastic increase in the amount of computation on randomized data moreover we gain more improvement in efficiency when the number of tests is large r code for our algorithm and the shortcut method are available at http people pcbi upenn edu pub
summary the systems biology markup language sbml is an established community xml format for the markup of biochemical models hucka et al with the introduction of sbml level version specific model entities such as species or reactions can now be annotated using ontological terms these annotations which are encoded using the resource description framework rdf provide the facility to specify definite terms to individual components allowing software to unambiguously identify such components and thus link the models to existing data resources kell mendes libsbml bornstein et al is an application programming interface library for the manipulation of sbml files while libsbml provides the facilities for reading and writing such annotations from and to models it is beyond the scope of libsbml to provide interpretation of these terms the libannotationsbml library introduced here acts as a layer on top of libsbml linking sbml annotations to the web services that describe these ontological terms two applications that use this library are described sbmlsynonymextractor finds name synonyms of sbml model entities and sbmlreactionbalancer checks sbml files to determine whether specifed reactions are elementally balanced availability http mcisb sourceforge net contact neil swainston manchester uk
background micrornas mirnas a growing class of small rnas with crucial regulatory roles at the post transcriptional level are usually found to be clustered on chromosomes however with the exception of a few individual cases so far little is known about the functional consequence of this conserved clustering of mirna loci in animal genomes such clusters often contain non homologous mirna genes one hypothesis to explain this heterogeneity suggests that clustered mirnas are functionally related by virtue of co targeting downstream pathways results integrating of mirna cluster information with protein protein interaction ppi network data our research supports the hypothesis of the functional coordination of clustered mirnas and links it to the topological features of mirnas targets in ppi network specifically our results demonstrate that clustered mirnas jointly regulate proteins in close proximity of the ppi network the possibility that two proteins yield to this coordinated regulation is negatively correlated with their distance in ppi network guided by the knowledge of this preference we found several network communities enriched with target genes of mirna clusters in addition our results demonstrate that the variance of this propensity can also partly be explained by protein s connectivity and mirna s conservation conclusion in summary this work supports the hypothesis of intra cluster coordination and investigates the extent of coordination
in tagging systems users can annotate items of interest with free form terms a good understanding of usage characteristics of such systems is necessary to improve the design of current and next generation of tagging systems to this end this work explores three aspects of user behavior in citeulike and connotea two systems that include tagging features to support online personalized management of scientific publications first this study characterizes the degree to which users re tag previously published items and reuse tags to of the daily activity can be characterized as re tagging and about of the activity as tag reuse second we use the pairwise similarity between users activity to characterize the interest sharing in the system we present the interest sharing distribution across the system show that this metric encodes information about existing usage patterns and attempt to correlate interest sharing levels to indicators of collaboration such as co membership in discussion groups and semantic similarity of tag vocabularies finally we show that interest sharing leads to an implicit structure that exhibit a natural segmentation throughout the paper we discuss the potential impact of our findings on the design of mechanisms that support systems
background genes that play an important role in tumorigenesis are expected to show association between dna copy number and rna expression optimal power to find such associations can only be achieved if analysing copy number and gene expression jointly furthermore some copy number changes extend over larger chromosomal regions affecting the expression levels of multiple resident genes results we propose to analyse copy number and expression array data using gene sets rather than individual genes the proposed model is robust and sensitive we re analysed two publicly available datasets as illustration these two independent breast cancer datasets yielded similar patterns of association between gene dosage and gene expression levels in spite of different platforms having been used our comparisons show a clear advantage to using sets of genes expressions to detect associations with long spanning low amplitude copy number aberrations in addition our model allows for using additional explanatory variables and does not require mapping between copy number and expression probes conclusion we developed a general and flexible tool for integration of multiple microarray data sets and showed how the identification of genes whose expression is affected by copy number aberrations provides a powerful approach to prioritize putative targets for validation
pnas it is commonly accepted that proteins have evolutionarily conserved dimensional structures uniquely defined by their amino acid sequence here we question the direct association of structure to sequence by comparing multiple models of identical proteins rapidly growing structural databases contain models of proteins determined independently multiple times we have collected these models in the database of the redundant sets of protein structures and then derived their conformational states by clustering the models with low root mean square deviations rmsds the distribution of conformational states represented in these sets is wider than commonly believed in fact exceeding the possible range of structure determination errors by at least an order of magnitude we argue that differences among the models represent the natural distribution of conformational states our results suggest that we should change the common notion of a protein structure by augmenting a single dimensional model by the width of the ensemble distribution this width must become an indispensible attribute of the protein description we show that every protein contains regions of high rigidity solid like and regions of high mobility liquid like in different and characteristic contribution we also show that the extent of local flexibility is correlated with the functional class of the protein this study suggests that the protein folding problem has no unique solution and should be limited to defining the folding class of the solid like fragments even though they may constitute only a small part of the protein these results limit the capability of modeling protein structures with multiple states
although personalized search has been under way for many years and many personalization algorithms have been investigated it is still unclear whether personalization is consistently effective on different queries for different users and under different search contexts in this paper we study this problem and provide some findings we present a large scale evaluation framework for personalized search based on query logs and then evaluate five personalized search algorithms including two click based ones and three topical interest based ones using day query logs of windows live search by analyzing the results we reveal that personalized web search does not work equally well under various situations it represents a significant improvement over generic web search for some queries while it has little effect and even harms query performance under some situations we propose click entropy as a simple measurement on whether a query should be personalized we further propose several features to automatically predict when a query will benefit from a specific personalization algorithm experimental results show that using a personalization algorithm for queries selected by our prediction model is better than using it simply for queries
abstract background the detection of enriched dna or rna fragments by tiling microarrays has become more and more popular these microarrays contain a high number of small probes covering genomic loci however to achieve high coverage the probe sequences cannot be selected for their hybridization properties the affinity of the probes towards their targets varies in a sequence dependent manner in order to remove this bias a number of approaches have been developed and shown to increase the detection of enriched dna or rna fragments however these approaches also employ a peak detection algorithm that is different from the one used previously thus it seems possible that the enhancement of detection is due to the peak detection algorithm rather than the sequence dependent normalization results we compared three different sequence dependent probe level normalization procedures to a naive sequence independent normalization technique in order to achieve maximal comparability we used the normalized intensity values as input to a single peak detection algorithm a so called spike in data set served as benchmark for the performance we will show that the sequence dependent normalization procedures do not perform better than the naive approach suggesting that the benefit of using these normalization approaches is limited furthermore we will show that the naive approach does well because it effectively removes the sequence dependent component of the measured intensities with the help of the control hybridization experiment conclusions sequence dependent normalization of microarray data hardly improves the detection of enriched dna or rna fragments the success of the sequence independent naive approach is only possible due to the control experiment and requires proper scaling of the intensities
this research explores new ways to augment the search and discovery of relations between web entities using multiple types and sources of social information our goal is to allow the search for all object types such as documents persons and tags while retrieving related objects of all types we implemented a social search engine using a unified approach where the search space is expanded to represent heterogeneous information objects that are interrelated by several relation types our solution is based on multifaceted search which provides an efficient update mechanism for relations between objects as well as efficient search over the heterogeneous data we describe a social search engine positioned within a large enterprise applied over social data gathered from several web applications we conducted a large user study with over people to evaluate the contribution of social data for search our results demonstrate the high precision of social search results and confirm the strong relationship of users and tags to the retrieved
with the advent of web tagging became a popular feature people tag diverse kinds of content e g products at amazon music at last fm images at flickr etc clicking on a tag enables the users to explore related content in this paper we investigate how such tag based queries initialized by the clicking activity can be enhanced with automatically produced contextual information so that the search result better fits to the actual aims of the user we introduce the socialhits algorithm and present an experiment where we compare different algorithms for ranking users tags and resources in a way
collaborative tagging systems are now popular tools for organising and sharing information on the web while collaborative tagging offers many advantages over the use of controlled vocabularies they also suffer from problems such as the existence of polysemous tags we investigate how the different contexts in which individual tags are used can be revealed automatically without consulting any external resources we consider several different network representations of tags and documents and apply a graph clustering algorithm on these networks to obtain groups of tags or documents corresponding to the different meanings of an ambiguous tag our experiments show that networks which explicitly take the social context into account are more likely to give a better picture of the semantics of tag
the rapidly increasing popularity of web knowledge and content sharing systems and growing amount of shared data make discovering relevant content and finding contacts a difficult enterprize typically folksonomies provide a rich set of structures and social relationships that can be mined for a variety of recommendation purposes in this paper we propose a formal model to characterize users items and annotations in web environments our objective is to construct social recommender systems that predict the utility of items users or groups based on the multi dimensional social environment of a given user based on this model we introduce recommendation mechanisms for content sharing frameworks our comprehensive evaluation shows the viability of our approach and emphasizes the key role of social meta knowledge for constructing effective recommendations in applications
contemporary scholarly discourse follows many alternative routes in addition to the three century old tradition of publication in peer reviewed journals the field of high energy physics hep has explored alternative communication strategies for decades initially via the mass mailing of paper copies of preliminary manuscripts then via the inception of the first online repositories and digital libraries this field is uniquely placed to answer recurrent questions raised by the current trends in scholarly communication is there an advantage for scientists to make their work available through repositories often in preliminary form is there an advantage to publishing in open access journals do scientists still read journals or do they use digital repositories the analysis of citation data demonstrates that free and immediate online dissemination of preprints creates an immense citation advantage in hep whereas publication in open access journals presents no discernible advantage in addition the analysis of clickstreams in the leading digital library of the field shows that hep scientists seldom read journals preferring instead
homozygosity for the g allele of at increases colorectal cancer crc risk fold we report here that the risk allele g shows copy number increase during crc development our computer algorithm enhancer element locator eel identified an enhancer element that contains the element drove expression of a reporter gene in a pattern that is consistent with regulation by the key crc pathway wnt affects a binding site for the wnt regulated transcription factor with the risk allele g showing stronger binding in vitro and in vivo genome wide chip assay revealed the element as the strongest binding site within mb of myc an unambiguous correlation between genotype and myc expression was not detected and additional work is required to scrutinize all possible targets of the enhancer our work provides evidence that the common crc predisposition associated with arises from enhanced responsiveness to wnt signaling nature america inc all reserved
recent reports have shown that most of the genome is transcribed and that transcription frequently occurs concurrently on both dna strands in diploid genomes the expression level of each allele conditions the degree to which sequence polymorphisms affect the phenotype it is thus essential to quantify expression in an allele and strand specific manner using a custom designed tiling array and a new computational approach we piloted measuring allele and strand specific expression in yeast confident quantitative estimates of allele specific expression were obtained for about half of the coding and non coding transcripts of a heterozygous yeast strain of which transcripts showed significant allelic differential expression greater than fold the data revealed complex allelic differential expression on opposite strands furthermore combining allele specific expression with linkage mapping enabled identifying allelic variants that act in cis and in trans to regulate allelic expression in the heterozygous strain our results provide the first high resolution analysis of differential expression on all four strands of an genome
for adam smith wealth was related to the division of labor as people and firms specialize in different activities economic efficiency increases suggesting that development is associated with an increase in the number of individual activities and with the complexity that emerges from the interactions between them here we develop a view of economic growth and development that gives a central role to the complexity of a country s economy by interpreting trade data as a bipartite network in which countries are connected to the products they export and show that it is possible to quantify the complexity of a country s economy by characterizing the structure of this network furthermore we show that the measures of complexity we derive are correlated with a country s level of income and that deviations from this relationship are predictive of future growth this suggests that countries tend to converge to the level of income dictated by the complexity of their productive structures indicating that development efforts should focus on generating the conditions that would allow complexity to emerge to generate sustained growth prosperity
we describe an open source portable javascript based genome browser jbrowse that can be used to navigate genome annotations over the web jbrowse helps preserve the user s sense of location by avoiding discontinuous transitions instead offering smoothly animated panning zooming navigation and track selection unlike most existing genome browsers where the genome is rendered into images on the webserver and the role of the client is restricted to displaying those images jbrowse distributes work between the server and client and therefore uses significantly less server overhead than previous genome browsers we report benchmark results empirically comparing server and client side rendering strategies review the architecture and design considerations of jbrowse and describe a simple wiki plug in that allows users to upload and share tracks
we recently showed that the mammalian genome encodes large intergenic noncoding linc rnas that are clearly conserved across mammals and thus functional gene expression patterns have implicated these lincrnas in diverse biological processes including cell cycle regulation immune surveillance and embryonic stem cell pluripotency however the mechanism by which these lincrnas function is unknown here we expand the catalog of human lincrnas to approximately by analyzing chromatin state maps of various human cell types inspired by the observation that the well characterized lincrna hotair binds the polycomb repressive complex prc we tested whether many lincrnas are physically associated with remarkably we observe that approximately of lincrnas expressed in various cell types are bound by and that additional lincrnas are bound by other chromatin modifying complexes also we show that sirna mediated depletion of certain lincrnas associated with leads to changes in gene expression and that the up regulated genes are enriched for those normally silenced by we propose a model in which some lincrnas guide chromatin modifying complexes to specific genomic loci to regulate expression
motivation recently many univariate and several multivariate approaches have been suggested for testing differential expression of gene sets between different phenotypes however despite a wealth of literature studying their performance on simulated and real biological data still there is a need to quantify their relative performance when they are testing different null hypotheses results in this article we compare the performance of univariate and multivariate tests on both simulated and biological data in the simulation study we demonstrate that high correlations equally affect the power of both univariate as well as multivariate tests in addition for most of them the power is similarly affected by the dimensionality of the gene set and by the percentage of genes in the set for which expression is changing between two phenotypes the application of different test statistics to biological data reveals that three statistics sum of squared t tests hotelling s t n statistic testing different null hypotheses find some common but also some complementing differentially expressed gene sets under specific settings this demonstrates that due to complementing null hypotheses each test projects on different aspects of the data and for the analysis of biological data it is beneficial to use all three tests simultaneously instead of focusing exclusively on one
in slapstick comedy the worst thing that could happen usually does the person with a sore toe manages to stub it sometimes twice such errors also arise in daily life and research traces the tendency to do precisely the worst thing to ironic processes of mental control these monitoring processes keep us watchful for errors of thought speech and action and enable us to avoid the worst thing in most situations but they also increase the likelihood of such errors when we attempt to exert control under mental load stress time pressure or distraction ironic errors in attention and memory occur with identifiable brain activity and prompt recurrent unwanted thoughts attraction to forbidden desires expression of objectionable social prejudices production of movement errors and rebounds of negative experiences such as anxiety pain and depression such ironies can be overcome when effective control strategies are deployed and mental load minimized
cellular responses in the secondary visual cortex to simple as well as complex visual stimuli have been well studied however the role of area in visual memory remains unexplored we found that layer neurons of are crucial for the processing of object recognition memory orm using the protein regulator of g protein signaling rgs as a tool we found that the expression of this protein into layer neurons of rat brain area promoted the conversion of a normal short term orm that normally lasts for minutes into long term memory detectable even after many months furthermore elimination of the same layer neurons by means of injection of a selective cytotoxin resulted in the complete loss of normal as well as protein mediated enhanced science
tracking new topics ideas and memes across the web has been an issue of considerable interest recent work has developed methods for tracking topic shifts over long time scales as well as abrupt spikes in the appearance of particular named entities however these approaches are less well suited to the identification of content that spreads widely and then fades over time scales on the order of days the time scale at which we perceive news and events we develop a framework for tracking short distinctive phrases that travel relatively intact through on line text developing scalable algorithms for clustering textual variants of such phrases we identify a broad class of memes that exhibit wide spread and rich variation on a daily basis as our principal domain of study we show how such a meme tracking approach can provide a coherent representation of the news cycle the daily rhythms in the news media that have long been the subject of qualitative interpretation but have never been captured accurately enough to permit actual quantitative analysis we tracked million mainstream media sites and blogs over a period of three months with the total of million articles and we find a set of novel and persistent temporal patterns in the news cycle in particular we observe a typical lag of hours between the peaks of attention to a phrase in the news media and in blogs respectively with divergent behavior around the overall peak and a heartbeat like pattern in the handoff between news and blogs we also develop and analyze a mathematical model for the kinds of temporal variation that the exhibits
array manufacturers originally designed single nucleotide polymorphism snp arrays to genotype human dna at thousands of snps across the genome simultaneously in the decade since their initial development the platform s applications have expanded to include the detection and characterization of copy number variation whether somatic inherited or de novo as well as loss of heterozygosity in cancer cells the technology s impressive contributions to insights in population and molecular genetics have been fueled by advances in computational methodology and indeed these insights and methodologies have spurred developments in the arrays themselves this review describes the most commonly used snp array platforms surveys the computational methodologies used to convert the raw data into inferences at the dna level and details the broad range of applications although the long term future of snp arrays is unclear cost considerations ensure their relevance for at least the next several years even as emerging technologies seem poised to take over for at least some applications researchers working with these new sources of data are adopting the computational approaches originally developed for arrays
ultra high throughput sequencing is used to analyse the transcriptome or interactome at unprecedented depth on a genome wide scale these techniques yield short sequence reads that are then mapped on a genome sequence to predict putatively transcribed or protein interacting regions we argue that factors such as background distribution sequence errors and read length impact on the prediction capacity of sequence census experiments here we suggest a computational approach to measure these factors and analyse their influence on both transcriptomic and epigenomic assays this investigation provides new clues on both methodological and biological issues for instance by analysing chromatin immunoprecipitation read sets we estimate that of reads are affected by snps we show that although the nucleotide error probability is low it significantly increases with the position in the sequence choosing a read length above bp practically eliminates the risk of finding irrelevant positions while above bp the number of uniquely mapped reads decreases with our procedure we obtain false positives among genomic locations hence even rare signatures should identify biologically relevant regions if they are mapped on the genome this indicates that digital transcriptomics may help to characterize the wealth of yet undiscovered low abundance nar
the gene ontology go is a collaborative effort that provides structured vocabularies for annotating the molecular function biological role and cellular location of gene products in a highly systematic way and in a species neutral manner with the aim of unifying the representation of gene function across different organisms each contributing member of the go consortium independently associates go terms to gene products from the organism s they are annotating here we introduce the reference genome project which brings together those independent efforts into a unified framework based on the evolutionary relationships between genes in these different organisms the reference genome project has two primary goals to increase the depth and breadth of annotations for genes in each of the organisms in the project and to create data sets and tools that enable other genome annotation efforts to infer go annotations for homologous genes in their organisms in addition the project has several important incidental benefits such as increasing annotation consistency across genome databases and providing important improvements to the go s logical structure and biological content gaudet al
in the late sixties the canadian psychologist laurence j peter advanced an apparently paradoxical principle named since then after him which can be summarized as follows every new member in a hierarchical organization climbs the hierarchy until he she reaches his her level of maximum incompetence despite its apparent unreasonableness such a principle would realistically act in any organization where the mechanism of promotion rewards the best members and where the competence at their new level in the hierarchical structure does not depend on the competence they had at the previous level usually because the tasks of the levels are very different to each other here we show by means of agent based simulations that if the latter two features actually hold in a given model of an organization with a hierarchical structure then not only is the peter principle unavoidable but also it yields in turn a significant reduction of the global efficiency of the organization within a game theory like approach we explore different promotion strategies and we find counterintuitively that in order to avoid such an effect the best ways for improving the efficiency of a given organization are either to promote each time an agent at random or to promote randomly the best and the worst members in terms competence
edge detection is a signal processing algorithm common in artificial intelligence and image recognition programs we have constructed a genetically encoded edge detection algorithm that programs an isogenic community of e coli to sense an image of light communicate to identify the light dark edges and visually present the result of the computation the algorithm is implemented using multiple genetic circuits an engineered light sensor enables cells to distinguish between light and dark regions in the dark cells produce a diffusible chemical signal that diffuses into light regions genetic logic gates are used so that only cells that sense light and the diffusible signal produce a positive output a mathematical model constructed from first principles and parameterized with experimental measurements of the component circuits predicts the performance of the complete program quantitatively accurate models will facilitate the engineering of more complex biological behaviors and inform bottom up studies of natural genetic networks
motivation there is a strong demand in the genomic community to develop effective algorithms to reliably identify genomic variants indel detection using next gen data is difficult and identification of long structural variations is extremely challenging results we present pindel a pattern growth approach to detect breakpoints of large deletions and medium sized insertions from paired end short reads we use both simulated reads and real data to demonstrate the efficiency of the computer program and accuracy of the results availability the binary code and a short user manual can be freely downloaded from http www ebi ac uk approximately kye pindel contact k ye lumc nl sanger uk
recently the abundance of digital data enabled the implementation of graph based ranking algorithms that provide system level analysis for ranking publications and authors here we take advantage of the entire physical review publication archive to construct authors networks where weighted edges as measured from opportunely normalized citation counts define a proxy for the mechanism of scientific credit transfer on this network we define a ranking method based on a diffusion algorithm that mimics the spreading of scientific credits on the network we compare the results obtained with our algorithm with those obtained by local measures such as the citation count and provide a statistical analysis of the assignment of major career awards in the area of physics a web site where the algorithm is made available to perform customized rank analysis can be found at the address a href http www physauthorsrank org this http a
social media systems have encouraged end user participation in the internet for the purpose of storing and distributing internet content sharing opinions and maintaining relationships collaborative tagging allows users to annotate the resulting user generated content and enables effective retrieval of otherwise uncategorised data however compared to professional web content production collaborative tagging systems face the challenge that end users assign tags in an uncontrolled manner resulting in unsystematic and inconsistent metadata this paper introduces a framework for the personalization of social media systems we pinpoint three tasks that would benefit from personalization collaborative tagging collaborative browsing and collaborative search we propose a ranking model for each task that integrates the individual users tagging history in the recommendation of tags and content to align its suggestions to the individual user preferences we demonstrate on two real data sets that for all three tasks the personalized ranking should take into account both the users own preference and the opinion others
signaling cascades are triggered by environmental stimulation and propagate the signal to regulate transcription systematic reconstruction of the underlying regulatory mechanisms requires pathway targeted informative experimental data however practical experimental design approaches are still in their infancy here we propose a framework that iterates design of experiments and identification of regulatory relationships downstream of a given pathway the experimental design component called meed aims to minimize the amount of laboratory effort required in this process to avoid ambiguity in the identification of regulatory relationships the choice of experiments maximizes diversity between expression profiles of genes regulated through different mechanisms the framework takes advantage of expert knowledge about the pathways under study formalized in a predictive logical model by considering model predicted dependencies between experiments meed is able to suggest a whole set of experiments that can be carried out simultaneously our framework was applied to investigate interconnected signaling pathways in yeast in comparison with other approaches meed suggested the most informative experiments for unambiguous identification of transcriptional regulation in system
all students today are increasingly expected to develop technological fluency digital citizenship and other twenty first century competencies despite wide variability in the quality of learning opportunities schools provide social network sites snss available via the internet may provide promising contexts for learning to supplement school based experiences this qualitative study examines how high school students from low income families in the usa use the sns i myspace i for identity formation and informal learning the analysis revealed that snss used outside of school allowed students to formulate and explore various dimensions of their identity and demonstrate twenty first century skills however students did not perceive a connection between their online activities and learning in classrooms we discuss how learning with such technologies might be incorporated into the students overall learning ecology to reduce educational inequities and how current institutionalized approaches might shift to accommodate change
whilst recent studies suggest that over of british undergraduate students are regularly using social networking sites we still know very little about how this phenomenon impacts on the student experience and in particular how it influences students social integration into university life this paper explores how pre registration engagement with a university i facebook i network influences students post registration social networks research was conducted with first year undergraduates at a british university using an online survey students reported that they specifically joined i facebook i pre registration as a means of making new friends at university as well as keeping in touch with friends and family at home the survey data also illustrate that once at university i facebook i was part of the social glue that helped students settle into university life however care must be taken not to over privilege i facebook i it is clearly only one aspect of students more general social networking practices and face to face interrelationships and interactions remain important students thought i facebook i was used most importantly for social reasons not for formal teaching purposes although it was sometimes used informally for purposes
drug combinations are a promising strategy to overcome the compensatory mechanisms and unwanted off target effects that limit the utility of many potential drugs however enthusiasm for this approach is tempered by concerns that the therapeutic synergy of a combination will be accompanied by synergistic side effects using large scale simulations of bacterial metabolism and multi dose experiments relevant to diverse diseases we provide evidence that synergistic drug combinations are generally more specific to particular cellular contexts than are single agent activities we highlight six combinations whose selective synergy depends on multitarget drug activity for one anti inflammatory example we show how such selectivity is achieved through differential expression of the drugs targets in cell types associated with therapeutic but not toxic effects and validate its therapeutic relevance in a rat model of asthma the context specificity of synergistic combinations creates many opportunities for therapeutically relevant selectivity and enables improved control of complex systems
as we approach the completed sequencing of microbial genomes the field of microbial genomics is poised at a crossroads the future holds great promise for far reaching advancements in microbiology as well as in diverse related sciences but realizing that potential will require meeting the challenges that have accompanied the rapid development of the underlying technology and the exponential growth of data new technologies provide unprecedented opportunities but also call for conceptual shifts experience gained in the first decade of genomics can guide the improved approaches now needed for the selection of genome sequencing projects and their funding for genome publication and annotation as well as for data analysis and access equipped with these new tools and policies microbiologists will have a unique opportunity for unprecedented exploration of our microbial planet the dramatic advancements in sequencing technology achieved during the past decade have mediated a rapid transition from single gene to whole genome studies in so doing they also transformed what had been an almost purely experimental discipline into a predominantly theoretical predictive
this paper reflects on the current position of virtual learning environments vles in universities and speculates about likely future directions for e learning using accepted models of technology innovation and looking at current web trends it considers the extent to which e learning is truly embedded in institutions how web is being used by the upcoming generation and what this might mean for teachers learners and higher education institutions the paper concludes that optimism about the impact of e learning on higher education based on the market penetration of vles is misplaced and suggests that ivan illich s concept of learning webs may be a more reliable guide to developments
inhibition of the tor signalling pathway by genetic or pharmacological intervention extends lifespan in invertebrates including yeast nematodes and fruitflies however whether inhibition of mtor signalling can extend lifespan in a mammalian species was unknown here we report that rapamycin an inhibitor of the mtor pathway extends median and maximal lifespan of both male and female mice when fed beginning at days of age on the basis of age at mortality rapamycin led to an increase of for females and for males the effect was seen at three independent test sites in genetically heterogeneous mice chosen to avoid genotype specific effects on disease susceptibility disease patterns of rapamycin treated mice did not differ from those of control mice in a separate study rapamycin fed to mice beginning at days of age also increased survival in both males and females based on an interim analysis conducted near the median survival point rapamycin may extend lifespan by postponing death from cancer by retarding mechanisms of ageing or both to our knowledge these are the first results to demonstrate a role for mtor signalling in the regulation of mammalian lifespan as well as pharmacological extension of lifespan in both genders these findings have implications for further development of interventions targeting mtor for the treatment and prevention of age diseases
complex networks have been studied intensively for a decade but research still focuses on the limited case of a single non interacting modern systems are coupled and therefore should be modelled as interdependent networks a fundamental property of interdependent networks is that failure of nodes in one network may lead to failure of dependent nodes in other networks this may happen recursively and can lead to a cascade of failures in fact a failure of a very small fraction of nodes in one network may lead to the complete fragmentation of a system of several interdependent networks a dramatic real world example of a cascade of failures concurrent malfunction is the electrical blackout that affected much of italy on september the shutdown of power stations directly led to the failure of nodes in the internet communication network which in turn caused further breakdown of power here we develop a framework for understanding the robustness of interacting networks subject to such cascading failures we present exact analytical solutions for the critical fraction of nodes that on removal will lead to a failure cascade and to a complete fragmentation of two interdependent networks surprisingly a broader degree distribution increases the vulnerability of interdependent networks to random failure which is opposite to how a single network behaves our findings highlight the need to consider interdependent network properties in designing networks
background most analyses of microarray data are based on point estimates of expression levels and ignore the uncertainty of such estimates by determining uncertainties from affymetrix genechip data and propagating these uncertainties to downstream analyses it has been shown that we can improve results of differential expression detection principal component analysis and clustering previously implementations of these uncertainty propagation methods have only been available as separate packages written in different languages previous implementations have also suffered from being very costly to compute and in the case of differential expression detection have been limited in the experimental designs to which they can be applied results puma is a bioconductor package incorporating a suite of analysis methods for use on affymetrix genechip data puma extends the differential expression detection methods of previous work from the class case to the multi factorial case puma can be used to automatically create design and contrast matrices for typical experimental designs which can be used both within the package itself but also in other bioconductor packages the implementation of differential expression detection methods has been parallelised leading to significant decreases in processing time on a range of computer architectures puma incorporates the first r implementation of an uncertainty propagation version of principal component analysis and an implementation of a clustering method based on uncertainty propagation all of these techniques are brought together in a single easy to use package with clear task based documentation conclusion for the first time the puma package makes a suite of uncertainty propagation methods available to a general audience these methods can be used to improve results from more traditional analyses of microarray data puma also offers improvements in terms of scope and speed of execution over previously available methods puma is recommended for anyone working with the affymetrix genechip platform for gene expression analysis and can also be applied generally
we present single molecule sequencing digital gene expression smsdge a high throughput amplification free method for accurate quantification of the full range of cellular polyadenylated rna transcripts using a helicos genetic analysis system smsdge involves a reverse transcription and polya tailing sample preparation procedure followed by sequencing that generates a single read per transcript we applied smsdge to the transcriptome of saccharomyces cerevisiae strain using of the available channels in a single sequencing run yielding on average million aligned reads per channel using spiked in rna accurate quantitative measurements were obtained over four orders of magnitude high correlation was demonstrated across independent flow cell channels instrument runs and sample preparations transcript counting in smsdge is highly efficient due to the representation of each transcript molecule by a single read this efficiency coupled with the high throughput enabled by the single molecule sequencing platform provides an alternative method for profiling
historically the majority of new drugs have been generated from natural products secondary metabolites and from compounds derived from natural products during the past years pharmaceutical industry research into natural products has declined in part because of an emphasis on high throughput screening of synthetic libraries currently there is substantial decline in new drug approvals and impending loss of patent protection for important medicines however untapped biological resources smart screening methods robotic separation with structural analysis metabolic engineering and synthetic biology offer exciting technologies for new natural product drug discovery advances in rapid genetic sequencing coupled with manipulation of biosynthetic pathways may provide a vast resource for the future discovery of pharmaceutical science
background transcription initiation is a key component in the regulation of gene expression mrna full length sequencing techniques have enhanced our understanding of mammalian transcription start sites tsss revealing different initiation patterns on a genomic scale results to identify tsss in drosophila melanogaster we applied a hierarchical clustering strategy on available expressed sequence tags ests and identified a high quality set of tsss for approximately genes we distinguished two initiation patterns peaked tsss and broad tss cluster groups peaked promoters were found to contain location specific sequence elements conversely broad promoters were associated with non location specific elements in alignments across other drosophila genomes conservation levels of sequence elements exceeded within the melanogaster subgroup but dropped considerably for distal species elements in broad promoters had lower levels of conservation than those in peaked promoters when characterizing the distributions of ests of tsss showed distinct associations to one out of eight different spatiotemporal conditions available whole genome tiling array time series data revealed different temporal patterns of embryonic activity across the majority of genes with distinct alternative promoters many genes with maternally inherited transcripts were found to have alternative promoters utilized later in development core promoters of maternally inherited transcripts showed differences in motif composition compared to zygotically active promoters conclusions our study provides a comprehensive map of drosophila tsss and the conditions under which they are utilized distinct differences in motif associations with initiation pattern and spatiotemporal utilization illustrate the complex regulatory code of initiation
schizophrenia is a complex disorder caused by both genetic and environmental factors and their interactions research on pathogenesis has traditionally focused on neurotransmitter systems in the brain particularly those involving dopamine schizophrenia has been considered a separate disease for over a century but in the absence of clear biological markers diagnosis has historically been based on signs and symptoms a fundamental message emerging from genome wide association studies of copy number variations cnvs associated with the disease is that its genetic basis does not necessarily conform to classical nosological disease boundaries certain cnvs confer not only high relative risk of schizophrenia but also of other psychiatric the structural variations associated with schizophrenia can involve several genes and the phenotypic syndromes or the genomic disorders have not yet been single nucleotide polymorphism snp based genome wide association studies with the potential to implicate individual genes in complex diseases may reveal underlying biological pathways here we combined snp data from several large genome wide scans and followed up the most significant association signals we found significant association with several markers spanning the major histocompatibility complex mhc region on chromosome a marker located upstream of the neurogranin gene nrgn on and a marker in intron four of transcription factor on our findings implicating the mhc region are consistent with an immune component to schizophrenia risk whereas the association with nrgn and points to perturbation of pathways involved in brain development memory cognition
development normally occurs similarly in all individuals within an isogenic population but mutations often affect the fates of individual organisms this phenomenon known as partial penetrance has been observed in diverse developmental systems however it remains unclear how the underlying genetic network specifies the set of possible alternative fates and how the relative frequencies of these fates here we identify a stochastic cell fate determination process that operates in bacillus subtilis sporulation mutants and show how it allows genetic control of the penetrance of multiple fates mutations in an intercompartmental signalling process generate a set of discrete alternative fates not observed in wild type cells including rare formation of two viable twin spores rather than one within a single cell by genetically modulating chromosome replication and septation we can systematically tune the penetrance of each mutant fate furthermore signalling and replication perturbations synergize to significantly increase the penetrance of twin sporulation these results suggest a potential pathway for developmental evolution between monosporulation and twin sporulation through states of intermediate twin penetrance furthermore time lapse microscopy of twin sporulation in wild type clostridium oceanicum shows a strong resemblance to twin sporulation in these b subtilis together the results suggest that noise can facilitate developmental evolution by enabling the initial expression of discrete morphological traits at low penetrance and allowing their stabilization by gradual adjustment of parameters
recent advances in sequencing technologies have initiated an era of personal genome sequences to date human genome sequences have been reported for individuals with ancestry in three distinct geographical regions a yoruba african two individuals of northwest european origin and a person from here we provide a highly annotated whole genome sequence for a korean individual known as the genome of was determined by an exacting combined approach that included whole genome shotgun sequencing coverage targeted bacterial artificial chromosome sequencing and high resolution comparative genomic hybridization using custom microarrays featuring more than probes alignment to the ncbi reference a composite of several ethnic disclosed nearly single nucleotide polymorphisms snps including non synonymous snps and deletion or insertion polymorphisms indels snp and indel densities were strongly correlated genome wide applying very conservative criteria yielded highly reliable copy number variants for clinical considerations potential medical phenotypes were annotated for non synonymous snps coding domain indels and structural variants the integration of several human whole genome sequences derived from several ethnic groups will assist in understanding genetic ancestry migration patterns and bottlenecks
microrna offset rnas mornas were recently detected as highly abundant class of small rnas in a basal chordate using short read sequencing data we show here that mornas are also produced from human microrna precursors albeit at quite low expression levels the expression levels of mornas are unrelated to those of the associated micrornas surprisingly microrna precursors that also show mornas are typically evolutionarily old comprising more than half of the microrna families that were present in early bilateria while evidence for mornas was found only for a relative small fraction of microrna families of recent origin contact studla bioinf uni leipzig de supplementary information supplementary data are available at bioinformatics online and in machine readable form at http www bioinf uni leipzig de publications bioinformatics
different synonymous codons are favored by natural selection for translation efficiency and accuracy in different organisms the rules governing the identities of favored codons in different organisms remain obscure in fact it is not known whether such rules exist or whether favored codons are chosen randomly in evolution in a process akin to a series of frozen accidents here we study this question by identifying for the first time the favored codons in bacteria archea and fungi we use a number of tests to show that the identified codons are indeed likely to be favored and find that across all studied organisms the identity of favored codons tracks the gc content of the genomes once the effect of the genomic gc content on selectively favored codon choice is taken into account additional universal amino acid specific rules governing the identity of favored codons become apparent our results provide for the first time a clear set of rules governing the evolution of selectively favored codon usage based on these results we describe a putative scenario for how evolutionary shifts in the identity of selectively favored codons can occur without even temporary weakening of natural selection for bias
summary why are genes harmful when they are overexpressed by testing possible causes of overexpression phenotypes in yeast we identify intrinsic protein disorder asan important determinant of dosage sensitivity disordered regions are prone to make promiscuous molecular interactions when their concentration is increased and we demonstrate that this is the likely cause of pathology when genes are overexpressed we validate our findings in two animals drosophila melanogaster and caenorhabditis elegans in mice and humans the same properties are strongly associated with dosage sensitive oncogenes such that mass action driven molecular interactions may be a frequent cause of cancer dosage sensitive genes are tightly regulated at the transcriptional rna and protein levels which may serve to prevent harmful increases in protein concentration under physiological conditions mass action driven interaction promiscuity is a single theoretical framework that can be used to understand predict and possibly treat the effects of increased gene expression in evolution disease
john nash showed that within a complex system individuals are best off if they make the best decision that they can taking into account the decisions of the other individuals here we investigate whether similar principles influence the evolution of signaling networks in multicellular animals specifically by analyzing a set of metazoan species we observed a striking negative correlation of genomically encoded tyrosine content with biological complexity as measured by the number of cell types in each organism we discuss how this observed tyrosine loss correlates with the expansion of tyrosine kinases in the evolution of the metazoan lineage and how it may relate to the optimization of signaling systems in multicellular animals we propose that this phenomenon illustrates genome wide adaptive evolution to accommodate beneficial perturbation
customer preferences for products are drifting over time product perception and popularity are constantly changing as new selection emerges similarly customer inclinations are evolving leading them to ever redefine their taste thus modeling temporal dynamics should be a key when designing recommender systems or general customer preference models however this raises unique challenges within the eco system intersecting multiple products and customers many different characteristics are shifting simultaneously while many of them influence each other and often those shifts are delicate and associated with a few data instances this distinguishes the problem from concept drift explorations where mostly a single concept is tracked classical time window or instance decay approaches cannot work as they lose too much signal when discarding data instances a more sensitive approach is required which can make better distinctions between transient effects and long term patterns the paradigm we offer is creating a model tracking the time changing behavior throughout the life span of the data this allows us to exploit the relevant components of all data instances while discarding only what is modeled as being irrelevant accordingly we revamp two leading collaborative filtering recommendation approaches evaluation is made on a large movie rating dataset by netflix results are encouraging and better than those previously reported on dataset
the quantum walk is the quantum analogue of the well known random walk which forms the basis for models and applications in many realms of science its properties are markedly different from the classical counterpart and might lead to extensive applications in quantum information science in our experiment we implemented a quantum walk on the line with single neutral atoms by deterministically delocalizing them over the sites of a one dimensional spin dependent optical lattice with the use of site resolved fluorescence imaging the final wave function is characterized by local quantum state tomography and its spatial coherence is demonstrated our system allows the observation of the quantum to classical transition and paves the way for applications such as quantum automata
schizophrenia is a severe mental disorder with a lifetime risk of about characterized by hallucinations delusions and cognitive deficits with heritability estimated at up to we performed a genome wide association study of european individuals with schizophrenia and controls here we show using two analytic approaches the extent to which common genetic variation underlies the risk of schizophrenia first we implicate the major histocompatibility complex second we provide molecular genetic evidence for a substantial polygenic component to the risk of schizophrenia involving thousands of common alleles of very small effect we show that this component also contributes to the risk of bipolar disorder but not to several non diseases
these lectures are intended to provide a brief pedagogical review of dark matter for the newcomer to the subject we begin with a discussion of the astrophysical evidence for dark matter the standard weakly interacting massive particle wimp scenario the motivation particle models and detection techniques is then reviewed we provide a brief sampling of some recent variations to the standard wimp scenario as well as some alternatives axions and sterile neutrinos exercises are provided for reader
gr second generation sequencing technologies deliver dna sequence data at unprecedented high throughput common to most biological applications is a mapping of the reads to an almost identical or highly similar reference genome due to the large amounts of data efficient algorithms and implementations are crucial for this task we present an efficient read mapping tool called razers it allows the user to align sequencing reads of arbitrary length using either the hamming distance or the edit distance our tool can work either lossless or with a user defined loss rate at higher speeds given the loss rate we present an approach that guarantees not to lose more reads than specified this enables the user to adapt to the problem at hand and provides a seamless tradeoff between sensitivity and time
we describe a new method tag seq which employs ultra high throughput sequencing of base pair cdna tags for sensitive and cost effective gene expression profiling we compared tag seq data to longsage data and observed improved representation of several classes of rare transcripts including transcription factors antisense transcripts and intronic sequences the latter possibly representing novel exons or genes we observed increases in the diversity abundance and dynamic range of such rare transcripts and took advantage of the greater dynamic range of expression to identify in cancers and normal libraries altered expression ratios of alternative transcript isoforms the strand specific information of tag seq reads further allowed us to detect altered expression ratios of sense and antisense s as transcripts between cancer and normal libraries s as transcripts were enriched in known cancer genes while transcript isoforms were enriched in mirna targeting sites we found that transcript abundance had a stronger gc bias in longsage than tag seq such that at rich tags were less abundant than gc rich tags in longsage tag seq also performed better in gene discovery identifying of genes detected by longsage and profiling a distinct subset of the transcriptome characterized by at rich genes which was expressed at levels below those detectable by longsage overall tag seq is sensitive to rare transcripts has less sequence composition bias relative to longsage and allows differential expression analysis for a greater range of transcripts including transcripts encoding important molecules
abstract background since public cheminformatic databases and their collective functionality for exploring relationships between compounds protein sequences literature and assay data have advanced dramatically in parallel commercial sources that extract and curate such relationships from journals and patents have also been expanding this work updates a previous comparative study of databases chosen because of their bioactive content availability of downloads and facility to select informative subsets results where they could be calculated extracted compounds per journal article were in the range of to but compound per protein counts increased with document numbers chemical structure filtration to facilitate standardised comparisons typically reduced source counts by between and the pair wise overlaps between databases and subsets were determined as well as changes between and while all compound sets have increased pubchem has doubled to million the comparison matrix shows not only overlap but also unique content across all sources many of the detailed differences could be attributed to individual strategies for data selection and extraction while there was a big increase in patent derived structures entering pubchem since gvkbio contains over million unique structures from this source venn diagrams showed extensive overlap between compounds extracted by independent expert curation from journals by gvkbio wombat both commercial and bindingdb public but each included unique content in contrast the approved drug collections from gvkbio mddr commercial and drugbank public showed surprisingly low overlap aggregating all commercial sources established that while million compounds overlapped with pubchem million did not conclusion on the basis of chemical structure content per se public sources have covered an increasing proportion of commercial databases over the last two years however commercial products included in this study provide links between compounds and information from patents and journals at a larger scale than current public efforts they also continue to capture a significant proportion of unique content our results thus demonstrate not only an encouraging overall expansion of data supported bioactive chemical space but also that both commercial and public sources are complementary for exploration
tagging has emerged as a powerful mechanism that enables users to nd organize and understand online entities recommender systems similarly enable users to efciently navigate vast collections of items algorithms combining tags with recommenders may deliver both the automation inherent in recommenders and the exibility and conceptual comprehensibility inherent in tagging systems in this paper we explore tagommenders recommender algorithms that predict users preferences for items based on their inferred preferences for tags we describe tag preference inference algorithms based on users interactions with tags and movies and evaluate these algorithms based on tag preference ratings collected from movielens users we design and evaluate algorithms that predict users ratings for movies based on their inferred tag preferences our tag based algorithms generate better recommendation rankings than state of the art algorithms and they may lead to exible recommender systems that leverage the characteristics of items users nd important
the dna of eukaryotic genomes is wrapped in nucleosomes which strongly distort and occlude the dna from access to most dna binding proteins an understanding of the mechanisms that control nucleosome positioning along the dna is thus essential to understanding the binding and action of proteins that carry out essential genetic functions new genome wide data on in vivo and in vitro nucleosome positioning greatly advance our understanding of several factors that can influence nucleosome positioning including dna sequence preferences dna methylation histone variants and posttranslational modifications higher order chromatin structure and the actions of transcription factors chromatin remodelers and other dna binding proteins we discuss how these factors function and ways in which they might be integrated into a unified framework that accounts for both the preservation of nucleosome positioning and the dynamic nucleosome repositioning that occur across biological conditions cell types developmental processes disease
we study the impact of human activity patterns on information diffusion to this end we ran a viral email experiment involving in which we were able to track a specific piece of information through the social network we found that contrary to traditional models information travels at an unexpectedly slow pace by using a branching model which accurately describes the experiment we show that the large heterogeneity found in the response time is responsible for the slow dynamics of information at the collective level given the generality of our result we discuss the important implications of this finding while modeling human dynamical phenomena
change is a fundamental ingredient of interaction patterns in biology technology the economy and science itself interactions within and between organisms change transportation patterns by air land and sea all change the global financial flow changes and the frontiers of scientific research change networks and clustering methods have become important tools to comprehend instances of these large scale structures but without methods to distinguish between real trends and noisy data these approaches are not useful for studying how networks change only if we can assign significance to the partitioning of single networks can we distinguish meaningful structural changes from random fluctuations here we show that bootstrap resampling accompanied by significance clustering provides a solution to this problem to connect changing structures with the changing function of networks we highlight and summarize the significant structural changes with alluvial diagrams and realize de solla price s vision of mapping change in science studying the citation pattern between about scientific journals over the past decade we find that neuroscience has transformed from an interdisciplinary specialty to a mature and stand discipline
gr recent large scale tumor resequencing studies have identified a number of mutations that might be involved in tumorigenesis analysis of the frequency of specific mutations across different tumors has been able to identify some but not all of the mutated genes that contribute to tumor initiation and progression one reason for this is that other functionally important genes are likely to be mutated more rarely and only in specific contexts thus for example mutation in one member of a collection of functionally related genes may result in the same net effect and or mutations in certain genes may be observed less frequently if they play functional roles in later stages of tumor development such as metastasis we modified and applied a network reconstruction and coexpression module identification based approach to identify functionally related gene modules targeted by somatic mutations in cancer this method was applied to available breast cancer colorectal cancer and glioblastoma sequence data and identified wnt tgf beta cross talk wnt vegf signaling and mapk focal adhesion kinase pathways as targets of rare driver mutations in breast colorectal cancer and glioblastoma respectively these mutations do not appear to alter genes that play a central role in these pathways but rather contribute to a more refined shaping or tuning of the functioning of these pathways in such a way as to result in the inhibition of their tumor suppressive signaling arms and thereby conserve or enhance tumor processes
collaborative filtering is the most popular approach to build recommender systems and has been successfully employed in many applications however it cannot make recommendations for so called cold start users that have rated only a very small number of items in addition these methods do not know how confident they are in their recommendations trust based recommendation methods assume the additional knowledge of a trust network among users and can better deal with cold start users since users only need to be simply connected to the trust network on the other hand the sparsity of the user item ratings forces the trust based approach to consider ratings of indirect neighbors that are only weakly trusted which may decrease its precision in order to find a good trade off we propose a random walk model combining the trust based and the collaborative filtering approach for recommendation the random walk model allows us to define and to measure the confidence of a recommendation we performed an evaluation on the epinions dataset and compared our model with existing trust based and collaborative methods
we present a database of copy number variations cnvs detected in disease free individuals using high density snp based oligonucleotide microarrays this large cohort comprised mainly of caucasians and african americans was analyzed for cnvs in a single study using a uniform array platform and computational process we have catalogued and characterized individual cnvs of which were identified in multiple unrelated individuals these nonunique cnvs mapped to distinct regions of genomic variation spanning of the genome of these were previously unreported and are rare our annotation and analysis confirmed and extended previously reported correlations between cnvs and several genomic features such as repetitive dna elements segmental duplications and genes we demonstrate the utility of this data set in distinguishing cnvs with pathologic significance from normal variants together this analysis and annotation provides a useful resource to assist with the assessment of cnvs in the contexts of human variation disease susceptibility and clinical diagnostics
gr clinseq is a pilot project to investigate the use of whole genome sequencing as a tool for clinical research by piloting the acquisition of large amounts of dna sequence data from individual human subjects we are fostering the development of hypothesis generating approaches for performing research in genomic medicine including the exploration of issues related to the genetic architecture of disease implementation of genomic technology informed consent disclosure of genetic information and archiving analyzing and displaying sequence data in the initial phase of clinseq we are enrolling roughly participants the evaluation of each includes obtaining a detailed family and medical history as well as a clinical evaluation the participants are being consented broadly for research on many traits and for whole genome sequencing initially sanger based sequencing of genes thought to be relevant to atherosclerosis is being performed with the resulting data analyzed for rare high penetrance variants associated with specific clinical traits the participants are also being consented to allow the contact of family members for additional studies of sequence variants to explore their potential association with specific phenotypes here we present the general considerations in designing clinseq preliminary results based on the generation of an initial mb of sequence data the findings for several genes that serve as positive controls for the project and our views about the potential implications of clinseq the early experiences with clinseq illustrate how large scale medical sequencing can be a practical productive and critical component of research in medicine
motivation cross platform microarray analysis is an increasingly important research tool but researchers still lack open source tools for storing integrating and analyzing large amounts of microarray data obtained from different array platforms results an open source integrated microarray database and analysis suite webarraydb http www webarraydb org has been developed that features convenient uploading of data for storage in a miame minimal information about a microarray experiment compliant fashion and allows data to be mined with a large variety of r based tools including data analysis across multiple platforms different methods for probe alignment normalization and statistical analysis are included to account for systematic bias student s t test moderated t tests non parametric tests and analysis of variance or covariance anova ancova are among the choices of algorithms for differential analysis of data users also have the flexibility to define new factors and create new analysis models to fit complex experimental designs all data can be queried or browsed through a web browser the computations can be performed in parallel on symmetric multiprocessing smp systems or linux clusters availability the software package is available for the use on a public web server http www webarraydb org or can be downloaded contact gmail com mcclelland michael gmail com yipengw gmail com supplementary information supplementary data are available at bioinformatics bioinformatics
motivation antibody based chromatin immunoprecipitation assay followed by high throughput sequencing technology chip seq is a relatively new method to study the binding patterns of specific protein molecules over the entire genome chip seq technology allows scientist to get more comprehensive results in shorter time here we present a non linear normalization algorithm and a mixture modeling method for comparing chip seq data from multiple samples and characterizing genes based on their rna polymerase ii pol ii binding patterns results we apply a two step non linear normalization method based on locally weighted regression loess approach to compare chip seq data across multiple samples and model the difference using an exponential normalk mixture model fitted model is used to identify genes associated with differential binding sites based on local false discovery rate fdr these genes are then standardized and hierarchically clustered to characterize their pol ii binding patterns as a case study we apply the analysis procedure comparing normal breast cancer to tamoxifen resistant oht cell line we find enriched regions that are associated with cancer p our findings also imply that there may be a dysregulation of cell cycle and gene expression control pathways in the tamoxifen resistant cells these results show that the non linear normalization method can be used to analyze chip seq data across multiple samples availability data are available at http www bmi osu edu khuang data chip rnapii contact taslim osu edu khuang bmi osu edusupplementary information supplementary data are available at online
abstract background the creation of accurate quantitative systems biology markup language sbml models is a time intensive manual process often complicated by the many data sources and formats required to annotate even a small and well scoped model ideally the retrieval and integration of biological knowledge for model annotation should be performed quickly precisely and with a minimum of manual effort results here we present rule based mediation a method of semantic data integration applied to systems biology model annotation the heterogeneous data sources are first syntactically converted into ontologies which are then aligned to a small domain ontology by applying a rule base we demonstrate proof of principle of this application of rule based mediation using off the shelf semantic web technology through two use cases for sbml model annotation existing tools and technology provide a framework around which the system is built reducing development time and increasing usability conclusions integrating resources in this way accommodates multiple formats with different semantics and provides richly modelled biological knowledge suitable for annotation of sbml models this initial work establishes the feasibility of rule based mediation as part of an automated sbml model annotation system availability detailed information on the project files as well as further information on and comparisons with similar projects is available from the project page at http cisban silico cs ncl ac rbm
schistosoma mansoni is responsible for the neglected tropical disease schistosomiasis that affects million people in countries here we present analysis of the megabase nuclear genome of the blood fluke it encodes at least genes with an unusual intron size distribution and new families of micro exon genes that undergo frequent alternative splicing as the first sequenced flatworm and a representative of the lophotrochozoa it offers insights into early events in the evolution of the animals including the development of a body pattern with bilateral symmetry and the development of tissues into organs our analysis has been informed by the need to find new drug targets the deficits in lipid metabolism that make schistosomes dependent on the host are revealed and the identification of membrane receptors ion channels and more than proteases provide new insights into the biology of the life cycle and new targets bioinformatics approaches have identified metabolic chokepoints and a chemogenomic screen has pinpointed schistosome proteins for which existing drugs may be active the information generated provides an invaluable resource for the research community to develop much needed new control tools for the treatment and eradication of this important and disease
in recent years strikingly consistent patterns of biodiversity have been identified over space time organism type and geographical a neutral theory assuming no environmental selection or organismal interactions has been shown to predict many patterns of ecological this theory is based on a mechanism by which new species arise similarly to point mutations in a population without sexual reproduction here we report the simulation of populations with sexual reproduction mutation and dispersal we found simulated time dependence of speciation rates speciesarea relationships and species abundance distributions consistent with the behaviours found in from our results we predict steady speciation rates more species in one dimensional environments than two dimensional environments three scaling regimes of speciesarea relationships and lognormal distributions of species abundance with an excess of rare species and a tail that may be approximated by fisher s logarithmic series these are consistent with dependences reported for among others global and flowering marine invertebrate ray finned british and north american mammal fossils from and panamanian quantitative comparisons of specific cases are remarkably successful our biodiversity results provide additional evidence that species diversity arises without specific physical this is similar to heavy traffic flows where traffic jams can form even without or
gr new high throughput sequencing technologies are generating large amounts of sequence data allowing the development of targeted large scale resequencing studies for these studies accurate identification of polymorphic sites is crucial heterozygous sites are particularly difficult to identify especially in regions of low coverage we present a new strategy for identifying heterozygous sites in a single individual by using a machine learning approach that generates a heterozygosity score for each chromosomal position our approach also facilitates the identification of regions with unequal representation of two alleles and other poorly sequenced regions the availability of confidence scores allows for a principled combination of sequencing results from multiple samples we evaluate our method on a gold standard data genotype set from hapmap we are able to classify sites in this data set as heterozygous or homozygous with accuracy in de novo data our probabilistic heterozygote detection probhd is able to identify of heterozygous sites at a false call rate fcr as estimated based on independent genotyping results in direct comparison of probhd with high coverage genomes sequencing available for a subset of our data we observe overall agreement for genotype calls and close to agreement for heterozygote calls overall our data indicate that high throughput resequencing of human genomic regions requires careful attention to systematic biases in sample preparation as well as sequence contexts and that their impact can be alleviated by machine learning based sequence analyses allowing more accurate extraction of true variants
deletions and duplications of chromosomal segments copy number variants cnvs are a major source of variation between individual humans and are an underlying factor in human evolution and in many diseases including mental illness developmental disorders and cancer cnvs form at a faster rate than other types of mutation and seem to do so by similar mechanisms in bacteria yeast and humans here we review current models of the mechanisms that cause copy number variation non homologous end joining mechanisms are well known but recent models focus on perturbation of dna replication and replication of non contiguous dna segments for example cellular stress might induce repair of broken replication forks to switch from high fidelity homologous recombination to non homologous repair thus promoting copy change
summary multi mapping sequence tags are a significant impediment to short read sequencing platforms these tags are routinely omitted from further analysis leading to experimental bias and reduced coverage here we present mumrescuelite a low resource requirement version of the mumrescue software that has been used by several next generation sequencing projects to probabilistically reincorporate multi mapping tags into mapped short read data availability and implementation mumrescuelite is written in python executables and documentation are available from http genome gsc riken jp osc english software contact geoff faulkner roslin ed ac bioinformatics
background phylogenetic studies using expressed sequence tags est are becoming a standard approach to answer evolutionary questions such studies are usually based on large sets of newly generated unannotated and error prone est sequences from different species a first crucial step in est based phylogeny reconstruction is to identify groups of orthologous sequences from these data sets appropriate target genes are selected and redundant sequences are eliminated to obtain suitable sequence sets as input data for tree reconstruction software generating such data sets manually can be very time consuming thus software tools are needed that carry out these steps automatically results we developed a flexible and user friendly software pipeline running on desktop machines or computer clusters that constructs data sets for phylogenomic analyses it automatically searches assembled est sequences against databases of orthologous groups og assigns ests to these predefined ogs translates the sequences into proteins eliminates redundant sequences assigned to the same og creates multiple sequence alignments of identified orthologous sequences and offers the possibility to further process this alignment in a last step by excluding potentially homoplastic sites and selecting sufficiently conserved parts our software pipeline can be used as it is but it can also be adapted by integrating additional external programs this makes the pipeline useful for non bioinformaticians as well as to bioinformatic experts the software pipeline is especially designed for ests but it can also handle protein sequences conclusion orthoselect is a tool that produces orthologous gene alignments from assembled ests our tests show that orthoselect detects orthologs in est libraries with high accuracy in the absence of a gold standard for orthology prediction we compared predictions by orthoselect to a manually created and published phylogenomic data set our tool was not only able to rebuild the data set with a specificity of but it detected four percent more orthologous sequences furthermore the results orthoselect produces are in absolut agreement with the results of other programs but our tool offers a significant speedup and additional functionality e g handling of ests computing sequence alignments and refining them to our knowledge there is currently no fully automated and freely available tool for this purpose thus orthoselect is a valuable tool for researchers in the field of phylogenomics who deal with large quantities of est sequences orthoselect is written in perl and runs on linux mac os x the tool can be downloaded at http gobics de fabian php
gr each human carries a large number of deleterious mutations together these mutations make a significant contribution to human disease identification of deleterious mutations within individual genome sequences could substantially impact an individual s health through personalized prevention and treatment of disease yet distinguishing deleterious mutations from the massive number of nonfunctional variants that occur within a single genome is a considerable challenge using a comparative genomics data set of vertebrate species we show that a likelihood ratio test lrt can accurately identify a subset of deleterious mutations that disrupt highly conserved amino acids within protein coding sequences which are likely to be unconditionally deleterious the lrt is also able to identify known human disease alleles and performs as well as two commonly used heuristic methods sift and polyphen application of the lrt to three human genomes reveals deleterious mutations per individual of which are estimated to be at allele frequency however the overlap between predictions made by the lrt sift and polyphen is low of predictions are unique to one of the three methods and only of predictions are shared across all three methods our results indicate that only a small subset of deleterious mutations can be reliably identified but that this subset provides the raw material for medicine
central memory t cells provide a pool of lymph node homing ag experienced cells that are capable of responding rapidly after a secondary infection we have previously described a population of central memory t cells in leishmania major infected mice that were capable of mediating immunity to a secondary infection in this study we show that the leishmania specific central memory t cells require il to produce ifn gamma demonstrating that this population needs additional signals to develop into cells in contrast effector cells isolated from immune mice produced ifn gamma in vitro or in vivo in the absence of il in addition we found that when central memory t cells were adoptively transferred into il deficient hosts many of the cells became il producers these studies indicate that the central memory t cell population generated during l major infection is capable of developing into either or effectors thus continued il production may be required to ensure the development of cells from this central memory t cell pool a finding that has direct relevance to the design of vaccines dependent upon central memory cells
background comparative genomics has revealed extensive horizontal gene transfer among prokaryotes a development that is often considered to undermine the tree of life concept however the possibility remains that a statistical central trend still exists in the phylogenetic forest of life results a comprehensive comparative analysis of a forest of phylogenetic trees for prokaryotic genes revealed a consistent phylogenetic signal particularly among nearly universal trees despite high levels of topological inconsistency probably due to horizontal gene transfer horizontal transfers seemed to be distributed randomly and did not obscure the central trend the nearly universal trees were topologically similar to numerous other trees thus the nearly universal trees might reflect a significant central tendency although they cannot represent the forest completely however topological consistency was seen mostly at shallow tree depths and abruptly dropped at the level of the radiation of archaeal and bacterial phyla suggesting that early phases of evolution could be non tree like biological big bang simulations of evolution under compressed cladogenesis or biological big bang yielded a better fit to the observed dependence between tree inconsistency and phylogenetic depth for the compressed cladogenesis model conclusions horizontal gene transfer is pervasive among prokaryotes very few gene trees are fully consistent making the original tree of life concept obsolete a central trend that most probably represents vertical inheritance is discernible throughout the evolution of archaea and bacteria although compressed cladogenesis complicates unambiguous resolution of the relationships between the major archaeal and clades
millions of contemporary young adults use social networking sites however little is known about how much why and how they use these sites in this study undergraduates completed a diary like measure each day for a week reporting daily time use and responding to an activities checklist to assess their use of the popular social networking site facebook at the end of the week they also completed a follow up survey results indicated that students use facebook approximately throughout the day as part of their daily routine students communicated on facebook using a one to many style in which they were the creators disseminating content to their friends even so they spent more time observing content on facebook than actually posting content facebook was used most often for social interaction primarily with friends with whom the students had a pre established relationship offline in addition to classic identity markers of emerging adulthood such as religion political ideology and work young adults also used media preferences to express their identity implications of social networking site use for the development of identity and peer relationships discussed
the ruby programming language has a lot to offer to any scientist with electronic data to process not only is the initial learning curve very shallow but its reflection and meta programming capabilities allow for the rapid creation of relatively complex applications while still keeping the code short and readable this paper provides a gentle introduction to this scripting language for researchers without formal informatics training such as many wet lab scientists we hope this will provide such researchers an idea of how powerful a tool ruby can be for their data management tasks and encourage them to learn more about it aerts and law licensee biomed ltd
high density strand specific cdna sequencing ssrna seq was used to analyze the transcriptome of salmonella enterica serovar typhi s typhi by mapping sequence data to the entire s typhi genome we analyzed the transcriptome in a strand specific manner and further defined transcribed regions encoded within prophages pseudogenes previously un annotated and or untranslated regions utr an additional novel candidate non coding rnas were identified beyond those previously annotated proteomic analysis was combined with transcriptome data to confirm and refine the annotation of a number of hpothetical genes ssrna seq was also combined with microarray and proteome analysis to further define the s typhi ompr regulon and identify novel ompr regulated transcripts thus ssrna seq provides a novel and powerful approach to the characterization of the transcriptome
a major challenge in current biology is to understand the genetic basis of variation for quantitative traits we review the principles of quantitative trait locus mapping and summarize insights about the genetic architecture of quantitative traits that have been obtained over the past decades we are currently in the midst of a genomic revolution which enables us to incorporate genetic variation in transcript abundance and other intermediate molecular phenotypes into a quantitative trait locus mapping framework this systems genetics approach enables us to understand the biology inside the black box that lies between genotype and phenotype in terms of causal networks of genes
human learning is distinguished by the range and complexity of skills that can be learned and the degree of abstraction that can be achieved compared with those of other species homo sapiens is also the only species that has developed formal ways to enhance learning teachers schools and curricula human infants have an intense interest in people and their behavior and possess powerful implicit learning mechanisms that are affected by social interaction neuroscientists are beginning to understand the brain mechanisms underlying learning and how shared brain systems for perception and action support social learning machine learning algorithms are being developed that allow robots and computers to learn autonomously new insights from many different fields are converging to create a new science of learning that may transform practices
analysis of neandertal dna holds great potential for investigating the population history of this group of hominins but progress has been limited due to the rarity of samples and damaged state of the dna we present a method of targeted ancient dna sequence retrieval that greatly reduces sample destruction and sequencing demands and use this method to reconstruct the complete mitochondrial dna mtdna genomes of five neandertals from across their geographic range we find that mtdna genetic diversity in neandertals that lived to years ago was approximately one third of that in contemporary modern humans together with analyses of mtdna protein evolution these data suggest that the long term effective population size of neandertals was smaller than that of modern humans and extant apes
background r is the statistical language commonly used by many life scientists in omics data analysis at the same time these complex analyses benefit from a workflow approach such as used by the open source workflow management system taverna however taverna had limited support for r because it supported just a few data types and only a single output also there was no support for graphical output and persistent sessions altogether this made using r in taverna impractical findings we have developed an r plugin for taverna rshell which provides r functionality within workflows designed in taverna in order to fully support the r language our rshell plugin directly uses the r interpreter the rshell plugin consists of a taverna processor for r scripts and an rshell session manager that communicates with the r server we made the rshell processor highly configurable allowing the user to define multiple inputs and outputs also various data types are supported such as strings numeric data and images to limit data transport between multiple rshell processors the rshell plugin also supports persistent sessions here we will describe the architecture of rshell and the new features that are introduced in version i e i support for r up to and including r version ii support for persistent sessions to limit data transfer iii support for vector graphics output through pdf iv syntax highlighting of the r code v improved usability through fewer port types our new rshell processor is backwards compatible with workflows that use older versions of the rshell processor we demonstrate the value of the rshell processor by a use case workflow that maps oligonucleotide probes designed with dna sequence information from vega onto the ensembl genome assembly conclusion our rshell plugin enables taverna users to employ r scripts within their workflows in a highly way
reading is essential in modern societies but many children have dyslexia a difficulty in learning to read dyslexia often arises from impaired phonological awareness the auditory analysis of spoken language that relates the sounds of language to print behavioral remediation especially at a young age is effective for many but not all children neuroimaging in children with dyslexia has revealed reduced engagement of the left temporo parietal cortex for phonological processing of print altered white matter connectivity and functional plasticity associated with effective intervention behavioral and brain measures identify infants and young children at risk for dyslexia and preventive intervention is often effective a combination of evidence based teaching practices and cognitive neuroscience measures could prevent dyslexia from occurring in the majority of children who would otherwise dyslexia
motivation the patterns of sequence similarity and divergence present within functionally diverse evolutionarily related proteins contain implicit information about corresponding biochemical similarities and differences a first step toward accessing such information is to statistically analyze these patterns which in turn requires that one first identify and accurately align a very large set of protein sequences ideally the set should include many distantly related functionally divergent subgroups because it is extremely difficult if not impossible for fully automated methods to align such sequences correctly researchers often resort to manual curation based on detailed structural and biochemical information however multiply aligning vast numbers of sequences in this way is clearly impractical results this problem is addressed using multiply aligned profiles for global alignment of protein sequences mapgaps the mapgaps program uses a set of multiply aligned profiles both as a query to detect and classify related sequences and as a template to multiply align the sequences it relies on karlin altschul statistics for sensitivity and on psi blast and other heuristics for speed using as input a carefully curated multiple profile alignment for p loop gtpases mapgaps correctly aligned weakly conserved sequence motifs within distantly related gtpases of known structure by comparison the sequence and structurally based alignment methods hmmalign and misaligned at least and of these regions respectively when applied to a dataset of million protein sequences mapgaps identified classified and aligned with comparable accuracy nearly half a million putative p loop gtpase sequences availability a c implementation of mapgaps is available at http mapgaps igs umaryland edu contact aneuwald som umaryland edu supplementary information supplementary data are available at bioinformatics bioinformatics
we describe the genome sequencing of an anonymous individual of african origin using a novel ligation based sequencing assay that enables a unique form of error correction that improves the raw accuracy of the aligned reads to allowing us to accurately call snps with as few as two reads per allele we collected several billion mate paired reads yielding approximately haploid coverage of aligned sequence and close to clone coverage over of the reference genome is covered with at least one uniquely placed read and is spanned by at least one uniquely placed mate paired clone we identify over million snps of which are novel mate paired data are used to physically resolve haplotype phases of nearly two thirds of the genotypes obtained and produce phased segments of up to kb we detect intra read indels indels between mate paired reads inversions and four gene fusions we use a novel approach for detecting indels between mate paired reads that are smaller than the standard deviation of the insert size of the library and discover deletions in common with those detected with our intra read approach dozens of mutations previously described in omim and hundreds of nonsynonymous single nucleotide and structural variants in genes previously implicated in disease are identified in this individual there is more genetic variation in the human genome still to be uncovered and we provide guidance for future surveys in populations and biopsies
lysine acetylation is a reversible posttranslational modification of proteins and plays a key role in regulating gene expression technological limitations have so far prevented a global analysis of lysine acetylation s cellular roles we used high resolution mass spectrometry to identify lysine acetylation sites on proteins and quantified acetylation changes in response to the deacetylase inhibitors suberoylanilide hydroxamic acid and ms lysine acetylation preferentially targets large macromolecular complexes involved in diverse cellular processes such as chromatin remodeling cell cycle splicing nuclear transport and actin nucleation acetylation impaired phosphorylation dependent interactions of and regulated the yeast cyclin dependent kinase our data demonstrate that the regulatory scope of lysine acetylation is broad and comparable with that of other major posttranslational science
gene expression can mean the difference between a functional and non functional genome between health and disease and with the development of transgenic crops the difference between survival and starvation in dna protein interactions principles and protocols third edition this vital subject is brought up to date with protocols exploring the most cutting edge developments in the field including in vivo and genome wide interaction techniques addressing topics such as chromatin immunoprecipitation topological studies photocrosslinking fret and imaging techniques the volume fully updates and expands upon the successful previous editions written in the convenient and informative methods in molecular biology series format chapters include introductions to their respective subjects lists of the necessary materials and reagents step by step readily reproducible laboratory protocols and notes on troubleshooting and avoiding known pitfalls comprehensive and authoritative dna protein interactions principles and protocols third edition serves as an ideal guide for all those exploring this dynamic essential and increasingly affordable area research
we present a tool that assesses the enrichment of significant associations from genome wide association studies gwas in a pathway context the snp ratio test srt compares the proportion of significant to all snps within genes that are part of a pathway and computes an empirical p value based on comparisons to ratios in datasets where the assignment of case control status has been randomized we applied the srt to a parkinson s disease gwas dataset using the kegg database revealing significance for parkinson s disease and related pathways availability https sourceforge net snpratiotest
genomic experiments produce multiple views of biological systems among them are dna sequence and copy number variation and mrna and protein abundance understanding these systems needs integrated bioinformatic analysis public databases such as ensembl provide relationships and mappings between the relevant sets of probe and target molecules however the relationships can be biologically complex and the content of the databases is dynamic we demonstrate how to use the computational environment r to integrate and jointly analyze experimental datasets employing biomart web services to provide the molecule mappings we also discuss typical problems that are encountered in making gene to transcript to protein mappings the approach provides a flexible programmable and reproducible basis for state of the art bioinformatic integration
protein domains are the common currency of protein structure and function over such protein families have now been collected in the pfam database using these data along with animal gene phylogenies from treefam allowed us to investigate the gain and loss of protein domains most gains and losses of domains occur at protein termini we show that the nature of changes is similar after speciation or duplication events however changes in domain architecture happen at a higher frequency after gene duplication we suggest that the bias towards protein termini is largely because insertion and deletion of domains at most positions in a protein are likely to disrupt the structure of existing domains we can also use pfam to trace the evolution of specific families for example the immunoglobulin superfamily can be traced over million years during its expansion into one of the largest families in the human genome it can be shown that this protein family has its origins in basic animals such as the poriferan sponges where it is found in cell surface receptor proteins we can trace how the structure and sequence of this family diverged during vertebrate evolution into constant and variable domains that are found in the antibodies of our immune system as well as in neural and proteins
pnas a plausible representation of the relational information among entities in dynamic systems such as a living cell or a social community is a stochastic network that is topologically rewiring and semantically evolving over time although there is a rich literature in modeling static or temporally invariant networks little has been done toward recovering the network structure when the networks are not observable in a dynamic context in this article we present a machine learning method called tesla which builds on a temporally smoothed regularized logistic regression formalism that can be cast as a standard convex optimization problem and solved efficiently by using generic solvers scalable to large networks we report promising results on recovering simulated time varying networks and on reverse engineering the latent sequence of temporally rewiring political and academic social networks from longitudinal data and the evolving gene networks over genes during the life cycle of from a microarray time course at a resolution limited only by frequency
next generation dna sequencing technologies have revolutionized diverse genomics applications including de novo genome sequencing snp detection chromatin immunoprecipitation and transcriptome analysis here we apply deep sequencing to genome scale fitness profiling to evaluate yeast strain collections in parallel this method barcode analysis by sequencing or bar seq outperforms the current benchmark barcode microarray assay in terms of both dynamic range and throughput when applied to a complex chemogenomic assay bar seq quantitatively identifies drug targets with performance superior to the benchmark microarray assay we also show that bar seq is well suited for a multiplex format we completely re sequenced and re annotated the yeast deletion collection using deep sequencing found that approximately of the barcodes and common priming sequences varied from expectation and used this revised list of barcode sequences to improve data quality together this new assay and analysis routine provide a deep sequencing based toolkit for identifying gene environment interactions on a genome scale
motivation permutation testing is very popular for analyzing microarray data to identify differentially expressed de genes estimating false discovery rates fdrs is a very popular way to address the inherent multiple testing problem however combining these approaches may be problematic when sample sizes are unequal results with unbalanced data permutation tests may not be suitable because they do not test the hypothesis of interest in addition permutation tests can be biased using biased p values to estimate the fdr can produce unacceptable bias in those estimates results also show that the approach of pooling permutation null distributions across genes can produce invalid p values since even non de genes can have different permutation null distributions we encourage researchers to use statistics that have been shown to reliably discriminate de genes but caution that associated p values may be either invalid or a less effective metric for discriminating genes
summary understanding the relationships between pathways and the altered expression of their components in disease conditions can be addressed in a visual data analysis process caleydo uses novel visualization techniques to support life science experts in their analysis of gene expression data in the context of pathways and functions of individual genes pathways and gene expression visualizations are placed in a scene where selected entities i e genes are visually connected this allows caleydo to seamlessly integrate interactive gene expression visualization with cross database pathway exploration availability the caleydo visualization framework is freely available on www caleydo org for non commercial use it runs on windows and linux and requires a capable graphics card contact caleydo icg tugraz at streit icg tugraz at supplementary information supplementary data are available at bioinformatics bioinformatics
nodes in real world networks are usually organized in local modules these groups called communities are intuitively defined as subgraphs with a larger density of internal connections than of external links in this work we define a measure aimed at quantifying the statistical significance of single communities extreme and order statistics are used to predict the statistics associated with individual clusters in random graphs these distributions allows us to define one community significance as the probability that a generic clustering algorithm finds such a group in a random graph the method is successfully applied in the case of real world networks for the evaluation of the significance of communities
micrornas mirnas have emerged as key post transcriptional regulators of gene expression involved in diverse physiological and pathological processes although mirnas can function as both tumour suppressors and oncogenes in tumour a widespread downregulation of mirnas is commonly observed in human cancers and promotes cellular transformation and this indicates an inherent significance of small rnas in tumour suppression however the connection between tumour suppressor networks and mirna biogenesis machineries has not been investigated in depth here we show that a central tumour suppressor enhances the post transcriptional maturation of several mirnas with growth suppressive function including mir mir and mir in response to dna damage in cells and human diploid fibroblasts interacts with the drosha processing complex through the association with dead box rna helicase also known as and facilitates the processing of primary mirnas to precursor mirnas we also found that transcriptionally inactive mutants interfere with a functional assembly between drosha complex and leading to attenuation of mirna processing activity these findings suggest that transcription independent modulation of mirna biogenesis is intrinsically embedded in a tumour suppressive program governed by our study reveals a previously unrecognized function of in mirna processing which may underlie key aspects of biology
with the advent of ultra high throughput sequencing technologies increasingly researchers are turning to deep sequencing for gene expression studies here we present a set of rigorous methods for normalization quantification of noise and co expression analysis of deep sequencing data using these methods on cap analysis of gene expression cage samples of transcription start sites we construct genome wide promoteromes in human and mouse consisting of a three tiered hierarchy of transcription start sites transcription start clusters and transcription regions
background we previously established that six sequence specific transcription factors that initiate anterior posterior patterning in drosophila bind to overlapping sets of thousands of genomic regions in blastoderm embryos while regions bound at high levels include known and probable functional targets more poorly bound regions are preferentially associated with housekeeping genes and or genes not transcribed in the blastoderm and are frequently found in protein coding sequences or in less conserved non coding dna suggesting that many are likely non functional results here we show that an additional transcription factors that regulate other aspects of embryo patterning show a similar quantitative continuum of function and binding to thousands of genomic regions in vivo collectively the regulators show a surprisingly high overlap in the regions they bind given that they belong to dna binding domain families specify distinct developmental fates and can act via different cis regulatory modules we demonstrate however that quantitative differences in relative levels of binding to shared targets correlate with the known biological and transcriptional regulatory specificities of these factors conclusions it is likely that the overlap in binding of biochemically and functionally unrelated transcription factors arises from the high concentrations of these proteins in nuclei which coupled with their broad dna binding specificities directs them to regions of open chromatin we suggest that most animal transcription factors will be found to show a similar broad overlapping pattern of binding in vivo with specificity achieved by modulating the amount rather than the identity of factor
systematic reviews and meta analyses are essential to summarize evidence relating to efficacy and safety of health care interventions accurately and reliably the clarity and transparency of these reports however is not optimal poor reporting of systematic reviews diminishes their value to clinicians policy makers and other users since the development of the quorom ality f eporting f eta analysis statementa reporting guideline published in have been several conceptual methodological and practical advances regarding the conduct and reporting of systematic reviews and meta analyses also reviews of published systematic reviews have found that key information about these studies is often poorly reported realizing these issues an international group that included experienced authors and methodologists developed prisma preferred reporting items for systematic reviews and meta analyses as an evolution of the original quorom guideline for systematic reviews and meta analyses of evaluations of health care interventions the prisma statement consists of a item checklist and a four phase flow diagram the checklist includes items deemed essential for transparent reporting of a systematic review in this explanation and elaboration document we explain the meaning and rationale for each checklist item for each item we include an example of good reporting and where possible references to relevant empirical studies and methodological literature the prisma statement this document and the associated web site should be helpful resources to improve reporting of systematic reviews and analyses
for decades we tacitly assumed that the components of such complex systems as the cell the society or the internet are randomly wired together in the past decade an avalanche of research has shown that many real networks independent of their age function and scope converge to similar architectures a universality that allowed researchers from different disciplines to embrace network theory as a common paradigm the decade old discovery of scale free networks was one of those events that had helped catalyze the emergence of network science a new research field with its distinct set of challenges and science
transcriptional regulatory circuits govern how cis and trans factors transform signals into messenger rna mrna expression levels with advances in quantitative and high throughput technologies that allow measurement of gene expression state in different conditions data that can be used to build and test models of transcriptional regulation is being generated at a rapid pace here we review experimental and computational methods used to derive detailed quantitative circuit models on a small scale and cruder genome wide models on a large scale we discuss the potential of combining small and large scale approaches to understand the working and wiring of transcriptional circuits
what is big data anyway gigabytes terabytes petabytes a brief personal memory may provide some perspective in the late at columbia university i had the chance to play around with what at the time was a truly enormous disk the ibm mss mass storage system the mss was actually a fully automatic robotic tape library and associated staging disks to make random access if not exactly instantaneous at least fully transparent in columbias configuration it stored a total of around it was already on its way out by the time i got my hands on it but in its heyday the early to mid it had been used to support access by social scientists to what was unquestionably big data at the time the entire u s census database presumably there was no other practical way to provide the researchers with ready access to a dataset that large at close to per gb disk
high throughput complementary dna sequencing rna seq is a powerful tool for whole transcriptome analysis supplying information about a transcript s expression level and structure however it is difficult to determine the polarity of transcripts and therefore identify which strand is transcribed here we present a simple cdna sequencing protocol that preserves information about a transcript s direction using saccharomyces cerevisiae and mouse brain transcriptomes as models we demonstrate that knowing the transcript s orientation allows more accurate determination of the structure and expression of genes it also helps to identify new genes and enables studying promoter associated and antisense transcription the transcriptional landscapes we obtained are online
objective to understand belief in a specific scientific claim by studying the pattern of citations among papers stating it design a complete citation network was constructed from all pubmed indexed english literature papers addressing the belief that beta amyloid a protein accumulated in the brain in alzheimer s disease is produced by and injures skeletal muscle of patients with inclusion body myositis social network theory and graph theory were used to analyse this network main outcome measures citation bias amplification and invention and their effects on determining authority results the network contained papers and citations addressing the belief with citation paths supporting it unfounded authority was established by citation bias against papers that refuted or weakened the belief amplification the marked expansion of the belief system by papers presenting no data addressing it and forms of invention such as the conversion of hypothesis into fact through citation alone extension of this network into text within grants funded by the national institutes of health and obtained through the freedom of information act showed the same phenomena present and sometimes used to justify requests for funding conclusion citation is both an impartial scholarly method and a powerful form of social communication through distortions in its social use that include bias amplification and invention citation can be used to generate information cascades resulting in unfounded authority of claims construction and analysis of a claim specific citation network may clarify the nature of a published belief system and expose distorted methods of citation
the current economic crisis illustrates a critical need for new and fundamental understanding of the structure and dynamics of economic networks economic systems are increasingly built on interdependencies implemented through trans national credit and investment networks trade relations or supply chains that have proven difficult to predict and control we need therefore an approach that stresses the systemic complexity of economic networks and that can be used to revise and extend established paradigms in economic theory this will facilitate the design of policies that reduce conflicts between individual interests and global efficiency as well as reduce the risk of global failure by making economic networks more science
biodiversity research typically focuses on species richness and has often neglected interactions either by assuming that such interactions are homogeneously distributed or by addressing only the interactions between a pair of species or a few species at a time in contrast a network approach provides a powerful representation of the ecological interactions among species and highlights their global interdependence understanding how the responses of pairwise interactions scale to entire assemblages remains one of the great challenges that must be met as society faces global ecosystem science
we live in an increasingly interconnected world of techno social systems in which infrastructures composed of different technological layers are interoperating within the social component that drives their use and development examples are provided by the internet the world wide web wifi communication technologies and transportation and mobility infrastructures the multiscale nature and complexity of these networks are crucial features in understanding and managing the networks the accessibility of new data and the advances in the theory and modeling of complex networks are providing an integrated framework that brings us closer to achieving true predictive power of the behavior of techno social science
attribution of biological robustness to the specific structural properties of a regulatory network is an important yet unsolved problem in systems biology it is widely believed that the topological characteristics of a biological control network largely determine its dynamic behavior yet the actual mechanism is still poorly understood here we define a novel structural feature of biological networks termed regulation entropy to quantitatively assess the influence of network topology on the robustness of the systems using the cell cycle control networks of the budding yeast saccharomyces cerevisiae and the fission yeast schizosaccharomyces pombe as examples we first demonstrate the correlation of this quantity with the dynamic stability of biological control networks and then we establish a significant association between this quantity and the structural stability of the networks and we further substantiate the generality of this approach with a broad spectrum of biological and random networks we conclude that the regulation entropy is an effective order parameter in evaluating the robustness of biological control networks our work suggests a novel connection between the topological feature and the dynamic property of biological networks
the breadth of genomic diversity found among organisms in nature allows populations to adapt to diverse however genomic diversity is difficult to generate in the laboratory and new phenotypes do not easily arise on practical although in vitro and directed evolution have created genetic variants with usefully altered phenotypes these methods are limited to laborious and serial manipulation of single genes and are not used for parallel and continuous directed evolution of gene networks or genomes here we describe multiplex automated genome engineering mage for large scale programming and evolution of cells mage simultaneously targets many locations on the chromosome for modification in a single cell or across a population of cells thus producing combinatorial genomic diversity because the process is cyclical and scalable we constructed prototype devices that automate the mage technology to facilitate rapid and continuous generation of a diverse set of genetic changes mismatches insertions deletions we applied mage to optimize the deoxy d xylulose phosphate dxp biosynthesis pathway in escherichia coli to overproduce the industrially important isoprenoid lycopene twenty four genetic components in the dxp pathway were modified simultaneously using a complex pool of synthetic dna creating over billion combinatorial genomic variants per day we isolated variants with more than fivefold increase in lycopene production within a significant improvement over existing metabolic engineering techniques our multiplex approach embraces engineering in the context of evolution by expediting the design and evolution of organisms with new and properties
we develop the concept of dragon kings corresponding to meaningful outliers which are found to coexist with power laws in the distributions of event sizes under a broad range of conditions in a large variety of systems these dragon kings reveal the existence of mechanisms of self organization that are not apparent otherwise from the distribution of their smaller siblings we present a generic phase diagram to explain the generation of dragon kings and document their presence in six different examples distribution of city sizes distribution of acoustic emissions associated with material failure distribution of velocity increments in hydrodynamic turbulence distribution of financial drawdowns distribution of the energies of epileptic seizures in humans and in model animals distribution of the earthquake energies we emphasize the importance of understanding dragon kings as being often associated with a neighborhood of what can be called equivalently a phase transition a bifurcation a catastrophe in the sense of rene thom or a tipping point the presence of a phase transition is crucial to learn how to diagnose in advance the symptoms associated with a coming dragon king several examples of predictions using the derived log periodic power law method are discussed including material failure predictions and the forecasts of the end of bubbles
we investigate tag and query logs to see if the terms people use to annotate websites are similar to the ones they use to query for them over a set of urls we compare the distribution of tags used to annotate each url with the distribution of query terms for clicks on the same url understanding the relationship between the distributions is important to determine how useful tag data may be for improving search results and conversely query data for improving tag prediction in our study we compare both term frequency distributions using vocabulary overlap and relative entropy we also test statistically whether the term counts come from the same underlying distribution our results indicate that the vocabulary used for tagging and searching for content are similar but not identical we further investigate the content of the websites to see which of the two distributions tag or query is most similar to the content of the annotated searched url finally we analyze the similarity for different categories of urls in our sample to see if the similarity between distributions is dependent on the topic of the website or the popularity of url
social tagging is becoming increasingly popular in many web applications where users can annotate resources e g web pages with arbitrary keywords i e tags a tag recommendation module can assist users in tagging process by suggesting relevant tags to them it can also be directly used to expand the set of tags annotating a resource the benefits are twofold improving user experience and enriching the index of resources however the former one is not emphasized in previous studies though a lot of work has reported that different users may describe the same concept in different ways we address the problem of personalized tag recommendation for text documents in particular we model personalized tag recommendation as a query and ranking problem and propose a novel graph based ranking algorithm for interrelated multi type objects when a user issues a tagging request both the document and the user are treated as a part of the query tags are then ranked by our graph based ranking algorithm which takes into consideration both relevance to the document and preference of the user finally the top ranked tags are presented to the user as suggestions experiments on a large scale tagging data set collected from del icio us have demonstrated that our proposed algorithm significantly outperforms algorithms which fail to consider the diversity of different interests
background bisulfite sequencing is a powerful technique to study dna cytosine methylation bisulfite treatment followed by pcr amplification specifically converts unmethylated cytosines to thymine coupled with next generation sequencing technology it is able to detect the methylation status of every cytosine in the genome however mapping high throughput bisulfite reads to the reference genome remains a great challenge due to the increased searching space reduced complexity of bisulfite sequence asymmetric cytosine to thymine alignments and multiple cpg heterogeneous methylation results we developed an efficient bisulfite reads mapping algorithm bsmap to address the above issues bsmap combines genome hashing and bitwise masking to achieve fast and accurate bisulfite mapping compared with existing bisulfite mapping approaches bsmap is faster more sensitive and more flexible conclusion bsmap is the first general purpose bisulfite mapping software it is able to map high throughput bisulfite reads at whole genome level with feasible memory and cpu usage it is freely available under gpl license at http code google com bsmap
motivation microarray normalization is a fundamental step in removing systematic bias and noise variability caused by technical and experimental artefacts several approaches suitable for large scale genome arrays have been proposed and shown to be effective in the reduction of systematic errors most of these methodologies are based on specific assumptions that are reasonable for whole genome arrays but possibly unsuitable for small microrna mirna platforms in this work we propose a novel normalization loessm and we investigate through simulated and real datasets the influence that normalizations for two colour mirna arrays have on the identification of differentially expressed genes results we show that normalizations usually applied to large scale arrays in several cases modify the actual structure of mirna data leading to large portions of false positives and false negatives nevertheless loessm is able to outperform other techniques in most experimental scenarios moreover when usual assumptions on differential expression distribution are missed channel effect has a strikingly negative influence on small arrays bias that cannot be removed by normalizations but rather by an appropriate experimental design we find that the combination of loessm with ecads an experimental design based on biological replicates dye swap recently proposed for channel effect reduction gives better results in most of the experimental conditions in terms of specificity sensitivity both on simulated and real data availability loessm r function is freely available at http gefu cribi unipd it papers mirna simulation contact chiara romualdi unipd it supplementary information supplementary data are available at bioinformatics bioinformatics
motivation in functional genomics it is frequently useful to correlate expression levels of genes to identify transcription factor binding sites tfbs via the presence of common sequence motifs the underlying assumption is that co expressed genes are more likely to contain shared tfbs and thus tfbs can be identified computationally indeed gene pairs with a very high expression correlation show a significant excess of shared binding sites in yeast we have tested this assumption in a more complex organism drosophila melanogaster by using experimentally determined tfbs and microarray expression data we have also examined the reverse relationship between the expression correlation and the extent of tfbs sharing results pairs of genes with shared tfbs show on average a higher degree of co expression than those with no common tfbs in drosophila however the reverse does not hold true gene pairs with high expression correlations do not share significantly larger numbers of tfbs exception to this observation exists when comparing expression of genes from the earliest stages of embryonic development interestingly semantic similarity between gene annotations biological process is much better associated with tfbs sharing as compared to the expression correlation we discuss these results in light of reverse engineering approaches to computationally predict regulatory sequences by using comparative genomics contact amarcoca asu bioinformatics
in past years comprehensive representations of cell signalling pathways have been developed by manual curation from literature which requires huge effort and would benefit from information stored in databases and from automatic retrieval and integration methods once a reconstruction of the network of interactions is achieved analysis of its structural features and its dynamic behaviour can take place mathematical modelling techniques are used to simulate the complex behaviour of cell signalling networks which ultimately sheds light on the mechanisms leading to complex diseases or helps in the identification of drug targets a variety of databases containing information on cell signalling pathways have been developed in conjunction with methodologies to access and analyse the data in principle the scenario is prepared to make the most of this information for the analysis of the dynamics of signalling pathways however are the knowledge repositories of signalling pathways ready to realize the systems biology promise in this article we aim to initiate this discussion and to provide some insights on issue
abstract background the automated extraction of gene and or protein interactions from the literature is one of the most important targets of biomedical text mining research in this paper we present a realistic evaluation of gene protein interaction mining relevant to potential non specialist users hence we have specifically avoided methods that are complex to install or require reimplementation and we coupled our chosen extraction methods with a state of the art biomedical named entity tagger results our results show that performance across different evaluation corpora is extremely variable that the use of tagged as opposed to gold standard gene and protein names has a significant impact on performance with a drop in f score of over percentage points being commonplace and that a simple keyword based benchmark algorithm when coupled with a named entity tagger outperforms two of the tools most widely used to extract gene protein interactions conclusions in terms of availability ease of use and performance the potential non specialist user community interested in automatically extracting gene and or protein interactions from free text is poorly served by current tools and systems the public release of extraction tools that are easy to install and use and that achieve state of art levels of performance should be treated as a high priority by the biomedical text community
social network systems like last fm play a significant role in web containing large amounts of multimedia enriched data that are enhanced both by explicit user provided annotations and implicit aggregated feedback describing the personal preferences of each user it is also a common tendency for these systems to encourage the creation of virtual networks among their users by allowing them to establish bonds of friendship and thus provide a novel and direct medium for the exchange data
nearest neighbor collaborative filtering provides a successful means of generating recommendations for web users however this approach suffers from several shortcomings including data sparsity and noise the cold start problem and scalability in this work we present a novel method for recommending items to users based on expert opinions our method is a variation of traditional collaborative filtering rather than applying a nearest neighbor algorithm to the user rating data predictions are computed using a set of expert neighbors from an independent dataset whose opinions are weighted according to their similarity to the user this method promises to address some of the weaknesses in traditional collaborative filtering while maintaining comparable accuracy we validate our approach by predicting a subset of the netflix data set we use ratings crawled from a web portal of expert reviews measuring results both in terms of prediction accuracy and recommendation list precision finally we explore the ability of our method to generate useful recommendations by reporting the results of a user study where users prefer the recommendations generated by approach
to understand how chromatin structure is organized by different histone variants we have measured the genome wide distribution of nucleosome core particles ncps containing the histone variants and z in human cells we find that a special class of ncps containing both variants is enriched at nucleosome free regions of active promoters enhancers and insulator regions we show that preparative methods used previously in studying nucleosome structure result in the loss of these unstable double variant ncps it seems likely that this instability facilitates the access of transcription factors to promoters and other regulatory sites in vivo other combinations of variants have different distributions consistent with distinct roles for histone variants in the modulation of expression
massively parallel sequencing offers an enormous potential for expression profiling in particular for interspecific comparisons currently different platforms for massively parallel sequencing are available which differ in read length and sequencing costs the technology offers the highest read length the other sequencing technologies are more cost effective on the expense of shorter reads reliable expression profiling by massively parallel sequencing depends crucially on the accuracy to which the reads could be mapped to the genes
background high throughput omics based data analysis play emerging roles in life sciences and molecular diagnostics this emphasizes the urgent need for user friendly windows based software interfaces that could process the diversity of large tab delimited raw data files generated by these methods depending on the study dozens to hundreds of these data tables are generated before the actual statistical or cluster analysis these data tables have to be combined and merged to expression matrices e g in case of gene expression analysis gene annotations as well as information concerning the samples analyzed may be appended renewed or extended often additional data values shall be computed or certain features must be filtered out results in order to perform these tasks we have developed a microsoft windows based application tablebutler which allows biologists or clinicians without substantial bioinformatics background to perform a plethora of data processing tasks required to analyze the large scale data conclusion tablebutler is a monolithic windows application it is implemented to handle join and preprocess large tab delimited ascii data files the intuitive user interface enables scientists e g biologists clinicians or others to setup workflows for their specific problems by simple drag and drop like operations for more details about tablebutler visit http www oncoexpress org tablebutler
abstract background integration of biological knowledge encoded in various lists of functionally related genes has become one of the most important aspects of analyzing genome wide functional genomics data in the context of cluster analysis functional coherence of clusters established through such analyses have been used to identify biologically meaningful clusters compare clustering algorithms and identify biological pathways associated with the biological process under investigation results we developed a computational framework for analytically and visually integrating knowledge based functional categories with the cluster analysis of genomics data the framework is based on the simple conceptually appealing and biologically interpretable gene specific functional coherence score clean score the score is derived by correlating the clustering structure as a whole with functional categories of interest we directly demonstrate that integrating biological knowledge in this way improves the reproducibility of conclusions derived from cluster analysis the clean score differentiates between the levels of functional coherence for genes within the same cluster based on their membership in enriched functional categories we show that this aspect results in higher reproducibility across independent datasets and produces more informative genes for distinguishing different sample types than the scores based on the traditional cluster wide analysis we also demonstrate the utility of the clean framework in comparing clusterings produced by different algorithms clean was implemented as an add on r package and can be downloaded at http uc edu clean the package integrates routines for calculating gene specific functional coherence scores and the open source interactive java based viewer functional treeview ftreeview conclusions our results indicate that using the gene specific functional coherence score improves the reproducibility of the conclusions made about clusters of co expressed genes over using the traditional cluster wide scores using gene specific coherence scores also simplifies the comparisons of clusterings produced by different clustering algorithms and provides a simple tool for selecting genes with a functionally coherent profile
wikipathways is a platform for creating updating and sharing biological pathways pathways can be edited and downloaded using the wiki style website here we present a soap web service that provides programmatic access to wikipathways that is complementary to the website we describe the functionality that this web service offers and discuss several use cases in detail exposing wikipathways through a web service opens up new ways of utilizing pathway information and assisting the community process
what enables individually simple insects like ants to act with such precision and purpose as a group how do trillions of individual neurons produce something as extraordinarily complex as consciousness what is it that guides self organizing structures like the immune system the world wide web the global economy and the human genome these are just a few of the fascinating and elusive questions that the science of complexity seeks to answer in this remarkably accessible and companionable book leading complex systems scientist melanie mitchell provides an intimate detailed tour of the sciences of complexity a broad set of efforts that seek to explain how large scale complex organized and adaptive behavior can emerge from simple interactions among myriad individuals comprehending such systems requires a wholly new approach one that goes beyond traditional scientific reductionism and that re maps long standing disciplinary boundaries based on her work at the santa fe institute and drawing on its interdisciplinary strategies mitchell brings clarity to the workings of complexity across a broad range of biological technological and social phenomena seeking out the general principles or laws that apply to all of them she explores as well the relationship between complexity and evolution artificial intelligence computation genetics information processing and many other fields richly illustrated and vividly written complexity a guided tour offers a comprehensive and eminently comprehensible overview of the ideas underlying complex systems science the current research at the forefront of this field and the prospects for the field s contribution to solving some of the most important scientific questions of time
search and recommendation systems must include contextual information to effectively model users interests in this paper we present a systematic study of the effectiveness of five variant sources of contextual information for user interest modeling post query navigation and general browsing behaviors far outweigh direct search engine interaction as an information gathering activity therefore we conducted this study with a focus on website recommendations rather than search results the five contextual information sources used are social historic task collection and user interaction we evaluate the utility of these sources and overlaps between them based on how effectively they predict users future interests our findings demonstrate that the sources perform differently depending on the duration of the time window used for future prediction and that context overlap outperforms any isolated source designers of website suggestion systems can use our findings to provide improved support for post query navigation and general behaviors
rna polymerase ii pol ii must overcome the barriers imposed by nucleosomes during transcription elongation we have developed an optical tweezers assay to follow individual pol ii complexes as they transcribe nucleosomal dna our results indicate that the nucleosome behaves as a fluctuating barrier that locally increases pause density slows pause recovery and reduces the apparent pause free velocity of pol ii the polymerase rather than actively separating dna from histones functions instead as a ratchet that rectifies nucleosomal fluctuations we also obtained direct evidence that transcription through a nucleosome involves transfer of the core histones behind the transcribing polymerase via a transient dna loop the interplay between polymerase dynamics and nucleosome fluctuations provides a physical basis for the regulation of eukaryotic science
understanding users search intent expressed through their search queries is crucial to web search and online advertisement web query classification qc has been widely studied for this purpose most previous qc algorithms classify individual queries without considering their context information however as exemplified by the well known example on query jaguar many web queries are short and ambiguous whose real meanings are uncertain without the context information in this paper we incorporate context information into the problem of query classification by using conditional random field crf models in our approach we use neighboring queries and their corresponding clicked urls web pages in search sessions as the context information we perform extensive experiments on real world search logs and validate the effectiveness and effciency of our approach we show that we can improve the score by as compared to other state of the baselines
p in recent years ontologies have become a mainstream topic in biomedical research when biological entities are described using a common schema such as an ontology they can be compared by means of their annotations this type of comparison is called semantic similarity since it assesses the degree of relatedness between two entities by the similarity in meaning of their annotations the application of semantic similarity to biomedical ontologies is recent nevertheless several studies have been published in the last few years describing and evaluating diverse approaches semantic similarity has become a valuable tool for validating the results drawn from biomedical studies such as gene clustering gene expression data analysis prediction and validation of molecular interactions and disease gene prioritization p p we review semantic similarity measures applied to biomedical ontologies and propose their classification according to the strategies they employ node based versus edge based and pairwise versus groupwise we also present comparative assessment studies and discuss the implications of their results we survey the existing implementations of semantic similarity measures and we describe examples of applications to biomedical research this will clarify how biomedical researchers can benefit from semantic similarity measures and help them choose the approach most suitable for their studies p p biomedical ontologies are evolving toward increased coverage formality and integration and their use for annotation is increasingly becoming a focus of both effort by biomedical experts and application of automated annotation procedures to create corpora of higher quality and completeness than are currently available given that semantic similarity measures are directly dependent on these evolutions we can expect to see them gaining more relevance and even becoming as essential as sequence similarity is today in biomedical p
the recently proposed concept of molecular connectivity maps enables researchers to integrate experimental measurements of genes proteins metabolites and drug compounds under similar biological conditions the study of these maps provides opportunities for future toxicogenomics and drug discovery applications we developed a computational framework to build disease specific drug protein connectivity maps we integrated gene protein and drug connectivity information based on protein interaction networks and literature mining without requiring gene expression profile information derived from drug perturbation experiments on disease samples we described the development and application of this computational framework using alzheimer s disease ad as a primary example in three steps first molecular interaction networks were incorporated to reduce bias and improve relevance of ad seed proteins second pubmed abstracts were used to retrieve enriched drug terms that are indirectly associated with ad through molecular mechanistic studies third and lastly a comprehensive ad connectivity map was created by relating enriched drugs and related proteins in literature we showed that this molecular connectivity map development approach outperformed both curated drug target databases and conventional information retrieval systems our initial explorations of the ad connectivity map yielded a new hypothesis that diltiazem and quinidine may be investigated as candidate drugs for ad treatment molecular connectivity maps derived computationally can help study molecular signature differences between different classes of drugs in specific disease contexts to achieve overall good data coverage and quality a series of statistical methods have been developed to overcome high levels of data noise in biological networks and literature mining results further development of computational molecular connectivity maps to cover major disease areas will likely set up a new model for drug development in which therapeutic toxicological profiles of candidate drugs can be checked computationally before costly clinical begin
studies correlating genetic variation to gene expression facilitate the interpretation of common human phenotypes and disease as functional variants may be operating in a tissue dependent manner we performed gene expression profiling and association with genetic variants single nucleotide polymorphisms on three cell types of individuals we detected cell type specific genetic effects with to of regulatory variants operating in a cell type specific manner and identified multiple expressive quantitative trait loci eqtls per gene unique or shared among cell types and positively correlated with the number of transcripts per gene cell type specific eqtls were found at larger distances from genes and at lower effect size similar to known enhancers these data suggest that the complete regulatory variant repertoire can only be uncovered in the context of cell specificity
the purpose of this article is to describe one good strategy for carrying out computational experiments i will not describe profound issues such as how to formulate hypotheses design experiments or draw conclusions rather i will focus on relatively mundane issues such as organizing files and directories and documenting progress these issues are important because poor organizational choices can lead to significantly slower research progress i do not claim that the strategies i outline here are optimal these are simply the principles and practices that i have developed over years of bioinformatics research augmented with various suggestions from other researchers with whom i have discussed issues
this review focuses on recent trends in multiple sequence alignment tools it describes the latest algorithmic improvements including the extension of consistency based methods to the problem of template based multiple sequence alignments some results are presented suggesting that template based methods are significantly more accurate than simpler alternative methods the validation of existing methods is also discussed at length with the detailed description of recent results and some suggestions for future validation strategies the last part of the review addresses future challenges for multiple sequence alignment methods in the genomic era most notably the need to cope with very large sequences the need to integrate large amounts of experimental data the need to accurately align non coding and non transcribed sequences and finally the need to integrate many alternative methods and approaches contact cedric notredame crg bioinformatics
summary mapping of next generation sequencing data derived from rna samples rnaseq presents different genome mapping challenges than data derived from dna for example tags that cross exon junction boundaries will often not map to a reference genome and the strand specificity of the data needs to be retained here we present rna mate a computational pipeline based on a recursive mapping strategy for placing strand specific rnaseq data onto a reference genome maximizing the mappable tags can provide significant savings in the cost of sequencing experiments this pipe line provides an automatic and integrated way to align color space sequencing data collate this information and generate files for examining gene expression data in a genomic context availability executables source code and exon junction libraries are available from http grimmond imb uq edu au rna mate contact n cloonan imb uq edu au supplementary information supplementary data are available at online
pnas we introduce a replica exchange parallel tempering method in which attempted configuration swaps are generated using nonequilibrium work simulations by effectively increasing phase space overlap this approach mitigates the need for many replicas we illustrate our method by using a model system and show that it is able to achieve the computational efficiency of ordinary replica exchange using replicas
whole genome shotgun assembly is the process of taking many short sequenced segments reads and reconstructing the genome from which they originated we demonstrate how the technique of bidirected network flow can be used to explicitly model the double stranded nature of dna for genome assembly by combining an algorithm for the chinese postman problem on bidirected graphs with the construction of a bidirected de bruijn graph we are able to find the shortest double stranded dna sequence that contains a given set of k long dna molecules this is the first exact polynomial time algorithm for the assembly of a double stranded genome furthermore we propose a maximum likelihood framework for assembling the genome that is the most likely source of the reads in lieu of the standard maximum parsimony approach which finds the shortest genome subject to some constraints in this setting we give a bidirected network flow based algorithm that by taking advantage of high coverage accurately estimates the copy counts of repeats in a genome our second algorithm combines these predicted copy counts with matepair data in order to assemble the reads into contigs we run our algorithms on simulated read data from escherichia coli and predict copy counts with extremely high accuracy while assembling contigs
the sirtuins are a highly conserved family of nad dependent enzymes that regulate lifespan in lower organisms recently the mammalian sirtuins have been connected to an ever widening circle of activities that encompass cellular stress resistance genomic stability tumorigenesis and energy metabolism here we review the recent progress in sirtuin biology the role these proteins have in various age related diseases and the tantalizing notion that the activity of this family of enzymes somehow regulates how long live
abstract background we have developed a simulation approach to help determine the optimal mixture of sequencing methods for most complete and cost effective transcriptome sequencing we compared simulation results for traditional capillary sequencing with next generation ng ultra high throughput technologies the simulation model was parameterized using mappings of cdna sequence reads to the arabidopsis genome ncbi accession we also generated sequences and de novo assemblies for the basal eudicot california poppy eschscholzia californica and the magnoliid avocado persea americana using a variety of methods for cdna synthesis results the arabidopsis reads tagged more than genes including new splice variants and extended utr regions of the total reads mb mapped exactly to known exons while mapped to introns spanned annotated intron exon boundaries and extended beyond the end of annotated utrs sequence based inference of relative gene expression levels correlated significantly with microarray data as expected ng sequencing of normalized libraries tagged more genes than non normalized libraries although non normalized libraries yielded more full length cdna sequences the arabidopsis data were used to simulate additional rounds of ng and traditional est sequencing and various combinations of each our simulations suggest a combination of flx and solexa sequencing for optimal transcriptome coverage at modest cost we have also developed estcalc http fgp huck psu edu ngsim pl an online webtool which allows users to explore the results of this study by specifying individualized costs and sequencing characteristics conclusions ng sequencing technologies are a highly flexible set of platforms that can be scaled to suit different project goals in terms of sequence coverage alone the ng sequencing is a dramatic advance over capillary based sequencing but ng sequencing also presents significant challenges in assembly and sequence accuracy due to short read lengths method specific sequencing errors and the absence of physical clones these problems may be overcome by hybrid sequencing strategies using a mixture of sequencing methodologies by new assemblers and by sequencing more deeply sequencing and microarray outcomes from multiple experiments suggest that our simulator will be useful for guiding ng transcriptome sequencing projects in a wide range organisms
motivation biomarker discovery and gene ranking is a standard task in genomic high throughput analysis typically the ordering of markers is based on a stabilized variant of the t score such as the moderated t or the sam statistic however these procedures ignore gene gene correlations which may have a profound impact on the gene orderings and on the power of the subsequent tests results we propose a simple procedure that adjusts gene wise t statistics to take account of correlations among genes the resulting correlation adjusted t scores cat scores are derived from a predictive perspective i e as a score for variable selection to discriminate group membership in two class linear discriminant analysis in the absence of correlation the cat score reduces to the standard t score moreover using the cat score it is straightforward to evaluate groups of features i e gene sets for computation of the cat score from small sample data we propose a shrinkage procedure in a comparative study comprising six different synthetic and empirical correlation structures we show that the cat score improves estimation of gene orderings and leads to higher power for fixed true discovery rate and vice versa finally we also illustrate the cat score by analyzing metabolomic data availability the shrinkage cat score is implemented in the r package st which is freely available under the terms of the gnu general public license version or later from cran http cran r project org web st
systems pharmacology is an emerging area of pharmacology which utilizes network analysis of drug action as one of its approaches by considering drug actions and side effects in the context of the regulatory networks within which the drug targets and disease gene products function network analysis promises to greatly increase our knowledge of the mechanisms underlying the multiple actions of drugs systems pharmacology can provide new approaches for drug discovery for complex diseases the integrated approach used in systems pharmacology can allow for drug action to be considered in the context of the whole genome network based studies are becoming an increasingly important tool in understanding the relationships between drug action and disease susceptibility genes this review discusses how analysis of biological networks has contributed to the genesis of systems pharmacology and how these studies have improved global understanding of drug targets suggested new targets and approaches for therapeutics and provided a deeper understanding of the effects of drugs taken together these types of analyses can lead to new therapeutic options while improving the safety and efficacy of existing medications contact ravi iyengar mssm bioinformatics
backgroundecologists and evolutionary biologists are becoming increasingly interested in networks as a framework to study plant animal mutualisms within their ecological context although such focus on networks has brought about important insights into the structure of these interactions relatively little is still known about the mechanisms behind these patterns scopethe aim in this paper is to offer an overview of the mechanisms influencing the structure of plant animal mutualistic networks a brief summary is presented of the salient network patterns the potential mechanisms are discussed and the studies that have evaluated them are reviewed this review shows that researchers of plant animal mutualisms have made substantial progress in the understanding of the processes behind the patterns observed in mutualistic networks at the same time we are still far from a thorough integrative mechanistic understanding we close with specific suggestions for directions of future research which include developing methods to evaluate the relative importance of mechanisms influencing network patterns and focusing research efforts on selected representative study systems throughout the aob
purpose this paper provides a number of examples of how web technologies and approaches library are being used within the library sector the paper acknowledges that there are a variety of risks associated with such approaches the paper describes the different types of risks and outlines a risk assessment and risk management approach which is being developed to minimize the dangers whilst allowing the benefits of library to be realized design methodology approach the paper outlines various risks and barriers which have been identified at a series of workshops run by ukoln for the cultural heritage sector in the uk a risk assessment and risk management approach which was initially developed to support use of web technologies at events organized by ukoln a national centre of expertise in digital information management based in the uk is described and its potential for use within the wider library community in conjunction with related approaches for addressing areas such as accessibility and protection of young people is described findings use of library approaches is becoming embedded across many libraries which seek to exploit the benefits which such technologies can provide the need to ensure that the associated risks are identified and appropriate mechanisms implemented to minimize such risks are beginning to be appreciated practical implications the areas described in this paper should be of relevance to many library organisations which are making use of library services originality value the paper should prove valuable to policy makers and web practitioners within libraries who may be aware of the potential benefits of library but have not considered risks
after a long history of overexploitation increasing efforts to restore marine ecosystems and rebuild fisheries are under way here we analyze current trends from a fisheries and conservation perspective in of well studied ecosystems the average exploitation rate has recently declined and is now at or below the rate predicted to achieve maximum sustainable yield for seven systems yet of assessed fish stocks worldwide still require rebuilding and even lower exploitation rates are needed to reverse the collapse of vulnerable species combined fisheries and conservation objectives can be achieved by merging diverse management actions including catch restrictions gear modification and closed areas depending on local context impacts of international fleets and the lack of alternatives to fishing complicate prospects for rebuilding fisheries in many poorer regions highlighting the need for a global perspective on rebuilding resources
arxiv org mediates contact with the literature for entire scholarly communities both through provision of archival access and through daily email and web announcements of new materials potentially many screenlengths long we confirm and extend a surprising correlation between article position in these initial announcements ordered by submission time and later citation impact due primarily to intentional self promotion on the part of authors a pure visibility effect was also present the subset of articles accidentally in early positions fared measurably better in the long term citation record than those lower down astrophysics articles announced in position for example overall received a median number of citations textbackslash higher while those there accidentally had a textbackslash visibility boost for two large subcommunities of theoretical high energy physics hep th and hep ph articles announced in position had median numbers of citations textbackslash and textbackslash larger than for positions and the subsets there accidentally had visibility boosts of textbackslash and textbackslash we also consider the positional effects on early readership the median numbers of early full text downloads for astro ph hep th and hep ph articles announced in position were textbackslash textbackslash and textbackslash higher than for lower positions respectively and those there accidentally had medians visibility boosted by textbackslash textbackslash and textbackslash finally we correlate a variety of readership features with long term citations using machine learning methods thereby extending previous results on the predictive power of early readership in a broader context we conclude with some observations on impact metrics and dangers of mechanisms
web users are spending more of their time and creative energies within online social networking systems while many of these networks allow users to export their personal data or expose themselves to third party web archiving some do not facebook one of the most popular social networking websites is one example of a walled garden where users activities are trapped we examine a variety of techniques for extracting users activities from facebook and by extension other social networking systems for the personal archive and for the third party archiver our framework could be applied to any walled garden where personal user data is locked
the current research explores whether librarians whose main work focuses on information are familiar with new technological changes and innovations and whether they make use of different web applications the research examines whether personality characteristics resistance to change cognitive appraisal empowerment and extroversion or introversion as well as computer expertise motivation importance and capacity towards studying and integrating different applications of web in future influence librarians use of web different questionnaires were distributed to randomly israeli librarians throughout the country the research revealed that personality characteristics as well as computer expertise motivation importance and capacity towards studying and integrating different applications of web in the future influence librarians use of web these findings have theoretical as well as practical implications elsevier inc all reserved
a major problem worldwide is the potential loss of fisheries forests and water resources understanding of the processes that lead to improvements in or deterioration of natural resources is limited because scientific disciplines use different concepts and languages to describe and explain complex social ecological systems sess without a common framework to organize findings isolated knowledge does not cumulate until recently accepted theory has assumed that resource users will never self organize to maintain their resources and that governments must impose solutions research in multiple disciplines however has found that some government policies accelerate resource destruction whereas some resource users have invested their time and energy to achieve sustainability a general framework is used to identify subsystem variables that affect the likelihood of self organization in efforts to achieve a sustainable science
gr transcriptional regulation is largely enacted by transcription factors tfs binding dna large numbers of tf binding motifs have been revealed by chip chip experiments followed by computational dna motif discovery however the success of motif discovery algorithms has been limited when applied to sequences bound in vivo such as those identified by chip chip because the observed tfdna interactions are not necessarily direct some tfs predominantly associate with dna indirectly through protein partners while others exhibit both direct and indirect binding here we present the first method for distinguishing between direct and indirect tfdna interactions integrating in vivo tf binding data in vivo nucleosome occupancy data and motifs from in vitro protein binding microarray experiments when applied to yeast chip chip data our method reveals that only of the data sets can be readily explained by direct binding of the profiled tf while can be explained by indirect dna binding in the remaining none of the motifs used in our analysis was able to explain the chip chip data either because the data were too noisy or because the set of motifs was incomplete as more in vitro tf dna binding motifs become available our method could be used to build a complete catalog of direct and indirect tfdna interactions our method is not restricted to yeast or to chip chip data but can be applied in any system for which both in vivo binding data and in vitro dna binding motifs available
pnas even though people in our contemporary technological society are depending on communication our understanding of the underlying laws of human communicational behavior continues to be poorly understood here we investigate the communication patterns in social internet communities in search of statistical laws in human interaction activity this research reveals that human communication networks dynamically follow scaling laws that may also explain the observed trends in economic growth specifically we identify a generalized version of gibrat s law of social activity expressed as a scaling law between the fluctuations in the number of messages sent by members and their level of activity gibrat s law has been essential in understanding economic growth patterns yet without an underlying general principle for its origin we attribute this scaling law to long term correlation patterns in human activity which surprisingly span from days to the entire period of the available data of more than year further we provide a mathematical framework that relates the generalized version of gibrat s law to the long term correlated dynamics which suggests that the same underlying mechanism could be the source of gibrat s law in economics ranging from large firms research and development expenditures gross domestic product of countries to city population growth these findings are also of importance for designing communication networks and for the understanding of the dynamics of social systems in which communication plays a role such as economic markets and systems
intrinsic protein disorder is a widespread phenomenon characterised by a lack of stable three dimensional structures and is considered to play an important role in protein protein interactions ppis this study examined the genome wide preference of disorder in ppis by using exhaustive disorder prediction in human ppis we categorised the ppis into three types interaction between disordered proteins interaction between structured proteins and interaction between a disordered protein and a structured protein with regard to the flexibility of molecular recognition and compared these three interaction types in an existing human ppi network with those in a randomised network although the structured regions were expected to become the identifiers for binding recognition this comparative analysis revealed unexpected results the occurrence of interactions between disordered proteins was significantly frequent and that between a disordered protein and a structured protein was significantly infrequent we found that this propensity was much stronger in interactions between nonhub proteins we also analysed the interaction types from a functional standpoint by using go which revealed that the interaction between disordered proteins frequently occurred in cellular processes regulation and metabolic processes the number of interactions especially in metabolic processes between disordered proteins was times as large as that in the randomised network another analysis conducted by using kegg pathways provided results where several signaling pathways and disease related pathways included many interactions between disordered proteins all of these analyses suggest that human ppis preferably occur between disordered proteins and that the flexibility of the interacting protein pairs may play an important role in human networks
this is a one semester course on bosonic string theory aimed at beginning graduate students the lectures assume a working knowledge of quantum field theory and general relativity contents the classical string the quantum string open strings and d branes introducing conformal field theory the polyakov path integral and ghosts string interactions the low energy effective action compactification and duality
research on open innovation has increasingly emphasised the role of communities in creating shaping and disseminating innovations however the comparability of many studies has been hampered by the lack of a precise definition of the community construct and the research on open innovation has to date not been well connected to insights from research on the role of transformational leaders and the networking of champions and promotors across organisational boundaries for this reason this paper introduces a new construct of innovation communities based on promotor theory which it defines as networks of promotors it proposes a comprehensive concept of the quality of interaction in innovation communities and presents findings of three case studies which explore the role of promotors and networks of promotors in open innovation the case studies reveal that such transformational leaders as promotors and especially their close and informal co operation across functional and organisational boundaries play a key role in innovation
summary pubmed ex is a browser extension that marks up pubmed search results with additional text mining information pubmed ex s page mark up which includes section categorization and gene disease and relation mark up can help researchers to quickly focus on key terms and provide additional information on them all text processing is performed server side freeing up user resources availability pubmed ex is freely available at http bws iis sinica edu tw pubmed ex and http iisr cse yzu edu tw pubmed ex contact thtsai saturn yzu edu tw supplementary information supplementary data are available at bioinformatics bioinformatics
summary experimental techniques that survey an entire genome demand flexible highly interactive visualization tools that can display new data alongside foundation datasets such as reference gene annotations the integrated genome browser igb aims to meet this need igb is an open source desktop graphical display tool implemented in java that supports real time zooming and panning through a genome layout of genomic features and datasets in moveable adjustable tiers incremental or genome scale data loading from remote web servers or local files and dynamic manipulation of quantitative data via genome graphs availability the application and source code are available from http igb bioviz org and http genoviz sourceforge net contact aloraine uncc bioinformatics
summary shortread is a package for input quality assessment manipulation and output of high throughput sequencing data shortread is provided in the r and bioconductor environments allowing ready access to additional facilities for advanced statistical analysis data transformation visualization and integration with diverse genomic resources availability and implementation this package is implemented in r and available at the bioconductor web site the package contains a vignette outlining typical work flows contact mtmorgan org
single stranded rna viruses encompass broad classes of infectious agents and cause the common cold cancer aids and other serious health threats viral replication is regulated at many levels including the use of conserved genomic rna structures most potential regulatory elements in viral rna genomes are uncharacterized here we report the structure of an entire hiv genome at single nucleotide resolution using shape a high throughput rna analysis technology the genome encodes protein structure at two levels in addition to the correspondence between rna and protein primary sequences a correlation exists between high levels of rna structure and sequences that encode inter domain loops in hiv proteins this correlation suggests that rna structure modulates ribosome elongation to promote native protein folding some simple genome elements previously shown to be important including the ribosomal gag pol frameshift stem loop are components of larger rna motifs we also identify organizational principles for unstructured rna regions including splice site acceptors and hypervariable regions these results emphasize that the hiv genome and potentially many coding rnas are punctuated by previously unrecognized regulatory motifs and that extensive rna structure constitutes an important component of the code
the leaders of the world are flying the economy by the seat of their pants say j doyne farmer and duncan foley there is however a better way to help guide financial policies in today s high tech age one naturally assumes that us president barack obama s economic team and its international counterparts are using sophisticated quantitative computer models to guide us out of the current economic crisis they not
agent based computational models can capture irrational behaviour complex social networks and global scale all essential in confronting says joshua m epstein as the world braces for an autumn wave of swine flu the relatively new technique of agent based computational modelling is playing a central part in mapping the disease s possible spread and designing policies for its mitigation classical epidemic modelling which began in the was built on equations
in this paper we report research results investigating microblogging as a form of electronic word of mouth for sharing consumer opinions concerning brands we analyzed more than microblog postings containing branding comments sentiments and opinions we investigated the overall structure of these microblog postings the types of expressions and the movement in positive or negative sentiment we compared automated methods of classifying sentiment in these microblogs with manual coding using a case study approach we analyzed the range frequency timing and content of tweets in a corporate account our research findings show that percnt of microblogs contain mention of a brand of the branding microblogs nearly percnt contained some expression of brand sentiments of these more than percnt were positive and percnt were critical of the company or product our comparison of automated and manual coding showed no significant differences between the two approaches in analyzing microblogs for structure and composition the linguistic structure of tweets approximate the linguistic patterns of natural language expressions we find that microblogging is an online tool for customer word of mouth communications and discuss the implications for corporations using microblogging as part of their overall marketing strategy wiley inc
background the full complement of dna mutations that are responsible for the pathogenesis of acute myeloid leukemia aml is not yet known methods we used massively parallel dna sequencing to obtain a very high level of coverage approximately of a primary cytogenetically normal de novo genome for aml with minimal maturation aml and a matched normal skin genome results we identified acquired somatic mutations within the coding sequences of genes and somatic point mutations in conserved or regulatory portions of the genome all mutations appeared to be heterozygous and present in nearly all cells in the tumor sample four of the mutations occurred in at least additional aml sample in samples that were tested mutations in nras and had been identified previously in patients with aml but two other mutations had not been identified one of these mutations in the gene was present in of additional aml genomes tested and was strongly associated with normal cytogenetic status it was present in of cytogenetically normal samples the other was a nongenic mutation in a genomic region with regulatory potential and conservation in higher mammals we detected it in one additional aml tumor the aml genome that we sequenced contains approximately point mutations of which only a small fraction are likely to be relevant to pathogenesis conclusions by comparing the sequences of tumor and skin genomes of a patient with aml we have identified recurring mutations that may be relevant pathogenesis
background proteins interact through specific binding interfaces that contain many residues in domains protein interactions thus occur on three different levels of a concept hierarchy whole proteins domains and residues each level offers a distinct and complementary set of features for computationally predicting interactions including functional genomic features of whole proteins evolutionary features of domain families and physical chemical features of individual residues the predictions at each level could benefit from using the features at all three levels however it is not trivial as the features are provided at different granularity results to link up the predictions at the three levels we propose a multi level machine learning framework that allows for explicit information flow between the levels we demonstrate using representative yeast interaction networks that our algorithm is able to utilize complementary feature sets to make more accurate predictions at the three levels than when the three problems are approached independently to facilitate application of our multi level learning framework we discuss three key aspects of multi level learning and the corresponding design choices that we have made in the implementation of a concrete learning algorithm architecture of information flow we show the greater flexibility of bidirectional flow over independent levels and unidirectional flow coupling mechanism of the different levels we show how this can be accomplished via augmenting the training sets at each level and discuss the prevention of error propagation between different levels by means of soft coupling sparseness of data we show that the multi level framework compounds data sparsity issues and discuss how this can be dealt with by building local models in information rich parts of the data our proof of concept learning algorithm demonstrates the advantage of combining levels and opens up opportunities for further research availability the software and a readme file can be downloaded at http networks gersteinlab org mll the programs are written in java and can be run on any platform with java or higher and apache ant or higher installed the software can be used without license
abstract background gene set analysis based on gene ontology go can be a promising method for the analysis of differential expression patterns however current studies that focus on individual go terms have limited analytical power because the complex structure of go introduces strong dependencies among the terms and some genes that are annotated to a go term cannot be found by statistically significant enrichment results we proposed a method for enriching clustered go terms based on semantic similarity namely cluster enrichment analysis based on go ceago to extend the individual term analysis method using an affymetrix chip dataset with simulated gene sets we illustrated that ceago was sensitive enough to detect moderate expression changes when compared to parent based individual term analysis methods the results showed that ceago may provide more accurate differentiation of gene expression results when used with two acute leukemia all and all aml microarray expression datasets ceago correctly identified specifically enriched go groups that were overlooked by other individual test methods conclusions by applying ceago to both simulated and real microarray data we showed that this approach could enhance the interpretation of microarray experiments ceago is currently available at http chgc sh cn en ceago
background genome wide association studies gwass and global profiling of gene expression microarrays are two major technological breakthroughs that allow hypothesis free identification of candidate genes associated with tumorigenesis it is not obvious whether there is a consistency between the candidate genes identified by gwas gwas genes and those identified by profiling gene expression microarray genes methodology principal findings we used the cancer genetic markers susceptibility database to retrieve single nucleotide polymorphisms from candidate genes for prostate cancer in addition we conducted a large meta analysis of gene expression data in normal prostate and prostate tumor tissue we identified genes that were interrogated by both gwass and microarrays on the basis of p values from gwass we selected most significantly associated genes for functional annotation by the database for annotation visualization and integrated discovery we also conducted functional annotation analysis using same number of the top genes identified in the meta analysis of the gene expression data we found that genes involved in cell adhesion were overrepresented among both the gwas and microarray genes conclusions significance we conclude that the results of these analyses suggest that combining gwas and microarray data would be a more effective approach than analyzing individual datasets and can help to refine the identification of candidate genes and functions associated with development
there is extensive natural variation in human gene expression as quantitative phenotypes expression levels of genes are heritable genetic linkage and association mapping have identified cis and trans acting dna variants that influence expression levels of human genes new insights into human gene regulation are emerging from genetic analyses of gene expression in cells at rest and following exposure to stimuli the integration of these genetic mapping results with data from co expression networks is leading to a better understanding of how expression levels of individual genes are regulated and how genes interact with each other these findings are important for basic understanding of gene regulation and of diseases that result from disruption of normal regulation
abstract nbsp nbsp state space methods have proven indispensable in neural data analysis however common methods for performing inference in state space models with non gaussian observations rely on certain approximations which are not always accurate here we review direct optimization methods that avoid these approximations but that nonetheless retain the computational efficiency of the approximate methods we discuss a variety of examples applying these direct optimization techniques to problems in spike train smoothing stimulus decoding parameter estimation and inference of synaptic properties along the way we point out connections to some related standard statistical methods including spline smoothing and isotonic regression finally we note that the computational methods reviewed here do not in fact depend on the state space setting at all instead the key property we are exploiting involves the bandedness of certain matrices we close by discussing some applications of this more general point of view including markov chain monte carlo methods for neural decoding and efficient estimation of spatially varying rates
while current computing practice abounds with innovations like online auctions blogs wikis twitter social networks and online social games few if any genuinely new theories have taken root in the corresponding ldquo top rdquo academic journals those creating computing progress increasingly see these journals as unreadable outdated and irrelevant yet as technology practice creates technology theory is if anything becoming even more conforming and less relevant we attribute this to the erroneous assumption that research rigor is excellence a myth contradicted by the scientific method itself excess rigor supports the demands of appointment grant and promotion committees but is drying up the wells of academic inspiration part i of this paper chronicles the inevitable limits of what can only be called a feudal academic knowledge exchange system with trends like exclusivity slowness narrowness conservatism self involvement and inaccessibility we predict an upcoming social upheaval in academic publishing as it shifts from a feudal to democratic form from knowledge managed by the few to knowledge managed by the many the technology trigger is socio technical advances the drive will be that only democratic knowledge exchange can scale up to support the breadth speed and flexibility modern cross disciplinary research needs part ii suggests the sort of socio technical design needed to bring this about
background p drug repositioning offers the possibility of faster development times and reduced risks in drug discovery with the rapid development of high throughput technologies and ever increasing accumulation of whole genome level datasets an increasing number of diseases and drugs can be comprehensively characterized by the changes they induce in gene expression protein metabolites and phenotypes p sec sec title methodology principal findings title p we performed a systematic large scale analysis of genomic expression profiles of human diseases and drugs to create a disease drug network a network of significant interactions was extracted from the million comparisons between publicly available transcriptomic profiles the network includes disease disease disease drug and drug drug relationships at least of the disease disease pairs were in the same disease area as determined by the medical subject headings mesh disease classification tree the remaining can drive a molecular level nosology by discovering relationships between seemingly unrelated diseases such as a connection between bipolar disorder and hereditary spastic paraplegia and a connection between actinic keratosis and cancer among the disease drug links connections with negative scores suggest new indications for existing drugs such as the use of some antimalaria drugs for crohn s disease and a variety of existing drugs for huntington s disease while the positive scoring connections can aid in drug side effect identification such as tamoxifen s undesired carcinogenic property from the drug drug relationships we discover relationships that aid in target and pathway deconvolution such as as a potential molecular target of lobeline and both apoptotic dna fragmentation and m dna damage checkpoint regulation as potential pathway targets of daunorubicin p sec sec title conclusions significance title p we have automatically generated thousands of disease and drug expression profiles using geo datasets and constructed a large scale disease drug network for effective and efficient drug repositioning as well as drug target pathway identification sec
abstract background although the use of clustering methods has rapidly become one of the standard computational approaches in the literature of microarray gene expression data analysis little attention has been paid to uncertainty in the results obtained results we present an r bioconductor port of a fast novel algorithm for bayesian agglomerative hierarchical clustering and demonstrate its use in clustering gene expression microarray data the method performs bottom up hierarchical clustering using a dirichlet process infinite mixture to model uncertainty in the data and bayesian model selection to decide at each step which clusters to merge conclusions biologically plausible results are presented from a well studied data set expression profiles of a thaliana subjected to a variety of biotic and abiotic stresses our method avoids several limitations of traditional methods for example how many clusters there should be and how to choose a principled metric
gr extracting sequence information from raw images of fluorescence is the foundation underlying several high throughput sequencing platforms some of the main challenges associated with this technology include reducing the error rate assigning accurate base specific quality scores and reducing the cost of sequencing by increasing the throughput per run to demonstrate how computational advancement can help to meet these challenges a novel model based basecalling algorithm bayescall is introduced for the illumina sequencing platform being founded on the tools of statistical learning bayescall is flexible enough to incorporate various features of the sequencing process in particular it can easily incorporate time dependent parameters and model residual effects this new approach significantly improves the accuracy over illumina s basecaller bustard particularly in the later cycles of a sequencing run for cycle data on a standard viral sample bayescall improves bustard s average per base error rate by about the probability of observing each base can be readily computed in bayescall and this probability can be transformed into a useful base specific quality score with a high discrimination ability a detailed study of bayescall s performance is here
understanding complex networks of protein protein interactions ppis is one of the foremost challenges of the post genomic era due to the recent advances in experimental bio technology including yeast hybrid tandem affinity purification tap and other high throughput methods for protein protein interaction ppi detection huge amounts of ppi network data are becoming available of major concern however are the levels of noise and incompleteness for example for screens it is thought that the false positive rate could be as high as and the false negative rate may range from to tap experiments are believed to have comparable levels noise
analysis of polymorphism and divergence in the non coding portion of the human genome yields crucial information about factors driving the evolution of gene regulation candidate cis regulatory regions spanning more than genes in african americans and european americans were re sequenced and aligned to the chimpanzee genome in order to identify potentially functional polymorphism and to characterize and quantify departures from neutral evolution distortions of the site frequency spectra suggest a general pattern of selective constraint on conserved non coding sites in the flanking regions of genes cncs moreover there is an excess of fixed differences that cannot be explained by a gamma model of deleterious fitness effects suggesting the presence of positive selection on cncs extensions of the mcdonald kreitman test identified candidate cis regulatory regions with high probabilities of positive and negative selection near many known human genes the biological characteristics of which exhibit genome wide trends that differ from patterns observed in protein coding regions notably there is a higher probability of positive selection in candidate cis regulatory regions near genes expressed in the fetal brain suggesting that a larger portion of adaptive regulatory changes has occurred in genes expressed during brain development overall we find that natural selection has played an important role in the evolution of candidate cis regulatory regions throughout evolution
although fully generative models have been successfully used to model the contents of text documents they are often awkward to apply to combinations of text data and document metadata in this paper we propose a dirichlet multinomial regression dmr topic model that includes a log linear prior on document topic distributions that is a function of observed features of the document such as author publication venue references and dates we show that by selecting appropriate features dmr topic models can meet or exceed the performance of several previously published topic models designed for data
circuit diagrams and unified modeling language diagrams are just two examples of standard visual languages that help accelerate work by promoting regularity removing ambiguity and enabling software tool support for communication of complex information ironically despite having one of the highest ratios of graphical to textual information biology still lacks standard graphical notations the recent deluge of biological knowledge makes addressing this deficit a pressing concern toward this goal we present the systems biology graphical notation sbgn a visual language developed by a community of biochemists modelers and computer scientists sbgn consists of three complementary languages process diagram entity relationship diagram and activity flow diagram together they enable scientists to represent networks of biochemical interactions in a standard unambiguous way we believe that sbgn will foster efficient and accurate representation visualization storage exchange and reuse of information on all kinds of biological knowledge from gene regulation to metabolism to signaling
metagenomics is providing an unprecedented view of the taxonomic diversity metabolic potential and ecological role of microbial communities in biomes as diverse as the mammalian gastrointestinal tract the marine water column and soils however we have found a systematic error in metagenomes generated by based pyrosequencing that leads to an overestimation of gene and taxon abundance between and of sequences in a typical metagenome are artificial replicates here we document the error in several published and original datasets and offer a web based solution http microbiomes msu edu replicates for identifying and removing artifacts
methods for the direct detection of copy number variation cnv genome wide have become effective instruments for identifying genetic risk factors for disease the application of next generation sequencing platforms to genetic studies promises to improve sensitivity to detect cnvs as well as inversions indels and snps new computational approaches are needed to systematically detect these variants from genome sequence data existing sequence based approaches for cnv detection are primarily based on paired end read mapping pem as reported previously by tuzun et al and korbel et al due to limitations of the pem approach some classes of cnvs are difficult to ascertain including large insertions and variants located within complex genomic regions to overcome these limitations we developed a method for cnv detection using read depth of coverage event wise testing ewt is a method based on significance testing in contrast to standard segmentation algorithms that typically operate by performing likelihood evaluation for every point in the genome ewt works on intervals of data points rapidly searching for specific classes of events overall false positive rate is controlled by testing the significance of each possible event and adjusting for multiple testing deletions and duplications detected in an individual genome by ewt are examined across multiple genomes to identify polymorphism between individuals we estimated error rates using simulations based on real data and we applied ewt to the analysis of chromosome from paired end shotgun sequence data on five individuals our results suggest that analysis of read depth is an effective approach for the detection of cnvs and it captures structural variants that are refractory to established pem methods
recent development of high throughput technology has accelerated interest in the development of molecular biomarker classifiers for safety assessment disease diagnostics and prognostics and prediction of response for patient assignment this article reviews and evaluates some important aspects and key issues in the development of biomarker classifiers development of a biomarker classifier for high throughput data involves two components i model building and ii performance assessment this article focuses on feature selection in model building and cross validation for performance assessment a frequency approach to feature selection is presented and compared to the conventional approach in terms of the predictive accuracy and stability of the selected feature set the two approaches are compared based on four biomarker classifiers each with a different feature selection method and well known classification algorithm in each of the four classifiers the feature predictor set selected by the frequency approach is more stable than the feature set selected by the approach
pnas although mind wandering occupies a large proportion of our waking life its neural basis and relation to ongoing behavior remain controversial we report an fmri study that used experience sampling to provide an online measure of mind wandering during a concurrent task analyses focused on the interval of time immediately preceding experience sampling probes demonstrate activation of default network regions during mind wandering a finding consistent with theoretical accounts of default network functions activation in medial prefrontal default network regions was observed both in association with subjective self reports of mind wandering and an independent behavioral measure performance errors on the concurrent task in addition to default network activation mind wandering was associated with executive network recruitment a finding predicted by behavioral theories of off task thought and its relation to executive resources finally neural recruitment in both default and executive network regions was strongest when subjects were unaware of their own mind wandering suggesting that mind wandering is most pronounced when it lacks meta awareness the observed parallel recruitment of executive and default network regionstwo brain systems that so far have been assumed to work in oppositionsuggests that mind wandering may evoke a unique mental state that may allow otherwise opposing networks to work in cooperation the ability of this study to reveal a number of crucial aspects of the neural recruitment associated with mind wandering underscores the value of combining subjective self reports with online measures of brain function for advancing our understanding of the neurophenomenology of experience
in human societies opinion formation is mediated by social interactions consequently taking place on a network of relationships and at the same time influencing the structure of the network and its evolution to investigate this coevolution of opinions and social interaction structure we develop a dynamic agent based network model by taking into account short range interactions like discussions between individuals long range interactions like a sense for overall mood modulated by the attitudes of individuals and external field corresponding to outside influence moreover individual biases can be naturally taken into account in addition the model includes the opinion dependent link rewiring scheme to describe network topology coevolution with a slower time scale than that of the opinion formation with this model comprehensive numerical simulations and mean field calculations have been carried out and they show the importance of the separation between fast and slow time scales resulting in the network to organize as well connected small communities of agents with the opinion
uncovering the community structure exhibited by real networks is a crucial step toward an understanding of complex systems that goes beyond the local organization of their constituents many algorithms have been proposed so far but none of them has been subjected to strict tests to evaluate their performance most of the sporadic tests performed so far involved small networks with known community structure and or artificial graphs with a simplified structure which is very uncommon in real systems here we test several methods against a recently introduced class of benchmark graphs with heterogeneous distributions of degree and community size the methods are also tested against the benchmark by girvan and newman proc natl acad sci u s a and on random graphs as a result of our analysis three recent algorithms introduced by rosvall and bergstrom proc natl acad sci u s a proc natl acad sci u s a blondel et al j stat mech theory exp and ronhovde and nussinov phys rev e have an excellent performance with the additional advantage of low computational complexity which enables one to analyze systems
background finding one small molecule query in a large target library is a challenging task in computational chemistry although several heuristic approaches are available using fragment based chemical similarity searches they fail to identify exact atom bond equivalence between the query and target molecules and thus cannot be applied to complex chemical similarity searches such as searching a complete or partial metabolic pathway in this paper we present a new maximum common subgraph mcs tool smsd small molecule subgraph detector to overcome the issues with current heuristic approaches to small molecule similarity searches the mcs search implemented in smsd incorporates chemical knowledge atom type match with bond sensitive and insensitive information while searching molecular similarity we also propose a novel method by which solutions obtained by each mcs run can be ranked using chemical filters such as stereochemistry bond energy etc results in order to benchmark and test the tool we performed a pair wise comparison between kegg ligands and pdb het group atoms in both cases the smsd was shown to be more efficient than the widely used mcs module implemented in the chemistry development kit cdk in generating mcs solutions from our test cases conclusion presently this tool can be applied to various areas of bioinformatics and chemo informatics for finding exhaustive mcs matches for example it can be used to analyse metabolic networks by mapping the atoms between reactants and products involved in reactions it can also be used to detect the mcs substructure searches in small molecules reported by metabolome experiments as well as in the screening of drug like compounds with similar substructures thus we present a robust tool that can be used for multiple applications including the discovery of new drug molecules this tool is freely available on http www ebi ac uk thornton srv smsd
time and cost are the enemies of cell biology the number of experiments required to rigorously dissect and comprehend a pathway of even modest complexity is daunting methods are needed to formulate biological pathways in a machine analysable fashion which would automate the process of considering all possible experiments in a complex pathway and identify those that command attention in this essay we describe a method that is based on the exploitation of computational tools that were originally developed to analyse reactive communicating computer systems such as mobile phones and web browsers in this approach the biological process is articulated as an executable computer program that can be interrogated using methods that were developed to analyse complex software systems using case studies of the fgf mapk and delta notch pathways we show that the application of this technology can yield interesting insights into the behaviour of signalling pathways which have subsequently been corroborated by experimental jcs
metagenomics projects collect dna from uncharacterized environments that may contain thousands of species per sample one main challenge facing metagenomic analysis is phylogenetic classification of raw sequence reads into groups representing the same or similar taxa a prerequisite for genome assembly and for analyzing the biological diversity of a sample new sequencing technologies have made metagenomics easier by making sequencing faster and more difficult by producing shorter reads than previous technologies classifying sequences from reads as short as base pairs has until now been relatively inaccurate requiring researchers to use older long read technologies we present phymm a classifier for metagenomic data that has been trained on complete curated genomes and can accurately classify reads as short as base pairs a substantial improvement over previous composition based classification methods we also describe how combining phymm with sequence alignment algorithms accuracy
learning to rank for information retrieval ir is a task to automatically construct a ranking model using training data such that the model can sort new objects according to their degrees of relevance preference or importance many ir problems are by nature ranking problems and many ir technologies can be potentially enhanced by using learning to rank techniques the objective of this tutorial is to give an introduction to this research direction specifically the existing learning to rank algorithms are reviewed and categorized into three approaches the pointwise pairwise and listwise approaches the advantages and disadvantages with each approach are analyzed and the relationships between the loss functions used in these approaches and ir evaluation measures are discussed then the empirical evaluations on typical learning to rank methods are shown with the letor collection as a benchmark dataset which seems to suggest that the listwise approach be the most effective one among all the approaches after that a statistical ranking theory is introduced which can describe different learning to rank algorithms and be used to analyze their query level generalization abilities at the end of the tutorial we provide a summary and discuss potential future work on learning rank
the pine alignment and snp identification pipeline pinesap provides a high throughput solution to single nucleotide polymorphism snp prediction using multiple sequence alignments from re sequencing data this pipeline integrates a hybrid of customized scripting existing utilities and machine learning in order to increase the speed and accuracy of snp calls the implementation of this pipeline results in significantly improved multiple sequence alignments and snp identifications when compared with existing solutions the use of machine learning in the snp identifications extends the pipeline s application to any eukaryotic species where full genome sequence information is unavailable availability all code used for this pipeline is freely available at the dendrome project website http dendrome ucdavis edu html
recent advances in high throughput dna sequencing technologies have enabled order of magnitude improvements in both cost and throughput here we report the use of single molecule methods to sequence an individual human genome we aligned billions of to bp reads bp average to approximately of the national center for biotechnology information ncbi reference genome with average coverage our results were obtained on one sequencing instrument by a single operator with four data collection runs single molecule sequencing enabled analysis of human genomic information without the need for cloning amplification or ligation we determined approximately million single nucleotide polymorphisms snps with a false positive rate of less than as validated by sanger sequencing and concordance with snp genotyping arrays we identified regions of copy number variation by analyzing coverage depth alone and validated of these using digital pcr this milestone should allow widespread application of genome sequencing to many aspects of genetics and human health including genomics
we present an algorithm pyronoise that clusters the flowgrams of pyrosequencing reads using a distance measure that models sequencing noise this infers the true sequences in a collection of amplicons we pyrosequenced a known mixture of microbial rdna sequences extracted from a lake and found that without noise reduction the number of operational taxonomic units is overestimated but using pyronoise it can be calculated
detection and characterization of genomic structural variation are important for understanding the landscape of genetic variation in human populations and in complex diseases such as cancer recent studies demonstrate the feasibility of detecting structural variation using next generation short insert paired end sequencing reads however the utility of these reads is not entirely clear nor are the analysis methods with which accurate detection can be achieved the algorithm breakdancer predicts a wide variety of structural variants including insertion deletions indels inversions and translocations we examined breakdancer s performance in simulation in comparison with other methods and in analyses of a sample from an individual with acute myeloid leukemia and of samples from the genomes trio individuals breakdancer sensitively and accurately detected indels ranging from base pairs to megabase pair that are difficult to detect via a single approach
a crucial question in the field of gene regulation is whether the location at which a transcription factor binds influences its effectiveness or the mechanism by which it regulates transcription comprehensive transcription factor binding maps are needed to address these issues and genome wide mapping is now possible thanks to the technological advances of chipchip and chipseq this review discusses how recent genomic profiling of transcription factors gives insight into how binding specificity is achieved and what features of chromatin influence the ability of transcription factors to interact with the genome it also suggests future experiments that may further our understanding of the causes and consequences of transcription interactions
the dynamics governing gene regulation have an important role in determining the phenotype of a cell or organism from processing extracellular signals to generating internal rhythms gene networks are central to many time dependent cellular processes recent technological advances now make it possible to track the dynamics of gene networks in single cells under various environmental conditions using microfluidic lab on a chip devices and researchers are using these new techniques to analyse cellular dynamics and discover regulatory mechanisms these technologies are expected to yield novel insights and allow the construction of mathematical models that more accurately describe the complex dynamics of regulation
how does auditory cortex represent auditory stimuli and how do these representations contribute to behavior recent experimental evidence suggests that activity in auditory cortex consists of sparse and highly synchronized volleys of activity observed both in anesthetized and awake animals many neurons are capable of remarkably precise activity with very low jitter or spike count variability most importantly animals are capable of exploiting such precise neuronal activity in making sensory decisions whether the ability of auditory cortex to exploit fine temporal differences in cortical activity is unique to auditory modality or represents a general strategy used by cortical circuits remains an question
motivation the explosion of next generation sequencing data has spawned the design of new algorithms and software tools to provide efficient mapping for different read lengths and sequencing technologies in particular abi s sequencer solid system poses a big computational challenge with its capacity to produce very large amounts of data and its unique strategy of encoding sequence data into color signals results we present the mapping software named perm periodic seed mapping that uses periodic spaced seeds to significantly improve mapping efficiency for large reference genomes when compared with state of the art programs the data structure in perm requires only bytes per base to index the human genome allowing entire genomes to be loaded to memory while multiple processors simultaneously map reads to the reference weight maximized periodic seeds offer full sensitivity for up to three mismatches and high sensitivity for four and five mismatches while minimizing the number random hits per query significantly speeding up the running time such sensitivity makes perm a valuable mapping tool for solid and solexa reads availability http code google com p perm contact tingchen usc edu supplementary information supplementary data are available at bioinformatics bioinformatics
sleep is often viewed as a vulnerable state that is incompatible with behaviours that nourish and propagate species this has led to the hypothesis that sleep has survived because it fulfills some universal but as yet unknown vital function i propose that sleep is best understood as a variant of dormant states seen throughout the plant and animal kingdoms and that it is itself highly adaptive because it optimizes the timing and duration of behaviour current evidence indicates that ecological variables are the main determinants of sleep duration and intensity species
screens for agents that specifically kill epithelial cancer stem cells cscs have not been possible due to the rarity of these cells within tumor cell populations and their relative instability in culture we describe here an approach to screening for agents with epithelial csc specific toxicity we implemented this method in a chemical screen and discovered compounds showing selective toxicity for breast cscs one compound salinomycin reduces the proportion of cscs by fold relative to paclitaxel a commonly used breast cancer chemotherapeutic drug treatment of mice with salinomycin inhibits mammary tumor growth invivo and induces increased epithelial differentiation of tumor cells in addition global gene expression analyses show that salinomycin treatment results in the loss of expression of breast csc genes previously identified by analyses of breast tissues isolated directly from patients this study demonstrates the ability to identify agents with specific toxicity for cscs
this is a written version of the opening talk at the international workshop on chiral dynamics at the university of bern switzerland july to be published in the proceedings of the workshop in it i reminisce about the early development of effective field theories of the strong interactions comment briefly on some other applications of effective field theories and then take up the idea that the standard model and general relativity are the leading terms in an effective field theory finally i cite recent calculations that suggest that the effective field theory of gravitation and matter is safe
the revolution in scientific publishing that has been promised since the is about to take place scientists have always read strategically working with many articles simultaneously to search filter scan link annotate and analyze fragments of content an observed recent increase in strategic reading in the online environment will soon be further intensified by two current trends i the widespread use of digital indexing retrieval and navigation resources and ii the emergence within many scientific disciplines of interoperable ontologies accelerated and enhanced by reading tools that take advantage of ontologies reading practices will become even more rapid and indirect transforming the ways in which scientists engage the literature and shaping the evolution of scientific science
massively parallel sequencing has reduced the cost and increased the throughput of genomic sequencing by more than three orders of magnitude and it seems likely that costs will fall and throughput improve even more in the next few years clinical use of massively parallel sequencing will provide a way to identify the cause of many diseases of unknown etiology through simultaneous screening of thousands of loci for pathogenic mutations and by sequencing biological specimens for the genomic signatures of novel infectious agents in addition to providing these entirely new diagnostic capabilities massively parallel sequencing may also replace arrays and sanger sequencing in clinical applications where they are currently being used routine clinical use of massively parallel sequencing will require higher accuracy better ways to select genomic subsets of interest and improvements in the functionality speed and ease of use of data analysis software in addition substantial enhancements in laboratory computer infrastructure data storage and data transfer capacity will be needed to handle the extremely large data sets produced clinicians and laboratory personnel will require training to use the sequence data effectively and appropriate methods will need to be developed to deal with the incidental discovery of pathogenic mutations and variants of uncertain clinical significance massively parallel sequencing has the potential to transform the practice of medical genetics and related fields but the vast amount of personal genomic data produced will increase the responsibility of geneticists to ensure that the information obtained is used in a medically and socially manner
complete formal verification is the only way to guarantee that a system is free of programming errors we present ent our experience in performing the formal machine checked verification of the microkernel from an abstract specification down to its c implementation we assume correctness of compiler assembly code and hardware and we used a unique design approach that fuses formal and operating systems techniques to our knowledge this is the first formal proof of functional correctness of a complete general purpose operating system kernel functional correctness means here that the implementation always strictly follows our high level abstract specification of kernel behaviour this encompasses traditional design and implementation safety properties such as the kernel will never crash and it will never perform an unsafe operation it also proves much more we can predict precisely how the kernel will behave in every possible situation a third generation microkernel of provenance comprises lines of c code and lines of assembler its performance is comparable to other high kernels
current spike sorting methods focus on clustering the neurons characteristic spike waveforms the resulting spike sorted data are typically used to estimate how covariates of interest modulate the firing rates of neurons however when these covariates do modulate the firing rates they provide information about spikes identities which thus far have been ignored for the purpose of spike sorting this letter describes a novel approach to spike sorting which incorporates both waveform information and tuning information obtained from the modulation of firing rates because it efficiently uses all the available information this spike sorter yields lower spike misclassification rates than traditional automatic spike sorters this theoretical result is verified empirically on several examples the proposed method does not require additional assumptions only its implementation is different it essentially consists of performing spike sorting and tuning estimation simultaneously rather than sequentially as is currently done we used an expectation maximization maximum likelihood algorithm to implement the new spike sorter we present the general form of this algorithm and provide a detailed implementable version under the assumptions that neurons are independent and spike according to poisson processes finally we uncover a systematic flaw of spike sorting based on waveform only
sequencing a genome to great depth can be highly informative about heterogeneity within an individual or a population here we address the problem of how to visualize the multiple layers of information contained in deep sequencing data we propose an interactive ajax based web viewer for browsing large datasets of aligned sequence reads by enabling seamless browsing and fast zooming the lookseq program assists the user to assimilate information at different levels of resolution from an overview of a genomic region to fine details such as heterogeneity within the sample a specific problem particularly if the sample is heterogeneous is how to depict information about structural variation lookseq provides a simple graphical representation of paired sequence reads that is more revealing about potential insertions and deletions than conventional methods lookseq is freely available at http lookseq net
the three dimensional structures of proteins are stabilized by the interactions between amino acid residues here we report a method where four distances are calculated between any two side chains to provide an exact spatial definition of their bonds the data were binned into a four dimensional grid and compared to a random model from which the preference for specific four distances was calculated a clear relation between the quality of the experimental data and the tightness of the distance distribution was observed with crystal structure data providing far tighter distance distributions than nmr data since the four distance data have higher information content than classical bond descriptions we were able to identify many unique inter residue features not found previously in proteins for example we found that the side chains of arg glu val and leu are not symmetrical in respect to the interactions of their head groups the described method may be developed into a function which computationally models accurately structures
abstract the illumina genome analyzer ga generates millions of short sequencing reads we present ibis improved base identification system an accurate fast and easy to use base caller that significantly reduces the error rate and increases the output of usable reads ibis is faster and more robust with respect to chemistry and technology than other publicly available packages ibis is freely available under the gpl from http bioinf eva mpg ibis
a common biological pathway reconstruction approachas implemented by many automatic biological pathway services such as the kaas and rast servers and the functional annotation of metagenomic sequencesstarts with the identification of protein functions or families e g ko families for the kegg database and the fig families for the seed database in the query sequences followed by a direct mapping of the identified protein families onto pathways given a predicted patchwork of individual biochemical steps some metric must be applied in deciding what pathways actually exist in the genome or metagenome represented by the sequences commonly and straightforwardly a complete biological pathway can be identified in a dataset if at least one of the steps associated with the pathway is found we report however that this nave mapping approach leads to an inflated estimate of biological pathways and thus overestimates the functional diversity of the sample from which the dna sequences are derived we developed a parsimony approach called minpath min imal set of path ways for biological pathway reconstructions using protein family predictions which yields a more conservative yet more faithful estimation of the biological pathways for a query dataset minpath identified far fewer pathways for the genomes collected in the kegg databaseas compared to the nave mapping approacheliminating some obviously spurious pathway annotations results from applying minpath to several metagenomes indicate that the common methods used for metagenome annotation may significantly overestimate the biological pathways encoded by communities
background the availability of newly sequenced vertebrate genomes along with more efficient and accurate alignment algorithms have enabled the expansion of the field of comparative genomics large scale genome rearrangement events modify the order of genes and non coding conserved regions on chromosomes while certain large genomic regions have remained intact over much of vertebrate evolution others appear to be hotspots for genomic breakpoints the cause of the non uniformity of breakpoints that occurred during vertebrate evolution is poorly understood results we describe a machine learning method to distinguish genomic regions where breakpoints would be expected to have deleterious effects called breakpoint refractory regions from those where they are expected to be neutral called breakpoint susceptible regions our predictor is trained using breakpoints that took place along the human lineage since amniote divergence based on our predictions refractory and susceptible regions have very distinctive features refractory regions are significantly enriched for conserved non coding elements as well as for genes involved in development whereas susceptible regions are enriched for housekeeping genes likely to have simpler transcriptional regulation conclusion we postulate that long range transcriptional regulation strongly influences chromosome break fixation in many regions the fitness cost of altering the spatial association between long range regulatory regions and their target genes may be so high that rearrangements are not allowed consequently only a limited identifiable fraction of the genome is susceptible to rearrangements
abstract background genomic analysis particularly for less well characterized organisms is greatly assisted by performing comparative analyses between different types of genome maps and across species boundaries various providers publish a plethora of on line resources collating genome mapping data from a multitude of species datasources range in scale and scope from small bespoke resources for particular organisms through larger web resources containing data from multiple species to large scale bioinformatics resources providing access to data derived from genome projects for model and non model organisms the heterogeneity of information held in these resources reflects both the technologies used to generate the data and the target users of each resource currently there is no common information exchange standard or protocol to enable access and integration of these disparate resources consequently data integration and comparison must be performed in an ad hoc manner results we have developed a simple generic xml schema genomicmappingdata xsd gmd to allow export and exchange of mapping data in a common lightweight xml document format this schema represents the various types of data objects commonly described across mapping datasources and provides a mechanism for recording relationships between data objects the schema is sufficiently generic to allow representation of any map type for example genetic linkage maps radiation hybrid maps sequence maps and physical maps it also provides mechanisms for recording data provenance and for cross referencing external datasources including for example ensembl pubmed and genbank the schema is extensible via the inclusion of additional datatypes which can be achieved by importing further schemas e g a schema defining relationship types we have built demonstration web services that export data from our arkdb database according to the gmd schema facilitating the integration of data retrieval into taverna workflows conclusions the data exchange standard we present here provides a useful generic format for transfer and integration of genomic and genetic mapping data the extensibility of our schema allows for inclusion of additional data and provides a mechanism for typing mapping objects via third party standards web services retrieving gmd compliant mapping data demonstrate that use of this exchange standard provides a practical mechanism for achieving data integration by facilitating syntactically and semantically controlled access to data
summary illumina produces a number of microarray based technologies for human genotyping an infinium beadchip is a two color platform that types between and single nucleotide polymorphisms snps per sample despite being widely used there is a shortage of open source software to process the raw intensities from this platform into genotype calls to this end we have developed the r bioconductor package crlmm for analyzing beadchip data after careful preprocessing our software applies the crlmm algorithm to produce genotype calls confidence scores and other quality metrics at both the snp and sample levels we provide access to the raw summary level intensity data allowing users to develop their own methods for genotype calling or copy number analysis if they wish availability and implementation the crlmm bioconductor package is available from http www bioconductor org data packages and documentation are available from http rafalab jhsph edu software html contact mritchie wehi edu au rafa jhu bioinformatics
motivation with the proliferation of microarray experiments and their availability in the public domain the use of meta analysis methods to combine results from different studies increases in microarray experiments where the sample size is often limited meta analysis offers the possibility to considerably increase the statistical power and give more accurate results results a moderated effect size combination method was proposed and compared to other meta analysis approaches all methods were applied to real publicly available datasets on prostate cancer and were compared in an extensive simulation study for various amounts of inter study variability although the proposed moderated effect size combination improved already existing effect size approaches the p value combination was found to provide a better sensitivity and a better gene ranking than the other meta analysis methods while effect size methods were more conservative availability an r package metama is available on the cran contact guillemette marot jouy fr
genome wide association studies suggest that common genetic variants explain only a modest fraction of heritable risk for common diseases raising the question of whether rare variants account for a significant fraction of unexplained heritability although dna sequencing costs have fallen markedly they remain far from what is necessary for rare and novel variants to be routinely identified at a genome wide scale in large cohorts we have therefore sought to develop second generation methods for targeted sequencing of all protein coding regions exomes to reduce costs while enriching for discovery of highly penetrant variants here we report on the targeted capture and massively parallel sequencing of the exomes of humans these include eight hapmap individuals representing three populations and four unrelated individuals with a rare dominantly inherited disorder freeman sheldon syndrome fss we demonstrate the sensitive and specific identification of rare and common variants in over megabases of coding sequence using fss as a proof of concept we show that candidate genes for mendelian disorders can be identified by exome sequencing of a small number of unrelated affected individuals this strategy may be extendable to diseases with more complex genetics through larger sample sizes and appropriate weighting of non synonymous variants by predicted impact
allostery describes altered protein function at one site due to a perturbation at another site one mechanism of allostery involves correlated motions which can occur even in the absence of substantial conformational change we present a novel method mutlnf to identify statistically significant correlated motions from equilibrium molecular dynamics simulations our approach analyzes both backbone and side chain motions using internal coordinates to account for the gear like twists that can take place even in the absence of the large conformational changes typical of traditional allosteric proteins we quantify correlated motions using a mutual information metric which we extend to incorporate data from multiple short simulations and to filter out correlations that are not statistically significant applying our approach to uncover mechanisms of cooperative small molecule binding in human interleukin we identify clusters of correlated residues from ns of molecular dynamics simulations interestingly two of the clusters with the strongest correlations highlight known cooperative small molecule binding sites and show substantial correlations between these sites these cooperative binding sites on interleukin are correlated not only through the hydrophobic core of the protein but also through a dynamic polar network of hydrogen bonding and electrostatic interactions since this approach identifies correlated conformations in an unbiased statistically robust manner it should be a useful tool for finding novel or orphan allosteric sites in proteins of biological and importance
many practical computing problems concern large graphs standard examples include the web graph and various social networks the scale of these graphs in some cases billions of vertices trillions of edges poses challenges to their efficient processing in this paper we present a computational model suitable for this task programs are expressed as a sequence of iterations in each of which a vertex can receive messages sent in the previous iteration send messages to other vertices and modify its own state and that of its outgoing edges or mutate graph topology this vertex centric approach is flexible enough to express a broad set of algorithms the model has been designed for efficient scalable and fault tolerant implementation on clusters of thousands of commodity computers and its implied synchronicity makes reasoning about programs easier distribution related details are hidden behind an abstract api the result is a framework for processing large graphs that is expressive and easy program
one of the most rapidly growing areas of physics and nanotechnology focuses on plasmonic effects on the nanometre scale with possible applications ranging from sensing and biomedicine to imaging and information however the full development of nanoplasmonics is hindered by the lack of devices that can generate coherent plasmonic fields it has been that in the same way as a laser generates stimulated emission of coherent photons a spaser could generate stimulated emission of surface plasmons oscillations of free electrons in metallic nanostructures in resonating metallic nanostructures adjacent to a gain medium but attempts to realize a spaser face the challenge of absorption loss in metal which is particularly strong at optical frequencies the to compensate loss by optical gain in localized and propagating surface plasmons has been implemented and even allowed the amplification of propagating surface plasmons in open still these experiments and the reported enhancement of the stimulated emission of dye molecules in the presence of metallic lack the feedback mechanism present in a spaser here we show that nm diameter nanoparticles with a gold core and dye doped silica shell allow us to completely overcome the loss of localized surface plasmons by gain and realize a spaser and in accord with the notion that only surface plasmon resonances are capable of squeezing optical frequency oscillations into a nanoscopic cavity to enable a true we show that outcoupling of surface plasmon oscillations to photonic modes at a wavelength of makes our system the smallest nanolaser reported to dateand to our knowledge the first operating at visible wavelengths we anticipate that now it has been realized experimentally the spaser will advance our fundamental understanding of nanoplasmonics and the development of applications
although allostery draws increasing attention not much is known about allosteric mechanisms here we argue that in all proteins allosteric signals transmit through multiple pre existing pathways which pathways dominate depend on protein topologies specific binding events covalent modifications and cellular environmental conditions further perturbation events at any site on the protein surface or in the interior will not create new pathways but only shift the pre existing ensemble of pathways drugs binding at different sites ormutational events in disease shift the ensemble toward the same conformations however the relative populations of the different states will change consequently the observed functional conformational and dynamic effects will be different this is the origin of allosteric functional modulation in dynamic proteins allostery does not necessarily need to invoke conformational rearrangements to control protein activity and pre existing pathways are always defaulted to during allostery regardless of the stimulant and perturbation site in protein
pnas data collected from mobile phones have the potential to provide insight into the relational dynamics of individuals this paper compares observational data from mobile phones with standard self report survey data we find that the information from these two data sources is overlapping but distinct for example self reports of physical proximity deviate from mobile phone records depending on the recency and salience of the interactions we also demonstrate that it is possible to accurately infer of friendships based on the observational data alone where friend dyads demonstrate distinctive temporal and spatial patterns in their physical proximity and calling patterns these behavioral patterns in turn allow the prediction of individual level outcomes such as satisfaction
the genomes of higher organisms are packaged in nucleosomes with functional histone modifications until now genome wide nucleosome and histone modification studies have focused on transcription start sites tsss where nucleosomes in rna polymerase ii rnapii occupied genes are well positioned and have histone modifications that are characteristic of expression status using public data we here show that there is a higher nucleosome positioning signal in internal human exons and that this positioning is independent of expression we observed a similarly strong nucleosome positioning signal in internal exons of caenorhabditis elegans among the histone modifications analyzed in man and had evidently higher signals in internal exons than in the following introns and were clearly related to exon expression these observations are suggestive of roles in splicing thus exons are not only characterized by their coding capacity but also by their nucleosome organization which seems evolutionarily conserved since it is present in both primates nematodes
abstract asi abs social tagging is one of the major phenomena transforming the world wide web from a static platform into an actively shared information space this paper addresses various aspects of social tagging including different views on the nature of social tagging how to make use of social tags and how to bridge social tagging with other web functionalities it discusses the use of facets to facilitate browsing and searching of tagging data and it presents an analogy between bibliometrics and tagometrics arguing that established bibliometric methodologies can be applied to analyze tagging behavior on the web based on the upper tag ontology uto a web crawler was built to harvest tag data from delicious flickr and youtube in september in total million objects including bookmarks photos and videos million taggers and million tags were collected and analyzed some tagging patterns and variations are identified discussed
wright s f statistics and especially f st provide important insights into the evolutionary processes that influence the structure of genetic variation within and among populations and they are among the most widely used descriptive statistics in population and evolutionary genetics estimates of f st can identify regions of the genome that have been the target of selection and comparisons of f st from different parts of the genome can provide insights into the demographic history of populations for these reasons and others f st has a central role in population and evolutionary genetics and has wide applications in fields that range from disease association mapping to forensic science this review clarifies how f st is defined how it should be estimated how it is related to similar statistics and how estimates of f st should interpreted
the systematic and quantitative molecular analysis of mutant organisms that has been pioneered by studies on mutant metabolomes and transcriptomes holds great promise for improving our understanding of how phenotypes emerge unfortunately owing to the limitations of classical biochemical analysis proteins have previously been excluded from such studies here we review how technical advances in mass spectrometry based proteomics can be applied to measure changes in protein abundance posttranslational modifications and proteinprotein interactions in mutants at the scale of the proteome we finally discuss examples that integrate proteomics data with genomic and phenomic information to build network centred models which provide a promising route for understanding how emerge
although several studies have provided important insights into the general principles of biological networks the link between network organization and the genome scale dynamics of the underlying entities genes mrnas and proteins and its role in systems behavior remain unclear here we show that transcription factor tf dynamics and regulatory network organization are tightly linked by classifying tfs in the yeast regulatory network into three hierarchical layers top core and bottom and integrating diverse genome scale datasets we find that the tfs have static and dynamic properties that are similar within a layer and different across layers at the protein level the top layer tfs are relatively abundant long lived and noisy compared with the core and bottom layer tfs although variability in expression of top layer tfs might confer a selective advantage as this permits at least some members in a clonal cell population to initiate a response to changing conditions tight regulation of the core and bottom layer tfs may minimize noise propagation and ensure fidelity in regulation we propose that the interplay between network organization and tf dynamics could permit differential utilization of the same underlying network by distinct members of a clonal population
motivation for the biologist running bioinformatics analyses involves a time consuming management of data and tools users need support to organize their work retrieve parameters reproduce their analyses they also need to be able to combine their analytic tools using a safe data flow software mechanism finally given that scientific tools can be difficult to install it is particularly helpful for biologists to be able to use these tools through a web user interface however providing a web interface for a set of tools raises the problem that a single web portal cannot offer all the existing and possible services it is the user again who has to cope with data copy among a number of different services a framework enabling portal administrators to build a network of cooperating services would therefore clearly be beneficial results we have designed a system mobyle to provide a flexible and usable web environment for defining and running bioinformatics analyses it embeds simple yet powerful data management features that allow the user to reproduce analyses and to combine tools using a hierarchical typing system mobyle offers invocation of services distributed over remote mobyle servers thus enabling a federated network of curated bioinformatics portals without the user having to learn complex concepts or to install sophisticated software while being focused on the end user the mobyle system also addresses the need for the bioinfomatician to automate remote services execution playmoby is a companion tool that automates the publication of biomoby web services using mobyle program definitions availability the mobyle system is distributed under the terms of the gnu on the project website http pasteur fr projects mobyle it is already deployed on three servers http mobyle pasteur fr http mobyle rpbs univ paris diderot fr and http lipm bioinfo toulouse inra fr mobyle the playmoby companion is distributed under the terms of the cecill license and is available at http lipm bioinfo toulouse inra fr biomoby playmoby contact mobyle support pasteur fr mobyle support rpbs univparis fr
pnas the mechanism of ligand binding coupled to conformational changes in macromolecules has recently attracted considerable interest the limiting cases are the induced fit mechanism binding first or conformational selection conformational change first described here are the criteria by which the sequence of events can be determined quantitatively the relative importance of the pathways is determined not by comparing rate constants a common misconception but instead by comparing the flux through each pathway the simple rules for calculating flux in multistep mechanisms are described and then applied to examples from the literature neither of which has previously been analyzed using the concept of flux the first example is the mechanism of conformational change in the binding of nadph to dihydrofolate reductase the second example is the mechanism of flavodoxin folding coupled to binding of its cofactor flavin mononucleotide in both cases the mechanism switches from being dominated by the conformational selection pathway at low ligand concentration to induced fit at high ligand concentration over a wide range of conditions a significant fraction of the flux occurs through both pathways such a mixed mechanism likely will be discovered for many cases of coupled conformational change and ligand binding when kinetic data are analyzed by using a flux based er
commodity computer systems contain more and more processor cores and exhibit increasingly diverse architectural tradeoffs including memory hierarchies interconnects instruction sets and variants and io configurations previous high performance computing systems have scaled in specific cases but the dynamic nature of modern client and server workloads coupled with the impossibility of statically optimizing an os for all workloads and hardware variants pose serious challenges for operating system structures we argue that the challenge of future multicore hardware is best met by embracing the networked nature of the machine rethinking os architecture using ideas from distributed systems we investigate a new os structure the multikernel that treats the machine as a network of independent cores assumes no inter core sharing at the lowest level and moves traditional os functionality to a distributed system of processes that communicate via message passing we have implemented a multikernel os to show that the approach is promising and we describe how traditional scalability problems for operating systems such as memory management can be effectively recast using messages and can exploit insights from distributed systems and networking an evaluation of our prototype on multicore systems shows that even on present day machines the performance of a multikernel is comparable with a conventional os and can scale better to support hardware
structure based and physics based coarse grained molecular force fields have become attractive approaches to gain mechanistic insight into the function of large biomolecular assemblies here we study how both approaches can be combined into a single representation that we term elnedin in this representation an elastic network is used as a structural scaffold to describe and maintain the overall shape of a protein and a physics based coarse grained model martini is used to describe both inter and intramolecular interactions in the system the results show that when used in molecular dynamics simulations elnedin models can be built so that the resulting structural and dynamical properties of a protein including its collective motions are comparable to those obtained using atomistic protein models we then evaluate the behavior of such models in long microsecond time scale simulations the modeling of very large macromolecular assemblies a viral capsid and the study of a proteinprotein association process the reassembly of the rop homodimer the results for this series of tests indicate that elnedin models allow microsecond time scale molecular dynamics simulations to be carried out readily that large biological entities such as the viral capsid of the cowpea mosaic virus can be stably modeled as assemblies of independent elnedin models and that elnedin models show significant promise for modeling proteinprotein processes
motivation the power of a microarray experiment derives from the identification of genes differentially regulated across biological conditions to date differential regulation is most often taken to mean differential expression and a number of useful methods for identifying differentially expressed de genes or gene sets are available however such methods are not able to identify many relevant classes of differentially regulated genes one important example concerns differentially co expressed dc genes results we propose an approach gene set co expression analysis gsca to identify dc gene sets the gsca approach provides a false discovery rate controlled list of interesting gene sets does not require that genes be highly correlated in at least one biological condition and is readily applied to data from individual or multiple experiments as we demonstrate using data from studies of lung cancer and diabetes availability the gsca approach is implemented in r and available at www biostat wisc edu kendzior gsca contact kendzior biostat wisc edu supplementary information supplementary data are available at bioinformatics bioinformatics
motivation prediction of microrna mirna target mrnas using machine learning approaches is an important area of research however most of the methods suffer from either high false positive or false negative rates one reason for this is the marked deficiency of negative examples or mirna non target pairs systematic identification of non target mrnas is still not addressed properly and therefore current machine learning approaches are compelled to rely on artificially generated negative examples for training results in this article we have identified tissue specific negative examples using a novel approach that involves expression profiling of both mirnas and mrnas mirna mrna structural interactions and seed site conservation the newly generated negative examples are validated with psilac dataset which elucidate the fact that the identified non targets are indeed non targets these high throughput tissue specific negative examples and a set of experimentally verified positive examples are then used to build a system called targetminer a support vector machine svm based classifier in addition to assessing the prediction accuracy on cross validation experiments targetminer has been validated with a completely independent experimental test dataset our method outperforms existing target prediction algorithms and provides a good balance between sensitivity and specificity that is not reflected in the existing methods we achieve a significantly higher sensitivity and specificity of and based on a pool of feature set and and using a set of selected feature set on the completely independent test dataset in order to establish the effectiveness of the systematically generated negative examples the svm is trained using a different set of negative data generated using the method in yousef et al a significantly higher false positive rate is observed when tested on the independent set while all other factors are kept the same again when an existing method nbmirtar is executed with the our proposed negative data we observe an improvement in its performance these clearly establish the effectiveness of the proposed approach of selecting the negative examples systematically availability targetminer is now available as an online tool at www isical ac in contact sanghami isical ac in isical ac in supplementary information supplementary data are available at bioinformatics bioinformatics
in recent years a flurry of new dna sequencing technologies have altered the landscape of genomics providing a vast amount of sequence information at a fraction of the costs that were previously feasible the task of assembling these sequences into a genome has however still remained an algorithmic challenge that is in practice answered by heuristic solutions in order to design better assembly algorithms and exploit the characteristics of sequence data from new technologies we need an improved understanding of the parametric complexity of the assembly problem in this article we provide a first theoretical study in this direction exploring the connections between repeat complexity read lengths overlap lengths and coverage in determining the hard instances of the assembly problem our work suggests at least two ways in which existing assemblers can be extended in a rigorous fashion in addition to delineating directions for future investigations
background variations in the composition of the human intestinal microbiota are linked to diverse health conditions high throughput molecular technologies have recently elucidated microbial community structure at much higher resolution than was previously possible here we compare two such methods pyrosequencing and a phylogenetic array and evaluate classifications based on two variable rrna gene regions methods and findings over million amplicon sequences were generated from the and regions of rrna genes in bacterial dna extracted from four fecal samples of elderly individuals the phylotype richness for individual samples was for reads and for reads and unique phylotypes when combining reads from all samples the rdp classifier was more efficient for the than for the far less conserved and shorter region but differences in community structure also affected efficiency even when analyzing only of the reads the majority of the microbial diversity was captured in two samples tested dna from the four samples was hybridized against the human intestinal tract hit chip a phylogenetic microarray for community profiling comparison of clustering of genus counts from pyrosequencing and hitchip data revealed highly similar profiles furthermore correlations of sequence abundance and hybridization signal intensities were very high for lower order ranks but lower at family level which was probably due to ambiguous taxonomic groupings conclusions the rdp classifier consistently assigned most sequences from human intestinal samples down to genus level with good accuracy and speed this is the deepest sequencing of single gastrointestinal samples reported to date but microbial richness levels have still not leveled out a majority of these diversities can also be captured with five times lower sampling depth hitchip hybridizations and resulting community profiles correlate well with pyrosequencing based compositions especially for lower order ranks indicating high robustness of both approaches however incompatible grouping schemes make exact difficult
we recently reported the chemical synthesis assembly and cloning of a bacterial genome in yeast to produce a synthetic cell the genome must be transferred from yeast to a receptive cytoplasm here we describe methods to accomplish this we cloned a mycoplasma mycoides genome as a yeast centromeric plasmid and then transplanted it into mycoplasma capricolum to produce a viable m mycoides cell while in yeast the genome was altered by using yeast genetic systems and then transplanted to produce a new strain of m mycoides these methods allow the construction of strains that could not be produced with genetic tools available for bacterium
abstract background high throughput bioinformatic analysis tools are needed to mine the large amount of structural data via knowledge based approaches the development of such tools requires a robust interface to access the structural data in an easy way for this the python scripting language is the optimal choice since its philosophy is to write an understandable source code results is an object oriented python module that adds a simple yet powerful interface to the python interpreter to process and analyse three dimensional protein structure files pdb files s strength arises from the combination of a very fast spatial access to the structural data due to the implementation of a binary space partitioning bsp tree b set theory and c functions that allow to combine a and b and that use human readable language in the search queries rather than complex computer language all these factors combined facilitate the rapid development of bioinformatic tools that can perform quick and complex analyses of protein structures conclusions is the perfect tool to quickly develop structural bioinformatic tools using the python language
small molecule drugs target many core metabolic enzymes in humans and pathogens often mimicking endogenous ligands the effects may be therapeutic or toxic but are frequently unexpected a large scale mapping of the intersection between drugs and metabolism is needed to better guide drug discovery to map the intersection between drugs and metabolism we have grouped drugs and metabolites by their associated targets and enzymes using ligand based set signatures created to quantify their degree of similarity in chemical space the results reveal the chemical space that has been explored for metabolic targets where successful drugs have been found and what novel territory remains to aid other researchers in their drug discovery efforts we have created an online resource of interactive maps linking drugs to metabolism these maps predict the effect space comprising likely target enzymes for each of the mddr drug classes in humans the online resource also provides species specific interactive drug metabolism maps for each of the model organisms and pathogens in the biocyc database collection chemical similarity links between drugs and metabolites predict potential toxicity suggest routes of metabolism and reveal drug polypharmacology the metabolic maps enable interactive navigation of the vast biological data on potential metabolic drug targets and the drug chemistry currently available to prosecute those targets thus this work provides a large scale approach to ligand based prediction of drug action in small metabolism
summary proteins display a hierarchy of structural features at primary secondary tertiary and higher order levels an organization that guides our current understanding of their biological properties and evolutionary origins here we reveal a structural organization distinct from this traditional hierarchy by statistical analysis of correlated evolution between amino acids applied to the serine proteases the analysis indicates a decomposition of the protein into three quasi independent groups of correlated amino acids that we term protein sectors each sector is physically connected in the tertiary structure has a distinct functional role and constitutes an independent mode of sequence divergence in the protein family functionally relevant sectors are evident in other protein families as well suggesting that they may be general features of proteins we propose that sectors represent a structural organization of proteins that reflects their histories
pnas the conformational flexibility of target proteins continues to be a major challenge in accurate modeling of proteininhibitor interactions a fundamental issue yet to be clarified is whether the observed conformational changes are controlled by the protein or induced by the inhibitor although the concept of induced fit has been widely adopted for describing the structural changes that accompany ligand binding there is growing evidence in support of the dominance of proteins intrinsic dynamics which has been evolutionarily optimized to accommodate its functional interactions the wealth of structural data for target proteins in the presence of different ligands now permits us to make a critical assessment of the balance between these two effects in selecting the bound forms we focused on three widely studied drug targets hiv reverse transcriptase map kinase and cyclin dependent kinase a total of structures determined for these enzymes in the presence of different inhibitors and unbound form permitted us to perform an extensive comparative analysis of the conformational space accessed upon ligand binding and its relation to the intrinsic dynamics before ligand binding as predicted by elastic network model analysis our results show that the ligand selects the conformer that best matches its structural and dynamic properties among the conformers intrinsically accessible to the protein in the unliganded form the results suggest that simple but robust rules encoded in the protein structure play a dominant role in predefining the mechanisms of ligand binding which may be advantageously exploited in inhibitors
background analyses of dna sequences from cultivated microorganisms have revealed genome wide taxa specific nucleotide compositional characteristics referred to as genome signatures these signatures have far reaching implications for understanding genome evolution and potential application in classification of metagenomic sequence fragments however little is known regarding the distribution of genome signatures in natural microbial communities or the extent to which environmental factors shape them results we analyzed metagenomic sequence data from two acidophilic biofilm communities including composite genomes reconstructed for nine archaea three bacteria and numerous associated viruses as well as thousands of unassigned fragments from strain variants and low abundance organisms genome signatures in the form of tetranucleotide frequencies analyzed by emergent self organizing maps segregated sequences from all known populations sharing to average amino acid identity and revealed previously unknown genomic clusters corresponding to low abundance organisms and a putative plasmid signatures were pervasive genome wide clusters were resolved because intra genome differences resulting from translational selection or protein adaptation to the intracellular ph approximately versus extracellular ph approximately environment were small relative to inter genome differences we found that these genome signatures stem from multiple influences but are primarily manifested through codon composition which we propose is the result of genome specific mutational biases conclusions an important conclusion is that shared environmental pressures and interactions among coevolving organisms do not obscure genome signatures in acid mine drainage communities thus genome signatures can be used to assign sequence fragments to populations an essential prerequisite if metagenomics is to provide ecological and biochemical insights into the functioning of communities
recent mirna transfection experiments show strong evidence that mirnas influence not only their target but also non target genes the precise mechanism of the extended regulatory effects of mirnas remains to be elucidated a hypothetical two layer regulatory network in which transcription factors tfs function as important mediators of mirna initiated regulatory effects was envisioned and a comprehensive strategy was developed to map such mirna centered regulatory cascades given gene expression profiles after mirna perturbation along with putative mirna gene and tf gene regulatory relationships highly likely degraded targets were fetched by a non parametric statistical test mirna regulated tfs and their downstream targets were mined out through linear regression modeling when applied to expression datasets this strategy discovered combinatorial regulatory networks centered around mirnas a tumor related regulatory network was diagrammed as an example with the important tumor related regulators and myc playing hub connector roles a web server is provided for query and analysis of all reported data in this article our results reinforce the growing awareness that non coding rnas may play key roles in the transcription regulatory network our strategy could be applied to reveal conditional regulatory pathways in many more contexts
chromatin has an impact on recombination repair replication and evolution of dna here we report that chromatin structure also affects laboratory dna manipulation in ways that distort the results of chromatin immunoprecipitation chip experiments we initially discovered this effect at the saccharomyces cerevisiae hmr locus where we found that silenced chromatin was refractory to shearing relative to euchromatin using input samples from chip seq studies we detected a similar bias throughout the heterochromatic portions of the yeast genome we also observed significant chromatin related effects at telomeres protein binding sites and genes reflected in the variation of input seq coverage experimental tests of candidate regions showed that chromatin influenced shearing at some loci and that chromatin could also lead to enriched or depleted dna levels in prepared samples independently of shearing effects our results suggested that assays relying on immunoprecipitation of chromatin will be biased by intrinsic differences between regions packaged into different chromatin structures biases which have been largely ignored to date these results established the pervasiveness of this bias genome wide and suggested that this bias can be used to detect differences in chromatin structures across genome
summary microorganisms are ubiquitous in nature and constitute intrinsic parts of almost every ecosystem a culture independent and powerful way to study microbial communities is metagenomics in such studies functional analysis is performed on fragmented genetic material from multiple species in the community the recent advances in high throughput sequencing have greatly increased the amount of data in metagenomic projects at present there is an urgent need for efficient statistical tools to analyse these data we have created shotgunfunctionalizer an r package for functional comparison of metagenomes the package contains tools for importing annotating and visualizing metagenomic data produced by shotgun high throughput sequencing shotgunfunctionalizer contains several statistical procedures for assessing functional differences between samples both for individual genes and for entire pathways in addition to standard and previously published methods we have developed and implemented a novel approach based on a poisson model this procedure is highly flexible and thus applicable to a wide range of different experimental designs we demonstrate the potential of shotgunfunctionalizer by performing a regression analysis on metagenomes sampled at multiple depths in the pacific ocean availability http shotgun zool gu secontact dalevi chalmers se erik kristiansson zool gu sesupplementary information supplementary data are available at online
histone acetyltransferases hats and deacetylases hdacs function antagonistically to control histone acetylation as acetylation is a histone mark for active transcription hats have been associated with active and hdacs with inactive genes we describe here genome wide mapping of hats and hdacs binding on chromatin and find that both are found at active genes with acetylated histones our data provide evidence that hats and hdacs are both targeted to transcribed regions of active genes by phosphorylated rna pol ii furthermore the majority of hdacs in the human genome function to reset chromatin by removing acetylation at active genes inactive genes that are primed by mll mediated histone methylation are subject to a dynamic cycle of acetylation and deacetylation by transient hat hdac binding preventing pol ii from binding to these genes but poising them for future activation silent genes without any methylation signal show no evidence of being bound hdacs
chronic media multitasking is quickly becoming ubiquitous although processing multiple incoming streams of information is considered a challenge for human cognition a series of experiments addressed whether there are systematic differences in information processing styles between chronically heavy and light media multitaskers a trait media multitasking index was developed to identify groups of heavy and light media multitaskers these two groups were then compared along established cognitive control dimensions results showed that heavy media multitaskers are more susceptible to interference from irrelevant environmental stimuli and from irrelevant representations in memory this led to the surprising result that heavy media multitaskers performed worse on a test of task switching ability likely due to reduced ability to filter out interference from the irrelevant task set these results demonstrate that media multitasking a rapidly growing societal trend is associated with a distinct approach to fundamental processing
since the publication of robert k merton s theory of cumulative advantage in science matthew effect several empirical studies have tried to measure its presence at the level of papers individual researchers institutions or countries however these studies seldom control for the intrinsic ldquoqualityrdquo of papers or of researchers ldquobetterrdquo however defined papers or researchers could receive higher citation rates because they are indeed of better quality using an original method for controlling the intrinsic value of papers identical duplicate papers published in different journals with different impact factors this paper shows that the journal in which papers are published have a strong influence on their citation rates as duplicate papers published in high impact journals obtain on average twice as many citations as their identical counterparts published in journals with lower impact factors the intrinsic value of a paper is thus not the only reason a given paper gets cited or not there is a specific matthew effect attached to journals and this gives to papers published there an added value over and above their quality
pnas despite decades of debate it remains unclear whether human bipedalism evolved from a terrestrial knuckle walking ancestor or from a more generalized arboreal ape ancestor proponents of the knuckle walking hypothesis focused on the wrist and hand to find morphological evidence of this behavior in the human fossil record these studies however have not examined variation or development of purported knuckle walking features in apes or other primates data that are critical to resolution of this long standing debate here we present novel data on the frequency and development of putative knuckle walking features of the wrist in apes and monkeys we use these data to test the hypothesis that all knuckle walking apes share similar anatomical features and that these features can be used to reliably infer locomotor behavior in our extinct ancestors contrary to previous expectations features long assumed to indicate knuckle walking behavior are not found in all african apes show different developmental patterns across species and are found in nonknuckle walking primates as well however variation among african ape wrist morphology can be clearly explained if we accept the likely independent evolution of fundamentally different biomechanical modes of knuckle walking an extended wrist posture in an arboreal environment versus a neutral columnar hand posture in a terrestrial environment the presence of purported knuckle walking features in the hominin wrist can thus be viewed as evidence of arboreality not terrestriality and provide evidence that human bipedalism evolved from a more arboreal ancestor occupying the ecological niche common to all apes
objectives the research sought to determine the value of pubmed filters and combinations of filters in literature selected for systematic reviews on therapy related clinical questions methods references to included and excluded articles were extracted from reviews published prior to january in the cochrane database of systematic reviews and sent to pubmed with and without filters sensitivity specificity and precision were calculated from the percentages of unfiltered and filtered references retrieved for each review and averaged over all reviews results sensitivity of the sensitive clinical queries filter was reasonable specificity and precision were low the specific clinical queries and the single term medline specific filters performed comparably sensitivity vs specificity vs precision vs combining the abridged index medicus aim and single term medline specific two terms medline optimized or specific clinical queries filters yielded the highest precision conclusions sensitive and specific clinical queries filters used to answer questions about therapy will result in a list of clinical trials but cannot be expected to identify only methodologically sound trials the specific clinical queries filters are not suitable for questions regarding therapy that cannot be answered with randomized controlled trials combining aim with specific pubmed filters yields the highest precision in the dataset
biclustering extends the traditional clustering techniques by attempting to find all subgroups of genes with similar expression patterns under to be identified subsets of experimental conditions when applied to gene expression data still the real power of this clustering strategy is yet to be fully realized due to the lack of effective and efficient algorithms for reliably solving the general biclustering problem we report a qualitative biclustering algorithm qubic that can solve the biclustering problem in a more general form compared to existing algorithms through employing a combination of qualitative or semi quantitative measures of gene expression data and a combinatorial optimization technique one key unique feature of the qubic algorithm is that it can identify all statistically significant biclusters including biclusters with the so called scaling patterns a problem considered to be rather challenging another key unique feature is that the algorithm solves such general biclustering problems very efficiently capable of solving biclustering problems with tens of thousands of genes under up to thousands of conditions in a few minutes of the cpu time on a desktop computer we have demonstrated a considerably improved biclustering performance by our algorithm compared to the existing algorithms on various benchmark sets and data sets of our own qubic was written in ansi c and tested using gcc version on linux its source code is available at http csbl bmb uga edu approximately maqin bicluster a server version of qubic is also available request
motivation with the availability of many omics data such as transcriptomics proteomics or metabolomics the integrative or joint analysis of multiple datasets from different technology platforms is becoming crucial to unravel the relationships between different biological functional levels however the development of such an analysis is a major computational and technical challenge as most approaches suffer from high data dimensionality new methodologies need to be developed and validated results integromics efficiently performs integrative analyses of two types of omics variables that are measured on the same samples it includes a regularized version of canonical correlation analysis to enlighten correlations between two datasets and a sparse version of partial least squares pls regression that includes simultaneous variable selection in both datasets the usefulness of both approaches has been demonstrated previously and successfully applied in various integrative studies availability integromics is freely available from http cran r project org or from the web site companion http math univ toulouse fr biostat that provides full documentation and tutorials contact k lecao uq edu au supplementary information supplementary data are available at online
next generation sequencing has opened the door to genomic analysis of nonmodel organisms technologies generating long sequence reads bp are increasingly used in evolutionary studies of nonmodel organisms but the short sequence reads bp that can be produced at lower cost are thought to be of limited utility for de novo sequencing applications here we tested this assumption by short read sequencing the transcriptomes of the tropical disease vectors aedes aegypti and anopheles gambiae for which complete genome sequences are available comparison of our results to the reference genomes allowed us to accurately evaluate the quantity quality and functional and evolutionary information content of our test data we produced more than billion nucleotides of sequenced data per species that assembled into more than test contigs larger than bp per species and covered of the aedes reference transcriptome remarkably the substitution error rate in the test contigs was per site with very few indels or assembly errors test contigs of both species were enriched for genes involved in energy production and protein synthesis and underrepresented in genes involved in transcription and differentiation ortholog prediction using the test contigs was accurate across hundreds of millions of years of evolution our results demonstrate the considerable utility of short read transcriptome sequencing for genomic studies of nonmodel organisms and suggest an approach for assessing the information content of next generation data for studies
in cancer cells genetic alterations can activate proto oncogenes thereby contributing to tumorigenesis however the protein products of oncogenes are sometimes overexpressed without alteration of the proto oncogene helping to explain this phenomenon we found that when compared to similarly proliferating nontransformed cell lines cancer cell lines often expressed substantial amounts of mrna isoforms with shorter untranslated regions utrs these shorter isoforms usually resulted from alternative cleavage and polyadenylation apa the apa had functional consequences with the shorter mrna isoforms exhibiting increased stability and typically producing ten fold more protein in part through the loss of microrna mediated repression moreover expression of the shorter mrna isoform of the proto oncogene imp led to far more oncogenic transformation than did expression of the full length annotated mrna the high incidence of apa in cancer cells with consequent loss of repressive elements suggests a pervasive role for apa in oncogene activation without alteration
theoretical models suggest that social networks influence the evolution ofcooperation but to date there have been few experimental studies observational data suggest that a wide variety of behaviors may spread in humansocial networks but subjects in such studies can choose to befriend peoplewith similar behaviors posing difficulty for causal inference here weexploit a seminal set of laboratory experiments that originally showed thatvoluntary costly punishment can help sustain cooperation in these experiments subjects were randomly assigned to a sequence of different groups in order toplay a series of single shot public goods games with strangers this featureallowed us to draw networks of interactions to explore how cooperative anduncooperative behavior spreads from person to person to person we show that in both an ordinary public goods game and in a public goods game withpunishment focal individuals are influenced by fellow group members contribution behavior in future interactions with other individuals who werenot a party to the initial interaction furthermore this influence persistsfor multiple periods and spreads up to three degrees of separation from personto person to person to person the results suggest that each additionalcontribution a subject makes to the public good in the first period is tripledover the course of the experiment by other subjects who are directly orindirectly influenced to contribute more as a consequence these are the firstresults to show experimentally that cooperative behavior cascades in networks
with the sheer growth of online user data it becomes challenging to develop preference learning algorithms that are sufficiently flexible in modeling but also affordable in computation in this paper we develop nonparametric matrix factorization methods by allowing the latent factors of two low rank matrix factorization methods the singular value decomposition svd and probabilistic principal component analysis ppca to be data driven with the dimensionality increasing with data size we show that the formulations of the two nonparametric models are very similar and their optimizations share similar procedures compared to traditional parametric low rank methods nonparametric models are appealing for their flexibility in modeling complex data dependencies however this modeling advantage comes at a computational price it is highly challenging to scale them to large scale problems hampering their application to applications such as collaborative filtering in this paper we introduce novel optimization algorithms which are simple to implement which allow learning both nonparametric matrix factorization models to be highly efficient on large scale problems our experiments on eachmovie and netflix the two largest public benchmarks to date demonstrate that the nonparametric models make more accurate predictions of user ratings and are computationally comparable or sometimes even faster in training in comparison with previous state of the art parametric matrix models
with the development of high throughput experimental techniques such as microarray mass spectrometry and large scale mutagenesis there is an increasing need to automatically annotate gene sets and identify the involved pathways although many pathway analysis tools are developed new tools are still needed to meet the requirements for flexible or advanced analysis purpose here we developed an r based software package subpathwayminer for flexible pathway identification subpathwayminer facilitates sub pathway identification of metabolic pathways by using pathway structure information additionally subpathwayminer also provides more flexibility in annotating gene sets and identifying the involved pathways entire pathways and sub pathways i subpathwayminer is able to provide the most up to date pathway analysis results for users ii subpathwayminer supports multiple species eukaryotes bacteria and archaea and different gene identifiers entrez gene ids ncbi gi ids uniprot ids pdb ids etc in the kegg gene database iii the system is quite efficient in cooperating with other r based tools in biology subpathwayminer is freely available at http cran r project org web packages nar
abstract genome assembly has long been one of the major challenges in genome sequencing where the product is dependent on genome coverage data quality the properties of the sequenced genome as well as the efficiency and accuracy of the algorithms employed for assembly we describe a new assembly concept where a genome assembly with low sequence coverage either throughout the genome or locally due to cloning bias is considerably improved through assisted assembly in the assisted assembly process a related reference genome is used to validate and leverage the information captured in the low coverage reads we show that the information provided by aligning the wgs reads of the target against a reference genome can be effectively used to substantially improve the assembly of the target both by covering more of the genome to and long range connectivity in some cases more than and by improving the quality of the resulting assembly we then show the validity of the assisting process by testing the algorithm on a low coverage assembly for which a high quality draft exists that of canis familiaris in this paper we describe the algorithms the methodological validation using the dog genome as well as some real applications such as the fold coverage assemblies of the low coverage mammals used to inform the human genome and the fold coverage assembly of falciparum
summary neural circuits display complex activity patterns both spontaneously and when responding to a stimulus or generating a motor output how are these two forms of activity related we develop a procedure called force learning for modifying synaptic strengths either external to or within a model neural network to change chaotic spontaneous activity into a wide variety of desired activity patterns force learning works even though the networks we train are spontaneously chaotic and we leave feedback loops intact and unclamped during learning using this approach we construct networks that produce a wide variety of complex output patterns input output transformations that require memory multiple outputs that can be switched by control inputs and motor patterns matching human motion capture data our results reproduce data on premovement activity in motor and premotor cortex and suggest that synaptic plasticity may be a more rapid and powerful modulator of network activity than appreciated
resolving individual atoms has always been the ultimate goal of surface microscopy the scanning tunneling microscope images atomic scale features on surfaces but resolving single atoms within an adsorbed molecule remains a great challenge because the tunneling current is primarily sensitive to the local electron density of states close to the fermi level we demonstrate imaging of molecules with unprecedented atomic resolution by probing the short range chemical forces with use of noncontact atomic force microscopy the key step is functionalizing the microscope s tip apex with suitable atomically well defined terminations such as co molecules our experimental findings are corroborated by ab initio density functional theory calculations comparison with theory shows that pauli repulsion is the source of the atomic resolution whereas van der waals and electrostatic forces only add a diffuse background
a single base pair resolution silkworm genetic variation map was constructed from domesticated and wild silkworms each sequenced to approximately threefold coverage representing of the genome we identified million single nucleotide polymorphisms many indels and structural variations we find that the domesticated silkworms are clearly genetically differentiated from the wild ones but they have maintained large levels of genetic variability suggesting a short domestication event involving a large number of individuals we also identified signals of selection at candidate genes that may have been important during domestication some of which have enriched expression in the silk gland midgut and testis these data add to our understanding of the domestication processes and may have applications in devising pest control strategies and advancing the use of silkworms as efficient science
lactase persistence lp is common among people of european ancestry but with the exception of some african middle eastern and southern asian groups is rare or absent elsewhere in the world lactase gene haplotype conservation around a polymorphism strongly associated with lp in europeans c t indicates that the derived allele is recent in origin and has been subject to strong positive selection furthermore ancient dna work has shown that the t derived allele was very rare or absent in early neolithic central europeans it is unlikely that lp would provide a selective advantage without a supply of fresh milk and this has lead to a gene culture coevolutionary model where lactase persistence is only favoured in cultures practicing dairying and dairying is more favoured in lactase persistent populations we have developed a flexible demic computer simulation model to explore the spread of lactase persistence dairying other subsistence practices and unlinked genetic markers in europe and western asia s geographic space using data on t allele frequency and farming arrival dates across europe and approximate bayesian computation to estimate parameters of interest we infer that the t allele first underwent selection among dairying farmers around years ago in a region between the central balkans and central europe possibly in association with the dissemination of the neolithic linearbandkeramik culture over central europe furthermore our results suggest that natural selection favouring a lactase persistence allele was not higher in northern latitudes through an increased requirement for dietary vitamin d our results provide a coherent and spatially explicit picture of the coevolution of lactase persistence and dairying europe
various online social networks osns have been developed rapidly on the internet researchers have analyzed different properties of such osns mainly focusing on the formation and evolution of the networks as well as the information propagation over the networks in knowledge sharing osns such as blogs and question answering systems issues on how users participate in the network and how users generate contribute knowledge are vital to the sustained and healthy growth of the networks however related discussions have not been reported in the literature
in large social networks nodes users entities are influenced by others for various reasons for example the colleagues have strong influence on one s work while the friends have strong influence on one s daily life how to differentiate the social influences from different angles topics how to quantify the strength of those social influences how to estimate the model on real large networks to address these fundamental questions we propose topical affinity propagation tap to model the topic level social influence on large networks in particular tap can take results of any topic modeling and the existing network structure to perform topic level influence propagation with the help of the influence analysis we present several important applications on real data sets such as what are the representative nodes on a given topic how to identify the social influences of neighboring nodes on a particular node to scale to real large networks tap is designed with efficient distributed learning algorithms that is implemented and tested under the map reduce framework we further present the common characteristics of distributed learning algorithms for map reduce finally we demonstrate the effectiveness and efficiency of tap on real large sets
summary understanding the key process of human mutation is important for many aspects of medical genetics and human evolution in the past estimates of mutation rates have generally been inferred from phenotypic observations or comparisons of homologous sequences among closely related species and here we apply new sequencing technology to measure directly one mutation rate that of base substitutions on the human y chromosome the y chromosomes of two individuals separated by generations were flow sorted and sequenced by illumina solexa paired end sequencing to an average depth of or respectively candidate mutations were further examined by capillary sequencing in cell line and blood dna from the donors and additional family members twelve mutations were confirmed in mb eight of these had occurred in vitro and four in vivo the latter could be placed in different positions on the pedigree and led to a mutation rate measurement of mutations nucleotide generation ci consistent with estimates of mutations nucleotide generation for the same y chromosomal region from published human chimpanzee comparisons depending on the generation and split assumed
disruptions in local chromatin structure often indicate features of biological interest such as regulatory regions we find that sonication of cross linked chromatin when combined with a size selection step and massively parallel short read sequencing can be used as a method sono seq to map locations of high chromatin accessibility in promoter regions sono seq sites frequently correspond to actively transcribed promoter regions as evidenced by their co association with rna polymerase ii chip regions transcription start sites histone lysine trimethylation marks and cpg islands signals over other sites such as those bound by the ctcf insulator are also observed the pattern of breakage by sono seq overlaps with but is distinct from that observed for faire and dnase i hypersensitive sites our results demonstrate that sono seq can be a useful and simple method by which to map many local alterations in chromatin structure furthermore our results provide insights into the mapping of binding sites by using chip seq experiments and the value of reference samples that should be used in experiments
motivation deregulated signaling cascades are known to play a crucial role in many pathogenic processes among them are tumor initiation and progression in the recent past modern experimental techniques that allow for measuring the amount of mrna transcripts of almost all known human genes in a tissue or even in a single cell have opened new avenues for studying the activity of the signaling cascades and for understanding the information flow in the networks results we present a novel dynamic programming algorithm for detecting deregulated signaling cascades the so called fidepa finding deregulated paths algorithm interprets differences in the expression profiles of tumor and normal tissues it relies on the well known gene set enrichment analysis gsea and efficiently detects all paths in a given regulatory or signaling network that are significantly enriched with differentially expressed genes or proteins since our algorithm allows for comparing a single tumor expression profile with the control group it facilitates the detection of specific regulatory features of a tumor that may help to optimize tumor therapy to demonstrate the capabilities of our algorithm we analyzed a glioma expression dataset with respect to a directed graph that combined the regulatory networks of the kegg and transpath database the resulting glioma consensus network that encompasses all detected deregulated paths contained many genes and pathways that are known to be key players in glioma or cancer related pathogenic processes moreover we were able to correlate clinically relevant features like necrosis or metastasis with the detected paths availability c source code is freely available bina can be downloaded from http www bnplusplus org contact ack bioinf uni sb de supplementary information supplementary data are available at online
abstract background micrornas mirnas have critical functions in various biological processes mirna profiling is an important tool for the identification of differentially expressed mirnas in normal cellular and disease processes a technical challenge remains for high throughput mirna expression analysis as the number of mirnas continues to increase with in silico prediction and experimental verification our study critically evaluated the performance of a novel mirna expression profiling approach quantitative rt pcr array qpcr array compared to mirna detection with oligonucleotide microchip microarray results high reproducibility with qpcr array was demonstrated by comparing replicate results from the same rna sample pre amplification of the mirna cdna improved sensitivity of the qpcr array and increased the number of detectable mirnas furthermore the relative expression levels of mirnas were maintained after pre amplification when the performance of qpcr array and microarrays were compared using different aliquots of the same rna a low correlation between the two methods r indicated considerable variability between the two assay platforms higher variation between replicates was observed in mirnas with low expression in both assays finally a higher false positive rate of differential mirna expression was observed using the microarray compared to the qpcr array conclusions our studies demonstrated high reproducibility of taqman qpcr array comparison between different reverse transcription reactions and qpcr arrays performed on different days indicated that reverse transcription reactions did not introduce significant variation in the results the use of cdna pre amplification increased the sensitivity of mirna detection although there was variability associated with pre amplification in low abundance mirnas the latter did not involve any systemic bias in the estimation of mirna expression comparison between microarray and qpcr array indicated superior sensitivity and specificity of array
summary many signaling systems show adaptationthe ability to reset themselves after responding toa stimulus we computationally searched all possible three node enzyme network topologies to identify those that could perform adaptation only two major core topologies emerge as robust solutions a negative feedback loop with a buffering node and an incoherent feedforward loop with a proportioner node minimal circuits containing these topologies are within proper regions of parameter space sufficient to achieve adaptation more complex circuits that robustly perform adaptation all contain at least one of these topologies at their core this analysis yields a design table highlighting a finite set of adaptive circuits despite the diversity of possible biochemical networks it may be common to find that only a finite set of core topologies can execute a particular function these design rules provide a framework for functionally classifying complex natural networks and a manual for engineering networks for a video summary of this article see the paperflickfile with the supplemental data online
we examine how the biomedical informatics bmi community especially consortia that share data and applications can take advantage of a new resource called cloud computing clouds generally offer resources on demand in most clouds charges are pay per use based on large farms of inexpensive dedicated servers sometimes supporting parallel computing substantial economies of scale potentially yield costs much lower than dedicated laboratory systems or even institutional data centers overall even with conservative assumptions for applications that are not i o intensive and do not demand a fully mature environment the numbers suggested that clouds can sometimes provide major improvements and should be seriously considered for bmi methodologically it was very advantageous to formulate analyses in terms of component technologies focusing on these specifics enabled us to bypass the cacophony of alternative definitions e g exactly what does a cloud include and to analyze alternatives that employ some of the component technologies e g an institution s data center relative analyses were another great simplifier rather than listing the absolute strengths and weaknesses of cloud based systems e g for security or data preservation we focus on the changes from a particular starting point e g individual lab systems we often find a rough parity in principle but one needs to examine individual acquisitions is a loosely managed lab moving to a well managed cloud or a tightly managed hospital data center moving to a poorly cloud
balancing selection is potentially an important biological force for maintaining advantageous genetic diversity in populations including variation that is responsible for long term adaptation to the environment by serving as a means to maintain genetic variation it may be particularly relevant to maintaining phenotypic variation in natural populations nevertheless its prevalence and specific targets in the human genome remain largely unknown we have analyzed the patterns of diversity and divergence of genes in two human populations using an unbiased single nucleotide polymorphism data set a genome wide approach and a method that incorporates demography in neutrality tests we identified an unbiased catalog of genes with signatures of long term balancing selection which includes immunity genes as well as genes encoding keratins and membrane channels the catalog also shows enrichment in functional categories involved in cellular structure patterns are mostly concordant in the two populations with a small fraction of genes showing population specific signatures of selection power considerations indicate that our findings represent a subset of all targets in the genome suggesting that although balancing selection may not have an obvious impact on a large proportion of human genes it is a key force affecting the evolution of a number of genes in molbev
despite their importance in gene innovation and phenotypic variation duplicated regions have remained largely intractable owing to difficulties in accurately resolving their structure copy number and sequence content we present an algorithm mrfast to comprehensively map next generation sequence reads which allows for the prediction of absolute copy number variation of duplicated segments and genes we examine three human genomes and experimentally validate genome wide copy number differences we estimate that on average genes vary in copy number between any two individuals and find that these genic differences overwhelmingly correspond to segmental duplications odds ratio p x our method can distinguish between different copies of highly identical genes providing a more accurate assessment of gene content and insight into functional constraint without the limitations of array technology
eukaryotic genomes contain large amount of repetitive dna most of which is derived from transposable elements tes progress has been made to develop computational tools for ab initio identification of repeat families but there is an urgent need to develop tools to automate the annotation of tes in genome sequences here we introduce repclass a tool that automates the classification of te sequences using control repeat libraries we show that the program can classify accurately virtually any known te types combining repclass to ab initio repeat finding in the genomes of caenorhabditis elegans and drosophila melanogaster allowed us to recover the contrasting te landscape characteristic of these species unexpectedly repclass also uncovered several novel te families in both genomes augmenting the te repertoire of these model species when applied to the genomes of distant caenorhabditis and drosophila species the approach revealed a remarkable conservation of te composition profile within each genus despite substantial interspecific covariations in genome size and in the number of tes and te families lastly we applied repclass to analyze fungal genomes from a wide taxonomic range most of which have not been analyzed for te content previously the results showed that te diversity varies widely across the fungi kingdom and appears to positively correlate with genome size in particular for dna transposons together these data validate repclass as a powerful tool to explore the repetitive dna landscapes of eukaryotes and to shed light onto the evolutionary forces shaping te diversity and architecture
in allosteric regulation an effector molecule binding a protein at one site induces conformational changes which alter structure and function at a distant active site two key challenges in the computational modeling of allostery are the prediction of the structure of one allosteric state starting from the structure of the other and elucidating the mechanisms underlying the conformational coupling of the effector and active sites here we approach these two challenges using the rosetta high resolution structure prediction methodology we find that the method can recapitulate the relaxation of effector bound forms of single domain allosteric proteins into the corresponding ligand free states particularly when sampling is focused on regions known to change conformation most significantly analysis of the coupling between contacting pairs of residues in large ensembles of conformations spread throughout the landscape between and around the two allosteric states suggests that the transitions are built up from blocks of tightly coupled interacting sets of residues that are more loosely coupled to another
proteins frequently accomplish their biological function by collective atomic motions yet the identification of collective motions related to a specific protein function from e g a molecular dynamics trajectory is often non trivial here we propose a novel technique termed functional mode analysis that aims to detect the collective motion that is directly related to a particular protein function based on an ensemble of structures together with an arbitrary functional quantity that quantifies the functional state of the protein the technique detects the collective motion that is maximally correlated to the functional quantity the functional quantity could e g correspond to a geometric electrostatic or chemical observable or any other variable that is relevant to the function of the protein in addition the motion that displays the largest likelihood to induce a substantial change in the functional quantity is estimated from the given protein ensemble two different correlation measures are applied first the pearson correlation coefficient that measures linear correlation only and second the mutual information that can assess any kind of interdependence detecting the maximally correlated motion allows one to derive a model for the functional state in terms of a single collective coordinate the new approach is illustrated using a number of biomolecules including a polyalanine helix lysozyme trp cage and leucine protein
sparse coding that is modelling data vectors as sparse linear combinations of basis elements is widely used in machine learning neuroscience signal processing and statistics this paper focuses on the large scale matrix factorization problem that consists of learning the basis set adapting it to specific data variations of this problem include dictionary learning in signal processing non negative matrix factorization and sparse principal component analysis in this paper we propose to address these tasks with a new online optimization algorithm based on stochastic approximations which scales up gracefully to large datasets with millions of training samples and extends naturally to various matrix factorization formulations making it suitable for a wide range of learning problems a proof of convergence is presented along with experiments with natural images and genomic data demonstrating that it leads to state of the art performance in terms of speed and optimization for both small and datasets
hundreds of different factors adorn the eukaryotic genome binding to it in large number these dna binding factors dbfs include nucleosomes transcription factors tfs and other proteins and protein complexes such as the origin recognition complex orc dbfs compete with one another for binding along the genome yet many current models of genome binding do not consider different types of dbfs together simultaneously additionally binding is a stochastic process that results in a continuum of binding probabilities at any position along the genome but many current models tend to consider positions as being either binding sites or not here we present a model that allows a multitude of dbfs each at different concentrations to compete with one another for binding sites along the genome the result is an occupancy profile a probabilistic description of the dna occupancy of each factor at each position we implement our model efficiently as the software package compete we demonstrate genome wide and at specific loci how modeling nucleosome binding alters tf binding and vice versa and illustrate how factor concentration influences binding occupancy binding cooperativity between nearby tfs arises implicitly via mutual competition with nucleosomes our method applies not only to tfs but also recapitulates known occupancy profiles of a well studied replication origin with and without orc binding importantly the sequence preferences our model takes as input are derived from in vitro experiments this ensures that the calculated occupancy profiles are the result of the forces of competition represented explicitly in our model and the inherent sequence affinities of the dbfs
chromatin modifications have been implicated in the regulation of gene expression while association of certain modifications with expressed or silent genes has been established it remains unclear how changes in chromatin environment relate to changes in gene expression in this article we used chip seq chromatin immunoprecipitation with massively parallel sequencing to analyze the genome wide changes in chromatin modifications during activation of total human t cells by t cell receptor tcr signaling surprisingly we found that the chromatin modification patterns at many induced and silenced genes are relatively stable during the short term activation of resting t cells active chromatin modifications were already in place for a majority of inducible protein coding genes even while the genes were silent in resting cells similarly genes that were silenced upon t cell activation retained positive chromatin modifications even after being silenced to investigate if these observations are also valid for mirna coding genes we systematically identified promoters for known mirna genes using epigenetic marks and profiled their expression patterns using deep sequencing we found that chromatin modifications can poise mirna coding genes as well our data suggest that mirna and protein coding genes share similar mechanisms of regulation by chromatin modifications which poise inducible genes for activation in response to stimuli
pnas directed networks are ubiquitous and are necessary to represent complex systems with asymmetric interactionsfrom food webs to the world wide web despite the importance of edge direction for detecting local and community structure it has been disregarded in studying a basic type of global diversity in networks the tendency of nodes with similar numbers of edges to connect this tendency called assortativity affects crucial structural and dynamic properties of real world networks such as error tolerance or epidemic spreading here we demonstrate that edge direction has profound effects on assortativity we define a set of four directed assortativity measures and assign statistical significance by comparison to randomized networks we apply these measures to three network classesonline social networks food webs and word adjacency networks our measures i reveal patterns common to each class ii separate networks that have been previously classified together and iii expose limitations of several existing theoretical models we reject the standard classification of directed networks as purely assortative or disassortative many display a class specific mixture likely reflecting functional or historical constraints contingencies and forces guiding the evolution
this paper aims to add to innovation management theory and practice by exploring the interrelationship between innovation idea quality and idea providers network connectivity using social network analysis the study uses a database from a company that has worked systematically with idea management over a long period of time and today has a well established information technology system that collects ideas from a large number of employees in addition to the idea database a number of interviews with key individuals within innovation were conducted to create rich contextual knowledge and understand more in detail how ideas are handled in the company the analysis indicated that there is a clear interrelationship between the network connectivity and the quality of the innovation ideas created the analysis was done for all the innovation ideas and then for ideas created by single individuals and by groups respectively in all three analyses the proportion of high quality innovation ideas increased as a step function between the least connected group and the group thereafter there is apparently a need for a certain amount of relations to increase the proportion of high quality innovation ideas generated regarding only ideas provided by single individuals more connections within the network resulted in a higher proportion of high quality ideas a different pattern was seen for ideas provided by groups as the proportion of high quality innovation ideas grew with some increase in the connectivity of groups but declined with a further increase in connectivity the findings suggest a number of implications for ideation management to increase the number of high quality innovation ideas created by individuals the possibility to interact with other people should be supported and facilitated however in these settings where individuals work with others in different groups the most connected groups perform worst in terms of the proportion of high quality ideas generated which points to the necessity to consider a multitude of factors when ideation
compounds with high intramolecular energy barriers represent challenging targets for free energy calculations because of the difficulty to obtain sufficient conformational sampling existing approaches are therefore computationally very demanding thus preventing practical applications for such compounds we present an enhanced sampling one step perturbation method es os to tackle this problem in a highly efficient way a single molecular dynamics simulation of a judiciously chosen reference state using two sets of soft core interactions is sufficient to determine conformational distributions of chemically similar compounds and the free energy differences between them the es os method is applied to a set of five biologically relevant substituted gtp analogs having high energy barriers between the anti and the syn conformations of the base with respect to the ribose part the reliability of es os is verified by comparing the results to hamiltonian replica exchange simulations of gtp and br gtp and the experimentally determined coupling constant for gmp in water additional simulations in vacuum and octanol allow us to calculate differences in the solvation free energies and in lipophilicities log p free energy contributions from individual conformational regions are also calculated and their relationship with the overall free energy is derived leading to a set of multiconformational free energy formulas these relationships are of general applicability and can be used in free energy calculations for a more diverse set compounds
background recent studies have shown that the regulatory effect of micrornas can be investigated by examining expression changes of their target genes given this it is useful to define an overall metric of regulatory effect for a specific microrna and see how this changes across different conditions results here we define a regulatory effect score re score to measure the inhibitory effect of a microrna in a sample essentially the average difference in expression of its targets versus non targets then we compare the re scores of various micrornas between two breast cancer subtypes estrogen receptor positive er and negative er we applied this approach to five microarray breast cancer datasets and found that the expression of target genes of most micrornas was more repressed in er than er that is micrornas appear to have higher re scores in er breast cancer these results are robust to the microrna target prediction method to interpret these findings we analyzed the level of microrna expression in previous studies and found that higher microrna expression was not always accompanied by higher inhibitory effects however several key microrna processing genes especially and dicer were differentially expressed between er and er breast cancer which may explain the different regulatory effects of micrornas in these two breast cancer subtypes conclusions the re score is a promising indicator to measure micrornas inhibitory effects most micrornas exhibit higher re scores in er than in er samples suggesting that they have stronger inhibitory effects in er cancers
the rapid emergence and exploding usage of online social networking forums which are frequented by millions present clinicians with new ethical and professional challenges particularly among a younger generation of physicians and patients the use of online social networking forums has become widespread in this article we discuss ethical challenges facing the patientdoctor relationship as a result of the growing use of online social networking forums we draw upon one heavily used and highly trafficked forum facebook to illustrate the elements of these online environments and the ethical challenges peculiar to their novel form of exchange finally we present guidelines for clinicians to negotiate responsibly and professionally their possible uses of these forums
background the analysis of high throughput gene expression data with respect to sets of genes rather than individual genes has many advantages a variety of methods have been developed for assessing the enrichment of sets of genes with respect to differential expression in this paper we provide a comparative study of four of these methods fisher s exact test gene set enrichment analysis gsea random sets rs and gene list analysis with prediction accuracy glapa the first three methods use associative statistics while the fourth uses predictive statistics we first compare all four methods on simulated data sets to verify that fisher s exact test is markedly worse than the other three approaches we then validate the other three methods on seven real data sets with known genetic perturbations and then compare the methods on two cancer data sets where our a priori knowledge is limited results the simulation study highlights that none of the three method outperforms all others consistently gsea and rs are able to detect weak signals of deregulation and they perform differently when genes in a gene set are both differentially up and down regulated glapa is more conservative and large differences between the two phenotypes are required to allow the method to detect differential deregulation in gene sets this is due to the fact that the enrichment statistic in glapa is prediction error which is a stronger criteria than classical two sample statistic as used in rs and gsea this was reflected in the analysis on real data sets as gsea and rs were seen to be significant for particular gene sets while glapa was not suggesting a small effect size we find that the rank of gene set enrichment induced by glapa is more similar to rs than gsea more importantly the rankings of the three methods share significant overlap conclusion the three methods considered in our study recover relevant gene sets known to be deregulated in the experimental conditions and pathologies analyzed there are differences between the three methods and gsea seems to be more consistent in finding enriched gene sets although no method uniformly dominates over all data sets our analysis highlights the deep difference existing between associative and predictive methods for detecting enrichment and the use of both to better interpret results of pathway analysis we close with suggestions for users of gene methods
context as of the international committee of medical journal editors required investigators to register their trials prior to participant enrollment as a precondition for publishing the trial s findings in member journals objective to assess the proportion of registered trials with results recently published in journals with high impact factors to compare the primary outcomes specified in trial registries with those reported in the published articles and to determine whether primary outcome reporting bias favored significant outcomes data sources and study selection medline via pubmed was searched for reports of randomized controlled trials rcts in medical areas cardiology rheumatology and gastroenterology indexed in in the general medical journals and specialty journals with the highest impact factors data extraction for each included article we obtained the trial registration information using a standardized data extraction form results of the included trials were adequately registered ie registered before the end of the trial with the primary outcome clearly specified trial registration was lacking for published reports trials were registered after the completion of the study were registered with no or an unclear description of the primary outcome were registered with no or an unclear description of the primary outcome and were registered after the completion of the study and had an unclear description of the primary outcome among articles with trials adequately registered of showed some evidence of discrepancies between the outcomes registered and the outcomes published the influence of these discrepancies could be assessed in only half of them and in these statistically significant results were favored in of conclusion comparison of the primary outcomes of rcts registered with their subsequent publication indicated that selective outcome reporting prevalent
complex dynamical systems ranging from ecosystems to financial markets and the climate can have tipping points at which a sudden shift to a contrasting dynamical regime may occur although predicting such critical points before they are reached is extremely difficult work in different scientific fields is now suggesting the existence of generic early warning signals that may indicate for a wide class of systems if a critical threshold approaching
the origin of new genes is extremely important to evolutionary innovation most new genes arise from existing genes through duplication or recombination the origin of new genes from noncoding dna is extremely rare and very few eukaryotic examples are known we present evidence for the de novo origin of at least three human protein coding genes since the divergence with chimp each of these genes has no protein coding homologs in any other genome but is supported by evidence from expression and importantly proteomics data the absence of these genes in chimp and macaque cannot be explained by sequencing gaps or annotation error high quality sequence data indicate that these loci are noncoding dna in other primates furthermore chimp gorilla gibbon and macaque share the same disabling sequence difference supporting the inference that the ancestral sequence was noncoding over the alternative possibility of parallel gene inactivation in multiple primate lineages the genes are not well characterized but interestingly one of them was first identified as an up regulated gene in chronic lymphocytic leukemia this is the first evidence for entirely novel human specific protein coding genes originating from ancestrally noncoding sequences we estimate that of human genes may have originated through this mechanism leading to a total expectation of such cases in a genome of protein genes
laser driven plasma based accelerators which are capable of supporting fields in excess of gv m are reviewed this includes the laser wakefield accelerator the plasma beat wave accelerator the self modulated laser wakefield accelerator plasma waves driven by multiple laser pulses and highly nonlinear regimes the properties of linear and nonlinear plasma waves are discussed as well as electron acceleration in plasma waves methods for injecting and trapping plasma electrons in plasma waves are also discussed limits to the electron energy gain are summarized including laser pulse diffraction electron dephasing laser pulse energy depletion and beam loading limitations the basic physics of laser pulse evolution in underdense plasmas is also reviewed this includes the propagation self focusing and guiding of laser pulses in uniform plasmas and with preformed density channels instabilities relevant to intense short pulse laser plasma interactions such as raman self modulation and hose instabilities are discussed experiments demonstrating key physics such as the production of high quality electron bunches at energies of gev summarized
copy number variation cnv is a source of genetic diversity in humans numerous cnvs are being identified with various genome analysis platforms including array comparative genomic hybridization acgh single nucleotide polymorphism snp genotyping platforms and next generation sequencing cnv formation occurs by both recombination based and replication based mechanisms and de novo locus specific mutation rates appear much higher for cnvs than for snps by various molecular mechanisms including gene dosage gene disruption gene fusion position effects etc cnvs can cause mendelian or sporadic traits or be associated with complex diseases however cnv can also represent benign polymorphic variants cnvs especially gene duplication and exon shuffling can be a predominant mechanism driving gene and evolution
background the elucidation of networks from a compendium of gene expression data is one of the goals of systems biology and can be a valuable source of new hypotheses for experimental researchers for arabidopsis there exist several thousand microarrays which form a valuable resource from which to learn results a novel bayesian network based algorithm to infer gene regulatory networks from gene expression data is introduced and applied to learn parts of the transcriptomic network in arabidopsis thaliana from a large number thousands of separate microarray experiments starting from an initial set of genes of interest a network is grown by iterative addition to the model of the gene from another defined set of genes which gives the best learned network structure the gene set for iterative growth can be as large as the entire genome a number of networks are inferred and analysed these show i an agreement with the current literature on the circadian clock network ii the ability to model other networks and iii that the learned network hypotheses can suggest new roles for poorly characterized genes through addition of relevant genes from an unconstrained list of over possible genes to demonstrate the latter point the method is used to suggest that particular gata transcription factors are regulators of photosynthetic genes additionally the performance in recovering a known network from different amounts of synthetically generated data is evaluated conclusion our results show that plausible regulatory networks can be learned from such gene expression data alone this work demonstrates that network hypotheses can be generated from existing gene expression data for use by biologists
to understand the process by which antibiotic resistance genes are acquired by human pathogens we functionally characterized the resistance reservoir in the microbial flora of healthy individuals most of the resistance genes we identified using culture independent sampling have not been previously identified and are evolutionarily distant from known resistance genes by contrast nearly half of the resistance genes we identified in cultured aerobic gut isolates a small subset of the gut microbiome are identical to resistance genes harbored by major pathogens the immense diversity of resistance genes in the human microbiome could contribute to future emergence of antibiotic resistance in human science
massively parallel pyrosequencing of the small subunit ribosomal rna gene has revealed that the extent of rare microbial populations in several environments the rare biosphere is orders of magnitude higher than previously thought one important caveat with this method is that sequencing error could artificially inflate diversity estimates although the per base error of rdna amplicon pyrosequencing has been shown to be as good as or lower than sanger sequencing no direct assessments of pyrosequencing errors on diversity estimates have been reported using only escherichia coli as a reference template we find that rdna diversity is grossly overestimated unless relatively stringent read quality filtering and low clustering thresholds are applied in particular the common practice of removing reads with unresolved bases and anomalous read lengths is insufficient to ensure accurate estimates of microbial diversity furthermore common and reproducible homopolymer length errors can result in relatively abundant spurious phylotypes further confounding data interpretation we suggest that stringent quality based trimming of pyrotags and clustering thresholds no greater than identity should be used to avoid overestimates of the biosphere
we integrate genomic features to construct an evidence weighted functional linkage network comprising human genes the functional linkage network is used to prioritize candidate genes for diseases and to reliably disclose hidden associations between disease pairs having dissimilar phenotypes such as hypercholesterolemia and alzheimer s disease many of these disease disease associations are supported by epidemiology but with no previous genetic basis such associations can drive novel hypotheses on molecular mechanisms of diseases therapies
broadly neutralizing antibodies bnabs which develop over time in some hiv infected individuals define critical epitopes for hiv vaccine design using a systematic approach we have examined neutralization breadth in the sera of about hiv infected individuals primarily infected with non clade b viruses and have selected donors for monoclonal antibody mab generation we then used a high throughput neutralization screen of antibody containing culture supernatants from about activated memory b cells from a clade a infected african donor to isolate two potent mabs that target a broadly neutralizing epitope this epitope is preferentially expressed on trimeric envelope protein and spans conserved regions of variable loops of the subunit the results provide a framework for the design of new vaccine candidates for the elicitation of responses
summary the increasing availability of large network datasets along with the progresses in experimental high throughput technologies have prompted the need for tools allowing easy integration of experimental data with data derived form network computational analysis in order to enrich experimental data with network topological parameters we have developed the cytoscape plug in centiscape the plug in computes several network centrality parameters and allows the user to analyze existing relationships between experimental data provided by the users and node centrality values computed by the plug in centiscape allows identifying network nodes that are relevant from both experimental and topological viewpoints centiscape also provides a boolean logic based tool that allows easy characterization of nodes whose topological relevance depends on more than one centrality finally different graphic outputs and the included description of biological significance for each computed centrality facilitate the analysis by the end users not expert in graph theory thus allowing easy node categorization and experimental prioritization availability centiscape can be downloaded via the cytoscape web site http chianti ucsd edu plugins index php tutorial centrality descriptions and example data are available at http profs sci univr it scardoni centiscape centiscapepage php contact giovanni scardoni gmail com supplementary information supplementary data are available at bioinformatics bioinformatics
models of mammalian regulatory networks controlling gene expression have been inferred from genomic data but have largely not been validated we present an unbiased strategy to systematically perturb candidate regulators and monitor cellular transcriptional responses we applied this approach to derive regulatory networks that control the transcriptional response of mouse primary dendritic cells to pathogens our approach revealed the regulatory functions of transcription factors chromatin modifiers and rna binding proteins which enabled the construction of a network model consisting of core regulators and fine tuners that help to explain how pathogen sensing pathways achieve specificity this study establishes a broadly applicable comprehensive and unbiased approach to reveal the wiring and functions of a regulatory network controlling a major transcriptional response in primary cells
the public goods game is the classic laboratory paradigm for studying collective action problems each participant chooses how much to contribute to a common pool that returns benefits to all participants equally the ideal outcome occurs if everybody contributes the maximum amount but the self interested strategy is not to contribute anything most previous studies have found punishment to be more effective than reward for maintaining cooperation in public goods games the typical design of these studies however represses future consequences for today s actions in an experimental setting we compare public goods games followed by punishment reward or both in the setting of truly repeated games in which player identities persist from round to round we show that reward is as effective as punishment for maintaining public cooperation and leads to higher total earnings moreover when both options are available reward leads to increased contributions and payoff whereas punishment has no effect on contributions and leads to lower payoff we conclude that reward outperforms punishment in repeated public goods games and that human cooperation in such repeated settings is best supported by positive interactions with science
background life sciences make heavily use of the web for both data provision and analysis however the increasing amount of available data and the diversity of analysis tools call for machine accessible interfaces in order to be effective http based web service technologies like the simple object access protocol soap and representational state transfer rest services are today the most common technologies for this in bioinformatics however these methods have severe drawbacks including lack of discoverability and the inability for services to send status notifications several complementary workarounds have been proposed but the results are ad hoc solutions of varying quality that can be difficult to use results we present a novel approach based on the open standard extensible messaging and presence protocol xmpp consisting of an extension io data to comprise discovery asynchronous invocation and definition of data types in the service that xmpp cloud services are capable of asynchronous communication implies that clients do not have to poll repetitively for status but the service sends the results back to the client upon completion implementations for bioclipse and taverna are presented as are various xmpp cloud services in bio and cheminformatics conclusion xmpp with its extensions is a powerful protocol for cloud services that demonstrate several advantages over traditional http based web services services are discoverable without the need of an external registry asynchronous invocation eliminates the need for ad hoc solutions like polling and input and output types defined in the service allows for generation of clients on the fly without the need of an external semantics description the many advantages over existing technologies make xmpp a highly interesting candidate for next generation online services bioinformatics
shor s quantum factoring algorithm finds the prime factors of a large number exponentially faster than any other known method a task that lies at the heart of modern information security particularly on the internet this algorithm requires a quantum computer a device that harnesses the massive parellism afforded by quantum superposition and entanglement of quantum bits or qubits we report the demonstration of a compiled version of shor s algorithm on an integrated waveguide silica on silicon chip that guides four single photon qubits through the computation factor
a major challenge in ecology is forecasting the effects of species extinctions a pressing problem given current human impacts on the planet consequences of species losses such as secondary extinctions are difficult to forecast because species are not isolated but interact instead in a complex network of ecological relationships because of their mutual dependence the loss of a single species can cascade in multiple coextinctions here we show that an algorithm adapted from the one google uses to rank web pages can order species according to their importance for coextinctions providing the sequence of losses that results in the fastest collapse of the network moreover we use the algorithm to bridge the gap between qualitative who eats whom and quantitative at what rate descriptions of food webs we show that our simple algorithm finds the best possible solution for the problem of assigning importance from the perspective of secondary extinctions in all analyzed networks our approach relies on network structure but applies regardless of the specific dynamical model of species interactions because it identifies the subset of coextinctions common to all possible models those that will happen with certainty given the complete loss of prey of a given predator results show that previous measures of importance based on the concept of hubs or number of connections as well as centrality measures do not identify the most effective extinction sequence the proposed algorithm provides a basis for further developments in the analysis of extinction risk ecosystems
communication between distant sites often defines the biological role of a protein amino acid long range interactions are as important in binding specificity allosteric regulation and conformational change as residues directly contacting the substrate the maintaining of functional and structural coupling of long range interacting residues requires coevolution of these residues networks of interaction between coevolved residues can be reconstructed and from the networks one can possibly derive insights into functional mechanisms for the protein family we propose a combinatorial method for mapping conserved networks of amino acid interactions in a protein which is based on the analysis of a set of aligned sequences the associated distance tree and the combinatorics of its subtrees the degree of coevolution of all pairs of coevolved residues is identified numerically and networks are reconstructed with a dedicated clustering algorithm the method drops the constraints on high sequence divergence limiting the range of applicability of the statistical approaches previously proposed we apply the method to four protein families where we show an accurate detection of functional networks and the possibility to treat sets of protein sequences of divergence
we undertook a two stage genome wide association study gwas of alzheimer s disease ad involving over individuals the most powerful ad gwas to date in stage cases and controls we replicated the established association with the apolipoprotein e apoe locus most significant snp p x and observed genome wide significant association with snps at two loci not previously associated with the disease at the clu also known as apoj gene p x and to the picalm gene p x these associations were replicated in stage cases and controls producing compelling evidence for association with alzheimer s disease in the combined dataset p x odds ratio p x ratio
an introduction to the theory of the renormalization group in the context of quantum field theories of relevance to particle physics is presented in the form of lectures delivered to the british universities summer school in theoretical elementary particle physics busstepp emphasis is placed on gaining a physical understand of the running of the couplings and the wilsonian version of the renormalization group is related to conventional perturbative calculations with dimensional regularization and minimal subtraction an introduction is given to some of the remarkable renormalization group properties of theories
the black hole information paradox is a very poorly understood problem it is often believed that hawking s argument is not precisely formulated and a more careful accounting of naturally occurring quantum corrections will allow the radiation process to become unitary we show that such is not the case by proving that small corrections to the leading order hawking computation cannot remove the entanglement between the radiation and the hole we formulate hawking s argument as a theorem assuming traditional physics at the horizon and usual assumptions of locality we will be forced into mixed states or remnants we also argue that one cannot explain away the problem by invoking ads cft duality we conclude with recent results on the quantum physics of black holes which show the the interior of black holes have a fuzzball structure this nontrivial structure of microstates resolves the information paradox and gives a qualitative picture of how classical intuition can break down in black physics
summary we report on a major new version of the rmap software for mapping reads from short read sequencing technology general improvements to accuracy and space requirements are included along with novel functionality included in the rmap software package are tools for mapping paired end reads mapping using more sophisticated use of quality scores collecting ambiguous mapping locations and mapping bisulfite treated reads availability the applications described in this note are available for download at http www cmb usc edu people andrewds rmap and are distributed as open source software under the the software has been tested on linux and os x platforms contact andrewds usc edu mzhang cshl edu the rmap algorithm was introduced by smith et al as one of the earliest available programs for mapping reads from the illumina second generation sequencing technology one important contribution of rmap was to incorporate the use of quality scores directly into the mapping process read positions with too low a quality score were not considered while mapping and that quality score cutoff could be adjusted by the user subsequently numerous mapping algorithm have appeared langmead et al li h et al li r et al lin et al schatz yanovsky et al with improvements in both efficiency and breadth of functionality e g ability to map paired end reads integrated snp calling investigators requiring solutions to mapping problems now have many options as new applications of short read sequencing emerge many variations on the analysis task of read mapping emerge diversity in performance characteristics of existing mapping tools becomes potentially valuable we report the first major update to rmap the basic algorithmic framework in rmap is still to preprocess reads and scan the genome but several modifications have been made and much additional functionality has been included importantly rmap has a memory footprint that depends on the number of reads being mapped this feature allows rmap to be used effectively in cluster environments with commodity nodes because partitioning the reads allows natural parallelizations with linear reduction in memory requirements per processor core used included in this release of the rmap software package is functionality for mapping paired end reads making more sophisticated use of quality scores collecting mapping locations for ambiguously mapping reads and mapping bisulfite treated bioinformatics
motivation we address the issue of finding a three way gene interaction i e two interacting genes in expression under the genotypes of another gene given a dataset in which expressions and genotypes are measured at once for each individual this issue can be a general switching mechanism in expression of two genes being controlled by categories of another gene and finding this type of interaction can be a key to elucidating complex biological systems the most suitable method for this issue is likelihood ratio test using logistic regressions which we call interaction test but a serious problem of this test is computational intractability at a genome wide level results we developed a fast method for this issue which improves the speed of interaction test by around times for any size of datasets keeping highly interacting genes with an accuracy of approximately we applied our method to approximately x three way combinations generated from a dataset on human brain samples and detected three way gene interactions with small p values to check the reliability of our results we first conducted permutations by which we can show that the obtained p values are significantly smaller than those obtained from permuted null examples we then used geo gene expression omnibus to generate gene expression datasets with binary classes to confirm the detected three way interactions by using these datasets and interaction tests the result showed us some datasets with significantly small p values strongly supporting the reliability of the detected three way interactions availability software is available from http www bic kyoto u ac jp pathway kayano way html contact kayano kuicr kyoto u ac jp supplementary information supplementary data are available at online
computer scientists have recently undermined our faith in the privacy protecting power of anonymization the name for techniques for protecting the privacy of individuals in large databases by deleting information like names and social security numbers these scientists have demonstrated they can often reidentify or deanonymize individ uals hidden in anonymized data with astonishing ease by understand ing this research we will realize we have made a mistake labored be neath a fundamental misunderstanding which has assured us much less privacy than we have assumed this mistake pervades nearly every information privacy law regulation and debate yet regulators and legal scholars have paid it scant attention we must respond to the surprising failure of anonymization and this article provides the tools to so
the most striking feature of quantum mechanics is the existence of superposition states where an object appears to be in different situations at the same time the existence of such states has been previously tested with small objects such as atoms ions electrons and photons zoller et al eur phys j d and even with molecules arndt et al nature more recently it has been shown that it is possible to create superpositions of collections of photons deleglise et al nature atoms hammerer et al arxiv or cooper pairs friedman et al nature very recent progress in optomechanical systems may soon allow us to create superpositions of even larger objects such as micro sized mirrors or cantilevers marshall et al phys rev lett kippenberg and vahala science marquardt and girvin physics favero and karrai nature photon and thus to test quantum mechanical phenomena at larger scales here we propose a method to cool down and create quantum superpositions of the motion of sub wavelength arbitrarily shaped dielectric objects trapped inside a high finesse cavity at a very low pressure our method is ideally suited for the smallest living organisms such as viruses which survive under low vacuum pressures rothschild and mancinelli nature and optically behave as dielectric objects ashkin and dziedzic science this opens up the possibility of testing the quantum nature of living organisms by creating quantum superposition states in very much the same spirit as the original schrodinger s cat gedanken paradigm schrodinger naturwissenschaften we anticipate that our paper will be a starting point for experimentally addressing fundamental questions such as the role of life and consciousness in mechanics
motivation next generation parallel sequencing technologies produce large quantities of short sequence reads due to experimental procedures various types of artifacts are commonly sequenced alongside the targeted rna or dna sequences identification of such artifacts is important during the development of novel sequencing assays and for the downstream analysis of the sequenced libraries results here we present tagdust a program identifying artifactual sequences in large sequencing runs given a user defined cutoff for the false discovery rate tagdust identifies all reads explainable by combinations and partial matches to known sequences used during library preparation we demonstrate the quality of our method on sequencing runs performed on illumina s genome analyzer platform availability executables and documentation are available from http genome gsc riken jp osc english software contact timolassmann gmail bioinformatics
this report is a review of the literature of marine natural product chemistry for earlier reports published in this journal cover the period from to december over the past two years marine natural product chemistry has graduated from an expanding to a mature field approximately the same number of new compounds have been described but more marine natural products are being selected for synthesis and for in depth investigations of their biological properties both from the biomedical and ecological biofouling viewpoints the format for this review is identical to that of its immediate predecessor the review does not provide a comprehensive coverage of all research involving chemicals from marine organisms but concentrates on reports of novel marine natural products with interesting biological and pharmaceutical properties biochemical studies involving marine organisms studies of the biosynthesis of marine natural products and reports of primary metabolites are specifically omitted wherever possible the biological and pharmacological properties of new marine natural products have been reported but papers detailing pharmacological studies are considered to be beyond the scope of this review in the area of synthetic organic chemistry the review focuses on reports of the total synthesis of marine natural products that confirm or redefine chemical structures no attempt has been made to review the patent literature or conference abstracts although recent experience has shown that these sources are probably the most important for those seeking new structures synthesis
chromatin immunoprecipitation followed by sequencing chip seq is a technique for genome wide profiling of dna binding proteins histone modifications or nucleosomes owing to the tremendous progress in next generation sequencing technology chip seq offers higher resolution less noise and greater coverage than its array based predecessor chip chip with the decreasing cost of sequencing chip seq has become an indispensable tool for studying gene regulation and epigenetic mechanisms in this review i describe the benefits and challenges in harnessing this technique with an emphasis on issues related to experimental design and data analysis chip seq experiments generate large quantities of data and effective computational analysis will be crucial for uncovering mechanisms
the molecular biology revolution led to an intense focus on the study of interactions between dna rna and protein biosynthesis in order to develop a more comprehensive understanding of the cell one consequence of this focus was a reduced attention to whole system physiology making it difficult to link molecular biology to clinical medicine equipped with the tools emerging from the genomics revolution we are now in a position to link molecular states to physiological ones through the reverse engineering of molecular networks that sense dna and environmental perturbations and as a result drive variations in physiological states associated disease
in contrast to protein coding sequences the significance of variation in non coding dna in human disease has been minimally explored a great number of recent genome wide association studies suggest that non coding variation is a significant risk factor for common disorders but the mechanisms by which this variation contributes to disease remain largely obscure distant acting transcriptional enhancers a major category of functional non coding dna are involved in many developmental and disease relevant processes genome wide approaches to their discovery and functional characterization are now available and provide a growing knowledge base for the systematic exploration of their role in human biology and susceptibility
the regulation of gene transcription involves a dynamic balance between packaging regulatory sequences into chromatin and allowing transcriptional regulators access to these sequences access is restricted by the nucleosomes but these can be repositioned or ejected by enzymes known as nucleosome remodellers in addition the dna sequence can impart stiffness or curvature to the dna thereby affecting the position of nucleosomes on the dna influencing particular promoter architectures recent genome wide studies in yeast suggest that constitutive and regulated genes have architectures that differ in terms of nucleosome position turnover remodelling requirements and noise
in the eukaryotic genome the thousands of genes that encode messenger rna are transcribed by a molecular machine called rna polymerase ii analysing the distribution and status of rna polymerase ii across a genome has provided crucial insights into the long standing mysteries of transcription and its regulation these studies identify points in the transcription cycle where rna polymerase ii accumulates after encountering a rate limiting step when coupled with genome wide mapping of transcription factors these approaches identify key regulatory steps and factors and importantly provide an understanding of the mechanistic generalities as well as the rich diversities of regulation
web is often attributed with a high potential to address today s challenges in knowledge management and distributed collaboration this is due to the focus on innovative and creative sociotechnical concepts that are strongly influenced by informal communication and collaboration this development has already reached industry using the term enterprise different possibilities to use social software in enterprises are researched but also in academia cooperation to generate new knowledge and add it to the scientific discourse may radically change under open web conditions in addition teaching and learning scenarios might be moved towards technology enhanced lifelong learning communities in this article we will give an overview of the influence that web has on academia and what innovative forms of cooperation might emerge from this before we focus on academia and show the potential of web in this domain we first describe web as a sociotechnical phenomenon and show how technical and social systems differ in order to define sociotechnical communities stcs and the criteria them
context until recently web of science was the only database available to track citation counts for published articles other databases are now available but their relative performance has not been established objective to compare the citation count profiles of articles published in general medical journals among the citation databases of web of science scopus and google scholar design cohort study of articles published in jama lancet or the new england journal of medicine between october and march total citation counts for each article up to june were retrieved from web of science scopus and google scholar article characteristics were analyzed in linear regression models to determine interaction with the databases main outcome measures number of citations received by an article since publication and article characteristics associated with citation in databases results google scholar and scopus retrieved more citations per article with a median of interquartile range iqr to and iqr to respectively than web of science median iqr to p for both comparisons compared with web of science scopus retrieved more citations from non english language sources median vs and reviews vs and fewer citations from articles vs editorials vs and letters vs all p on a transformed scale fewer citations were found in google scholar to articles with declared industry funding nonstandardized regression coefficient confidence interval ci to reporting a study of a drug or medical device ci to or with group authorship ci to in multivariable analysis group authorship was the only characteristic that differed among the databases google scholar had significantly fewer citations to group authored articles ci to compared with web of science conclusion web of science scopus and google scholar produced quantitatively and qualitatively different citation counts for articles published in general medical jama
phytophthora infestans is the most destructive pathogen of potato and a model organism for the oomycetes a distinct lineage of fungus like eukaryotes that are related to organisms such as brown algae and diatoms as the agent of the irish potato famine in the mid nineteenth century p infestans has had a tremendous effect on human history resulting in famine and population displacement to this day it affects world agriculture by causing the most destructive disease of potato the fourth largest food crop and a critical alternative to the major cereal crops for feeding the world s population current annual worldwide potato crop losses due to late blight are conservatively estimated at billion management of this devastating pathogen is challenged by its remarkable speed of adaptation to control strategies such as genetically resistant cultivars here we report the sequence of the p infestans genome which at approximately megabases mb is by far the largest and most complex genome sequenced so far in the chromalveolates its expansion results from a proliferation of repetitive dna accounting for approximately of the genome comparison with two other phytophthora genomes showed rapid turnover and extensive expansion of specific families of secreted disease effector proteins including many genes that are induced during infection or are predicted to have activities that alter host physiology these fast evolving effector genes are localized to highly dynamic and expanded regions of the p infestans genome this probably plays a crucial part in the rapid adaptability of the pathogen to host plants and underpins its potential
the ability of a transcription factor tf to regulate its targets is modulated by a variety of genetic and epigenetic mechanisms resulting in highly context dependent regulatory networks however high throughput methods for the identification of proteins that affect tf activity are still largely unavailable here we introduce an algorithm modulator inference by network dynamics mindy for the genome wide identification of post translational modulators of tf activity within a specific cellular context when used to dissect the regulation of myc activity in human b lymphocytes the approach inferred novel modulators of myc function which act by distinct mechanisms including protein turnover transcription complex formation and selective enzyme recruitment mindy is generally applicable to study the post translational modulation of mammalian tfs in any cellular context as such it can be used to dissect context specific signaling pathways and combinatorial regulation
in a seminal paper w maddison proposed minimizing deep coalescences or mdc as an optimization criterion for inferring the species tree from a set of incongruent gene trees assuming the incongruence is exclusively due to lineage sorting in a subsequent paper maddison and knowles provided and implemented a search heuristic for optimizing the mdc criterion given a set of gene trees however the heuristic is not guaranteed to compute optimal solutions and its hill climbing search makes it slow in practice in this paper we provide two exact solutions to the problem of inferring the species tree from a set of gene trees under the mdc criterion in other words our solutions are guaranteed to find the tree that minimizes the total number of deep coalescences from a set of gene trees one solution is based on a novel integer linear programming ilp formulation and another is based on a simple dynamic programming dp approach powerful ilp solvers such as cplex make the first solution appealing particularly for very large scale instances of the problem whereas the dp based solution eliminates dependence on proprietary tools and its simplicity makes it easy to integrate with other genomic events that may cause gene tree incongruence using the exact solutions we analyze a data set of loci from eight yeast species a data set of loci from eight apicomplexan species and several simulated data sets we show that the mdc criterion provides very accurate estimates of the species tree topologies and that our solutions are very fast thus allowing for the accurate analysis of genome scale data sets further the efficiency of the solutions allow for quick exploration of sub optimal solutions which is important for a parsimony based criterion such as mdc as we show we show that searching for the species tree in the compatibility graph of the clusters induced by the gene trees may be sufficient in practice a finding that helps ameliorate the computational requirements of optimization solutions further we study the statistical consistency and convergence rate of the mdc criterion as well as its optimality in inferring the species tree finally we show how our solutions can be used to identify potential horizontal gene transfer events that may have caused some of the incongruence in the data thus augmenting maddison s original framework we have implemented our solutions in the phylonet software package which is freely available at http bioinfo cs rice phylonet
with few exceptions current methods for short read mapping make use of simple seed heuristics to speed up the search most of the underlying matching models neglect the necessity to allow not only mismatches but also insertions and deletions current evaluations indicate however that very different error models apply to the novel high throughput sequencing methods while the most frequent error type in illumina reads are mismatches reads produced by s gs flx predominantly contain insertions and deletions indels even though sequencers are able to produce longer reads the method is frequently applied to small rna mirna and sirna sequencing fast and accurate matching in particular of short reads with diverse errors is therefore a pressing practical problem we introduce a matching model for short reads that can besides mismatches also cope with indels it addresses different error models for example it can handle the problem of leading and trailing contaminations caused by primers and poly a tails in transcriptomics or the length dependent increase of error rates in these contexts it thus simplifies the tedious and error prone trimming step for efficient searches our method utilizes index structures in the form of enhanced suffix arrays in a comparison with current methods for short read mapping the presented approach shows significantly increased performance not only for reads but also for illumina reads our approach is implemented in the software segemehl available at http www bioinf uni leipzig de segemehl
motivation the computational identification of non coding rna ncrna genes represents one of the most important and challenging problems in computational biology existing methods for ncrna gene prediction rely mostly on homology information thus limiting their applications to ncrna genes with known homologues results we present a novel de novo prediction algorithm for ncrna genes using features derived from the sequences and structures of known ncrna genes in comparison to decoys using these features we have trained a neural network based classifier and have applied it to escherichia coli and sulfolobus solfataricus for genome wide prediction of ncrnas our method has an average prediction sensitivity and specificity of and respectively for identifying windows with potential for ncrna genes in e coli by combining windows of different sizes and using positional filtering strategies we predicted candidate ncrnas and recovered of known ncrnas in e coli we experimentally investigated six novel candidates using northern blot analysis and found expression of three candidates one represents a potential new ncrna one is associated with stable mrna decay intermediates and one is a case of either a potential riboswitch or transcription attenuator involved in the regulation of cell division in general our approach enables the identification of both cis and trans acting ncrnas in partially or completely sequenced microbial genomes without requiring homology or structural conservation availability the source code and results are available at http csbl bmb uga edu publications materials tran contact xyn bmb uga edu supplementary information supplementary data are available at bioinformatics bioinformatics
abstract various efforts to integrate biological knowledge into networks of interactions have produced a lively microbial systems biology putting molecular biology and computer sciences in perspective we review another trend in systems biology in which recursivity and information replace the usual concepts of differential equations feedback and feedforward loops and the like noting that the processes of gene expression separate the genome from the cell machinery we analyse the role of the separation between machine and program in computers however computers do not make computers for cells to make cells requires a specific organization of the genetic program which we investigate using available knowledge microbial genomes are organized into a paleome the name emphasizes the role of the corresponding functions from the time of the origin of life comprising a constructor and a replicator and a cenome emphasizing community relevant genes made up of genes that permit life in a particular context the cell duplication process supposes rejuvenation of the machine and replication of the program the paleome also possesses genes that enable information to accumulate in a ratchet like process down the generations the systems biology must include the dynamics of information creation in its developments
organizing data into sensible groupings is one of the most fundamental modes of understanding and learning as an example a common scheme of scientific classification puts organisms into a system of ranked taxa domain kingdom phylum class etc cluster analysis is the formal study of methods and algorithms for grouping or clustering objects according to measured or perceived intrinsic characteristics or similarity cluster analysis does not use category labels that tag objects with prior identifiers i e class labels the absence of category information distinguishes data clustering unsupervised learning from classification or discriminant analysis supervised learning the aim of clustering is to find structure in data and is therefore exploratory in nature clustering has a long and rich history in a variety of scientific fields one of the most popular and simple clustering algorithms k means was first published in in spite of the fact that k means was proposed over years ago and thousands of clustering algorithms have been published since then k means is still widely used this speaks to the difficulty in designing a general purpose clustering algorithm and the ill posed problem of clustering we provide a brief overview of clustering summarize well known clustering methods discuss the major challenges and key issues in designing clustering algorithms and point out some of the emerging and useful research directions including semi supervised clustering ensemble clustering simultaneous feature selection during data clustering and large scale clustering
sequencing by synthesis technologies can reduce the cost of generating de novo genome assemblies we report a method for assembling draft genome sequences of eukaryotic organisms that integrates sequence information from different sources and demonstrate its effectiveness by assembling an approximately mb draft genome sequence for the forest pathogen grosmannia clavigera an ascomycete fungus we also developed a method for assessing draft assemblies using illumina paired end read data and demonstrate how we are using it to guide future sequence finishing our results demonstrate that eukaryotic genome sequences can be accurately assembled by combining illumina and sanger data
recently next generation sequencing has been introduced as a promising new platform for assessing the copy number of transcripts while the existing microarray technology is considered less reliable for absolute quantitative expression measurements nonetheless so far results from the two technologies have only been compared based on biological data leading to the conclusion that although they are somewhat correlated expression values differ significantly here we use synthetic rna samples resembling human microrna samples to find that microarray expression measures actually correlate better with sample rna content than expression measures obtained from sequencing data in addition microarrays appear highly sensitive and perform equivalently to next generation sequencing in terms of reproducibility and relative quantification
the study of human micrornas is seriously hampered by the lack of proper tools allowing genome wide identification of mirna targets we performed ribonucleoprotein immunoprecipitation gene chip rip chip using antibodies against wild type human in untreated hodgkin lymphoma hl cell lines ten to thirty percent of the gene transcripts from the genome were enriched in the ip fraction of untreated cells representing the hl mirna targetome in silico analysis indicated that approximately of these gene transcripts represent targets of the abundantly co expressed mirnas to identify targets of mir rip chip with anti mir treated cells was performed and gene transcripts were identified these genes were analyzed for mir target sites in the utrs coding regions and utrs fifty one percent of them had mir target sites in the utr while of them were predicted mir targets by targetscan luciferase reporter assay confirmed targeting of mir to the utrs of out of genes in conclusion we report a method which can establish the mirna targetome in untreated human cells and identify mirna specific targets in a high throughput manner this approach is applicable to identify mirna targets in any human tissue sample or purified cell population in an unbiased and physiologically manner
our knowledge of yeast genomes remains largely dominated by the extensive studies on saccharomyces cerevisiae and the consequences of its ancestral duplication leaving the evolution of the entire class of hemiascomycetes only partly explored we concentrate here on five species of saccharomycetaceae a large subdivision of hemiascomycetes that we call protoploid because they diverged from the s cerevisiae lineage prior to its genome duplication we determined the complete genome sequences of three of these species kluyveromyces lachancea thermotolerans and saccharomyces lachancea kluyveri two members of the newly described lachancea clade and zygosaccharomyces rouxii we included in our comparisons the previously available sequences of kluyveromyces lactis and ashbya eremothecium gossypii despite their broad evolutionary range and significant individual variations in each lineage the five protoploid saccharomycetaceae share a core repertoire of approximately protein families and a high degree of conserved synteny synteny blocks were used to define gene orthology and to infer ancestors far from representing minimal genomes without redundancy the five protoploid yeasts contain numerous copies of paralogous genes either dispersed or in tandem arrays that altogether constitute a third of each genome ancient conserved paralogs as well as novel lineage specific paralogs identified
rna interference rnai a gene silencing pathway triggered by double stranded rna is conserved in diverse eukaryotic species but has been lost in the model budding yeast saccharomyces cerevisiae here we show that rnai is present in other budding yeast species including saccharomyces castellii and candida albicans these species use noncanonical dicer proteins to generate small interfering rnas which mostly correspond to transposable elements and y subtelomeric repeats in s castellii rnai mutants are viable but have excess y messenger rna levels in s cerevisiae introducing dicer and argonaute of s castellii restores rnai and the reconstituted pathway silences endogenous retrotransposons these results identify a previously unknown class of dicer proteins bring the tool of rnai to the study of budding yeasts and bring the tools of budding yeast to the study rnai
background production of proteins as therapeutic agents research reagents and molecular tools frequently depends on expression in heterologous hosts synthetic genes are increasingly used for protein production because sequence information is easier to obtain than the corresponding physical dna protein coding sequences are commonly re designed to enhance expression but there are no experimentally supported design principles principal findings to identify sequence features that affect protein expression we synthesized and expressed in e coli two sets of genes encoding two commercially valuable proteins a dna polymerase and a single chain antibody genes differing only in synonymous codon usage expressed protein at levels ranging from undetectable to of cellular protein using partial least squares regression we tested the correlation of protein production levels with parameters that have been reported to affect expression we found that the amount of protein produced in e coli was strongly dependent on the codons used to encode a subset of amino acids favorable codons were predominantly those read by trnas that are most highly charged during amino acid starvation not codons that are most abundant in highly expressed e coli proteins finally we confirmed the validity of our models by designing synthesizing and testing new genes using codon biases predicted to perform well conclusion the systematic analysis of gene design parameters shown in this study has allowed us to identify codon usage within a gene as a critical determinant of achievable protein expression levels in e coli we propose a biochemical basis for this as well as design algorithms to ensure high protein production from synthetic genes replication of this methodology should allow similar design algorithms to be empirically derived for any system
abstract background micrornas mirnas are endogenous small rna molecules that modulate the gene expression at the post transcription levels in many eukaryotic cells their widespread and important role in animals is gauged by estimates that of all genes are mirna targets results we perform a systematic investigation of the relationship between mirna regulation and their targets evolution in two mammals human and mouse we find genes with longer utrs are regulated by more distinct types of mirnas these genes correspondingly tend to have slower evolutionary rates at the protein level housekeeping genes are another class of genes that evolve slowly however they have a distinctly different type of regulation with shorter utrs to avoid mirna targeting conclusions our analysis suggests a two way evolutionary mechanism for mirna targets on the basis of their cellular roles and the length of their utrs functionally critical genes that are spatially or temporally expressed are stringently regulated by mirnas while housekeeping genes however conserved are selected to have shorter utrs to avoid regulation
genetically encodable optical reporters such as green fluorescent protein have revolutionized the observation and measurement of cellular states however the inverse challenge of using light to control precisely cellular behaviour has only recently begun to be addressed semi synthetic chromophore tethered and naturally occurring channel rhodopsins have been used to perturb directly neuronal the difficulty of engineering light sensitive proteins remains a significant impediment to the optical control of most cell biological processes here we demonstrate the use of a new genetically encoded light control system based on an optimized reversible proteinprotein interaction from the phytochrome signalling network of arabidopsis thaliana because proteinprotein interactions are one of the most general currencies of cellular information this system can in principle be generically used to control diverse functions here we show that this system can be used to translocate target proteins precisely and reversibly to the membrane with micrometre spatial resolution and at the second timescale we show that light gated translocation of the upstream activators of rho family gtpases which control the actin cytoskeleton can be used to precisely reshape and direct the cell morphology of mammalian cells the light gated proteinprotein interaction that has been optimized here should be useful for the design of diverse light programmable reagents potentially enabling a new generation of perturbative quantitative experiments in biology
pnas the environments we humans encounter daily are sources of exposure to diverse microbial communities some of potential concern to human health in this study we used culture independent technology to investigate the microbial composition of biofilms inside showerheads as ecological assemblages in the human indoor environment showers are an important interface for human interaction with microbes through inhalation of aerosols and showerhead waters have been implicated in disease although opportunistic pathogens commonly are cultured from shower facilities there is little knowledge of either their prevalence or the nature of other microorganisms that may be delivered during shower usage to determine the composition of showerhead biofilms and waters we analyzed rrna gene sequences from showerhead sites around the united states we find that variable and complex but specific microbial assemblages occur inside showerheads particularly striking was the finding that sequences representative of non tuberculous mycobacteria ntm and other opportunistic human pathogens are enriched to high levels in many showerhead biofilms fold above background water contents we conclude that showerheads may present a significant potential exposure to aerosolized microbes including documented opportunistic pathogens the health risk associated with showerhead microbiota needs investigation in persons with compromised immune or pulmonary er
annotating the function of all human genes is a critical yet formidable challenge current gene annotation efforts focus on centralized curation resources but it is increasingly clear that this approach does not scale with the rapid growth of the biomedical literature the gene wiki utilizes an alternative and complementary model based on the principle of community intelligence directly integrated within the online encyclopedia wikipedia the goal of this effort is to build a gene specific review article for every gene in the human genome where each article is collaboratively written continuously updated and community reviewed previously we described the creation of gene wiki stubs for approximately human genes here we describe ongoing systematic improvements to these articles to increase their utility moreover we retrospectively examine the community usage and improvement of the gene wiki providing evidence of a critical mass of users and editors gene wiki articles are freely accessible within the wikipedia web site and additional links and information are available at http en wikipedia org portal
meta analyses combining gene expression microarray experiments offer new insights into the molecular pathophysiology of disease not evident from individual experiments although the established technical reproducibility of microarrays serves as a basis for meta analysis pathophysiological reproducibility across experiments is not well established in this study we carried out a large scale analysis of disease associated experiments obtained from ncbi geo and evaluated their concordance across a broad range of diseases and tissue types on evaluating experiments representing diseases and tissues from microarrays we find evidence for a general pathophysiological concordance between experiments measuring the same disease condition furthermore we find that the molecular signature of disease across tissues is overall more prominent than the signature of tissue expression across diseases the results offer new insight into the quality of public microarray data using pathophysiological metrics and support new directions in meta analysis that include characterization of the commonalities of disease irrespective of tissue as well as the creation of multi tissue systems models of disease pathology using data
background discovering the functions of all genes is a central goal of contemporary biomedical research despite considerable effort we are still far from achieving this goal in any metazoan organism collectively the growing body of high throughput functional genomics data provides evidence of gene function but remains difficult to interpret results we constructed the first network of functional relationships for drosophila melanogaster by integrating most of the available comprehensive sets of genetic interaction protein protein interaction and microarray expression data the complete integrated network covers of the currently known genes which we refined to a high confidence network that includes functional relationships among genes an analysis of the network revealed a remarkable concordance with prior knowledge using the network we were able to infer a set of high confidence gene ontology biological process annotations on of the roughly previously unannotated genes we also show that this approach is a means of inferring annotations on a class of genes that cannot be annotated based solely on sequence similarity lastly we demonstrate the utility of the network through reanalyzing gene expression data to both discover clusters of coregulated genes and compile a list of candidate genes related to specific biological processes conclusions here we present the the first genome wide functional gene network in d melanogaster the network enables the exploration mining and reanalysis of experimental data as well as the interpretation of new data the inferred annotations provide testable hypotheses of previously genes
background wiki technology has become a ubiquitous mechanism for dissemination of information and places strong emphasis on collaboration we aimed to leverage wiki technology to allow small groups of researchers to collaborate around a specific domain for example a biological pathway automatically gathered seed data could be modified by the group and enriched with domain specific information results we describe a software system biokb implemented as a plugin for the twiki engine and designed to facilitate construction of a field specific wiki containing collaborative and automatically generated content features of this system include query of publicly available resources such as kegg ihop and mesh to generate seed content for topics simple definition of structure for topics of different types via an administration page and interactive incorporation of relevant pubmed references an exemplar is shown for the use of this system in the creation of the raaswiki knowledgebase on the renin angiotensin aldosterone system raas raaswiki has been seeded with data by use of biokb and will be the subject of ongoing development into an extensive knowledgebase on the raas conclusion the biokb system is available from http www bioinf mvm ed ac uk twiki bin view twiki biokbplugin as a plugin for the engine
gr mirnas are nt rna molecules that play important roles in post transcriptional regulation we have performed small rna sequencing in the nematodes caenorhabditis elegans c briggsae c remanei and pristionchus pacificus which have diverged up to million years ago to establish the repertoire and evolutionary dynamics of mirnas in these species in addition to previously known mirna genes from c elegans and c briggsae we demonstrate expression of many of their homologs in c remanei and p pacificus and identified in total more than novel expressed mirna genes the majority of which belong to p pacificus interestingly more than half of all identified mirna genes are conserved at the seed level in all four nematode species whereas only a few mirnas appear to be species specific in our compendium of mirnas we observed evidence for known mechanisms of mirna evolution including antisense transcription and arm switching as well as mirna family expansion through gene duplication in addition we identified a novel mode of mirna evolution termed hairpin shifting in which an alternative hairpin is formed with up or downstream sequences leading to shifting of the hairpin and creation of novel mirna species finally we identified rnas in all four nematodes including p pacificus where the upstream rna motif is more diverged the identification and systematic analysis of small rna repertoire in four nematode species described here provides a valuable resource for understanding the evolutionary dynamics of mirna mediated regulation
redgreen colour blindness which results from the absence of either the long l or the middle m wavelength sensitive visual photopigments is the most common single locus genetic disorder here we explore the possibility of curing colour blindness using gene therapy in experiments on adult monkeys that had been colour blind since birth a third type of cone pigment was added to dichromatic retinas providing the receptoral basis for trichromatic colour vision this opened a new avenue to explore the requirements for establishing the neural circuits for a new dimension of colour sensation classic visual deprivation have led to the expectation that neural connections established during development would not appropriately process an input that was not present from birth therefore it was believed that the treatment of congenital vision disorders would be ineffective unless administered to the very young however here we show that the addition of a third opsin in adult redgreen colour deficient primates was sufficient to produce trichromatic colour vision behaviour thus trichromacy can arise from a single addition of a third cone class and it does not require an early developmental process this provides a positive outlook for the potential of gene therapy to cure adult disorders
understanding the evolutionary origins of behaviour is a central aim in the study of biology and may lead to insights into human disorders synaptic transmission is observed in a wide range of invertebrate and vertebrate organisms and underlies their behaviour proteomic studies of the molecular components of the highly complex mammalian postsynaptic machinery point to an ancestral molecular machinery in unicellular organisms the protosynapse that existed before the evolution of metazoans and neurons and hence challenges existing views on the origins of the brain the phylogeny of the molecular components of the synapse provides a new model for studying synapse diversity and complexity and their implications for evolution
pnas small bowel transplants provide an exceptional opportunity for long term study of the microbial ecology of the human small bowel the ileostomy created at time of transplant for ongoing monitoring of the allograft provides access to samples of ileal effluent and mucosal biopsies in this study we used qpcr to assay the bacterial population of the small bowel lumen of small bowel transplant patients over time surprisingly the posttransplant microbial community was found to be dominated by lactobacilli and enterobacteria both typically facultative anaerobes this represents an inversion of the normal community that is dominated instead by the strictly anaerobic bacteroides and clostridia we found this inverted community also in patients with ileostomies who did not receive a transplant suggesting that the ileostomy itself is the primary ecological determinant shaping the microbiota after surgical closure of the ileostomy the community reverted to the normal structure therefore we hypothesized that the ileostomy allows oxygen into the otherwise anaerobic distal ileum thus driving the transition from one microbial community structure to another supporting this hypothesis metabolomic profiling of both communities demonstrated an enrichment for metabolites associated with aerobic respiration in samples from patients with open ileostomies viewed from an ecological perspective the two communities constitute alternative stable states of the human ileum that the small bowel appears to function normally despite these dramatic shifts suggests that its ecological resilience is greater than realized
metabolic pathways have traditionally been described in terms of biochemical reactions and metabolites with the use of structural genomics and systems biology we generated a three dimensional reconstruction of the central metabolic network of the bacterium thermotoga maritima the network encompassed proteins of which were determined by experiment and were modeled structural analysis revealed that proteins forming the network are dominated by a small number only of basic shapes folds performing diverse but mostly related functions most of these folds are already present in the essential core approximately of the network and its expansion by nonessential proteins is achieved with relatively few additional folds thus integration of structural data with networks analysis generates insight into the function mechanism and evolution of networks
background with the increasing number of expression profiling technologies researchers today are confronted with choosing the technology that has sufficient power with minimal sample size in order to reduce cost and time these depend on data variability partly determined by sample type preparation and processing objective measures that help experimental design given own pilot data are thus fundamental results relative power and sample size analysis were performed on two distinct data sets the first set consisted of affymetrix array data derived from a nutrigenomics experiment in which weak intermediate and strong pparalpha agonists were administered to wild type and pparalpha null mice our analysis confirms the hierarchy of pparalpha activating compounds previously reported and the general idea that larger effect sizes positively contribute to the average power of the experiment a simulation experiment was performed that mimicked the effect sizes seen in the first data set the relative power was predicted but the estimates were slightly conservative the second more challenging data set describes a microarray platform comparison study using hippocampal deltac doublecortin like kinase transgenic mice that were compared to wild type mice which was combined with results from solexa illumina deep sequencing runs as expected the choice of technology greatly influences the performance of the experiment solexa illumina deep sequencing has the highest overall power followed by the microarray platforms agilent and affymetrix interestingly solexa illumina deep sequencing displays comparable power across all intensity ranges in contrast with microarray platforms that have decreased power in the low intensity range due to background noise this means that deep sequencing technology is especially more powerful in detecting differences in the low intensity range compared to microarray platforms conclusion power and sample size analysis based on pilot data give valuable information on the performance of the experiment and can thereby guide further decisions on experimental design solexa illumina deep sequencing is the technology of choice if interest lies in genes expressed in the low intensity range researchers can get guidance on experimental design using our approach on their own pilot data implemented as a bioconductor package sspa http bioconductor org packages release bioc html html
abstract genome resequencing with short reads generally relies on alignments against a single reference genomemapper supports simultaneous mapping of short reads against multiple genomes by integrating related genomes e g individuals of the same species into a single graph structure it constitutes the first approach for handling multiple references and introduces representations for alignments against complex structures demonstrated benefits include access to polymorphisms that cannot be identified by alignments against the reference alone download genomemapper at org
motivation splice junction microarrays and rna seq are two popular ways of quantifying splice variants within a cell unfortunately isoform expressions cannot always be determined from the expressions of individual exons and splice junctions while this issue has been noted before the extent of the problem on various platforms has not yet been explored nor have potential remedies been presented results we propose criteria that will guarantee identifiability of an isoform deconvolution model on exon and splice junction arrays and in rna seq we show that up to of alternatively spliced human genes selected from the refseq database lead to identifiable gene models in rna seq with similar results in mouse however in the human exon array only of these genes lead to identifiable models and even in the most comprehensive splice junction array only lead to identifiable models contact whwong stanford edu supplementary information supplementary data are available at bioinformatics bioinformatics
bayesian statistical methods have recently made great inroads into many areas of science and this advance is now extending to the assessment of association between genetic variants and disease or other phenotypes we review these methods focusing on single snp tests in genome wide association studies we discuss the advantages of the bayesian approach over classical frequentist approaches in this setting and provide a tutorial on basic analysis steps including practical guidelines for appropriate prior specification we demonstrate the use of bayesian methods for fine mapping in candidate regions discuss meta analyses and provide guidance for refereeing manuscripts that contain analyses
background micrornas are small endogenously expressed non coding rna molecules that regulate target gene expression through translation repression or messenger rna degradation microrna regulation is performed through pairing of the microrna to sites in the messenger rna of protein coding genes since experimental identification of mirna target genes poses difficulties computational microrna target prediction is one of the key means in deciphering the role of micrornas in development and disease results diana microt is an algorithm for microrna target prediction which is based on several parameters calculated individually for each microrna and combines conserved and non conserved microrna recognition elements into a final prediction score which correlates with protein production fold change specifically for each predicted interaction the program reports a signal to noise ratio and a precision score which can be used as an indication of the false positive rate of the prediction conclusion recently several computational target prediction programs were benchmarked based on a set of microrna target genes identified by the psilac method in this assessment diana microt was found to achieve the highest precision among the most widely used microrna target prediction programs reaching approximately the diana microt prediction results are available online in a user friendly web server at http www microrna microt
the enlargement and species specific elaboration of the cerebral neocortex during evolution holds the secret to the mental abilities of humans however the genetic origin and cellular mechanisms that generated the distinct evolutionary advancements are not well understood this article describes how novelties that make us human may have been introduced during evolution based on findings in the embryonic cerebral cortex in different mammalian species the data on the differences in gene expression new molecular pathways and novel cellular interactions that have led to these evolutionary advances may also provide insight into the pathogenesis and therapies for human specific disorders
sharp wave ripple spw r complexes in the hippocampus entorhinal cortex are believed to be important for transferring labile memories from the hippocampus to the neocortex for long term storage we found that selective elimination of spw rs during post training consolidation periods resulted in performance impairment in rats trained on a hippocampus dependent spatial memory task our results provide evidence for a prominent role of hippocampal spw rs in consolidation
motivation from the scientific community a lot of effort has been spent on the correct identification of gene and protein names in text while less effort has been spent on the correct identification of chemical names dictionary based term identification has the power to recognize the diverse representation of chemical information in the literature and map the chemicals to their database identifiers results we developed a dictionary for the identification of small molecules and drugs in text combining information from umls mesh chebi drugbank kegg hmdb and chemidplus rule based term filtering manual check of highly frequent terms and disambiguation rules were applied we tested the combined dictionary and the dictionaries derived from the individual resources on an annotated corpus and conclude the following i each of the different processing steps increase precision with a minor loss of recall ii the overall performance of the combined dictionary is acceptable precision recall for trivial names iii the combined dictionary performed better than the dictionary in the chemical recognizer iv the performance of a dictionary based on chemidplus alone is comparable to the performance of the combined dictionary availability the combined dictionary is freely available as an xml file in simple knowledge organization system format on the web site http www biosemantics org chemlist contact k hettne erasmusmc nlsupplementary information supplementary data are available at online
transcriptome analysis has been a key area of biological inquiry for decades over the years research in the field has progressed from candidate gene based detection of rnas using northern blotting to high throughput expression profiling driven by the advent of microarrays next generation sequencing technologies have revolutionized transcriptomics by providing opportunities for multidimensional examinations of cellular transcriptomes in which high throughput expression data are obtained at a single resolution
we analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust ner system in particular we address issues such as the representation of text chunks the inference approach needed to combine local ner decisions the sources of prior knowledge and how to use them within an ner system in the process of comparing several solutions to these challenges we reach some surprising conclusions as well as develop an ner system that achieves f score on the conll ner shared task the best reported result for dataset
over the past several years it has become clear that alterations in the expression of microrna mirna genes contribute to the pathogenesis of most if not all human malignancies these alterations can be caused by various mechanisms including deletions amplifications or mutations involving mirna loci epigenetic silencing or the dysregulation of transcription factors that target specific mirnas because malignant cells show dependence on the dysregulated expression of mirna genes which in turn control or are controlled by the dysregulation of multiple protein coding oncogenes or tumour suppressor genes these small rnas provide important opportunities for the development of future mirna therapies
motivation the online resources in the life sciences are characterized by a great fragmentation and one of the pressing issues of bioinformatics is making the integration of these resources a smoother and more flexible process than it is currently here we present i cite a browser extension which implements a client side model of integration which improves the navigation within the rapidly increasing life science literature and links terms from it to corresponding non textual data availability http i org
micrornas mirnas and small nucleolar rnas snornas are two classes of small non coding regulatory rnas which have been much investigated in recent years while their respective functions in the cell are distinct they share interesting genomic similarities and recent sequencing projects have identified processed forms of snornas that resemble mirnas here we investigate a possible evolutionary relationship between mirnas and box h aca snornas a comparison of the genomic locations of reported mirnas and snornas reveals an overlap of specific members of these classes to test the hypothesis that some mirnas might have evolved from snorna encoding genomic regions reported mirna encoding regions were scanned for the presence of box h aca snorna features twenty mirna precursors show significant similarity to h aca snornas as predicted by snogps these include molecules predicted to target known ribosomal rna pseudouridylation sites in vivo for which no guide snorna has yet been reported the predicted folded structures of these twenty h aca snorna like mirna precursors reveal molecules which resemble the structures of known box h aca snornas the genomic regions surrounding these predicted snorna like mirnas are often similar to regions around snorna retroposons including the presence of transposable elements target site duplications and poly a tails we further show that the precursors of five h aca snorna like mirnas mir mir mir mir and mir bind to dyskerin a specific protein component of functional box h aca small nucleolar ribonucleoprotein complexes suggesting that these molecules have retained some h aca snorna functionality the detection of small rna molecules that share features of mirnas and snornas suggest that these classes of rna may have an relationship
resumen se presenta el resultado de un estudio de los sitiosweb de repositorios de publicaciones digitales de libre acceso de pases europeos seanaliza hasta qu punto cumplencon algunas de las caractersticas que podran hacer de ellossitios web de calidad ms all delos aspectos de contenido cuyacalidad se da por supuesto o almenos no es objeto de anlisisen esta ocasin el objetivo escomprobar si los sitios web directamente vinculados con elcompromiso del libre acceso ala informacin satisfacen criterios de accesibilidad posicionamiento y calidad del cdigo web es decir en este ltimo casoobservacin de los estndaresabiertos por tanto se estudiauna serie de elementos que afectan directamente a la accesibilidad el posicionamiento web y lacalidad del cdigo fuente entrelos resultados obtenidos cabedestacar que la accesibilidaddebe mejorarse el cdigo htmlsolamente es aceptable y el posicionamiento es bueno es decir se observan malas prcticas queno deberan darse en sitios cuyoobjetivo es facilitar precisamente el libre acceso a informacin
sec title background title p many journals now require authors share their data with other investigators either by depositing the data in a public repository or making it freely available upon request these policies are explicit but remain largely untested we sought to determine how well authors comply with such policies by requesting data from authors who had published in one of two journals with clear data sharing policies p sec sec title methods and findings title p we requested data from ten investigators who had published in either plos medicine or plos clinical trials all responses were carefully documented in the event that we were refused data we reminded authors of the journal s data sharing guidelines if we did not receive a response to our initial request a second request was made following the ten requests for raw data three investigators did not respond four authors responded and refused to share their data two email addresses were no longer valid and one author requested further details a reminder of plos s explicit requirement that authors share data did not change the reply from the four authors who initially refused only one author sent an original data set p sec sec title conclusions title p we received only one of ten raw data sets requested this suggests that journal policies requiring data sharing do not lead to authors making their data sets available to independent investigators sec
background to effectively apply evolutionary concepts in genome scale studies large numbers of phylogenetic trees have to be automatically analysed at a level approaching human expertise complex architectures must be recognized within the trees so that associated information can be extracted results here we present a new software library phylopattern for automating tree manipulations and analysis phylopattern includes three main modules which address essential tasks in high throughput phylogenetic tree analysis node annotation pattern matching and tree comparison phylopattern thus allows the programmer to focus on i the use of predefined or user defined annotation functions to perform immediate or deferred evaluation of node properties ii the search for user defined patterns in large phylogenetic trees iii the pairwise comparison of trees by dynamically generating patterns from one tree and applying them to the other conclusion phylopattern greatly simplifies and accelerates the work of the computer scientist in the evolutionary biology field the library has been used to automatically identify phylogenetic evidence for domain shuffling or gene loss events in the evolutionary histories of protein sequences however any workflow that relies on phylogenetic tree analysis could be automated phylopattern
background high throughput sequencing technology has become popular and widely used to study protein and dna interactions chromatin immunoprecipitation followed by sequencing of the resulting samples produces large amounts of data that can be used to map genomic features such as transcription factor binding sites and histone modifications methods our proposed statistical algorithm bayespeak uses a fully bayesian hidden markov model to detect enriched locations in the genome the structure accommodates the natural features of the solexa illumina sequencing data and allows for overdispersion in the abundance of reads in different regions moreover a control sample can be incorporated in the analysis to account for experimental and sequence biases markov chain monte carlo algorithms are applied to estimate the posterior distributions of the model parameters and posterior probabilities are used to detect the sites of interest conclusion we have presented a flexible approach for identifying peaks from chip seq reads suitable for use on both transcription factor binding and histone modification data our method estimates probabilities of enrichment that can be used in downstream analysis the method is assessed using experimentally verified data and is shown to provide high confidence calls with low false rates
motivation aligning protein sequences with the best possible accuracy requires sophisticated algorithms since the optimal alignment is not guaranteed to be the correct one it is expected that even the best alignment will contain sites that do not respect the assumption of positional homology because formulating rules to identify these sites is difficult it is common practice to manually remove them although considered necessary in some cases manual editing is time consuming and not reproducible we present here an automated editing method based on the classification of valid and invalid sites results a support vector machine svm classifier is trained to reproduce the decisions made during manual editing with an accuracy of this implies that manual editing can be made reproducible and applied to large scale analyses we further demonstrate that it is possible to retrain extend the training of the classifier by providing examples of multiple sequence alignment msa annotation near optimal training can be achieved with only annotated sites or roughly three samples of protein sequence alignments availability this method is implemented in the software manuel licensed under the gpl a web based application for single and batch job is available at http fester cs dal ca manuel supplementary information supplementary data are available at online
we consider evidence for deviations from general relativity gr in the growth of large scale structure using two parameters gamma and eta to quantify the modification we consider the integrated sachs wolfe effect isw in the wmap cosmic microwave background data the cross correlation between the isw and galaxy distributions from and sdss surveys and the weak lensing shear field from the hubble space telescope s cosmos survey along with measurements of the cosmic expansion history we find current data driven by the cosmos weak lensing measurements disfavors gr on cosmic scales preferring eta lt at lt z lt at the level
high throughput genomic sequencing has focused attention on understanding differences between species and between individuals when this genetic variation affects protein sequences the rate of amino acid substitution reflects both darwinian selection for functionally advantageous mutations and selectively neutral evolution operating within the constraints of structure and function during neutral evolution whereby mutations accumulate by random drift amino acid substitutions are constrained by factors such as the formation of intramolecular and intermolecular interactions and the accessibility to water or lipids surrounding the protein these constraints arise from the need to conserve a specific architecture and to retain interactions that mediate functions in protein families superfamilies
how mirnas recognize their target sites is a puzzle that many experimental and computational studies aimed to solve several features such as perfect pairing of the mirna seed additional pairing in the region of the mirna relative position in the utr and the a u content of the environment of the putative site have been found to be relevant here we have used a large number of previously published data sets to assess the power that various sequence and structure features have in distinguishing between putative sites that do and those that do not appear to be functional we found that although different data sets give widely different answers when it comes to ranking the relative importance of these features the sites inferred from most transcriptomics experiments as well as from comparative genomics appear similar at this level this suggests that mirna target sites have been selected in evolution on their ability to trigger mrna degradation to understand at what step in the mirna induced response individual features play a role we transfected human cells with mirnas and analyzed the association of argonaute mirna complexes with target mrnas and the degradation of these messages we found that structural features of the target site are only important for argonaute binding while sequence features such as the a u content of the utr are important for degradation
india has been underrepresented in genome wide surveys of human variation we analyse diverse groups in india to provide strong evidence for two ancient populations genetically divergent that are ancestral to most indians today one the ancestral north indians ani is genetically close to middle easterners central asians and europeans whereas the other the ancestral south indians asi is as distinct from ani and east asians as they are from each other by introducing methods that can estimate ancestry without accurate ancestral populations we show that ani ancestry ranges from in most indian groups and is higher in traditionally upper caste and indo european speakers groups with only asi ancestry may no longer exist in mainland india however the indigenous andaman islanders are unique in being asi related groups without ani ancestry allele frequency differences between groups in india are larger than in europe reflecting strong founder effects whose signatures have been maintained for thousands of years owing to endogamy we therefore predict that there will be an excess of recessive diseases in india which should be possible to screen and genetically
our understanding of human biology and disease is ultimately dependent on a complete understanding of the genome and its functions the recent application of microarray and sequencing technologies to transcriptomics has changed the simplistic view of transcriptomes to a more complicated view of genome wide transcription where a large fraction of transcripts emanates from unannotated parts of and underlined our limited knowledge of the dynamic state of transcription most of this broad body of knowledge was obtained indirectly because current transcriptome analysis methods typically require rna to be converted to complementary dna cdna before measurements even though the cdna synthesis step introduces multiple biases and artefacts that interfere with both the proper characterization and quantification of furthermore cdna synthesis is not particularly suitable for the analysis of short degraded and or small quantity rna samples here we report direct single molecule rna sequencing without prior conversion of rna to cdna we applied this technology to sequence femtomole quantities of poly a saccharomyces cerevisiae rna using a surface coated with poly dt oligonucleotides to capture the rnas at their natural poly a tails and initiate sequencing by synthesis we observed transcript end heterogeneity and polyadenylated small nucleolar rnas this study provides a path to high throughput and low cost direct rna sequencing and achieving the ultimate goal of a comprehensive and bias free understanding transcriptomes
background sswap simple semantic web architecture and protocol pronounced swap is an architecture protocol and platform for using reasoning to semantically integrate heterogeneous disparate data and services on the web sswap was developed as a hybrid semantic web services technology to overcome limitations found in both pure web service technologies and pure semantic web technologies results there are currently over resources published in sswap approximately two dozen are custom written services for qtl quantitative trait loci and mapping data for legumes and grasses grains the remaining are wrappers to nucleic acids research database and web server entries as an architecture sswap establishes how clients users of data services and ontologies providers suppliers of data services and ontologies and discovery servers semantic search engines interact to allow for the description querying discovery invocation and response of semantic web services as a protocol sswap provides the vocabulary and semantics to allow clients providers and discovery servers to engage in semantic web services the protocol is based on the sanctioned first order description logic language owl dl as an open source platform a discovery server running at http sswap info as in to swap info uses the description logic reasoner pellet to integrate semantic resources the platform hosts an interactive guide to the protocol at http sswap info protocol jsp developer tools at http sswap info developer jsp and a portal to third party ontologies at http sswapmeet sswap info a swap meet conclusion sswap addresses the three basic requirements of a semantic web services architecture i e a common syntax shared semantic and semantic discovery while addressing three technology limitations common in distributed service systems i e i the fatal mutability of traditional interfaces ii the rigidity and fragility of static subsumption hierarchies and iii the confounding of content structure and presentation sswap is novel by establishing the concept of a canonical yet mutable owl dl graph that allows data and service providers to describe their resources to allow discovery servers to offer semantically rich search engines to allow clients to discover and invoke those resources and to allow providers to respond with semantically tagged data sswap allows for a mix and match of terms from both new and legacy third party ontologies in graphs
context web applications such as social networking sites are creating new challenges for medical professionalism the scope of this problem in undergraduate medical education is not well defined objective to assess the experience of us medical schools with online posting of unprofessional content by students and existing medical school policies to address online posting design setting and participants an anonymous electronic survey was sent to deans of student affairs their representatives or counterparts from each institution in the association of american medical colleges data were collected in march and april main outcome measures percentage of schools reporting incidents of students posting unprofessional content online type of professionalism infraction disciplinary actions taken existence of institution policies and plans for policy development results sixty percent of us medical schools responded of these schools reported incidents of students posting unprofessional online content violations of patient confidentiality were reported by student use of profanity frankly discriminatory language depiction of intoxication and sexually suggestive material were commonly reported of schools that reported an incident and responded to the question about disciplinary actions gave informal warning and reported student dismissal policies that cover student posted online content were reported by of deans of schools without such policies were actively developing new policies to cover online content deans reporting incidents were significantly more likely to report having such a policy vs p believing these issues could be effectively addressed vs p and having higher levels of concern p conclusion many responding schools had incidents of unprofessional student online postings but they may not have adequate policy place
the extent to which evolution is reversible has long fascinated most previous work on the reversibility of morphological and life history has been indecisive because of uncertainty and bias in the methods used to infer ancestral states for such further despite theoretical work on the factors that could contribute to there is little empirical evidence on its causes because sufficient understanding of the mechanistic basis for the evolution of new or ancestral phenotypes is seldom by studying the reversibility of evolutionary changes in protein structure and function these limitations can be overcome here we show using the evolution of hormone specificity in the vertebrate glucocorticoid receptor as a case study that the evolutionary path by which this protein acquired its new function soon became inaccessible to reverse exploration using ancestral gene reconstruction protein engineering and x ray crystallography we demonstrate that five subsequent restrictive mutations which optimized the new specificity of the glucocorticoid receptor also destabilized elements of the protein structure that were required to support the ancestral conformation unless these ratchet like epistatic substitutions are restored to their ancestral states reversing the key function switching mutations yields a non functional protein reversing the restrictive substitutions first however does nothing to enhance the ancestral function our findings indicate that even if selection for the ancestral function were imposed direct reversal would be extremely unlikely suggesting an important role for historical contingency in evolution
the identification and modeling of patterns of human activity have important ramifications for applications ranging from predicting disease spread to optimizing resource allocation because of its relevance and availability written correspondence provides a powerful proxy for studying human activity one school of thought is that human correspondence is driven by responses to received correspondence a view that requires a distinct response mechanism to explain e mail and letter correspondence observations we demonstrate that like e mail correspondence the letter correspondence patterns of writers performers politicians and scientists are well described by the circadian cycle task repetition and changing communication needs we confirm the universality of these mechanisms by rescaling letter and e mail correspondence statistics to reveal their underlying science
micrornas mirnas are major regulators of gene expression and thereby modulate many biological processes computational methods have been instrumental in understanding how mirnas bind to mrnas to induce their repression but have proven inaccurate here we describe a novel method that combines expression data from human and mouse to discover conserved patterns of expression between orthologous mirnas and mrna genes this method allowed us to predict thousands of putative mirna targets using the luciferase reporter assay we confirmed out of of our predictions in addition this method predicted many mirnas that act as expression enhancers we show that many mirna enhancer effects are mediated through the repression of negative transcriptional regulators and that this effect could be as common as the widely reported repression activity of mirnas our findings suggest that the indirect enhancement of gene expression by mirnas could be an important component of mirna regulation that has been widely neglected date
background academic social tagging systems such as connotea and citeulike provide researchers with a means to organize personal collections of online references with keywords tags and to share these collections with others one of the side effects of the operation of these systems is the generation of large publicly accessible metadata repositories describing the resources in the collections in light of the well known expansion of information in the life sciences and the need for metadata to enhance its value these repositories present a potentially valuable new resource for application developers here we characterize the current contents of two scientifically relevant metadata repositories created through social tagging this investigation helps to establish how such socially constructed metadata might be used as it stands currently and to suggest ways that new social tagging systems might be designed that would yield better aggregate products results we assessed the metadata that users of citeulike and connotea associated with citations in pubmed with the following metrics coverage of the document space density of metadata tags per document rates of inter annotator agreement and rates of agreement with mesh indexing citeulike and connotea were very similar on all of the measurements in comparison to pubmed document coverage and per document metadata density were much lower for the social tagging systems inter annotator agreement within the social tagging systems and the agreement between the aggregated social tagging metadata and mesh indexing was low though the latter could be increased through voting conclusion the most promising uses of metadata from current academic social tagging repositories will be those that find ways to utilize the novel relationships between users tags and documents exposed through these systems for more traditional kinds of indexing based applications such as keyword based search to benefit substantially from socially generated metadata in the life sciences more documents need to be tagged and more tags are needed for each document these issues may be addressed both by finding ways to attract more users to current systems and by creating new user interfaces that encourage more collectively useful individual behaviour
low rank matrix approximations such as the truncated singular value decomposition and the rank revealing qr decomposition play a central role in data analysis and scientific computing this work surveys recent research which demonstrates that emph randomization offers a powerful tool for performing low rank matrix approximation these techniques exploit modern computational architectures more fully than classical methods and open the possibility of dealing with truly massive data sets in particular these techniques offer a route toward principal component analysis pca for petascale data this paper presents a modular framework for constructing randomized algorithms that compute partial matrix decompositions these methods use random sampling to identify a subspace that captures most of the action of a matrix the input matrix is then compressed either explicitly or implicitly to this subspace and the reduced matrix is manipulated deterministically to obtain the desired low rank factorization in many cases this approach beats its classical competitors in terms of accuracy speed and robustness these claims are supported by extensive numerical experiments and a detailed analysis
choosing good problems is essential for being a good scientist but what is a good problem and how do you choose one the subject is not usually discussed explicitly within our profession scientists are expected to be smart enough to figure it out on their own and through the observation of their teachers this lack of explicit discussion leaves a vacuum that can lead to approaches such as choosing problems that can give results that merit publication in valued journals resulting in a job and tenure main textthe premise of this essay is that a fuller discussion of our topic including its subjective and emotional aspects can enrich our science and our well being a good choice means that you can competently discover new knowledge that you find fascinating and that allows self expression we will discuss simple principles of choosing scientific problems that have helped me my students and many fellow scientists these principles might form a basis for teaching this subject generally to scientists starting point choosing a problem is an act of nurturingwhat is the goal of starting a lab it is sometimes easy to pick up a default value common in current culture such as the goal of my lab is to publish the maximum number of papers of the highest quality however in this essay we will frame the goal differently a lab is a nurturing environment that aims to maximize the potential of students as scientists and as human beings choices such as these are crucial from valueseven if they are not consciously statedflow all of the decisions made in the lab big and small how the lab looks when students can take a vacation and as we will now discuss what problems to choose within the nurturing lab we aim to choose a problem for our students and for ourselves in order to foster growth and self motivated research the two dimensions of problem choiceto choose a scientific problem let us begin with a simple graph as a starting point for discussion we will compare problems by imagining two axes the first is feasibilitythat is whether a problem is hard or easy in units such as the expected time to complete the project this axis is a function of theskills of the researchers and of the technology in the lab it is important to remember that problems that are easy on paper are often hard in reality and that problems that are hard on paper are nearly impossible in reality the second axis is interest the increase in knowledge expected from the project we generally value science that ventures deep into unknown waters problems can be ranked in terms of the distance from the known shores by the amount inwhich they increase verifiable knowledge we will call this the interest of the problem in a forthcoming section we will discuss the subjective nature of the interest axis but first let us first consider aspects of problem choice using our diagram looking at the range of problems in this two dimensional space one sees that many projects in current research are of the easy but not too interesting variety also known as low hanging fruit many other projects in science today are unfortunately both difficult and have low interest partially stemming from a view that hard equals good a few problems are grand challenges tough problems with the potential to considerably advance understanding but most often we would like problems in the top right quadrant both feasible and with high interest likely to extend our knowledge significantly the diagram suggests a way to choose between problems using the pareto front principle of optimization theory if problem a is better on both axes than problem b one can erase b from the diagram applying this criterion to all problems one is left only with problems for which there are no problems clearly better in both feasibility and interest these remaining problems are on the pareto front to decide which problem to select along the front depends on how we weigh the two axes for example a beginning graduate student needs a problem that is easy positive feedback can thus be rapidly provided bolstering confidence these problems are on the bottom right of the pareto front the second problem in graduate school can move up the interest axis postdocs need projects in the top right quadrant since time is limited beginning pis who need to select a field on which to spend many years and with which to train students may seek a grand challenge that can be divided into many good smaller projects thus the optimal problems move along the pareto front as a function of the life stages of the scientist take your timea common mistake made in choosing problems is taking the first problem that comes to mind since a typical project takes years even it if seems doable in months rapid choice leads to much frustration and bitterness in our profession it takes time to find a good problem and every week spent in choosing one can save months or years later on in my lab we have a rule for new students and postdocs do not commit to a problem before months have elapsed in these months the new student or postdoc reads discusses and plans the state of mind is focused on being rather than doing the temptation to start working arises but a rule is a rule after months or more a celebration marks the beginning of the research phasewith a well planned project taking time is not always easy one must be supported to resist the urge oh we must producelet s not waste time and start working i am under no illusion that everyone is free to choose their own problems or has the time needed for an extended search taking time can be especially difficult when funding is insufficient and grant deadlines approach in such difficult situations nurturing is not enough and you need to find support and do all you can to get into a better situation even so for many of us dealing with the difficulties of running a lab taking time to choose problems can make a huge difference the subjectivity of the interest axislet us now look in more detail on the axis of problem interest who decides how to rank the interest of problems one of the fundamental aspects of science is that the interest of a problem is subjective and personal this subjectivity however makes things confusing the confusion is due to the mixing of two voicesone is a loud voice of the interests of those around us in conferences in our department etc the other is a faint voice in our breast that says this is interesting to me ranking problems with consideration to the inner voice makes you more likely to choose problems that will satisfy you in the long term the inner voice can be strengthened and guided if one is lucky enough to have caring mentors a scientist often needs a supportive environment to begin to listen to this voice one way to help listening to the inner voice is to ask if i was the only person on earth which of these problems would i work on an honest answer can help minimize compromises another good sign of the inner voice are ideas and questions that come back again and again to your mind for months or years these are likely to be the basis of good projects more so than ideas that have occurred to you in recent days another good test when asked to describe our research to an acquaintance how does it feel to describe each project it is remarkable that listening to our own idiosyncratic voice leads to better science it makes research self motivated and the routine of research more rewarding in science the more you interest yourself the larger the probability that you will interest your audience self expressionwhat is the essence of the inner voice the projects that a particular researcher finds interesting are an expression of a personal filter a way of perceiving the world this filter is associated with a set of values the beliefs of what is good beautiful and true versus what is bad ugly and false our unique filter is what we bring to the table as scientists a multiplicity in styles and questions based on the uniqueness of scientists is the basis of a viable and creative science to choose a good problem therefore we need to reflect on our own world view and as mentors we can help students in the late phases of their phd or in the postdoc stage to strengthen their inner voice a mentor can help by listening to a student describe what they like in science in life outside of science what moment made them decide to become scientists and what scientific work they admire we sometimes begin to see patterns in what the student is talking about there emerges a map of values in the way that deep rocks in an ocean are discernable by the waves made on the surface is this student motivated by visual aesthetics or by abstract ideas by supporting the dogma or by undermining commonly held truths likes techniques or logical proofs basic understanding or applied work and so on this can help the mentor select a project in which the student has the potential forself expression as mentioned above when one can achieve self expression in science work becomes revitalizing self driven and laden with personal meaning it may also have a better chance of discovering something profound the schema of researchwhat happens after we choosea problem before we end i d like to discuss the mental picture or schema we hold of what research will look like a common schema is expressed in the way papers are written one starts at point a which is the question and proceeds by the shortest path to point b the answer there is a danger if one accepts this schema to regard students as a means to an end an arrow to b furthermore for those that hold this schema any deviation from the path experiments that don t work students that become depressed etc is intolerable deviation causes stress because of the cognitive dissonance between reality and the mental schema however one can adopt a second schema one that resembles more the course of most projects as before one starts at point a and moves toward the goal at point b soon enough things move off course and the path meanders and loops back experiments stop working all assumptions seem wrong and nothing makes sense the researcher has entered a phase linked with negative emotions that may be called the cloud then in the midst of confusion one senses a new problem in the materials at hand let s call this new problem c if c ismore interesting and feasible than b one can choose to go toward it after a few more detours c is reached the researchers can pause to celebrate before taking time to think about the next problem in this second schema the meandering of research is seen as an integral part of our craft rather than a nuisance the mentors task is to support students through the cloud that seems to guard the entry into the unknown and with this schema we have more space to see that problem c exists and may be more worthwhile than continuing to plod toward b in the nurturing schema we celebrate the courage and openness of scientists sailing into the unknown again and again takes courage seeing there something different from expectations and usually more rich and strange requires uncommon openness in summary take your time recall the rule to find among the problems available the one that is most feasible and most interesting to you rather than to others a good project draws upon your skills to achieve self expression acknowledgmentsthe ideas in this essay were presented to me as gifts in conversations and books or are the fruit of learning from my mistakes and are collected here and again offered as a gift especially memorable are discussions with ron milo galit lahav becky ward yuvalal liron michael elowitz angela depace evelyn fox keller and her writings especially reflections on gender and science and with members of my lab and colleagues who told me stories of mentoring and problem choice i would also like to thank my parents galia moran and our daughter gefen and mentors i balberg dov shvarts david mukamel and stan leibler harvard s positive psychology taught by tal ben shahar dan macadams for his books the person a new introduction to personality psychology and the narrative study of lives amir orian and the open circle approach to theatre and creative arts classes of jonathan fox for playback theatre and his book acts of service jerome bruner for his book acts of meaning erik erikson for childhood and society the weizmann institute for providing freedom to play mark kirschner and the harvard medical school department of systems biology for hospitality and a place to discuss these ideas with a well prepared audience and critical remarks by audience members in janelia farms who helped sharpen message
in we started an ar framework for mobile phones we intended its first generation as primarily a proof of feasibility the second generation was an attempt to port a fully featured pc based ar framework studierstube to a phone platform you can port existing applications and make them run on mobile phones however as we had to painfully experience ourselves this approach typically produces slow bloated and unstable software optimally using phones scarce resources requires different algorithms and architectural decisions than for pcs leading to a complete reengineering of an existing solution so for the third generation studierstube es we largely abandoned compatibility requirements and added new elements to the design such as an asymmetric client server technique that are specific to mobile devices in this first installment of our two part tale of studierstube es and what we ve learned along the way we describe the mobile phone platform s restrictions and how our software architecture allows fast development of mobile phone applications
background highly parallel sequencing technologies have become important tools in the analysis of sequence polymorphisms on a genomic scale however the development of customized software to analyze data produced by these methods has lagged behind methods principal findings here i describe a tool galign designed to identify polymorphisms between sequence reads obtained using illumina solexa technology and a reference genome the galign alignment tool does not use smith waterman matrices for sequence comparisons instead a simple algorithm comparing parsed sequence reads to parsed reference genome sequences is used galign output is geared towards immediate user application displaying polymorphism locations nucleotide changes and relevant predicted amino acid changes for ease of information processing to do so galign requires several accessory files easily derived from an annotated reference genome direct sequencing as well as in silico studies demonstrate that galign provides lesion predictions comparable in accuracy to available prediction programs accompanied by greater processing speed and more user friendly output we demonstrate the use of galign to identify mutations leading to phenotypic consequences in c elegans conclusion significance our studies suggest that galign is a useful tool for polymorphism discovery and is of immediate utility for sequence mining in elegans
this study explores web technologies in an academic library through focus groups with undergraduates at kent state university results reveal that students despite being heavy users are less sophisticated and expressive in their use of web than presumed students set clear boundaries between educational and social spaces on the web and the library may be best served by building web into its site and extending its services into course systems
pmid the activity within a living cell is based on a complex network of interactions among biomolecules exchanging information and energy through biochemical processes these events occur on different scales from the nano to the macroscale spanning about orders of magnitude in the space domain and orders of magnitude in the time domain consequently many different modeling techniques each proper for a particular time or space scale are commonly used in addition a single process often spans more than a single time or space scale thus the necessity arises for combining the modeling techniques in multiscale approaches in this account i first review the different modeling methods for bio systems from quantum mechanics to the coarse grained and continuum like descriptions passing through the atomistic force field simulations special attention is devoted to their combination in different possible multiscale approaches and to the questions and problems related to their coherent matching in the space and time domains these aspects are often considered secondary but in fact they have primary relevance when the aim is the coherent and complete description of bioprocesses subsequently applications are illustrated by means of two paradigmatic examples i the green fluorescent protein gfp family and ii the proteins involved in the human immunodeficency virus hiv replication cycle the gfps are currently one of the most frequently used markers for monitoring protein trafficking within living cells nanobiotechnology and cell biology strongly rely on their use in fluorescence microscopy techniques a detailed knowledge of the actions of the virus specific enzymes of hiv specifically hiv protease and integrase is necessary to study novel therapeutic strategies against this disease thus the insight accumulated over years of intense study is an excellent framework for this account the foremost relevance of these two biomolecular systems was recently confirmed by the assignment of two of the nobel prizes in in chemistry for the discovery of gfp and in medicine for the discovery of hiv accordingly these proteins were studied with essentially all of the available modeling techniques making them ideal examples for studying the details of multiscale approaches in modeling
the genome projects have unearthed an enormous diversity of genes of unknown function that are still awaiting biological and biochemical characterization these genes as most others can be grouped into families based on sequence similarity the pfam database currently contains over such families referred to as domains of unknown function duf in a coordinated effort the four large scale centers of the nih protein structure initiative have determined the first three dimensional structures for more than of these duf families analysis of the first reveals that about two thirds of the duf families likely represent very divergent branches of already known and well characterized families which allows hypotheses to be formulated about their biological function the remainder can be formally categorized as new folds although about one third of these show significant substructure similarity to previously characterized folds these results infer that despite the enormous increase in the number and the diversity of new genes being uncovered the fold space of the proteins they encode is gradually becoming saturated the previously unexplored sectors of the protein universe appear to be primarily shaped by extreme diversification of known protein families which then enables organisms to evolve new functions and adapt to particular niches and habitats notwithstanding these duf families still constitute the richest source for discovery of the remaining protein folds topologies
micrornas mirnas regulate gene expression at the posttranscriptional level and are therefore important cellular components as is true for protein coding genes the transcription of mirnas is regulated by transcription factors tfs an important class of gene regulators that act at the transcriptional level the correct regulation of mirnas by tfs is critical and increasing evidence indicates that aberrant regulation of mirnas by tfs can cause phenotypic variations and diseases therefore a tf mirna regulation database would be helpful for understanding the mechanisms by which tfs regulate mirnas and understanding their contribution to diseases in this study we manually surveyed approximately reports in the literature and identified tf mirna regulatory relationships which were supported experimentally from publications we used these data to build a tf mirna regulatory database transmir http cmbi bjmu edu cn transmir which contains tfs and mirnas with regulatory pairs between tfs and mirnas in addition we included references to the published literature pubmed id information about the organism in which the relationship was found whether the tfs and mirnas are involved with tumors mirna function annotation and mirna associated disease annotation transmir provides a user friendly interface by which interested parties can easily retrieve tf mirna regulatory pairs by searching for either a mirna or tf
biomedical texts can be typically represented by four rhetorical categories introduction methods results and discussion imrad classifying sentences into these categories can benefit many other text mining tasks although many studies have applied different approaches for automatically classifying sentences in medline abstracts into the imrad categories few have explored the classification of sentences that appear in full text biomedical articles we first evaluated whether sentences in full text biomedical articles could be reliably annotated into the imrad format and then explored different approaches for automatically classifying these sentences into the imrad categories our results show an overall annotation agreement of with a kappa score of the best classification system is a multinomial naive bayes classifier trained on manually annotated data that achieved accuracy and an average f score of which is significantly higher than baseline systems a web version of this system is available online at http wood ims uwm edu contact hongyu uwm bioinformatics
background current search engines are keyword based semantic technologies promise a next generation of semantic search engines which will be able to answer questions current approaches either apply natural language processing to unstructured text or they assume the existence of structured statements over which they can reason results here we introduce a third approach goweb which combines classical keyword based web search with text mining and ontologies to navigate large results sets and facilitate question answering we evaluate goweb on three benchmarks of questions on genes and functions on symptoms and diseases and on proteins and diseases the first benchmark is based on the biocreative task and links gene names with functions goweb finds of the functional geneontology annotations the second benchmark is based on case reports and links symptoms with diseases goweb achieves success rate improving an existing approach by nearly the third benchmark is based on questions in the trec genomics challenge and links proteins to diseases goweb achieves a success rate of conclusion goweb s combination of classical web search with text mining and ontologies is a first step towards answering questions in the biomedical domain goweb is online at http www gopubmed goweb
background life scientists need help in coping with the plethora of fast growing and scattered knowledge resources ideally this knowledge should be integrated in a form that allows them to pose complex questions that address the properties of biological systems independently from the origin of the knowledge semantic web technologies prove to be well suited for knowledge integration knowledge production hypothesis formulation knowledge querying and knowledge maintenance results we implemented a semantically integrated resource named biogateway comprising the entire set of the obo foundry candidate ontologies the go annotation files the swiss prot protein set the ncbi taxonomy and several in house ontologies biogateway provides a single entry point to query these resources through sparql it constitutes a key component for a semantic systems biology approach to generate new hypotheses concerning systems properties in the course of developing biogateway we faced challenges that are common to other projects that involve large datasets in diverse representations we present a detailed analysis of the obstacles that had to be overcome in creating biogateway we demonstrate the potential of a comprehensive application of semantic web technologies to global biomedical data conclusion the time is ripe for launching a community effort aimed at a wider acceptance and application of semantic web technologies in the life sciences we call for the creation of a forum that strives to implement a truly semantic life science foundation for semantic systems biology access to the system and supplementary information such as a listing of the data sources in rdf and sample queries can be found at http www semantic systems biology biogateway
scientific communication is being enriched by the introduction of new ways of storage publication and dissemination of the results these include the services of the web which are still largely unknown to researchers in this context the objective of this paper is to illustrate how we can strategically use web services to disseminate and give greater visibility to scientific publications to this end we present a series of services of interest to scientific communication blogger twitter facebook slideshare y e lis and explain the role they can play in communicating scientific results it also shows how these services should be logically interrelated which we call the strategic dissemination cycle in the last section the authors collected a set of webmetric indicators classified into three groups social influence use recognition to evaluate the success of the dissemination strategy and the visibility of a work in web
google scholar is a search engine that specializes in scientific information and in the identification of the citations that academic papers receive making it a strong competitor for other citations indexes for this reason several studies have attempted to evaluate its capacity as a bibliometric tool due to this interest we present an introduction to its use and the advantages and disadvantages versus scopus and web of science first its way of collecting information and features of its interface are analyzed the following section describes the results that google scholar generates thirdly we analyze the coverage of information sources and the different document types to be found showing how this coverage universe offers different citations versus other products finally we specify the standardization problems of google scholar and offer a number of precautions that must be taken into account when using google scholar as an tool
reference managing tools are one of the most useful devices for researchers and librarians due to their ability to compile store and format information related to different products sources and types of records in recent years a new generation of reference managing software has appeared these new tools include applications from the new technological context that have contributed to reinforcing their capacity and potential an overview of these tools and their applications is offered we also makes a comparative analysis of the different products with a view to highlighting their strengths as well as the elements that could be improved in tool
micrornas mirnas are a class of short endogenously expressed rna molecules that regulate gene expression by binding directly to the messenger rna of protein coding genes they have been found to confer a novel layer of genetic regulation in a wide range of biological processes computational mirna target prediction remains one of the key means used to decipher the role of mirnas in development and disease here we introduce the basic idea behind the experimental identification of mirna targets and present some of the most widely used computational mirna target identification programs the review includes an assessment of the prediction quality of these programs and their combinations supplementary information supplementary data are available at online
este artculo analiza los principales puntos de convergencia y divergencia entre los lenguajes documentales y las ontologas en tanto que herramientas para la organizacin del conocimiento y para la recuperacin de informacin en el mbito de la web semntica se describen los aspectos fundamentales de una ontologa as como las principales caractersticas semnticas y estructurales de los lenguajes documentales para establecer una comparacin entre ellos the main points of convergence and divergence between controlled indexing languages and ontologies tools for knowledge organization and information retrieval in the semantic web are described fundamental aspects of ontologies are presented as well as the basic semantic and structural characteristics of traditional languages
la carencia de un modelo bien definido de representacin de la informacin en la web ha trado consigo problemas de cara a diversos aspectos relacionados con su procesamiento para intentar solucionarlos el organismo encargado de guiar la evolucin de la web ha propuesto su transformacin hacia una nueva web denominada web semntica en este trabajo se presentan las posibilidades que ofrece este nuevo escenario as como las dificultades para su consecucin prestando especial atencin a las ontologas herramientas de representacin del conocimiento fundamentales para la web semntica por ltimo se analiza el papel del profesional de la biblioteconoma y documentacin en este nuevo entorno the lack of a well defined model of information representation on the web has produced several problems related to processing information in an effort to resolve these problems the has proposed the semantic web project this new scenario offers both possibilities and difficulties for the future special attention is given to ontologies fundamental tools for the representation of knowledge on the semantic web finally the role of library and information professionals is considered in this context
despite its long existence and international acceptance network theory and analysis is a practically unknown approach in documentation both theoretically and methodologically speaking fortunately this trend is changing inasmuch as network theory and analysis may mean a quantitative and qualitative leap forward in the representation and analysis of the structure of all types of scientific domains whether geographic thematic or institutional the extraordinary advances that have taken place in recent years in the study and analysis of complex networks have been made possible by a number of parallel developments first of all with computerized data acquisition and handling large databases can be managed leading to the emergence of different real network topologies secondly the increase in computing power has made it possible to explore networks with millions of nodes thirdly there is the slow but sure breakdown of boundaries between disciplines this can be seen by researchers because of their ability to access and use databases that facilitate an understanding of the generic properties of networks
pnas ten complete mammalian genome sequences were compared by using the feature frequency profile ffp method of alignment free comparison this comparison technique reveals that the whole nongenic portion of mammalian genomes contains evolutionary information that is similar to their genic counterpartsthe intron and exon regions we partitioned the complete genomes of mammals such as human chimp horse and mouse into their constituent nongenic intronic and exonic components phylogenic species trees were constructed for each individual component class of genome sequence data as well as the whole genomes by using standard tree building algorithms with ffp distances the phylogenies of the whole genomes and each of the component classes exonic intronic and nongenic regions have similar topologies within the optimal feature length range and all agree well with the evolutionary phylogeny based on a recent large dataset multispecies and multigene based alignment in the strictest sense the ffp based trees are genome phylogenies not species phylogenies however the species phylogeny is highly related to the whole genome phylogeny furthermore our results reveal that the footprints of evolutionary history are spread throughout the entire length of the whole genome of an organism and are not limited to genes introns or short highly conserved nongenic sequences that can be adversely affected by factors such as a choice of sequences homoplasy and different mutation rates resulting in inconsistent phylogenies
gr genes interact in networks to orchestrate cellular processes analysis of these networks provides insights into gene interactions and functions here we took advantage of normal variation in human gene expression to infer gene networks which we constructed using correlations in expression levels of more than million gene pairs in immortalized b cells from three independent samples the resulting networks allowed us to identify biological processes and gene functions among the biological pathways we found processes such as translation and glycolysis that co occur in the same subnetworks we predicted the functions of poorly characterized genes including and and provided experimental evidence that is part of the endoplasmic reticulum associated secretory pathway we also found that a susceptibility gene of type diabetes interacts with which plays a role in glucose transport furthermore genes that predispose to the same diseases are clustered nonrandomly in the coexpression network suggesting that networks can provide candidate genes that influence disease susceptibility therefore our analysis of gene coexpression networks offers information on the role of human genes in normal and processes
we demonstrate that genome sequences approaching finished quality can be generated from short paired reads using base fragment and base jumping reads from five microbial genomes of varied gc composition and sizes up to mb generated assemblies with long accurate contigs and scaffolds velvet and euler sr were less accurate for example for escherichia coli the fraction of kb stretches that were perfect was velvet and sr
although widely used in practice the behavior and accuracy of the popular module identification technique called modularity maximization is not well understood here we present a broad and systematic characterization of its performance in practical situations first we generalize and clarify the recently identified resolution limit phenomenon second we show that the modularity function q exhibits extreme degeneracies that is the modularity landscape admits an exponential number of distinct high scoring solutions and does not typically exhibit a clear global maximum third we derive the limiting behavior of the maximum modularity for infinitely modular networks showing that it depends strongly on the size of the network and the number of module like subgraphs it contains finally using three real world examples of metabolic networks we show that the degenerate solutions can fundamentally disagree on the composition of even the largest modules together these results significantly extend and clarify our understanding of this popular method in particular they explain why so many heuristics perform well in practice at finding high scoring partitions why these heuristics can disagree on the composition of the identified modules and how the estimated value of should be interpreted further they imply that the output of any modularity maximization procedure should be interpreted cautiously in scientific contexts we conclude by discussing avenues for mitigating these behaviors such as combining information from many degenerate solutions or using models
background the development of effective environmental shotgun sequence binning methods remains an ongoing challenge in algorithmic analysis of metagenomic data while previous methods have focused primarily on supervised learning involving extrinsic data a first principles statistical model combined with a self training fitting method has not yet been developed results we derive an unsupervised maximum likelihood formalism for clustering short sequences by their taxonomic origin on the basis of their k mer distributions the formalism is implemented using a markov chain monte carlo approach in a k mer feature space we introduce a space transformation that reduces the dimensionality of the feature space and a genomic fragment divergence measure that strongly correlates with the method s performance pairwise analysis of over completely sequenced genomes reveals that the vast majority of genomes have sufficient genomic fragment divergence to be amenable for binning using the present formalism using a high performance implementation the binner is able to classify fragments as short as nt with accuracy over in simulations of low complexity communities of to species given sufficient genomic fragment divergence the method is available as an open source package called likelybin conclusion an unsupervised binning method based on statistical signatures of short environmental sequences is a viable stand alone binning method for low complexity samples for medium and high complexity samples we discuss the possibility of combining the current method with other methods as part of an iterative process to enhance the resolving power of sorting reads into taxonomic and or bins
caloric restriction cr protects against aging and disease but the mechanisms by which this affects mammalian life span are unclear we show in mice that deletion of ribosomal protein kinase a component of the nutrient responsive mtor mammalian target of rapamycin signaling pathway led to increased life span and resistance to age related pathologies such as bone immune and motor dysfunction and loss of insulin sensitivity deletion of induced gene expression patterns similar to those seen in cr or with pharmacological activation of adenosine monophosphate amp activated protein kinase ampk a conserved regulator of the metabolic response to cr our results demonstrate that influences healthy mammalian life span and suggest that therapeutic manipulation of and ampk might mimic cr and could provide broad protection against diseases of science
escherichia coli serves as an excellent model for the study of fundamental cellular processes such as metabolism signalling and gene expression understanding the function and organization of proteins within these processes is an important step towards a systems view of e coli integrating experimental and computational interaction data we present a reliable network of functional interactions between e coli proteins of its proteome these were combined with a recently generated set of high quality physical interactions between proteins and clustered to reveal discrete modules in addition to known protein complexes e g rna and dna polymerases we identified modules that represent biochemical pathways e g nitrate regulation and cell wall biosynthesis as well as batteries of functionally and evolutionarily related processes to aid the interpretation of modular relationships several case examples are presented including both well characterized and novel biochemical systems together these data provide a global view of the modular organization of the e coli proteome and yield unique insights into structural and evolutionary relationships in networks
for almost years topological analysis of different large scale biological networks metabolic reactions protein interactions transcriptional regulation has been highlighting some recurrent properties power law distribution of degree scale freeness small world which have been proposed to confer functional advantages such as robustness to environmental changes and tolerance to random mutations stochastic generative models inspired different scenarios to explain the growth of interaction networks during evolution the power law and the associated properties appeared so ubiquitous in complex networks that they were qualified as universal laws however these properties are no longer observed when the data are subjected to statistical tests in most cases the data do not fit the expected theoretical models and the cases of good fitting merely result from sampling artefacts or improper data representation the field of network biology seems to be founded on a series of myths i e widely believed but false ideas the weaknesses of these foundations should however not be considered as a failure for the entire domain network analysis provides a powerful frame for understanding the function and evolution of biological processes provided it is brought to an appropriate level of description by focussing on smaller functional modules and establishing the link between their topological properties and their behaviour
background the breadth of biological databases and their information content continues to increase exponentially unfortunately our ability to query such sources is still often suboptimal here we introduce and apply community voting database driven text classification and visual aids as a means to incorporate distributed expert knowledge to automatically classify database entries and to efficiently retrieve them results using a previously developed peptide database as an example we compared several machine learning algorithms in their ability to classify abstracts of published literature results into categories relevant to peptide research such as related or not related to cancer angiogenesis molecular imaging etc ensembles of bagged decision trees met the requirements of our application best no other algorithm consistently performed better in comparative testing moreover we show that the algorithm produces meaningful class probability estimates which can be used to visualize the confidence of automatic classification during the retrieval process to allow viewing long lists of search results enriched by automatic classifications we added a dynamic heat map to the web interface we take advantage of community knowledge by enabling users to cast votes in web style in order to correct automated classification errors which triggers reclassification of all entries we used a novel framework in which the database drives the entire vote aggregation and reclassification process to increase speed while conserving computational resources and keeping the method scalable in our experiments we simulate community voting by adding various levels of noise to nearly perfectly labelled instances and show that under such conditions classification can be improved significantly conclusion using pepbank as a model database we show how to build a classification aided retrieval system that gathers training data from the community is completely controlled by the database scales well with concurrent change events and can be adapted to add text classification capability to other biomedical databases the system can be accessed at http pepbank mgh edu
aggregate results from genome wide association studies gwas such as genotype frequencies for cases and controls were until recently often made available on public because they were thought to disclose negligible information concerning an individual s participation in a study homer et al recently suggested that a method for forensic detection of an individual s contribution to an admixed dna sample could be applied to aggregate gwas data using a likelihood based statistical framework we developed an improved statistic that uses genotype frequencies and individual genotypes to infer whether a specific individual or any close relatives participated in the gwas and if so what the participant s phenotype status is our statistic compares the logarithm of genotype frequencies in contrast to that of homer et al which is based on differences in either snp probe intensity or allele frequencies we derive the theoretical power of our test statistics and explore the empirical performance in scenarios with varying numbers of randomly chosen or top snps
gr genome assemblies are now available for nine primate species and large scale sequencing projects are underway or approved for six others an explicitly evolutionary and phylogenetic approach to comparative genomics called phylogenomics will be essential in unlocking the valuable information about evolutionary history and genomic function that is contained within these genomes however most phylogenomic analyses so far have ignored the effects of variation in ancestral populations on patterns of sequence divergence these effects can be pronounced in the primates owing to large ancestral effective population sizes relative to the intervals between speciation events in particular local genealogies can vary considerably across loci which can produce biases and diminished power in many phylogenomic analyses of interest including phylogeny reconstruction the identification of functional elements and the detection of natural selection at the same time this variation in genealogies can be exploited to gain insight into the nature of ancestral populations in this perspective i explore this area of intersection between phylogenetics and population genetics and its implications for primate phylogenomics i begin by lifting the hood on the conventional tree like representation of the phylogenetic relationships between species to expose the population genetic processes that operate along its branches next i briefly review an emerging literature that makes use of the complex relationships among coalescence recombination and speciation to produce inferences about evolutionary histories ancestral populations and natural selection finally i discuss remaining challenges and future prospects at this nexus of phylogenetics population genetics genomics
this paper aims to explore how the principles of a well known web service the worlds largest social music service last fm www last fm can be applied to research which potential it could have in the world of research e g an open and interdisciplinary database usage based reputation metrics and collaborative filtering and which challenges such a model would face in academia a real world application of these principles mendeley www mendeley com will be demoed at the ieee e conference
microbial engineering often requires fine control over protein expressionfor example to connect genetic or control flux through a metabolic to circumvent the need for trial and error optimization we developed a predictive method for designing synthetic ribosome binding sites enabling a rational control over the protein expression level experimental validation of predictions in escherichia coli showed that the method is accurate to within a factor of over a range of fold the design method also correctly predicted that reusing identical ribosome binding site sequences in different genetic contexts can result in different protein expression levels we demonstrate the method s utility by rationally optimizing protein expression to connect a genetic sensor to a synthetic circuit the proposed forward engineering approach should accelerate the construction and systematic optimization of large systems
summarybackgroundmicrobes engage in a remarkable array of cooperative behaviors secreting shared proteins that are essential for foraging shelter microbial warfare and virulence these proteins are costly rendering populations of cooperators vulnerable to exploitation by nonproducing cheaters arising by gene loss or migration in such conditions how can cooperation persist resultsour model predicts that differential gene mobility drives intragenomic variation in investment in cooperative traits more mobile loci generate stronger among individual genetic correlations at these loci higher relatedness and thereby allow the maintenance of more cooperative traits via kin selection by analyzing escherichia genomes we confirm that genes coding for secreted proteinsthe secretomeare very frequently lost and gained and are associated with mobile elements we show that homologs of the secretome are overrepresented among human gut metagenomics samples consistent with increased relatedness at secretome loci across multiple species the biosynthetic cost of secreted proteins is shown to be under intense selective pressure even more than for highly expressed proteins consistent with a cost of cooperation driving social dilemmas finally we demonstrate that mobile elements are in conflict with their chromosomal hosts over the chimeric ensemble s social strategy with mobile elements enforcing cooperation on their otherwise selfish hosts via the cotransfer of secretome genes with mafia strategy addictive systems toxin antitoxin and restriction modification conclusionour analysis matches the predictions of our model suggesting that horizontal transfer promotes cooperation as transmission increases local genetic relatedness at mobile loci and enforces cooperation on the resident genes as a consequence horizontal transfer promoted by agents such as plasmids phages or integrons drives microbial cooperation introductionhumans live in intimate mutualism with many microbes that are important for nutrient uptake and to stabilize niches prone to invasion by pathogens the human gastrointestinal tract records the highest cell densities for any known ecosystem with individuals from more than species that cooperate and compete while interacting with the host among them escherichia coli the workhorse of molecular biology is a major colonizer of the human gut where it establishes associations that are most frequently commensal but that can in some cases be highly virulent the genome of e coli shows a remarkable variability of its gene repertoire and e coli genomes have an average of protein coding genes of which many are strain specific and less than half are ubiquitous the core genome this dynamic gives the species great adaptability and ecological diversity the outcome of its interaction with the host depends on its ability to adhere to cell surfaces colonize tissues and produce metabolites and on its interplay with other bacteria many of the genes associated with ecological interactions are present in the genomes of both commensal and pathogenic strains suggesting that this species like many others should be regarded as a complex of strains oscillating between commensalism and pathogenicity and many interactions between e coli and other microbes eukaryotes or abiotic factors depend on secreted or outer envelope exposed molecules e g and these molecules although presumably costly to produce generate a range of benefits to any neighboring bacteria that are suitably equipped to profit from the expression of these molecules for example the molecules may scavenge for scarce resources e g siderophores aid in the construction of biofilms e g adhesive polymers kill competing lineages e g bacteriocins or aid direct exploitation of a host e g shiga toxins diffusion rates will have an important role in evolution of the trait thus outer membrane proteins are likely to benefit the most neighboring cells whereas secreted proteins may diffuse farther in the environment the shared rewards generated by these surface and secreted molecules ensures that their producers are prone to exploitation by nonproducer cheaters that prosper at the expense of more cooperative individuals the above traits are all cooperative from the perspective of individual cells that are equipped to benefit from the shared molecules for example bacteriocin production is aimed at eliminating competitors in the community and can be viewed as spiteful but it is still a cooperative trait among the individuals carrying the appropriate production and resistance genes a thorough discussion of logical identities between altruism and spite can be found elsewhere the recognition that microbial shared surface or secreted molecules are public goods vulnerable to exploitation by cheaters brings to the fore the need to understand social dilemmas raised by microbial cooperation and the evolution of cooperation can be favored by both nepotism and self interest cooperation may be self interested if it directly benefits the actor more extreme forms of altruistic cooperation may also be evolutionarily favored when they differentially help recipients who are statistically more likely to share the altruistic gene i e self interest returns at the level of the gene yet when genomes are highly dynamic and environments very diverse such as in microbial populations in the human gut how can cooperation persist in the face of cheaters constantly arising by gene loss or migration initial theoretical work has suggested that the invasion of cheaters in a population of cooperators would be prevented if the social trait were coded in conjugative plasmids because cheaters created by plasmid loss would re acquire the trait by reinfection with the plasmid sociality would be restored because of the infectivity of the social trait a key assumption of this model is that all plasmids carry the cooperative trait so any act of infection will also increase cooperation but what of the social dilemma between cooperative and cheating traits played out at the level of the mobile element in principle cheaters created by loss of the cooperative gene but not of the whole plasmid will still be able to invade because they are both infectious and social cheaters benefiting from the cooperative actions of their neighbors in order to understand the fate of cooperative traits in this broader strategy set allowing for both cooperative and noncooperative mobile elements we must return to the basic principles of social evolution theory and askwhat are the inclusive fitness consequences of carrying a cooperative trait as a function of gene mobility resultsa general result of social evolutionary theory is that an altruistic gene which confers a benefit b on another individual at a cost c to an actor can spread in a population if rb c where r is the genetic relatedness between two individuals and is measured with respect to the locus controlling the behavior in question our model fully described in supplemental data available online uses a standard recursion equation for relatedness in a patch structured population assuming a basic life cycle where individuals reproduce interact and migrate and finally population regulation occurs we extend this recursion to allow for horizontal gene transfer based on the formalism of unbiased horizontal transmission of cultural traits where the change in frequency of the horizontally transferred traits depends purely on its frequency in the local population and not on any allelic value the probability that two individuals carrying distinct alleles become identical at the focal locus in one time step will depend on gene mobility at this locus and the within patch homogeneity at this locus r t for a patch of a given size n the probability that two individuals carrying identical alleles remain identical in one time step depends on the rate of gene loss s when both within patch gene mobility and segregation loss tend to zero and the recursion equation converges to an equilibrium at r n where m is the among patch migration rate capturing relatedness or fst as a function of deme size and migration under purely vertical transmission incorporating genome dynamics into our calculation of relatedness within patches shows that horizontal transfer increases relatedness whereas gene loss reduces it because increased local relatedness favors cooperation and we conclude that horizontally transferred genes will be more likely to code for cooperative traits than those that are less infectiously mobile to test our model we inferred protein localization in genomes we analyzed e coli genomes and their plasmids and also e fergusonii which is the closest outgroup table gene dynamics in e coli is high enough to change significantly even at short time spans providing opportunity for social traits to be gained exchanged and lost to avoid overrepresenting gene families that have endured extensive recent duplication e g transposable elements we put together very closely related protein sequence identity families of orthologs proteins within a family henceforth named equivalogs are assumed to have similar functions and localization we inferred protein localization via psortb and secretion by type secretion systems via sequence similarity to known effectors by using a conservative approach see experimental procedures we inferred the localization of families for a total of proteins table this corresponds to of all families of equivalogs and of all genes as expected the majority of proteins are cytoplasmic many are associated with the inner membrane and proteins localized in the periplasm in the outer membrane and secreted are rarer secreted and outer membrane proteins are much less frequently ubiquitous i e present in the core genome or ancestral i e present in the last common ancestor of e coli than expected both p test inner membrane periplasmic and cytoplasmic localizations have similar fractions of genes in the core genome whereas only of outer membrane proteins and of secreted proteins are in the core genome the frequency with which external proteins are gained and lost is thus consistent with our prediction that cooperative traits are mobile this pattern of frequent gene gain and loss is entirely consistent with both theory and experimental evolution studies on microbial social traits which have repeatedly revealed how readily selection on cooperative traits can be reversed as a function of small changes to population structure e g and our hypothesis depends on the existence of genes coding for public goods especially for nonancestral secreted and outer membrane proteins in e coli s environment because many horizontally transferred genes lack homologs in the current databases we searched for homologs of e coli proteins from the secretome or from other localization in a large proteins human gut metagenome sample we first checked that the secretome genes had family sizes not significantly different from the remaining localizations p for both student s t and wilcoxon tests we then observed that in spite of similar family sizes many more genes of the secretome had homologs in the data set than for the other localizations p in all cases some of the hits might correspond to e coli cells in the sample our model assumes that secretome genes are present in other bacteria in the environment but makes no specific prediction about these genes occurring in the same or different species nevertheless we tried to further detail this point by estimating the number of e coli proteins in the metagenomic sample for this we separated the adult and infant data sets where e coli proportions are bacteria from the unweaned baby data set where e coli is more abundant the former contains proteins leading to an expected number of e coli proteins present in the metagenomic sample of which three are secreted or outer membrane associated thus when we find that most of the secretome has homologs in the environment this almost never corresponds to e coli genes present in the metagenomics sample but instead to hits to genes in other species genomes this suggests that cooperation by mobilization of public goods crosses species barriers establishing relatedness between previously unrelated bacteria many of the proteins we predict as secreted are annotated as virulence factors consistent with the view that microbial virulence is driven by cooperative bacterial traits and yet virulence factor labels often mask broader social traits for example we identified a secreted flagellin in enteroaggregative strains flagellins are involved in the inflammation process but they are also involved in immunomodulation by probiotic e coli strains similarly the toxins secreted by the enterohemorrhagic strain which are mobilized by a prophage have no effect in cattle where the strain isa commensal but can be highly virulent in humans this shows the thin line between mutualism and antagonism and suggests a broader and potentially multivalent role of secreted proteins in social interactions we found four times less effectors in commensals than in pathogens p in line with the available evidence for the role of in virulence but not in commensalism in e coli excluding the small set of effectors there is no significant difference in secretome size between commensals and pathogens both p secreted and outer membrane proteins perform a variety of functions that are not necessarily related with virulence even if their role in virulence is well described interestingly shigella stands out as having three times less secreted proteins than expected given genome size p shigella thrives within eukaryotic cells where they have little opportunity for social interactions the ensuing lower rates of transfer might then lead to loss of cooperative traits in these strains in spite of the strain s virulence and in agreement with our model these results highlight the ubiquity of microbial social life bacteria are social engineers and this engineering poses social dilemmas that affect but are not limited to virulence secreted proteins are metabolically costly for the producer they are poorly or not recycled and because they are public goods posing social dilemmas they are potentially rewarding for the cheaters they should therefore present traces of selection for the use of biosynthetically inexpensive amino acids the e coli biosynthetic cost of each amino acid from basic precursors was used to compute an average cost for each residue in each protein as in we found that proteins were cheaper when they were exterior this difference remains unchanged when controlling for gene expression levels or ancestrality and is even higher when controlling for g c content membrane proteins were compared separately because they have peculiar structures and cannot meaningfully be compared with the other proteins in terms of amino acid composition inner membrane proteins are more expensive than outer membrane proteins p wilcoxon test cytoplasmic proteins are more expensive than periplasmic and these in turn are more expensive than secreted proteins which are the least expensive of all proteins p wilcoxon test this suggests a selection gradient for lower cost in the most external proteins in both membrane associated and nonassociated proteins hence secreted proteins endure the strongest selection for low biosynthetic cost consistent with their potential exposure to social exploitation by nonproducer cheaters in fact secreted and outer membrane proteins are cheaper than highly expressed proteins suggesting that localization is more important than expressiveness in leading to selection for inexpensive amino acids indeed although the linear regression of expression levels measured by the codon adaptation index explains less than of the variance in protein cost the anova of the protein localization on protein cost accounts for the strong association between protein cost and localization shows that the costs of public good provision can be partly alleviated by selection of less costly amino acids our model suggests that relatedness increases because of horizontal gene transfer in consequence mobile genes should be more likely to offset the costs of investing in a cooperative trait via greater inclusive fitness benefits these results are expected to be applicable to mobile elements in general even integrative ones as long as they are not strongly deleterious therefore our prediction is that cooperative traits should be preferably coded in the most mobilizable regions of genomes some regions of the e coli genome constitute transfer hotspots whereas others are very stable indeed hotspots accumulate more than of all variable genes in the chromosomes of the strains analyzed in our study therefore genes are highly mobile if they are in plasmids very mobile if they are in hotpots and weakly mobile if they are in the remaining genome coldspots naturally for this analysis we removed all nonvolatile genes i e the core genome we then analyzed the position in the chromosome of genes coding for proteins with inferred localization secreted and outer membrane protein coding genes are more frequent in plasmids of all genes than in hotspots and than in coldspots where they are three times less frequent than in plasmids p test the effect is nearly two times stronger in secreted than in outer membrane proteins this confirms the prediction of the model that secreted and outer membrane proteins are more often located in more mobile regions plasmids contain times fewer genes than hotspots and times fewer than coldspots therefore even if relatively fewer genes coding for external proteins are in chromosomes these still account for a large fraction of the secretome both in hotspots and in coldspots some elements are more prone to mobilization because they are colocalized with integrases that facilitate the integration of such regions in other genomes we found a very high colocalization of integrases with secreted proteins and to a lesser extent with outer membrane proteins p wilcoxon test this further supports our model expectation that cooperative traits are encoded in regions that are highly mobile the frequency of conjugation depends on the donor but very little on the recipient bacterium and favoring the spread of cooperative genes and consequently the imposition of cooperator phenotypes on what were previously defector cells many species maintain cooperation via strategies of enforcement e g sanctions policing punishment and whereas most enforcement strategies entail death or isolation for defectors the transmission of mobile genes acts to enforce cooperation by reprogramming defectors however the volatility of mobile genetic elements may lead to a high rate of gene deletion with subsequent production of new cheaters represented by the segregation rate s in our model the maintenance of cooperation would be further facilitated if social traits were encoded along with stabilizing genetic elements type ii restriction modification and toxin antitoxin systems have been shown experimentally to result in the stabilization of plasmids but also chromosomally encoded laterally transferred genes and we therefore analyzed how these genes colocalized in the chromosome with genes not in the core genome and for which we could infer protein localization we found a highly significant copositioning of these stabilizing elements with the genes coding for secreted proteins and to a lesser extent with genes coding for outer membrane proteins p wilcoxon test the copositioning of mobile cooperative genes with addiction and restriction modification complexes will act to further enhance the enforcement of cooperation on the chimeric bacterial individual by punishing the loss of the cooperative trait discussionthe average human body carries asymptomatically more than coli cells yet infections by strains of e coli result in nearly one billion cases of diarrhea per year leading to more than a million human deaths shifts between mutualism and parasitism are largely the result of complex social interactions among microbes and the human host besides enterobacteria they also concern bacteria notorious for having high rates of genetic transfer such as neisseria streptococcus staphylococcus helicobacter or bacteroides most strains in these genera are commensal and are carried by a large percentage of the population but some strains in some circumstances can be deadly pathogens the disturbance in the social network of commensals caused by the loss of cooperative traits and the subsequent demographic depression may open the niche for other bacteria eventually pathogens inversely the fitness loss associated with the disruption of cooperative behavior of pathogens may facilitate therapy e g by weakening their invasive potential it is therefore important to know how networks of cooperators can be stably maintained or disrupted for public health reasons our results suggest that this may be of general utility to manipulate microbial social behavior the statistical association between gene mobility and engagement with the secretome is open to additional explanations some outer membrane proteins in bacterial pathogens and commensals are recognized by the immune system by grazing protozoa or by phages and are thus under diversifying or frequency dependent selection and in the best studied cases this leads to the selection of simple sequence repeats that generate variability at the promoter or at the protein sequence level to selection for gene multiplicity for gene conversion between homologous sequences or to signs of positive selection in the vast majority of cases this affects outer membrane proteins although diversifying selection surely acts upon some e coli outer membrane proteins it is unlikely to fully explain the patterns we observe in because the effect is especially strong among secreted not outer membrane proteins also e coli codes for few simple sequence repeats few large repeats putatively involved in gene conversion and few genes under strong positive selection suggesting that diversifying selection may not be so important in this species finally although immune challenges are more important for pathogens than for commensals we found no association between pathogenicity and secretome size in fact the highly virulent shigella strains have fewer outer membrane proteins than the remaining genomes further work will be necessary to disentangle the effects of different types of selection on the patterns found in more importantly even if all genes not in the core genome were under other forms of diversifying selection the secretome would still face social dilemmas the benefits of the secretome e g nutrient acquisition are indeed likely to be environment specific whereas the benefits of the core genome replication repair etc are more likely to be environment independent the fluctuating rewards of investment in the secretome may therefore promote their flexible gain and loss as part of an accessory genome yet within this accessory genome putatively under the action of diversifying frequency dependent or weak selection we systematically find deviations in the sense predicted by our model i e among noncore genes the ones coding for the secretome which are the ones most facing social dilemmas are more frequently in the environment in mobile elements and associated with enforcement factors although the inconsistent and accessory nature of many secretome traits may contribute to their emergence on mobile elements their maintenance will depend strongly on their resilience in the face of social cheatersbecause whenever they generate rewards these rewards will by their extracellular nature be shared therefore the social forces shaped by gene mobility and extracellular benefits outlined in our study will remain central to any understanding of the maintenance of social traits on mobile genes given the highly promiscuous nature of horizontal gene transfer in prokaryotes mobile genetic elements may bring inclusive fitness benefits to cooperation among species by helping heterospecifics that also contain the element the element e g a plasmid benefits a copy of itself plasmids and conjugative transposons are often highly promiscuous and their rates of exchange can be as high or higher between species than among conspecifics some plasmids have very broad ranges and can autonomously transfer into hosts that have diverged billions of years ago and for example transfer ranges of incp plasmids encompass proteobacteria firmicutes bacteroides and yeasts thus potentially all major microbial constituents of the human gut conjugation itself promotes biofilms that may further facilitate horizontal transfer our observation that social traits coded in e coli are frequent in the environment and overrepresented in its mobile elements opens the possibility of kin selection defined as the process by which inclusive fitness is maximized among these organisms broadening the conceptual framework for the evolutionary study of between species mutualisms as a result our analysis does not simply concern cooperation between a few strains of e coli but should be interpreted in a more global view of cooperation between microbial species related via their shared mobile elements our model relies on frequent transfer between microbes for the maintenance of cooperation further work will be necessary to determine what are the minimal rates of transfer to maintain cooperation in e coli s primary or secondary habitats one can suppose two extreme values for the time frame of transfer in the primary habitat dominant e coli strains persist for months in the host which is an upper bound and intestinal transit times average hr in humans which is a lower bound conjugation and transduction occur both in the lab and invivo at rates compatible with the lower bound given by intestinal transit and our results bring new light to the role of vehicles of lateral transfer such as plasmids integrative conjugative elements or temperate phages and help explain why they are so prevalent in bacterial populations and temperate phages code for a range of social traits e g toxins and have been demonstrated to generate higher relatedness coefficients at prophage loci than at their host chromosomes competition experiments between prophage carrying lysogenic and nonlysogenic bacteria resulted in the maintenance of host chromosomal diversity apart from monomorphism at the prophage loci integrons which are often coded in plasmids code for many proteins with peptide signals thus probably secreted these and other integrative highly dynamic gene capture systems may therefore constitute powerful generators of microbial social networks if plasmids are major players in the dynamics of social interactions as our data suggest it is essential to further characterize the diversity of genetic information they code for cooperation mediated by secreted proteins is metabolically costly protein biosynthesis is among the costliest of cellular processes because of both the costs of the raw materials amino acids and the costs of synthesis gene expression and the production of the necessary machinery recent data suggest that the process is more costly than the product among other reasons because amino acids can be recycled after proteolysis yet secreted proteins cannot be easily recycled and possibly so cannot the exterior domains of outer membrane proteins making them more expensive as a result we find evidence of selection for biosynthetically less expensive amino acids in these proteins that partly alleviate the burden of producing public goods naturally we cannot exclude that an unknown biophysical effect leads to the selection of biosynthetically inexpensive amino acids in the exact same way as predicted by our model the extent of selection for biosynthetic cost might seem surprising because accessory genes are traditionally supposed to be under weak selection yet we find evidence of selection for their amino acid composition the cost of secreted and outer membrane proteins might be under stronger selection as a result of both their limited recyclability and their involvement in cooperative processes vulnerable to exploitation by nonproducer cheats if cooperation is costly how can it arise and be maintained in bacterial populations enduring migration and gene deletions resulting in new cheaters we find on theoretical grounds that the mobilization of genes producing public goods favors the maintenance of cooperation by increasing relatedness at the mobile loci it is well established that relatedness favors investments in cooperation and by increasing relatedness at mobile loci between interacting individuals horizontal transfer should therefore favor cooperation coded by the mobile loci however the imposition of a social trait ona distinct recipient genetic background raises the potential for intragenomic conflict resulting ultimately from the intragenomic variation in mobility whereas the most mobile genes will gain a high inclusive fitness benefit defined by rb c from providing benefits to neighboring bacteria because they are more likely to carry a copy of the same gene other more static genes may experience a loss of inclusive fitness from aid to the same bacterial neighborsbecause they are less likely to be related at nonmobile lociand may therefore be selected to inhibit the uptake or carriage of more mobileand socialgenes we find that mobile genes act to enforce cooperation and on their chromosomal hosts via the cotransfer of cooperative genes with addictive systems that together impose a mafia and ultimatum to their host of cooperation or death thus we find that cooperation among bacterial individuals driven by mobile genetic elements is in turn the cause of conflict within these individuals between differentially mobile genes experimental procedures definition of the core genome and gene ancestrythe preliminary core and pan genome were computed as described elsewhere because paralogs with more than similarity and similar size are likely to have very similar functions and localizations we put such families together into families of equivalogs by using the pattern of presence absence of equivalogs we inferred their acquisition loss in the history of the e coli species by using maximum likelihood with the package ape in r we redid all the analysis with inferences of ancestrality based on orthologs instead of equivalogs with similar results data not shown the final core genome contains families with equivalogs in all genomes all genes of the core genome are inferred to be present in the ancestor genome definition of homologyprotein sequences of integrases and toxin antitoxin systems were fetched from swissprot http www expasy ch sprot type restriction modification systems from rebase http rebase neb com and effectors from homologs were searched in the genome with blastp and selected when e value we considered two genes as homologs if they aligned through more than of their sequence and were similar prediction of cellular localizationproteins specifically targeted for secretion can be identified from their sequences and we initially used more than programs to infer protein localization many of these programs provide putative localizations even when their likelihoods are low we wished to remain conservative in our analysis and use a reliable data set of proteins and therefore excluded them the other programs often gave similar results because psortb has been evaluated as one of the best performing programs we used it extensively the results of several tests of the psortb protein localization prediction program were integrated ecsvm the gram negative version of the support vector machine trained to identify extra cellular proteins omsvm the support vector machine trained to identify outer membrane proteins for gram negative bacteria ppsvm the support vector machine trained to identify periplasmic proteins for gram negative bacteria cmsvm the gram negative version of the support vector machine trained to identify cytoplasmic membrane proteins hmmtop which predicts transmembrane helices within the sequence where the presence of three or more transmembrane helices are predicted to belong to the inner membrane cytosvm the gram negative version of the support vector machine trained to identify cytoplasmic proteins and the final prediction we also used the tmhmm program to identify transmembrane helices in integral membrane proteins and kept proteins with at least three transmembrane helices type secretion systems have been thoroughly studied for their role in transport of effectors allowing the manipulation of eukaryotic cells by pathogenic e coli effectors can only be identified by sequence similarity with known effectors we therefore complemented our catalog of secreted proteins by searching for proteins similar to a recently published set of known effectors in proteobacteria metagenomic datathe contigs from healthy humans gut microbiomes of the human metagenome consortium japan hmgj http www metagenome jp were retrieved from genbank genes were identified with the program getorf from the emboss package http emboss sourceforge net acknowledgmentswe thank stuart west angus buckling jean marc ghigo didier mazel carmen bessa gomes jeff smith and benjamin le qur for comments suggestions and criticisms we thank sara silva for preparing the metagenomics data we thank the reviewers for challenging criticisms that significantly improved the manuscript authors have no financial conflict of interest t n was a recipient of a grant from fct sfrh bpd d j r is funded by the swiss national science foundation ambizione programme and the university of zrich forschungskredit s p b is funded by the wellcome trust e p c r and m t are funded by the cnrs and the pasteur
background hypothesis generation in molecular and cellular biology is an empirical process in which knowledge derived from prior experiments is distilled into a comprehensible model the requirement of automated support is exemplified by the difficulty of considering all relevant facts that are contained in the millions of documents available from pubmed semantic web provides tools for sharing prior knowledge while information retrieval and information extraction techniques enable its extraction from literature their combination makes prior knowledge available for computational analysis and inference while some tools provide complete solutions that limit the control over the modeling and extraction processes we seek a methodology that supports control by the experimenter over these critical processes results we describe progress towards automated support for the generation of biomolecular hypotheses semantic web technologies are used to structure and store knowledge while a workflow extracts knowledge from text we designed minimal proto ontologies in owl for capturing different aspects of a text mining experiment the biological hypothesis text and documents text mining and workflow provenance the models fit a methodology that allows focus on the requirements of a single experiment while supporting reuse and posterior analysis of extracted knowledge from multiple experiments our workflow is composed of services from the adaptive information disclosure application aida toolkit as well as a few others the output is a semantic model with putative biological relations with each relation linked to the corresponding evidence conclusion we demonstrated a do it yourself approach for structuring and extracting knowledge in the context of experimental research on biomolecular mechanisms the methodology can be used to bootstrap the construction of semantically rich biological models using the results of knowledge extraction processes models specific to particular experiments can be constructed that in turn link with other semantic models creating a web of knowledge that spans experiments mapping mechanisms can link to other knowledge resources such as obo ontologies or skos vocabularies aida web services can be used to design personalized knowledge extraction procedures in our example experiment we found three proteins nf kappa b and bax potentially playing a role in the interplay between nutrients and epigenetic regulation
recent advances in next generation sequencing have made it possible to precisely characterize all somatic coding mutations that occur during the development and progression of individual cancers here we used these approaches to sequence the genomes fold coverage and transcriptomes of an oestrogen receptor alpha positive metastatic lobular breast cancer at depth we found somatic non synonymous coding mutations present in the metastasis and measured the frequency of these somatic mutations in dna from the primary tumour of the same patient which arose years earlier five of the mutations in and were prevalent in the dna of the primary tumour removed at diagnosis years earlier six in and were present at lower frequencies were not detected in the primary tumour and two were undetermined the combined analysis of genome and transcriptome data revealed two new rna editing events that recode the amino acid sequence of and taken together our data show that single nucleotide mutational heterogeneity can be a property of low or intermediate grade primary breast cancers and that significant evolution can occur with progression
genome wide association studies have identified hundreds of genetic variants associated with complex human diseases and traits and have provided valuable insights into their genetic architecture most variants identified so far confer relatively small increments in risk and explain only a small proportion of familial clustering leading many to question how the remaining missing heritability can be explained here we examine potential sources of missing heritability and propose research strategies including and extending beyond current genome wide association approaches to illuminate the genetics of complex diseases and enhance its potential to enable effective disease prevention treatment
although autism is a highly heritable neurodevelopmental disorder attempts to identify specific susceptibility genes have thus far met with limited genome wide association studies using half a million or more markers particularly those with very large sample sizes achieved through meta analysis have shown great success in mapping genes for other complex genetic traits consequently we initiated a linkage and association mapping study using half a million genome wide single nucleotide polymorphisms snps in a common set of multiplex autism families affected offspring we identified regions of suggestive and significant linkage on chromosomes and respectively initial analysis did not yield genome wide significant associations however genotyping of top hits in additional families revealed an snp on chromosome between and that was significantly associated with autism p we also demonstrated that expression of is reduced in brains from autistic patients further implicating as an autism susceptibility gene the linkage regions reported here provide targets for rare variation screening whereas the discovery of a single novel association demonstrates the action of variants
background the rapid evolution of internet technologies and the collaborative approaches that dominate the field have stimulated the development of numerous bioinformatics resources to address this new framework several initiatives have tried to organize these services and resources in this paper we present the bioinformatics resource inventory biri a new approach for automatically discovering and indexing available public bioinformatics resources using information extracted from the scientific literature the index generated can be automatically updated by adding additional manuscripts describing new resources we have developed web services and applications to test and validate our approach it has not been designed to replace current indexes but to extend their capabilities with richer functionalities results we developed a web service to provide a set of high level query primitives to access the index the web service can be used by third party web services or web based applications to test the web service we created a pilot web application to access a preliminary knowledge base of resources we tested our tool using an initial set of abstracts almost of the resources described in the abstracts were correctly classified more than descriptions of functionalities were extracted conclusion these experiments suggest the feasibility of our approach for automatically discovering and indexing current and future bioinformatics resources given the domain independent characteristics of this tool it is currently being applied by the authors in other areas such as medical nanoinformatics biri is available at http edelman dia fi upm biri
structural variations of dna greater than kilobase in size account for most bases that vary among human genomes but are still relatively under ascertained here we use tiling oligonucleotide microarrays comprising probes to generate a comprehensive map of copy number variations cnvs greater than base pairs of which most have been validated independently for of these cnvs we generated reference genotypes from individuals of european african or east asian ancestry the predominant mutational mechanisms differ among cnv size classes retrotransposition has duplicated and inserted some coding and non coding dna segments randomly around the genome furthermore by correlation with known trait associated single nucleotide polymorphisms snps we identified loci with cnvs that are candidates for influencing disease susceptibility despite this having assessed the completeness of our map and the patterns of linkage disequilibrium between cnvs and snps we conclude that for complex traits the heritability void left by genome wide association studies will not be accounted for by cnvs
background the empirical frequencies of dna k mers in whole genome sequences provide an interesting perspective on genomic complexity and the availability of large segments of genomic sequence from many organisms means that analysis of k mers with non trivial lengths is now possible results we have studied the k mer spectra of more than species from archea bacteria and eukaryota particularly looking at the modalities of the distributions as expected most species have a unimodal k mer spectrum however a few species including all mammals have multimodal spectra these species coincide with the tetrapods genomic sequences are clearly very complex and cannot be fully explained by any simple probabilistic model yet we sought such an explanation for the observed modalities and discovered that low order markov models capture this property and some others fairly well conclusions multimodal spectra are characterized by specific ranges of values of c g content and of cpg dinucleotide suppression a range that encompasses all tetrapods analyzed other genomes like that of the protozoa entamoeba histolytica which also exhibits cpg suppression do not have multimodal k mer spectra groupings of functional elements of the human genome also have a clear modality and exhibit either a unimodal or multimodal behaviour depending on the two above values
progress in evolutionary genomics is tightly coupled with the development of new technologies to collect high throughput data the availability of next generation sequencing technologies has the potential to revolutionize genomic research and enable us to focus on a large number of outstanding questions that previously could not be addressed effectively indeed we are now able to study genetic variation on a genome wide scale characterize gene regulatory processes at unprecedented resolution and soon we expect that individual laboratories might be able to rapidly sequence new genomes however at present the analysis of next generation sequencing data is challenging in particular because most sequencing platforms provide short reads which are difficult to align and assemble in addition only little is known about sources of variation that are associated with next generation sequencing study designs a better understanding of the sources of error and bias in sequencing data is essential especially in the context of studies of variation at dynamic traits
we describe hi c a method that probes the three dimensional architecture of whole genomes by coupling proximity based ligation with massively parallel sequencing we constructed spatial proximity maps of the human genome with hi c at a resolution of megabase these maps confirm the presence of chromosome territories and the spatial proximity of small gene rich chromosomes we identified an additional level of genome organization that is characterized by the spatial segregation of open and closed chromatin to form two genome wide compartments at the megabase scale the chromatin conformation is consistent with a fractal globule a knot free polymer conformation that enables maximally dense packing while preserving the ability to easily fold and unfold any genomic locus the fractal globule is distinct from the more commonly used globular equilibrium model our results demonstrate the power of hi c to map the dynamic conformations of genomes
we describe a sensitive metabolite array for genome sequence independent functional analysis of metabolic phenotypes and networks the reactomes of cell populations and communities the array includes dye linked substrate compounds collectively representing central metabolic pathways of all forms of life application of cell extracts to the array leads to specific binding of enzymes to cognate substrates transformation to products and concomitant activation of the dye signals proof of principle was shown by reconstruction of the metabolic maps of model bacteria utility of the array for unsequenced organisms was demonstrated by reconstruction of the global metabolisms of three microbial communities derived from acidic volcanic pool deep sea brine lake and hydrocarbon polluted seawater enzymes of interest are captured on nanoparticles coated with cognate metabolites sequenced and their functions established
background recent improvements in dna microarray techniques have made a large variety of gene expression data available in public databases this data can be used to evaluate the strength of gene coexpression by calculating the correlation of expression patterns among different genes between many experiments however gene expression levels differ significantly across various tissues in higher organisms as well as in different cellular location in eukaryotes in different cell state thus the usual correlation measure can only evaluate the difference of tissues or cellular localizations and cannot adequately elucidate the functional relationship from the coexpression of genes method we propose a new measure of coexpression by expanding the generally used correlation into a multidimensional one we used principal component analyses to identify the major factors of gene expression correlation and then re calculate the correlation by subtracting the major components in order to remove biases cased by a few experiments the repeated subtractions of the major components yielded a set of correlation values for each pair of genes we observed the correlation changes when the first ten principal components were subtracted step by step in large scale arabidopsis expression data results we found two extreme patterns of correlation changes corresponding to stable and fragile coexpression our new indexes provided a good means to determine the functional relationships of the genes by examining a few examples and higher performance of gene ontology term prediction by using the support vector machine and the multidimensional correlation availability the results are available from the expression detail pages in atted ii http atted jp contact kinosita hgc jp supplementary information supplementary data are available at bioinformatics bioinformatics
the theoretical setting of hierarchical bayesian inference is gaining acceptance as a framework for understanding cortical computation in this paper we describe how bayesian belief propagation in a spatio temporal hierarchical model called hierarchical temporal memory htm can lead to a mathematical model for cortical circuits an htm node is abstracted using a coincidence detector and a mixture of markov chains bayesian belief propagation equations for such an htm node define a set of functional constraints for a neuronal implementation anatomical data provide a contrasting set of organizational constraints the combination of these two constraints suggests a theoretically derived interpretation for many anatomical and physiological features and predicts several others we describe the pattern recognition capabilities of htm networks and demonstrate the application of the derived circuits for modeling the subjective contour effect we also discuss how the theory and the circuit can be extended to explain cortical features that are not explained by the current model and describe testable predictions that can be derived from model
chronic fatigue syndrome cfs is a debilitating disease of unknown etiology that is estimated to affect million people worldwide studying peripheral blood mononuclear cells pbmcs from cfs patients we identified dna from a human gammaretrovirus xenotropic murine leukemia virus related virus xmrv in of patients as compared to of healthy controls cell culture experiments revealed that patient derived xmrv is infectious and that both cell associated and cell free transmission of the virus are possible secondary viral infections were established in uninfected primary lymphocytes and indicator cell lines after their exposure to activated pbmcs b cells t cells or plasma derived from cfs patients these findings raise the possibility that xmrv may be a contributing factor in the pathogenesis of science
in allostery a binding event at one site in a protein modulates the behavior of a distant site identifying residues that relay the signal between sites remains a challenge we have developed predictive models using support vector machines a widely used machine learning method the training data set consisted of residues classified as either hotspots or non hotspots based on experimental characterization of point mutations from a diverse set of allosteric proteins each residue had an associated set of calculated features two sets of features were used one consisting of dynamical structural network and informatic measures and another of structural measures defined by daily and gray the resulting models performed well on an independent data set consisting of hotspots and non hotspots from five allosteric proteins for the independent data set our top models using feature set recalled of known hotspots and among total hotspot predictions were actual hotspots hence these models have precision p and recall r the corresponding models for feature set had p and r we combined the features from each set that produced models with optimal predictive performance the top models using this hybrid feature set had r and p the best overall performance of any of the sets of models our methods identified hotspots in structural regions of known allosteric significance moreover our predicted hotspots form a network of contiguous residues in the interior of the structures in agreement with previous work in conclusion we have developed models that discriminate between known allosteric hotspots and non hotspots with high accuracy and sensitivity moreover the pattern of predicted hotspots corresponds to known functional motifs implicated in allostery and is consistent with previous work describing sparse networks of allosterically residues
virtual compound screening using molecular docking is widely used in the discovery of new lead compounds for drug design however this method is not completely reliable and therefore unsatisfactory in this study we used massive molecular dynamics simulations of protein ligand conformations obtained by molecular docking in order to improve the enrichment performance of molecular docking our screening approach employed the molecular mechanics poisson boltzmann and surface area method to estimate the binding free energies for the top ranking compounds obtained by docking to a target protein approximately molecular dynamics simulations were performed using multiple docking poses in about a week as a result the enrichment performance of the top compounds by our approach was improved by times that of the enrichment performance of molecular dockings this result indicates that the application of molecular dynamics simulations to virtual screening for lead discovery is both effective and practical however further optimization of the computational protocols is required for screening various proteins
background the kegg pathway database is a valuable collection of metabolic pathway maps nevertheless the production of simulation capable metabolic networks from kegg pathway data is a challenging complicated work regardless the already developed tools for this scope originally used for illustration purposes kegg pathways through kgml kegg markup language files can provide complete reaction sets and introduce species versioning which offers advantages for the scope of cellular metabolism simulation modelling in this project keggconverter is described implemented also as a web based application which uses as source kgml files in order to construct integrated pathway sbml models fully functional for simulation purposes results a case study of the integration of six human metabolic pathways from kegg depicts the ability of keggconverter to automatically produce merged and converted to sbml fully functional pathway models enhanced with default kinetics the suitability of the developed tool is demonstrated through a comparison with other state of the art relevant software tools for the same data fusion and conversion tasks thus illustrating the problems and the relevant workflows moreover keggconverter permits the inclusion of additional reactions in the resulting model which represent flux cross talk with neighbouring pathways providing in this way improved simulative accuracy these additional reactions are introduced by exploiting relevant semantic information for the elements of the kegg pathways database the architecture and functionalities of the web based application are presented conclusion keggconverter is capable of producing integrated analogues of metabolic pathways appropriate for simulation tasks by inputting only kgml files the web application acts as a user friendly shell which transparently enables the automated biochemically correct pathway merging conversion to sbml format proper renaming of the species and insertion of default kinetic properties for the pertaining reactions the tool is available at http www grissom keggconverter
hidden markov models hmms and their variants are widely used in bioinformatics applications that analyze and compare biological sequences designing a novel application requires the insight of a human expert to define the model s architecture the implementation of prediction algorithms and algorithms to train the model s parameters however can be a time consuming and error prone task we here present hmmconverter a software package for setting up probabilistic hmms pair hmms as well as generalized hmms and pair hmms the user defines the model itself and the algorithms to be used via an xml file which is then directly translated into efficient c code the software package provides linear memory prediction algorithms such as the hirschberg algorithm banding and the integration of prior probabilities and is the first to present computationally efficient linear memory algorithms for automatic parameter training users of hmmconverter can thus set up complex applications with a minimum of effort and also perform parameter training and data analyses for large data nar
the renewable based electricity generation technologies were assessed against a range of sustainability indicators using data obtained from the literature these indicators are cost of electricity generation greenhouse gas emissions and energy pay back time all the three parameters were found to have a very wide range for each technology for grading different renewable energy sources a new figure of merit has been proposed linking greenhouse gas emissions energy pay back time and cost of electricity generated by these renewable energy sources it has been found out that wind and small hydro are the most sustainable source for the generation
abstract background micrornas mirnas small non coding rnas of to nt play important roles in gene regulation in both animals and plants in the last few years the oligonucleotide microarray is one high throughput and robust method for detecting mirna expression however the approach is restricted to detecting the expression of known mirnas second generation sequencing is an inexpensive and high throughput sequencing method this new method is a promising tool with high sensitivity and specificity and can be used to measure the abundance of small rna sequences in a sample hence the expression profiling of mirnas can involve use of sequencing rather than an oligonucleotide array additionally this method can be adopted to discover novel mirnas results this work presents a systematic approach mirexpress for extracting mirna expression profiles from sequencing reads obtained by second generation sequencing technology a stand alone software package is implemented for generating mirna expression profiles from high throughput sequencing of rna without the need for sequenced genomes the software is also a database supported efficient and flexible tool for investigating mirna regulation moreover we demonstrate the utility of mirexpress in extracting mirna expression profiles from two illumina data sets constructed for the human and a plant species conclusions we develop mirexpress which is a database supported efficient and flexible tool for detecting mirna expression profile the analysis of two illumina data sets constructed from human and plant demonstrate the effectiveness of mirexpress to obtain mirna expression profiles and show the usability in finding mirnas
abstract background sequence similarity searching is an important and challenging task in molecular biology and next generation sequencing should further strengthen the need for faster algorithms to process such vast amounts of data at the same time the internal architecture of current microprocessors is tending towards more parallelism leading to the use of chips with two four and more cores integrated on the same die the main purpose of this work was to design an effective algorithm to fit with the parallel capabilities of modern microprocessors results a parallel algorithm for comparing large genomic banks and targeting middle range computers has been developed and implemented in plast software the algorithm exploits two key parallel features of existing and future microprocessors the simd programming model sse instruction set and the multithreading concept multicore compared to multithreaded blast software tests performed on an processor server have shown speedup ranging from to with a similar level of accuracy conclusions a parallel algorithmic approach driven by the knowledge of the internal microprocessor architecture allows significant speedup to be obtained while preserving standard sensitivity for similarity problems
background microarray experiments are increasing in size and samples are collected asynchronously over long time available data are re analysed as more samples are hybridized systematic use of collected data requires tracking of biomaterials array information raw data and assembly of annotations to meet the information tracking and data analysis challenges in microarray experiments we reimplemented and improved base version results the new base presented in this report is a comprehensive annotable local microarray data repository and analysis application providing researchers with an efficient information management and analysis tool the information management system tracks all material from biosource via sample and through extraction and labelling to raw data and analysis all items in base can be annotated and the annotations can be used as experimental factors in downstream analysis base stores all microarray experiment related data regardless if analysis tools for specific techniques or data formats are readily available the base team is committed to continue improving and extending base to make it usable for even more experimental setups and techniques and we encourage other groups to target their specific needs leveraging on the infrastructure provided by base conclusion base is a comprehensive management application for information data and analysis of microarray experiments available as free open source software at http base thep lu se under the terms of license
background the prediction of essential genes from molecular networks is a way to test the understanding of essentiality in the context of what is known about the network however the current knowledge on molecular network structures is incomplete yet and consequently the strategies aimed to predict essential genes are prone to uncertain predictions we propose that simultaneously evaluating different network structures and different algorithms representing gene essentiality centrality measures may identify essential genes in networks in a reliable fashion results by simultaneously analyzing different centrality measures on different reconstructed metabolic networks for saccharomyces cerevisiae we show that no single centrality measure identifies essential genes from these networks in a statistically significant way however the combination of at least centrality measures achieves a reliable prediction of most but not all of the essential genes no improvement is achieved in the prediction of essential genes when or centrality measures were combined conclusion the method reported here describes a reliable procedure to predict essential genes from molecular networks our results show that essential genes may be predicted only by combining centrality measures revealing the complex nature of the function of genes
motivation nowadays publishers of scientific journals face the tough task of selecting high quality articles that will attract as many readers as possible from a pool of articles this is due to the growth of scientific output and literature the possibility of a journal having a tool capable of predicting the citation count of an article within the first few years after publication would pave the way for new assessment systems results this article presents a new approach based on building several prediction models for the bioinformatics journal these models predict the citation count of an article within years after publication global models to build these models tokens found in the abstracts of bioinformatics papers have been used as predictive features along with other features like the journal sections and week post publication periods to improve the accuracy of the global models specific models have been built for each bioinformatics journal section data and text mining databases and ontologies gene expression genetics and population analysis genome analysis phylogenetics sequence analysis structural bioinformatics and systems biology in these new models the average success rate for predictions using the naive bayes and logistic regression supervised classification methods was and respectively within the nine sections and for year time horizon availability supplementary material on this experimental survey is available at http www dia fi upm es concha bioinformatics html contact aibanez fi upm bioinformatics
motivation next generation sequencing has become an important tool for genome wide quantification of dna and rna however a major technical hurdle lies in the need to map short sequence reads back to their correct locations in a reference genome here we investigate the impact of snp variation on the reliability of read mapping in the context of detecting allele specific expression ase results we generated million bp reads from mrna of each of two hapmap yoruba individuals when we mapped these reads to the human genome we found that at heterozygous snps there was a significant bias toward higher mapping rates of the allele in the reference sequence compared with the alternative allele masking known snp positions in the genome sequence eliminated the reference bias but surprisingly did not lead to more reliable results overall we find that even after masking approximately of snps still have an inherent bias toward more effective mapping of one allele filtering out inherently biased snps removes of the top signals of ase the remaining snps showing ase are enriched in genes previously known to harbor cis regulatory variation or known to show uniparental imprinting our results have implications for a variety of applications involving detection of alternate alleles from short read sequence data availability scripts written in perl and r for simulating short reads masking snp variation in a reference genome and analyzing the simulation output are available upon request from jfd raw short read data were deposited in geo http www ncbi nlm nih gov geo under accession number contact jdegner uchicago edu marioni uchicago edu gilad uchicago edu pritch uchicago edu supplementary information supplementary data are available at online
gr how many species inhabit our immediate surroundings a straightforward collection technique suitable for answering this question is known to anyone who has ever driven a car at highway speeds the windshield of a moving vehicle is subjected to numerous insect strikes and can be used as a collection device for representative sampling unfortunately the analysis of biological material collected in that manner as with most metagenomic studies proves to be rather demanding due to the large number of required tools and considerable computational infrastructure in this study we use organic matter collected by a moving vehicle to design and test a comprehensive pipeline for phylogenetic profiling of metagenomic samples that includes all steps from processing and quality control of data generated by next generation sequencing technologies to statistical analyses and data visualization to the best of our knowledge this is also the first publication that features a live online supplement providing access to exact analyses and workflows used in article
the human microbiome project hmp funded as an initiative of the nih roadmap for biomedical research http nihroadmap nih gov is a multi component community resource the goals of the hmp are to take advantage of new high throughput technologies to characterize the human microbiome more fully by studying samples from multiple body sites from each of at least normal volunteers to determine whether there are associations between changes in the microbiome and health disease by studying several different medical conditions and to provide both a standardized data resource and new technological approaches to enable such studies to be undertaken broadly in the scientific community the ethical legal and social implications of such research are being systematically studied as well the ultimate objective of the hmp is to demonstrate that there are opportunities to improve human health through monitoring or manipulation of the human microbiome the history and implementation of this new program are here
motivation much of a cell s regulatory response to changing environments occurs at the transcriptional level particularly in higher organisms transcription factors tfs micrornas and epigenetic modifications can combine to form a complex regulatory network part of this system can be modeled as a collection of regulatory modules co regulated genes the conditions under which they are co regulated and sequence level regulatory motifs results we present the combinatorial algorithm for expression and sequence based cluster extraction coalesce system for regulatory module prediction the algorithm is efficient enough to discover expression biclusters and putative regulatory motifs in metazoan genomes genes and very large microarray compendia conditions using bayesian data integration it can also include diverse supporting data types such as evolutionary conservation or nucleosome placement we validate its performance using a functional evaluation of co clustered genes known yeast and escherichea coli tf targets synthetic data and various metazoan data compendia in all cases coalesce performs as well or better than current biclustering and motif prediction tools with high accuracy in functional and tf target assignments and zero false positives on synthetic data coalesce provides an efficient and flexible platform within which large diverse data collections can be integrated to predict metazoan regulatory networks availability source code c is available at http function princeton edu sleipnir and supporting data and a web interface are provided at http function princeton edu coalesce contact ogt cs princeton edu hcoller princeton edu supplementary information supplementary data are available at online
electrophysiology the gold standard for investigating neuronal signalling is being challenged by a new generation of optical probes together with new forms of microscopy these probes allow us to measure and control neuronal signals with spatial resolution and genetic specificity that already greatly surpass those of electrophysiology we predict that the photon will progressively replace the electron for probing neuronal function particularly for targeted stimulation and silencing of neuronal populations although electrophysiological characterization of channels cells and neural circuits will remain necessary new combinations of electrophysiology and imaging should lead to transformational discoveries neuroscience
background in systems biology comparative analyses of molecular interactions across diverse species indicate that conservation and divergence of networks can be used to understand functional evolution from a systems perspective a key characteristic of these networks is their modularity which contributes significantly to their robustness as well as adaptability consequently analysis of modular network structures from a phylogenetic perspective may be useful in understanding the emergence conservation and diversification of functional modularity results in this paper we propose a phylogenetic framework for analyzing network modules with applications that extend well beyond network based phylogeny reconstruction our approach is based on identification of modular network components from each network separately followed by projection of these modules onto the networks of other species to compare different networks subsequently we use the conservation of various modules in each network to assess the similarity between different networks compared to traditional methods that rely on topological comparisons our approach has key advantages in i avoiding intractable graph comparison problems in comparative network analysis ii accounting for noise and missing data through flexible treatment of network conservation and iii providing insights on the evolution of biological systems through investigation of the evolutionary trajectories of network modules we test our method mophy on synthetic data generated by simulation of network evolution as well as existing protein protein interaction data for seven diverse species comprehensive experimental results show that mophy is promising in reconstructing evolutionary histories of extant networks based on conservation of modularity it is highly robust to noise and outperforms existing methods that quantify network similarity in terms of conservation of network topology conclusion these results establish modularity and network proximity as useful features in comparative network analysis and motivate detailed studies of the evolutionary histories of modules
dna cytosine methylation is a central epigenetic modification that has essential roles in cellular processes including genome regulation development and disease here we present the first genome wide single base resolution maps of methylated cytosines in a mammalian genome from both human embryonic stem cells and fetal fibroblasts along with comparative analysis of messenger rna and small rna components of the transcriptome several histone modifications and sites of dna protein interaction for several key regulatory factors widespread differences were identified in the composition and patterning of cytosine methylation between the two genomes nearly one quarter of all methylation identified in embryonic stem cells was in a non cg context suggesting that embryonic stem cells may use different methylation mechanisms to affect gene regulation methylation in non cg contexts showed enrichment in gene bodies and depletion in protein binding sites and enhancers non cg methylation disappeared upon induced differentiation of the embryonic stem cells and was restored in induced pluripotent stem cells we identified hundreds of differentially methylated regions proximal to genes involved in pluripotency and differentiation and widespread reduced methylation levels in fibroblasts associated with lower transcriptional activity these reference epigenomes provide a foundation for future studies exploring this key epigenetic modification in human disease development
advances in genetics and genomics have fuelled a revolution in discovery based or hypothesis generating research that provides a powerful complement to the more directly hypothesis driven molecular cellular and systems neuroscience genetic and functional genomic studies have already yielded important insights into neuronal diversity and function as well as disease one of the most exciting and challenging frontiers in neuroscience involves harnessing the power of large scale genetic genomic and phenotypic data sets and the development of tools for data integration and mining methods for network analysis and systems biology offer the promise of integrating these multiple levels of data connecting molecular pathways to nervous function
background although expression microarrays have become a standard tool used by biologists analysis of data produced by microarray experiments may still present challenges comparison of data from different platforms organisms and labs may involve complicated data processing and inferring relationships between genes remains difficult results starnet is a new web based tool that allows post hoc visual analysis of correlations that are derived from expression microarray data starnet facilitates user discovery of putative gene regulatory networks in a variety of species human rat mouse chicken zebrafish drosophila c elegans s cerevisiae arabidopsis and rice by graphing networks of genes that are closely co expressed across a large heterogeneous set of preselected microarray experiments for each of the represented organisms raw microarray data were retrieved from ncbi s gene expression omnibus for a selected affymetrix platform all pairwise pearson correlation coefficients were computed for expression profiles measured on each platform respectively these precompiled results were stored in a mysql database and supplemented by additional data retrieved from ncbi a web based tool allows user specified queries of the database centered at a gene of interest the result of a query includes graphs of correlation networks graphs of known interactions involving genes and gene products that are present in the correlation networks and initial statistical analyses two analyses may be performed in parallel to compare networks which is facilitated by the new heatseeker module conclusion starnet is a useful tool for developing new hypotheses about regulatory relationships between genes and gene products and has coverage for species interpretation of the correlation networks is supported with a database of previously documented interactions a test for enrichment of gene ontology terms and heat maps of correlation distances that may be used to compare two networks the list of genes in a starnet network may be useful in developing a list of candidate genes to use for the inference of causal networks the tool is freely available at http vanburenlab medicine tamhsc edu html and does not require registration
background to aid in bioinformatics data processing and analysis an increasing number of web based applications are being deployed although this is a positive circumstance in general the proliferation of tools makes it difficult to find the right tool or more importantly the right set of tools that can work together to solve real complex problems results magallanes magellan is a versatile platform independent java library of algorithms aimed at discovering bioinformatics web services and associated data types a second important feature of magallanes is its ability to connect available and compatible web services into workflows that can process data sequentially to reach a desired output given a particular input magallanes capabilities can be exploited both as an api or directly accessed through a graphic user interface the magallanes api is freely available for academic use and together with magallanes application has been tested in ms windowstm xp and unix like operating systems detailed implementation information including user manuals and tutorials is available at http www bitlab es com magallanes conclusion different implementations of the same client web page desktop applications web services etc have been deployed and are currently in use in real installations such as the national institute of bioinformatics spain and the acgt eu project this shows the potential utility and versatility of the software library including the integration of novel tools in the domain and with strong evidences in the line of facilitate the automatic discovering and composition workflows
genome wide studies reveal that transcription by rna polymerase ii pol ii is dynamically regulated to obtain a comprehensive view of a single transcription cycle we switched on transcription of five long human genes kbp with tumor necrosis factor alpha tnfalpha and monitored using microarrays rna fluorescence in situ hybridization and chromatin immunoprecipitation the appearance of nascent rna changes in binding of pol ii and two insulators the cohesin subunit and the ccctc binding factor ctcf and modifications of histone activation triggers a wave of transcription that sweeps along the genes at approximately kbp min splicing occurs cotranscriptionally a major checkpoint acts several kilobases downstream of the transcription start site to regulate polymerase transit and pol ii tends to stall at cohesin ctcf sites
the concept of social media is top of the agenda for many business executives today decision makers as well as consultants try to identify ways in which firms can make profitable use of applications such as wikipedia youtube facebook second life and twitter yet despite this interest there seems to be very limited understanding of what the term social media exactly means this article intends to provide some clarification we begin by describing the concept of social media and discuss how it differs from related concepts such as web and user generated content based on this definition we then provide a classification of social media which groups applications currently subsumed under the generalized term into more specific categories by characteristic collaborative projects blogs content communities social networking sites virtual game worlds and virtual social worlds finally we present pieces of advice for companies which decide to utilize social media kelley school of business university
words grammar and phonology are linguistically distinct yet their neural substrates are difficult to distinguish in macroscopic brain regions we investigated whether they can be separated in time and space at the circuit level using intracranial electrophysiology ice namely by recording local field potentials from populations of neurons using electrodes implanted in language related brain regions while people read words verbatim or grammatically inflected them present past or singular plural neighboring probes within broca s area revealed distinct neuronal activity for lexical milliseconds grammatical milliseconds and phonological milliseconds processing identically for nouns and verbs in a region activated in the same patients and task in functional magnetic resonance imaging this suggests that a linguistic processing sequence predicted on computational grounds is implemented in the brain in fine grained spatiotemporally patterned science
background classification using microarray datasets is usually based on a small number of samples for which tens of thousands of gene expression measurements have been obtained the selection of the genes most significant to the classification problem is a challenging issue in high dimension data analysis and interpretation a previous study with svm rce recursive cluster elimination suggested that classification based on groups of correlated genes sometimes exhibits better performance than classification using single genes large databases of gene interaction networks provide an important resource for the analysis of genetic phenomena and for classification studies using interacting genes we now demonstrate that an algorithm which integrates network information with recursive feature elimination based on svm exhibits good performance and improves the biological interpretability of the results we refer to the method as svm with recursive network elimination svm rne results initially one thousand genes selected by t test from a training set are filtered so that only genes that map to a gene network database remain the gene expression network analysis tool gxna is applied to the remaining genes to form n clusters of genes that are highly connected in the network linear svm is used to classify the samples using these clusters and a weight is assigned to each cluster based on its importance to the classification the least informative clusters are removed while retaining the remainder for the next classification step this process is repeated until an optimal classification is obtained conclusion more than accuracy can be obtained in classification of selected microarray datasets by integrating the interaction network information with the gene expression information from the microarrays the matlab version of svm rne can be downloaded from http web macam ac myousef
this paper presents the findings of a podcasting trial held in within the faculty of economics and business at the university of sydney australia the trial investigates the value of using short format podcasts to support assessment for postgraduate and undergraduate students a multi method approach is taken in investigating perceptions of the benefits of podcasting incorporating surveys focus groups and interviews the results show that a majority of students believe they gained learning benefits from the podcasts and appreciated the flexibility of the medium to support their learning and the lecturers felt the innovation helped diversify their pedagogical approach and support a diverse student population three primary conclusions are presented most students reject the mobile potential of podcasting in favour of their traditional study space at home what students and lecturers value about this podcasting design overlap the assessment focussed short format podcast design may be considered a successful podcasting model the paper finishes by identifying areas for future research on the effective use of podcasting in learning teaching
principal components analysis pca is a statistical method commonly used in population genetics to identify structure in the distribution of genetic variation across geographical location and ethnic background however while the method is often used to inform about historical demographic processes little is known about the relationship between fundamental demographic parameters and the projection of samples onto the primary axes here i show that for snp data the projection of samples onto the principal components can be obtained directly from considering the average coalescent times between pairs of haploid genomes the result provides a framework for interpreting pca projections in terms of underlying processes including migration geographical isolation and admixture i also demonstrate a link between pca and wright s f st and show that snp ascertainment has a largely simple and predictable effect on the projection of samples using examples from human genetics i discuss the application of these results to empirical data and the implications inference
the relationship between rates of genomic evolution and organismal adaptation remains uncertain despite considerable interest the feasibility of obtaining genome sequences from experimentally evolving populations offers the opportunity to investigate this relationship with new precision here we sequence genomes sampled through generations from a laboratory population of escherichia coli although adaptation decelerated sharply genomic evolution was nearly constant for generations such clock like regularity is usually viewed as the signature of neutral evolution but several lines of evidence indicate that almost all of these mutations were beneficial this same population later evolved an elevated mutation rate and accumulated hundreds of additional mutations dominated by a neutral signature thus the coupling between genomic and adaptive evolution is complex and can be counterintuitive even in a constant environment in particular beneficial substitutions were surprisingly uniform over time whereas neutral substitutions were variable
cis acting variants altering gene expression are a source of phenotypic differences the cis acting components of expression variation can be identified through the mapping of differences in allelic expression ae which is the measure of relative expression between two allelic transcripts we generated a map of ae associated snps using quantitative measurements of ae on illumina beadchips in lymphoblastoid cell lines derived from donors of european descent we identified common cis variants affecting of the measured refseq transcripts at permutation significance the pervasive influence of cis regulatory variants which explain of population variation in ae extend to full length transcripts and their isoforms as well as to unannotated transcripts these strong effects facilitate fine mapping of cis regulatory snps as demonstrated by dissection of heritable control of transcripts in the systemic lupus erythematosusassociated blk region in chromosome the dense collection of associations will facilitate large scale isolation of cis snps
modern high throughput gene perturbation screens are key technologies at the forefront of genetic research combined with rich phenotypic descriptors they enable researchers to observe detailed cellular reactions to experimental perturbations on a genome wide scale this review surveys the current state of the art in analyzing perturbation screens from a network point of view we describe approaches to make the step from the parts list to the wiring diagram by using phenotypes for network inference and integrating them with complementary data sources the first part of the review describes methods to analyze one or low dimensional phenotypes like viability or reporter activity the second part concentrates on high dimensional phenotypes showing global changes in cell morphology transcriptome proteome
empowered by technology and sampling efforts designed to facilitate genome wide association mapping human geneticists are now studying the geography of genetic variation in unprecedented detail with high genomic coverage and geographic resolution these studies are identifying loci with spatial signatures of selection such as extreme levels of differentiation and correlations with environmental variables collectively patterns at these loci are beginning to provide new insights into the process of human adaptation here we review the challenges of these studies and emerging results including how human population structure has influenced the response to novel pressures
sequence specific binding of a transcription factor to dna is the central event in any transcriptional regulatory network however relatively little is known about the evolutionary plasticity of transcription factors for example the exact functional consequence of an amino acid substitution on the dna binding specificity of most transcription factors is currently not predictable furthermore although the major structural families of transcription factors have been identified the detailed dna binding repertoires within most families have not been characterized we studied the sequence recognition code and evolvability of the basic helix loop helix transcription factor family by creating all possible single point mutations of five dna contacting residues of max a human helix loop helix transcription factor and measured the detailed dna binding repertoire of each mutant our results show that the sequence specific repertoire of max accessible through single point mutations is extremely limited and we are able to predict of the naturally occurring diversity at these positions all naturally occurring basic regions were also found to be accessible through functional intermediates finally we observed a set of amino acids that are functional in vitro but are not found to be used naturally indicating that functionality alone is not sufficient selection
abstract background rare population variants are known to have important biomedical implications but their systematic discovery has only recently been enabled by advances in dna sequencing the design process of a discovery project remains formidable being limited to ad hoc mixtures of extensive computer simulation and pilot sequencing here the task is examined from a general mathematical perspective results we pose and solve the population sequencing design problem and subsequently apply standard optimization techniques that maximize the discovery probability emphasis is placed on cases whose discovery thresholds place them within reach of current technologies we find that parameter values characteristic of rare variant projects lead to a general yet remarkably simple set of optimization rules specifically optimal processing occurs at constant values of the per sample redundancy refuting current notions that sample size should be selected outright optimal project wide redundancy and sample size are then shown to be inversely proportional to the desired variant frequency a second family of constants governs these relationships permitting one to immediately establish the most efficient settings for a given set of discovery conditions our results largely concur with the empirical design of the thousand genomes project though they furnish some additional refinement conclusions the optimization principles reported here dramatically simplify the design process and should be broadly useful as rare variant projects become both more important and routine in future
abstract background biological networks characterize the interactions of biomolecules at a systems level one important property of biological networks is the modular structure in which nodes are densely connected with each other but between which there are only sparse connections in this report we attempted to find the relationship between the network topology and formation of modular structure by comparing gene co expression networks with random networks the organization of gene functional modules was also investigated results we constructed a genome wide arabidopsis gene co expression network agcn by using microarrays we then analyzed the topological properties of agcn and partitioned the network into modules by using an efficient graph clustering algorithm in the agcn hub genes formed a clique and they were densely connected only to a small subset of the network at the module level the network clustering results provide a systems level understanding of the gene modules that coordinate multiple biological processes to carry out specific biological functions for instance the photosynthesis module in agcn involves a very large number of genes which participate in various biological processes including photosynthesis electron transport pigment metabolism chloroplast organization and biogenesis cofactor metabolism protein biosynthesis and vitamin metabolism the cell cycle module orchestrated the coordinated expression of hundreds of genes involved in cell cycle dna metabolism and cytoskeleton organization and biogenesis we also compared the agcn constructed in this study with a graphical gaussian model ggm based arabidopsis gene network the photosynthesis protein biosynthesis and cell cycle modules identified from the ggm network had much smaller module sizes compared with the modules found in the agcn respectively conclusions this study reveals new insight into the topological properties of biological networks the preferential hub hub connections might be necessary for the formation of modular structure in gene co expression networks the study also reveals new insight into the organization of gene modules
fluorescence that is spontaneous emission is generally more sensitive than absorption measurement and is widely used in optical however many chromophores such as haemoglobin and cytochromes absorb but have undetectable fluorescence because the spontaneous emission is dominated by their fast non radiative yet the detection of their absorption is difficult under a microscope here we use stimulated emission which competes effectively with the nonradiative decay to make the chromophores detectable and report a new contrast mechanism for optical microscopy in a pumpprobe experiment on photoexcitation by a pump pulse the sample is stimulated down to the ground state by a time delayed probe pulse the intensity of which is concurrently increased we extract the miniscule intensity increase with shot noise limited sensitivity by using a lock in amplifier and intensity modulation of the pump beam at a high megahertz frequency the signal is generated only at the laser foci owing to the nonlinear dependence on the input intensities providing intrinsic three dimensional optical sectioning capability in contrast conventional one beam absorption measurement exhibits low sensitivity lack of three dimensional sectioning capability and complication by linear scattering of heterogeneous samples we demonstrate a variety of applications of stimulated emission microscopy such as visualizing chromoproteins non fluorescent variants of the green fluorescent protein monitoring lacz gene expression with a chromogenic reporter mapping transdermal drug distributions without histological sectioning and label free microvascular imaging based on endogenous contrast of haemoglobin for all these applications sensitivity is orders of magnitude higher than for spontaneous emission or absorption contrast permitting nonfluorescent reporters for imaging
during the crossing of the place field of a pyramidal cell in the rat hippocampus the firing phase of the cell decreases with respect to the local theta rhythm this phase precession is usually studied on the basis of data in which many place field traversals are pooled together here we study properties of phase precession in single trials we found that single trial and pooled trial phase precession were different with respect to phase position correlation phase time correlation and phase range whereas pooled trial phase precession may span degrees the most frequent single trial phase range was only degrees in pooled trials the correlation between phase and position r was stronger than the correlation between phase and time r whereas in single trials these correlations r for both were not significantly different next we demonstrated that phase precession exhibited a large trial to trial variability overall only a small fraction of the trial to trial variability in measures of phase precession e g slope or offset could be explained by other single trial properties such as running speed or firing rate whereas the larger part of the variability remains to be explained finally we found that surrogate single trials created by randomly drawing spikes from the pooled data are not equivalent to experimental single trials pooling over trials therefore changes basic measures of phase precession these findings indicate that single trials may be better suited for encoding temporally structured events than is suggested by the pooled jneurosci
genome wide mapping of nucleosomes has revealed a great deal about the relationships between chromatin structure and control of gene expression and has led to mechanistic hypotheses regarding the rules by which chromatin structure is established high throughput sequencing has recently become the technology of choice for chromatin mapping studies yet analysis of these experiments is still in its infancy here we introduce a pipeline for analyzing deep sequencing maps of chromatin structure and apply it to data from s cerevisiae we analyze a digestion series where nucleosomes are isolated from under and overdigested chromatin we find that certain classes of nucleosomes are unusually susceptible or resistant to overdigestion with promoter nucleosomes easily digested and mid coding region nucleosomes being quite stable we find evidence for highly sensitive nucleosomes located within nucleosome free regions suggesting that these regions are not always completely naked but instead are likely associated with easily digested nucleosomes finally since rna polymerase is the dominant energy consuming machine that operates on the chromatin template we analyze changes in chromatin structure when rna polymerase is inactivated via a temperature sensitive mutation we find evidence that rna polymerase plays a role in nucleosome eviction at promoters and is also responsible for retrograde shifts in nucleosomes during transcription loss of rna polymerase results in a relaxation of chromatin structure to more closely match in vitro nucleosome positioning preferences together these results provide analytical tools and experimental guidance for nucleosome mapping experiments and help disentangle the interlinked processes of transcription and packaging
motivation by default the r statistical environment does not make use of parallelism researchers may resort to expensive solutions such as cluster hardware for large analysis tasks graphics processing units gpus provide an inexpensive and computationally powerful alternative using r and the cuda toolkit from nvidia we have implemented several functions commonly used in microarray gene expression analysis for gpu equipped computers results r users can take advantage of the better performance provided by an nvidia gpu availability the package is available from cran the r project s repository of packages at http cran r project org web packages gputools more information about our gputools r package is available at http brainarray mbni med umich edu brainarray rgpgpu contact bucknerj umich bioinformatics
over the past decade there has been a growing public fascination with the complex connectedness of modern society this connectedness is found in many incarnations in the rapid growth of the internet and the web in the ease with which global communication now takes place and in the ability of news and information as well as epidemics and financial crises to spread around the world with surprising speed and intensity these are phenomena that involve networks incentives and the aggregate behavior of groups of people they are based on the links that connect us and the ways in which each of our decisions can have subtle consequences for the outcomes of everyone else networks crowds and markets combines different scientific perspectives in its approach to understanding networks and behavior drawing on ideas from economics sociology computing and information science and applied mathematics it describes the emerging field of study that is growing at the interface of all these areas addressing fundamental questions about how the social economic and technological worlds are connected the book is based on an inter disciplinary course entitled networks that we teach at cornell the book like the course is designed at the introductory undergraduate level with no formal prerequisites to support deeper explorations most of the chapters are supplemented with optional sections
despite only becoming popular at the beginning of this decade biomolecular networks are now frameworks that facilitate many discoveries in molecular biology the nodes of these networks are usually proteins specifically enzymes in metabolic networks whereas the links or edges are their interactions with other molecules these networks are made up of protein protein interactions or enzyme enzyme interactions through shared metabolites in the case of metabolic networks evolutionary analysis has revealed that changes in the nodes and links in protein protein interaction and metabolic networks are subject to different selection pressures owing to distinct topological features however many evolutionary constraints can be uncovered only if temporal and spatial aspects are included in the analysis
after nearly a decade metabolomics has begun to acquire some credence in the scientific community although its acceptance cannot be compared with that of its forerunners genomics and proteomics the legitimization of metabolomics as a valid scientific entity depends on the size of the research community it influences by far the most effective medium for inoculation is the web infrastructure public servers that accommodate experimental data simple formats and guidelines for their interpretation and connectivity between data and tools for analysis when these elements satisfy the condition to initiate a social epidemic metabolomics will be accepted as a fundamental data driven science that can unite hitherto independently conducted disciplines
abstract background information extraction ie is a component of text mining that facilitates knowledge discovery by automatically locating instances of interesting biomedical events from huge document collections as events are usually centred on verbs and nominalised verbs understanding the syntactic and semantic behaviour of these words is highly important corpora annotated with information concerning this behaviour can constitute a valuable resource in the training of ie components and resources results we have defined a new scheme for annotating sentence bound gene regulation events centred on both verbs and nominalised verbs for each event instance all participants arguments in the same sentence are identified and assigned a semantic role from a rich set of roles tailored to biomedical research articles together with a biological concept type linked to the gene regulation ontology to our knowledge our scheme is unique within the biomedical field in terms of the range of event arguments identified using the scheme we have created the gene regulation event corpus grec consisting of medline abstracts in which events relating to gene regulation and expression have been annotated by biologists a novel method of evaluating various different facets of the annotation task showed that average inter annotator agreement rates fall within the range of conclusion the grec is a unique resource within the biomedical field in that it annotates not only core relationships between entities but also a range of other important details about these relationships e g location temporal manner and environmental conditions as such it is specifically designed to support bio specific tool and resource development it has already been used to acquire semantic frames for inclusion within the biolexicon a lexical terminological resource to aid biomedical text mining initial experiments have also shown that the corpus may viably be used to train ie components such as semantic role labellers the corpus and annotation guidelines are freely available for purposes
motivation the massively parallel sequencing technology can be used by small research labs to generate genome sequences of their research interest however annotation of genomes still relies on the manual process which becomes a serious bottleneck to the high throughput genome projects recently automatic annotation methods are increasingly more accurate but there are several issues one important challenge in using automatic annotation methods is to distinguish annotation quality of orfs or genes the availability of such annotation quality of genes can reduce the human labor cost dramatically since manual inspection can focus only on genes with low annotation quality scores results in this paper we propose a novel annotation quality or confidence scoring scheme called annotation confidence score acs using a genome comparison approach the scoring scheme is computed by combining sequence and textual annotation similarity using a modified version of a logistic curve the most important feature of the proposed scoring scheme is to generate a score that reflects the excellence in annotation quality of genes by automatically adjusting the number of genomes used to compute the score and their phylogenetic distance extensive experiments with bacterial genomes showed that the proposed scoring scheme generated scores for annotation quality according to the quality of annotation regardless of the number of reference genomes and their phylogenetic distance availability http microbial informatics indiana edu acs contact indiana bioinformatics
summary high throughput rna sequencing rna seq is rapidly emerging as a major quantitative transcriptome profiling platform here we present degseq an r package to identify differentially expressed genes or isoforms for rna seq data from different samples in this package we integrated three existing methods and introduced two novel methods based on ma plot to detect and visualize gene expression difference availability the r package and a quick start vignette is available at http bioinfo au tsinghua edu cn software degseqcontact xwwang tsinghua edu cn zhangxg tsinghua edu cnsupplementary information supplementary data are available at online
summary high throughput sequencing technologies introduce novel demands on tools available for data analysis we have developed ngsview next generation sequence view a generally applicable flexible and extensible next generation sequence alignment editor the software allows for visualization and manipulation of millions of sequences simultaneously on a desktop computer through a graphical interface ngsview is available under an open source license and can be extended through a well documented api availability http ngsview net
we present seaview version a multiplatform program designed to facilitate multiple alignment and phylogenetic tree building from molecular sequence data through the use of a graphical user interface seaview version combines all the functions of the widely used programs seaview in its previous versions and and expands them by adding network access to sequence databases alignment with arbitrary algorithm maximum likelihood tree building with phyml and display printing and copy to clipboard of rooted or unrooted binary or multifurcating phylogenetic trees in relation to the wide present offer of tools and algorithms for phylogenetic analyses seaview is especially useful for teaching and for occasional users of such software seaview is freely available at http pbil univ fr software molbev
the primary mission of uniprot is to support biological research by maintaining a stable comprehensive fully classified richly and accurately annotated protein sequence knowledgebase with extensive cross references and querying interfaces freely accessible to the scientific community uniprot is produced by the uniprot consortium which consists of groups from the european bioinformatics institute ebi the swiss institute of bioinformatics sib and the protein information resource pir uniprot is comprised of four major components each optimized for different uses the uniprot archive the uniprot knowledgebase the uniprot reference clusters and the uniprot metagenomic and environmental sequence database uniprot is updated and distributed every weeks and can be accessed online for searches or download at http www org
micrornas are small non protein coding rna molecules known to regulate the expression of genes by binding to the utr region of mrnas micrornas are produced from longer transcripts which can code for more than one mature mirnas mirgen is a database that aims to provide comprehensive information about the position of human and mouse microrna coding transcripts and their regulation by transcription factors including a unique compilation of both predicted and experimentally supported data expression profiles of micrornas in several tissues and cell lines single nucleotide polymorphism locations microrna target prediction on protein coding genes and mapping of mirna targets of co regulated mirnas on biological pathways are also integrated into the database and user interface the mirgen database will be continuously maintained and freely available at http www microrna mirgen
alternative splicing can enhance transcriptome plasticity and proteome diversity in plants alternative splicing can be manifested at different developmental stages and is frequently associated with specific tissue types or environmental conditions such as abiotic stress we mapped the arabidopsis transcriptome at single base resolution using the illumina platform for ultrahigh throughput rna sequencing rna seq deep transcriptome sequencing confirmed a majority of annotated introns and identified thousands of novel alternatively spliced mrna isoforms our analysis suggests that at least of intron containing genes in arabidopsis are alternatively spliced this is significantly higher than previous estimates based on cdna expressed sequence tag sequencing random validation confirmed that novel splice isoforms empirically predicted by rna seq can be detected in vivo novel introns detected by rna seq were substantially enriched in nonconsensus terminal dinucleotide splice signals alternative isoforms with premature termination codons ptcs comprised the majority of alternatively spliced transcripts using an example of an essential circadian clock gene we show that intron retention can generate relatively abundant ptc isoforms and that this specific event is highly conserved among diverse plant species alternatively spliced ptc isoforms can be potentially targeted for degradation by the nonsense mediated mrna decay nmd surveillance machinery or regulate the level of functional transcripts by the mechanism of regulated unproductive splicing and translation rust we demonstrate that the relative ratios of the ptc and reference isoforms for several key regulatory genes can be considerably shifted under abiotic stress treatments taken together our results suggest that like in animals nmd and rust may be widespread in plants and may play important roles in regulating expression
background evolutionary trees are central to a wide range of biological studies in many of these studies tree nodes and branches need to be associated or annotated with various attributes for example in studies concerned with organismal relationships tree nodes are associated with taxonomic names whereas tree branches have lengths and oftentimes support values gene trees used in comparative genomics or phylogenomics are usually annotated with taxonomic information genome related data such as gene names and functional annotations as well as events such as gene duplications speciations or exon shufflings combined with information related to the evolutionary tree itself the data standards currently used for evolutionary trees have limited capacities to incorporate such annotations of different data types results we developed a xml language named phyloxml for describing evolutionary trees as well as various associated data items phyloxml provides elements for commonly used items such as branch lengths support values taxonomic names and gene names and identifiers by using property elements phyloxml can be adapted to novel and unforeseen use cases we also developed various software tools for reading writing conversion and visualization of phyloxml formatted data conclusion phyloxml is an xml language defined by a complete schema in xsd that allows storing and exchanging the structures of evolutionary trees as well as associated data more information about phyloxml itself the xsd schema as well as tools implementing and supporting phyloxml is available at http www org
the naked mole rat is the longest living rodent with a maximum lifespan exceeding years in addition to its longevity naked mole rats have an extraordinary resistance to cancer as tumors have never been observed in these rodents furthermore we show that a combination of activated ras and lt fails to induce robust anchorage independent growth in naked mole rat cells while it readily transforms mouse fibroblasts the mechanisms responsible for the cancer resistance of naked mole rats were unknown here we show that naked mole rat fibroblasts display hypersensitivity to contact inhibition a phenomenon we termed early contact inhibition contact inhibition is a key anticancer mechanism that arrests cell division when cells reach a high density in cell culture naked mole rat fibroblasts arrest at a much lower density than those from a mouse we demonstrate that early contact inhibition requires the activity of and prb tumor suppressor pathways inactivation of both and prb attenuates early contact inhibition contact inhibition in human and mouse is triggered by the induction of in contrast early contact inhibition in naked mole rat is associated with the induction of furthermore we show that the roles of and in the control of contact inhibition became temporally separated in this species the early contact inhibition is controlled by and regular contact inhibition is controlled by we propose that the additional layer of protection conferred by two tiered contact inhibition contributes to the remarkable tumor resistance of the naked rat
protein coding genes constitute only approximately of the human genome but harbor of the mutations with large effects on disease related traits therefore efficient strategies for selectively sequencing complete coding regions i e whole exome have the potential to contribute to the understanding of rare and common human diseases here we report a method for whole exome sequencing coupling roche nimblegen whole exome arrays to the illumina dna sequencing platform we demonstrate the ability to capture approximately of the targeted coding sequences with high sensitivity and specificity for detection of homozygous and heterozygous variants we illustrate the utility of this approach by making an unanticipated genetic diagnosis of congenital chloride diarrhea in a patient referred with a suspected diagnosis of bartter syndrome a renal salt wasting disease the molecular diagnosis was based on the finding of a homozygous missense mutation at a position in the known congenital chloride diarrhea locus that is virtually completely conserved in orthologues and paralogues from invertebrates to humans and clinical follow up confirmed the diagnosis to our knowledge whole exome or genome sequencing has not previously been used to make a genetic diagnosis five additional patients suspected to have bartter syndrome but who did not have mutations in known genes for this disease had homozygous deleterious mutations in these results demonstrate the clinical utility of whole exome sequencing and have implications for disease gene discovery and diagnosis
motivation the advent of next generation sequencing technologies has increased the accuracy and quantity of sequence data opening the door to greater opportunities in genomic research results in this article we present gnumap genomic next generation universal mapper a program capable of overcoming two major obstacles in the mapping of reads from next generation sequencing runs first we have created an algorithm that probabilistically maps reads to repeat regions in the genome on a quantitative basis second we have developed a probabilistic needleman wunsch algorithm which utilizes txt and txt files produced in the solexa illumina pipeline to improve the mapping accuracy for lower quality reads and increase the amount of usable data produced in a given experiment availability the source code for the software can be downloaded from http dna cs byu edu gnumap contact nathanlclement gmail com supplementary information supplementary data are available at bioinformatics bioinformatics
technical advances such as the development of molecular cloning sanger sequencing pcr and oligonucleotide microarrays are key to our current capacity to sequence annotate and study complete organismal genomes recent years have seen the development of a variety of so called next generation sequencing platforms with several others anticipated to become available shortly the previously unimaginable scale and economy of these methods coupled with their enthusiastic uptake by the scientific community and the potential for further improvements in accuracy and read length suggest that these technologies are destined to make a huge and ongoing impact upon genomic and post genomic biology however like the analysis of microarray data and the assembly and annotation of complete genome sequences from conventional sequencing data the management and analysis of next generation sequencing data requires and indeed has already driven the development of informatics tools able to assemble map and interpret huge quantities of relatively or extremely short nucleotide sequence data here we provide a broad overview of bioinformatics approaches that have been introduced for several genomics and functional genomics applications of next sequencing
ndash dna recognition the recognition of specific dna sequences by proteins is thought to depend on two types of mechanism one that involves the formation of hydrogen bonds with specific bases primarily in the major groove and one involving sequence dependent deformations of the dna helix by comprehensively analysing the three dimensional structures of proteindna complexes here we show that the binding of arginine residues to narrow minor grooves is a widely used mode for proteindna recognition this readout mechanism exploits the phenomenon that narrow minor grooves strongly enhance the negative electrostatic potential of the dna the nucleosome core particle offers a prominent example of this effect minor groove narrowing is often associated with the presence of a tracts at rich sequences that exclude the flexible tpa step these findings indicate that the ability to detect local variations in dna shape and electrostatic potential is a general mechanism that enables proteins to use information in the minor groove which otherwise offers few opportunities for the formation of base specific hydrogen bonds to achieve dna specificity
background statistical analysis of dna microarray data provides a valuable diagnostic tool for the investigation of genetic components of diseases to take advantage of the multitude of available data sets and analysis methods it is desirable to combine both different algorithms and data from different studies applying ensemble learning consensus clustering and cross study normalization methods for this purpose in an almost fully automated process and linking different analysis modules together under a single interface would simplify many microarray analysis tasks results we present arraymining net a web application for microarray analysis that provides easy access to a wide choice of feature selection clustering prediction gene set analysis and cross study normalization methods in contrast to other microarray related web tools multiple algorithms and data sets for an analysis task can be combined using ensemble feature selection ensemble prediction consensus clustering and cross platform data integration by interlinking different analysis tools in a modular fashion new exploratory routes become available e g ensemble sample classification using features obtained from a gene set analysis and data from multiple studies the analysis is further simplified by automatic parameter selection mechanisms and linkage to web tools and databases for functional annotation and literature mining conclusion arraymining net is a free web application for microarray analysis combining a broad choice of algorithms based on ensemble and consensus methods using automatic parameter selection and integration with databases
background the remarkable advance of metagenomics presents significant new challenges in data analysis metagenomic datasets metagenomes are large collections of sequencing reads from anonymous species within particular environments computational analyses for very large metagenomes are extremely time consuming and there are often many novel sequences in these metagenomes that are not fully utilized the number of available metagenomes is rapidly increasing so fast and efficient metagenome comparison methods are in great demand results the new metagenomic data analysis method rapid analysis of multiple metagenomes with a clustering and annotation pipeline rammcap was developed using an ultra fast sequence clustering algorithm fast protein family annotation tools and a novel statistical metagenome comparison method that employs a unique graphic interface rammcap processes extremely large datasets with only moderate computational effort it identifies raw read clusters and protein clusters that may include novel gene families and compares metagenomes using clusters or functional annotations calculated by rammcap in this study rammcap was applied to the two largest available metagenomic collections the global ocean sampling and the metagenomic profiling of nine biomes conclusion rammcap is a very fast method that can cluster and annotate one million metagenomic reads in only hundreds of cpu hours it is available from http tools camera net rammcap
methods for detecting nucleotide substitution rates that are faster or slower than expected under neutral drift are widely used to identify candidate functional elements in genomic sequences however most existing methods consider either reductions conservation or increases acceleration in rate but not both or assume that selection acts uniformly across the branches of a phylogeny here we examine the more general problem of detecting departures from the neutral rate of substitution in either direction possibly in a clade specific manner we consider four statistical phylogenetic tests for addressing this problem a likelihood ratio test a score test a test based on exact distributions of numbers of substitutions and the genomic evolutionary rate profiling gerp test all four tests have been implemented in a freely available program called phylop based on extensive simulation experiments these tests are remarkably similar in statistical power with mammalian species they all appear to be capable of fairly good sensitivity with low false positive rates in detecting strong selection at individual nucleotides moderate selection in bp elements and weaker or clade specific selection in longer elements by applying phylop to mammalian multiple alignments from the encode project we shed light on patterns of conservation acceleration in known and predicted functional elements approximate fractions of sites subject to constraint and differences in clade specific selection in the primate and glires clades we also describe new conservation tracks in the ucsc genome browser that display both phylop and phastcons scores for genome wide alignments of species
baryon acoustic oscillations bao are frozen relics left over from the pre decoupling universe they are the standard rulers of choice for century cosmology providing distance estimates that are for the first time firmly rooted in well understood linear physics this review synthesises current understanding regarding all aspects of bao cosmology from the theoretical and statistical to the observational and includes a map of the future landscape of bao surveys both spectroscopic photometric
recent experiments have established that information can be encoded in the spike times of neurons relative to the phase of a background oscillation in the local field potential a phenomenon referred to as phase of firing coding pofc these firing phase preferences could result from combining an oscillation in the input current with a stimulus dependent static component that would produce the variations in preferred phase but it remains unclear whether these phases are an epiphenomenon or really affect neuronal interactions only then could they have a functional role here we show that pofc has a major impact on downstream learning and decoding with the now well established spike timing dependent plasticity stdp to be precise we demonstrate with simulations how a single neuron equipped with stdp robustly detects a pattern of input currents automatically encoded in the phases of a subset of its afferents and repeating at random intervals remarkably learning is possible even when only a small fraction of the afferents exhibits pofc the ability of stdp to detect repeating patterns had been noted before in continuous activity but it turns out that oscillations greatly facilitate learning a benchmark with more conventional rate based codes demonstrates the superiority of oscillations and pofc for both stdp based learning and the speed of decoding the oscillation partially formats the input spike times so that they mainly depend on the current input currents and can be efficiently learned by stdp and then recognized in just one oscillation cycle this suggests a major functional role for oscillatory brain activity that has been widely reported jneurosci
the means we use to record the process of carrying out research remains tied to the concept of a paginated paper notebook despite the advances over the past decade in web based communication and publication tools the development of these tools offers an opportunity to re imagine what the laboratory record would look like if it were re built in a web native form in this paper i describe a distributed approach to the laboratory record based which uses the most appropriate tool available to house and publish each specific object created during the research process whether they be a physical sample a digital data object or the record of how one was created from another i propose that the web native laboratory record would act as a feed of relationships between these items this approach can be seen as complementary to rather than competitive with integrative approaches that aim to aggregate relevant objects together to describe knowledge the potential for the recent announcement of the google wave protocol to have a significant impact on realizing this vision is discussed along with the issues of security and provenance that are raised by such approach
summary protein dna interactions pdis mediate a broad range of functions essential for cellular differentiation function and survival however it is still a dauntingtask to comprehensively identify and profile sequence specific pdis in complex genomes here we have used a combined bioinformatics and protein microarray based strategy to systematically characterize the human protein dna interactome we identified pdis between dna motifs predicted to regulate transcription and human proteins of various functional classes among them we recovered many known pdis for transcription factors tfs we identified a large number of unanticipated pdis for known tfs as well as for previously uncharacterized tfs we also found that over three hundred unconventional dna binding proteins udbps which include rna binding proteins mitochondrial proteins and protein kinasesshowed sequence specific pdis one such udbp acts as a transcriptional repressor for interferon gamma induced genes suggesting important biological roles for proteins
background advances in automated dna sequencing technology have greatly increased the scale of genomic and metagenomic studies an increasingly popular means of increasing project throughput is by multiplexing samples during the sequencing phase this can be achieved by covalently linking short unique barcode dna segments to genomic dna samples for instance through incorporation of barcode sequences in pcr primers although several strategies have been described to insure that barcode sequences are unique and robust to sequencing errors these have not been integrated into the overall primer design process thus potentially introducing bias into pcr amplification and or sequencing steps results barcrawl is a software program that facilitates the design of barcoded primers for multiplexed high throughput sequencing the program bartab can be used to deconvolute dna sequence datasets produced by the use of multiple barcoded primers this paper describes the functions implemented by barcrawl and bartab and presents a proof of concept case study of both programs in which barcoded rrna primers were designed and validated by high throughput sequencing conclusion barcrawl and bartab can benefit researchers who are engaged in metagenomic projects that employ multiplexed specimen processing the source code is released under the gnu general public license and can be accessed at http www com
following the technological advances that have enabled genome wide analysis in most model organisms over the last decade there has been unprecedented growth in genomic and post genomic science with concomitant generation of an exponentially increasing volume of data and material resources as a result numerous repositories have been created to store and archive data organisms and material which are of substantial value to the whole community sustained access facilitating re use of these resources is essential not only for validation but for re analysis testing of new hypotheses and developing new technologies platforms a common challenge for most data resources and biological repositories today is finding financial support for maintenance and development to best serve the scientific community in this study we examine the problems that currently confront the data and resource infrastructure underlying the biomedical sciences we discuss the financial sustainability issues and potential business models that could be adopted by biological resources and consider long term preservation issues within the context of mouse functional genomics efforts europe
protein protein interaction networks pins are rich sources of information that enable the network properties of biological systems to be understood a study of the topological and statistical properties of budding yeast and human pins revealed that they are scale rich and configured as highly optimized tolerance hot networks that are similar to the router level topology of the internet this is different from claims that such networks are scale free and configured through simple preferential attachment processes further analysis revealed that there are extensive interconnections among middle degree nodes that form the backbone of the networks degree distributions of essential genes synthetic lethal genes synthetic sick genes and human drug target genes indicate that there are advantageous drug targets among nodes with middle to low degree nodes such network properties provide the rationale for combinatorial drugs that target less prominent nodes to increase synergetic efficacy and create fewer effects
small scale human societies range from foraging bands with a strong egalitarian ethos to more economically stratified agrarian and pastoral societies we explain this variation in inequality using a dynamic model in which a population s long run steady state level of inequality depends on the extent to which its most important forms of wealth are transmitted within families across generations we estimate the degree of intergenerational transmission of three different types of wealth material embodied and relational as well as the extent of wealth inequality in historical and contemporary populations we show that intergenerational transmission of wealth and wealth inequality are substantial among pastoral and small scale agricultural societies on a par with or even exceeding the most unequal modern industrial economies but are limited among horticultural and foraging peoples equivalent to the most egalitarian of modern industrial populations differences in the technology by which a people derive their livelihood and in the institutions and norms making up the economic system jointly contribute to this science
in the last several years a number of studies have described large scale structural variation in several genomes traditionally such methods have used whole genome array comparative genome hybridization or single nucleotide polymorphism arrays to detect large regions subject to copy number variation later techniques have been based on paired end mapping of sanger sequencing data providing better resolution and accuracy with the advent of next generation sequencing a new generation of methods is being developed to tackle the challenges of short reads while taking advantage of the high coverage the new sequencing technologies provide in this survey we describe these methods including their strengths and their limitations and future directions
the most important first step in understanding next generation sequencing data is the initial alignment or assembly that determines whether an experiment has succeeded and provides a first glimpse into the results in parallel with the growth of new sequencing technologies several algorithms that align or assemble the large data output of today s sequencing machines have been developed we discuss the current algorithmic approaches and future directions of these fundamental tools and provide specific examples for some commonly tools
genome wide measurements of protein dna interactions and transcriptomes are increasingly done by deep dna sequencing methods chip seq and rna seq the power and richness of these counting based measurements comes at the cost of routinely handling tens to hundreds of millions of reads whereas early adopters necessarily developed their own custom computer code to analyze the first chip seq and rna seq datasets a new generation of more sophisticated algorithms and software tools are emerging to assist in the analysis phase of these projects here we describe the multilayered analyses of chip seq and rna seq datasets discuss the software packages currently available to perform tasks at each layer and describe some upcoming challenges and features for future analysis tools we also discuss how software choices and uses are affected by specific aspects of the underlying biology and data structure including genome size positional clustering of transcription factor binding sites transcript discovery and quantification
evaluating multicomponent climate change mitigation strategies requires knowledge of the diverse direct and indirect effects of emissions methane ozone and aerosols are linked through atmospheric chemistry so that emissions of a single pollutant can affect several species we calculated atmospheric composition changes historical radiative forcing and forcing per unit of emission due to aerosol and tropospheric ozone precursor emissions in a coupled composition climate model we found that gas aerosol interactions substantially alter the relative importance of the various emissions in particular methane emissions have a larger impact than that used in current carbon trading schemes or in the kyoto protocol thus assessments of multigas mitigation policies as well as any separate efforts to mitigate warming from short lived pollutants should include gas interactions
predictive methods for the computational design of proteins search for amino acid sequences adopting desired structures that perform specific functions typically design of function is formulated as engineering new and altered binding activities into proteins progress in the design of functional protein protein interactions is directed toward engineering proteins to precisely control biological processes by specifically recognizing desired interaction partners while avoiding competitors the field is aiming for strategies to harness recent advances in high resolution computational modelingparticularly those exploiting protein conformational variabilityto engineer new functions and incorporate many functional simultaneously
a low complexity voice recognition method in a car environment is proposed models of noise in the car environment are adaptable in an easy way power computation and memory storage for the proposed method are very low some real world tests are conducted and show that the proposed voice recognition method has potential for application to car telephones
next generation sequencing technologies are now being exploited not only to analyse static genomes but also dynamic transcriptomes in an approach termed rna seq although these powerful and rapidly evolving technologies have only been available for a couple of years they are already making substantial contributions to our understanding of genome expression and regulation here we briefly describe technical issues accompanying rna seq data generation and analysis highlighting differences to array based approaches we then review recent biological insight gained from applying rna seq and related approaches to deeply sample transcriptomes in different cell types or physiological conditions these approaches are providing fascinating information about transcriptional and post transcriptional gene regulation and they are also giving unique insight into the richness of transcript structures and processing on a global scale and at resolution
drug discovery must be guided not only by medical need and commercial potential but also by the areas in which new science is creating therapeutic opportunities such as target identification and the understanding of disease mechanisms to systematically identify such areas of high scientific activity we use bibliometrics and related data mining methods to analyse over half a terabyte of data including pubmed abstracts literature citation data and patent filings these analyses reveal trends in scientific activity related to disease studied at varying levels down to individual genes and pathways and provide methods to monitor areas in which scientific advances are likely to create new opportunities
via mndoci there is a growing gap between the generation of massively parallel sequencing output and the ability to process and analyze the resulting data new users are left to navigate a bewildering maze of base calling alignment assembly and analysis tools with often incomplete documentation and no idea how to compare and validate their outputs bridging this gap is essential or the coveted genome will come with a analysis tag
background alanine scanning mutagenesis is a powerful experimental methodology for investigating the structural and energetic characteristics of protein complexes individual amino acids are systematically mutated to alanine and changes in free energy of binding deltadeltag measured several experiments have shown that protein protein interactions are critically dependent on just a few residues hot spots at the interface hot spots make a dominant contribution to the free energy of binding and if mutated they can disrupt the interaction as mutagenesis studies require significant experimental efforts there is a need for accurate and reliable computational methods such methods would also add to our understanding of the determinants of affinity and specificity in protein protein recognition results we present a novel computational strategy to identify hot spot residues given the structure of a complex we consider the basic energetic terms that contribute to hot spot interactions i e van der waals potentials solvation energy hydrogen bonds and coulomb electrostatics we treat them as input features and use machine learning algorithms such as support vector machines and gaussian processes to optimally combine and integrate them based on a set of training examples of alanine mutations we show that our approach is effective in predicting hot spots and it compares favourably to other available methods in particular we find the best performances using transductive support vector machines a semi supervised learning scheme when hot spots are defined as those residues for which deltadeltag greater than or equal to kcal mol our method achieves a precision and a recall respectively of and conclusion we have developed an hybrid scheme in which energy terms are used as input features of machine learning models this strategy combines the strengths of machine learning and energy based methods although so far these two types of approaches have mainly been applied separately to biomolecular problems the results of our investigation indicate that there are substantial benefits to be gained by integration
summary we develop a novel mining pipeline integrative next generation genome analysis pipeline ingap guided by a bayesian principle to detect single nucleotide polymorphisms snps insertion deletions indels by comparing high throughput pyrosequencing reads with a reference genome of related organisms ingap can be applied to the mapping of both roche and illumina reads with no restriction of read length experiments on simulated and experimental data show that this pipeline can achieve overall accuracy in snp detection and in the finding of indels all the detected snps indels can be further evaluated by a graphical editor in our pipeline ingap also provides functions of multiple genomes comparison and assistance of bacterial genome assembly availability ingap is available at http sites google com site nextgengenomics ingap contact scs bx psu edu supplementary information supplementary data are available at bioinformatics bioinformatics
although drugs are intended to be selective at least some bind to several physiological targets explaining side effects and efficacy because many drug target combinations exist it would be useful to explore possible interactions computationally here we compared us food and drug administration fda approved and investigational drugs against hundreds of targets defining each target by its ligands chemical similarities between drugs and ligand sets predicted thousands of unanticipated associations thirty were tested experimentally including the antagonism of the beta receptor by the transporter inhibitor prozac the inhibition of the hydroxytryptamine ht transporter by the ion channel drug vadilex and antagonism of the histamine h receptor by the enzyme inhibitor rescriptor overall new drug target associations were confirmed five of which were potent nm the physiological relevance of one the drug n n dimethyltryptamine dmt on serotonergic receptors was confirmed in a knockout mouse the chemical similarity approach is systematic and comprehensive and may suggest side effects and new indications for drugs
recommending news stories to users based on their preferences has long been a favourite domain for recommender systems research in this paper we describe a novel approach to news recommendation that harnesses real time micro blogging activity from a service such as twitter as the basis for promoting news stories from a user s favourite rss feeds a preliminary evaluation is carried out on an implementation of this technique that shows results
cellular functions are mediated through complex systems of macromolecules and metabolites linked through biochemical and physical interactions represented in interactome models as nodes and edges respectively better understanding of genotype to phenotype relationships in human disease will require modeling of how disease causing mutations affect systems or interactome properties here we investigate how perturbations of interactome networks may differ between complete loss of gene products node removal and interaction specific or edge specific edgetic alterations global computational analyses of approximately known causative mutations in human mendelian disorders revealed clear separations of mutations probably corresponding to those of node removal versus edgetic perturbations experimental characterization of mutant alleles in various disorders identified diverse edgetic interaction profiles of mutant proteins which correlated with distinct structural properties of disease proteins and disease mechanisms edgetic perturbations seem to confer distinct functional consequences from node removal because a large fraction of cases in which a single gene is linked to multiple disorders can be modeled by distinguishing edgetic network perturbations edgetic network perturbation models might improve both the understanding of dissemination of disease alleles in human populations and the development of molecular strategies
pmid a strategy in drug design is to consider enhancing the affinity of lead molecules with structural modifications that displace water molecules from a protein binding site because success of the approach is uncertain clarification of the associated energetics was sought in cases where similar structural modifications yield qualitatively different outcomes specifically free energy perturbation calculations were carried out in the context of monte carlo statistical mechanics simulations to investigate ligand series that feature displacement of ordered water molecules in the binding sites of scytalone dehydratase map kinase and egfr kinase the change in affinity for a ligand modification is found to correlate with the ease of displacement of the ordered water molecule however as in the egfr example the binding affinity may diminish if the free energy increase due to the removal of the bound water molecule is not more than compensated by the additional interactions of the water displacing moiety for accurate computation of the effects of ligand modifications a complete thermodynamic analysis is shown to be needed it requires identification of the location of water molecules in the proteinligand interface and evaluation of the free energy changes associated with their removal and with the introduction of the ligand modification direct modification of the ligand in free energy calculations is likely to trap the ordered molecule and provide misleading guidance for optimization
after drifting apart for years the two worlds of genetics quantitative genetics and molecular genetics are finally coming together in genome wide association gwa research which shows that the heritability of complex traits and common disorders is due to multiple genes of small effect size we highlight a polygenic framework supported by recent gwa research in which qualitative disorders can be interpreted simply as being the extremes of quantitative dimensions research that focuses on quantitative traits including the low and high ends of normal distributions could have far reaching implications for the diagnosis treatment and prevention of the problematic extremes of traits
abstract path abs for years the term gene has been synonymous with regions of the genome encoding mrnas that are translated into protein however recent genome wide studies have shown that the human genome is pervasively transcribed and produces many thousands of regulatory non protein coding rnas ncrnas including micrornas small interfering rnas piwi interacting rnas and various classes of long ncrnas it is now clear that these rnas fulfil critical roles as transcriptional and post transcriptional regulators and as guides of chromatin modifying complexes here we review the biology of ncrnas focusing on the fundamental mechanisms by which ncrnas facilitate normal development and physiology and when dysfunctional underpin disease we also discuss evidence that intergenic regions associated with complex diseases express ncrnas as well as the potential use of ncrnas as diagnostic markers and therapeutic targets taken together these observations emphasize the need to move beyond the confines of protein coding genes and highlight the fact that continued investigation of ncrna biogenesis and function will be necessary for a comprehensive understanding of human disease copyright pathological society of great britain and ireland published by john wiley ltd
the availability and utility of genome scale metabolic reconstructions have exploded since the first genome scale reconstruction was published a decade ago reconstructions have now been built for a wide variety of organisms and have been used toward five major ends contextualization of high throughput data guidance of metabolic engineering directing hypothesis driven discovery interrogation of multi species relationships and network property discovery in this review we examine the many uses and future directions of genome scale metabolic reconstructions and we highlight trends and opportunities in the field that will make the greatest impact on many fields biology
bet hedgingstochastic switching between phenotypic a canonical example of an evolutionary adaptation that facilitates persistence in the face of fluctuating environmental conditions although bet hedging is found in organisms ranging from bacteria to direct evidence for an adaptive origin of this behaviour is here we report the de novo evolution of bet hedging in experimental bacterial populations bacteria were subjected to an environment that continually favoured new phenotypic states initially our regime drove the successive evolution of novel phenotypes by mutation and selection however in two of replicates this trend was broken by the evolution of bet hedging genotypes that persisted because of rapid stochastic phenotype switching genome re sequencing of one of these switching types revealed nine mutations that distinguished it from the ancestor the final mutation was both necessary and sufficient for rapid phenotype switching nonetheless the evolution of bet hedging was contingent upon earlier mutations that altered the relative fitness effect of the final mutation these findings capture the adaptive evolution of bet hedging in the simplest of organisms and suggest that risk spreading strategies may have been among the earliest evolutionary solutions to life in environments
development requires the establishment of precise patterns of gene expression which are primarily controlled by transcription factors binding to cis regulatory modules although transcription factor occupancy can now be identified at genome wide scales decoding this regulatory landscape remains a daunting challenge here we used a novel approach to predict spatio temporal cis regulatory activity based only on in vivo transcription factor binding and enhancer activity data we generated a high resolution atlas of cis regulatory modules describing their temporal and combinatorial occupancy during drosophila mesoderm development the binding profiles of cis regulatory modules with characterized expression were used to train support vector machines to predict five spatio temporal expression patterns in vivo transgenic reporter assays demonstrate the high accuracy of these predictions and reveal an unanticipated plasticity in transcription factor binding leading to similar expression this data driven approach does not require previous knowledge of transcription factor sequence affinity function or expression making it applicable
genomes are organized into high level three dimensional structures and dna elements separated by long genomic distances can in principle interact functionally many transcription factors bind to regulatory dna elements distant from gene promoters although distal binding sites have been shown to regulate transcription by long range chromatin interactions at a few loci chromatin interactions and their impact on transcription regulation have not been investigated in a genome wide manner here we describe the development of a new strategy chromatin interaction analysis by paired end tag sequencing chia pet for the de novo detection of global chromatin interactions with which we have comprehensively mapped the chromatin interaction network bound by oestrogen receptor er in the human genome we found that most high confidence remote er binding sites are anchored at gene promoters through long range chromatin interactions suggesting that er functions by extensive chromatin looping to bring genes together for coordinated transcriptional regulation we propose that chromatin interactions constitute a primary mechanism for regulating transcription in genomes
resumen se analiza la convergencia que se est produciendo en el campo de las ontologas entre ingeniera del conocimiento y organizacin del conocimiento en el marco del proyecto de la web semntica se estudia el desarrollo de la investigacin sobre ontologas en las ciencias de la documentacin y en el conjunto de las disciplinas que se interesan por los problemas ontolgicos se contextualiza el actual frente de investigacin en el campo de las ontologas en el marco del desarrollo de internet y especialmente de la web semntica finalmente se analizan las implicaciones de futuro para el profesional de la informacin integracin en un campo transdisciplinar ms amplio y con un gran porvenir clarificar su posicin en l y asegurar una formacin adecuada en los nuevos estndares y tecnologas the emerging convergence in the field of ontologies between knowledge organization and knowledge engineering is examined in the context of the semantic web project we describe the development of research on ontologies in library and information sciences and in other disciplines interested in ontological problems the emergence of a research agenda on ontologies is discussed in the context of the development of the internet and specifically in relation to the semantic web project finally some implications of this convergence for the information professional are discussed the integration of knowledge organization into a broader transdisciplinary research arena with a very promising future the need to clarify the role and potential contributions of the information professional and the urgency of adequate education and training in the new standards technologies
glass formation in colloidal suspensions has many of the hallmarks of glass formation in molecular for hard sphere colloids which interact only as a result of excluded volume phase behaviour is controlled by volume fraction an increase in drives the system towards its glassy state analogously to a decrease in temperature t in molecular systems when increases above the viscosity starts to increase significantly and the system eventually moves out of equilibrium at the glass transition where particle crowding greatly restricts structural the large particle size makes it possible to study both structure and dynamics with light and colloidal suspensions have therefore provided considerable insight into the glass transition however hard sphere colloidal suspensions do not exhibit the same diversity of behaviour as molecular glasses this is highlighted by the wide variation in behaviour observed for the viscosity or structural relaxation time when the glassy state is approached in supercooled molecular this variation is characterized by the unifying concept of which has spurred the search for a universal description of dynamic arrest in glass forming liquids for fragile liquids is highly sensitive to changes in t whereas non fragile or strong liquids show a much lower tsensitivity in contrast hard sphere colloidal suspensions are restricted to fragile behaviour as determined by their ultimately limiting their utility in the study of the glass transition here we show that deformable colloidal particles when studied through their concentration dependence at fixed temperature do exhibit the same variation in fragility as that observed in the tdependence of molecular liquids at fixed volume their fragility is dictated by elastic properties on the scale of individual colloidal particles furthermore we find an equivalent effect in molecular systems where elasticity directly reflects fragility colloidal suspensions may thus provide new insight into glass formation in systems
ensembl genomes http www ensemblgenomes org is a new portal offering integrated access to genome scale data from non vertebrate species of scientific interest developed using the ensembl genome annotation and visualisation platform ensembl genomes consists of five sub portals for bacteria protists fungi plants and invertebrate metazoa designed to complement the availability of vertebrate genomes in ensembl many of the databases supporting the portal have been built in close collaboration with the scientific community which we consider as essential for maintaining the accuracy and usefulness of the resource a common set of user interfaces which include a graphical genome browser ftp blast search a query optimised data warehouse programmatic access and a perl api is provided for all domains data types incorporated include annotation of protein and non protein coding genes cross references to external resources and high throughput experimental data e g data from large scale studies of gene expression and polymorphism visualised in their genomic context additionally extensive comparative analysis has been performed both within defined clades and across the wider taxonomy and sequence alignments and gene trees resulting from this can be accessed through site
most human diseases are complex multi factorial diseases resulting from the combination of various genetic and environmental factors in the kegg database resource http www genome jp kegg diseases are viewed as perturbed states of the molecular system and drugs as perturbants to the molecular system disease information is computerized in two forms pathway maps and gene molecule lists the kegg pathway database contains pathway maps for the molecular systems in both normal and perturbed states in the kegg disease database each disease is represented by a list of known disease genes any known environmental factors at the molecular level diagnostic markers and therapeutic drugs which may reflect the underlying molecular system the kegg drug database contains chemical structures and or chemical components of all drugs in japan including crude drugs and tcm traditional chinese medicine formulas and drugs in the usa and europe this database also captures knowledge about two types of molecular networks the interaction network with target molecules metabolizing enzymes other drugs etc and the chemical structure transformation network in the history of drug development the new disease drug information resource named kegg medicus can be used as a reference knowledge base for computational analysis of molecular networks especially by integrating large scale datasets
genome sequencing of large numbers of individuals promises to advance the understanding treatment and prevention of human diseases among other applications we describe a genome sequencing platform that achieves efficient imaging and low reagent consumption with combinatorial probe anchor ligation chemistry to independently assay each base from patterned nanoarrays of self assembling dna nanoballs we sequenced three human genomes with this platform generating an average of to fold coverage per genome and identifying to million sequence variants per genome validation of one genome data set demonstrates a sequence accuracy of about false variant per kilobases the high accuracy affordable cost of for sequencing consumables and scalability of this platform enable complete human genome sequencing for the detection of rare variants in large scale studies
the exponentially increasing number of published papers million per year by one estimate makes it more and more difficult for us to manage the flood of scientific information each of us has acquired some protocol to find and organize journal articles and other references over the course of our careers most of those protocols are likely to have been formed by old routines or idleness rather than a structured approach to save time and frustration over the long run furthermore with the web revolution new ways of handling information are emerging oreilly for example traditional standalone tools for reference management like endnote are being supplemented by centralized resources like refworks and social bookmarking sites as described subsequently this fusion of personal and public information offers the promise of efficiency through better organization which in turn leads to better science how can seasoned scientists do better using these tools and those newer to the field start off in the right way to start to answer that question i present ten simple rules to master the search and organization of new literature this is not meant to be comprehensive it represents the experiences of a few and i welcome your thoughts through comments to this article on what you do to keep your organized
the gene ontology go consists of nearly classes for describing the activities and locations of gene products manual maintenance of an ontology of this size is a considerable effort and errors and inconsistencies inevitably arise reasoners can be used to assist with ontology development automatically placing classes in a subsumption hierarchy based on their properties however the historic lack of computable definitions within the go has prevented the user of these tools in this paper we present preliminary results of an ongoing effort to normalize the go by explicitly stating the definitions of compositional classes in a form that can be used by reasoners these definitions are partitioned into mutually exclusive cross product sets many of which reference other obo foundry candidate ontologies for chemical entities proteins biological qualities and anatomical entities using these logical definitions we are gradually beginning to automate many aspects of ontology development detecting errors and filling in missing relationships these definitions also enhance the go by weaving it into the fabric of a wider collection of interoperating ontologies increasing opportunities for data integration and enhancing analyses
elucidating the biogeography of bacterial communities on the human body is critical for establishing healthy baselines from which to detect differences associated with diseases to obtain an integrated view of the spatial and temporal distribution of the human microbiota we surveyed bacteria from up to sites in seven to nine healthy adults on four occasions we found that community composition was determined primarily by body habitat within habitats interpersonal variability was high whereas individuals exhibited minimal temporal variability several skin locations harbored more diverse communities than the gut and mouth and skin locations differed in their community assembly patterns these results indicate that our microbiota although personalized varies systematically across body habitats and time such trends may ultimately reveal how microbiome changes cause or disease
the human genome project has been recently complemented by whole genome assessment sequence of mammals and nonmammalian vertebrate species suitable for comparative genomic analyses here we anticipate a precipitous drop in costs and increase in sequencing efficiency with concomitant development of improved annotation technology and therefore propose to create a collection of tissue and dna specimens for vertebrate species specifically designated for whole genome sequencing in the very near future for this purpose we the genome community of scientists will assemble and allocate a biospecimen collection of some representative vertebrate species spanning evolutionary diversity across living mammals birds nonavian reptiles amphibians and fishes ca living species in this proposal we present precise counts for these individual species with specimens presently tagged and stipulated for dna sequencing by the dna sequencing has ushered in a new era of investigation in the biological sciences allowing us to embark for the first time on a truly comprehensive study of vertebrate evolution the results of which will touch nearly every aspect of vertebrate biological jhered
the web is ephemeral many resources have representations that change over time and many of those representations are lost forever a lucky few manage to reappear as archived resources that carry their own uris for example some content management systems maintain version pages that reflect a frozen prior state of their changing resources archives recurrently crawl the web to obtain the actual representation of resources and subsequently make those available via special purpose archived resources in both cases the archival copies have uris that are protocol wise disconnected from the uri of the resource of which they represent a prior state indeed the lack of temporal capabilities in the most common web protocol http prevents getting to an archived resource on the basis of the uri of its original this turns accessing archived resources into a significant discovery challenge for both human and software agents which typically involves following a multitude of links from the original to the archival resource or of searching archives for the original uri this paper proposes the protocol based memento solution to address this problem and describes a proof of concept experiment that includes major servers of archival content including wikipedia and the internet archive the memento solution is based on existing http capabilities applied in a novel way to add the temporal dimension the result is a framework in which archived resources can seamlessly be reached via the uri of their original protocol based time travel for web
targeted enrichment of specific loci of the human genome is a promising approach to enable sequencing based studies of genetic variation in large populations here we describe an enrichment approach based on microdroplet pcr which enables million amplifications in parallel we sequenced six samples enriched by microdroplet or traditional singleplex pcr using primers targeting exons of genes both methods generated similarly high quality data of the uniquely mapping reads fell within the targeted sequences coverage was uniform across of targeted bases sequence variants were called with accuracy and reproducibility between samples was high we scaled the microdroplet pcr to amplicons totaling mb of sequence sequenced the resulting sample with both illumina gaii and roche and obtained data with equally high specificity and sensitivity our results demonstrate that microdroplet technology is well suited for processing dna for massively parallel enrichment of specific subsets of the human genome for sequencing
dna sequencing by synthesis sbs technology using a polymerase or ligase enzyme as its core biochemistry has already been incorporated in several second generation dna sequencing systems with significant performance notwithstanding the substantial success of these sbs platforms challenges continue to limit the ability to reduce the cost of sequencing a human genome to or less achieving dramatically reduced cost with enhanced throughput and quality will require the seamless integration of scientific and technological effort across disciplines within biochemistry chemistry physics and engineering the challenges include sample preparation surface chemistry fluorescent labels optimizing the enzyme substrate system optics instrumentation understanding tradeoffs of throughput versus accuracy and read length phasing limitations by framing these challenges in a manner accessible to a broad community of scientists and engineers we hope to solicit input from the broader research community on means of accelerating the advancement of genome technology
bacterial genomes are organized by structural and functional elements including promoters transcription start and termination sites open reading frames regulatory noncoding regions untranslated regions and transcription units here we iteratively integrate high throughput genome wide measurements of rna polymerase binding locations and mrna transcript abundance sequences and translation into proteins to determine the organizational structure of the escherichia coli k genome integration of the organizational elements provides an experimentally annotated transcription unit architecture including alternative transcription start sites untranslated region boundaries and open reading frames of each transcription unit a total of transcription units were identified representing an increase of over current knowledge this comprehensive transcription unit architecture allows for the elucidation of condition specific uses of alternative sigma factors at the genome scale furthermore the transcription unit architecture provides a foundation on which to construct genome scale transcriptional and translational networks
background despite increasing interest in the noncoding fraction of transcriptomes the number species conservation and functions if any of many non protein coding transcripts remain to be discovered two extensive long intergenic noncoding rna ncrna transcript catalogues are now available for mouse over macrornas identified by cdna sequencing and long intergenic noncoding rna lincrna intervals that are predicted from chromatin state maps previously we showed that macrornas tend to be more highly conserved than putatively neutral sequence although only of bases are predicted as constrained by contrast over a thousand lincrnas were reported as being highly conserved this apparent difference may account for the surprisingly small fraction of transcripts that are represented in both catalogues here we sought to resolve the reported discrepancy between the evolutionary rates for these two sets results our analyses reveal lincrna and macrorna exon sequences to be subject to the same relatively low degree of sequence constraint nonetheless our observations are consistent with the functionality of a fraction of ncrna in these sets with up to a quarter of ncrna exons having evolved significantly slower than neighboring neutral sequence the more tissue specific macrornas are enriched in predicted rna secondary structures and thus may often act in trans whereas the more highly and broadly expressed lincrnas appear more likely to act in the cis regulation of adjacent transcription factor genes conclusions taken together our results indicate that each of the two ncrna catalogues unevenly and lightly samples the true much larger ncrna repertoire of mouse
micrornas mirnas regulate gene expression posttranscriptionally by interfering with a target mrna s translation stability or both we sought to dissect the respective contributions of translational inhibition and mrna decay to microrna regulation we identified direct targets of a specific mirna mir by virtue of their association with argonaute proteins core components of mirna effector complexes in response to mir transfection in human tissue culture cells in parallel we assessed mrna levels and obtained translation profiles using a novel global approach to analyze polysomes separated on sucrose gradients analysis of translation profiles for approximately genes in these proliferative human cells revealed that basic features of translation are similar to those previously observed in rapidly growing saccharomyces cerevisiae for approximately mrnas specifically recruited to argonaute proteins by mir we found reductions in both the mrna abundance and inferred translation rate spanning a large dynamic range the changes in mrna levels of these mir targets were larger than the changes in translation with average decreases of and respectively further there was no identifiable subgroup of mrna targets for which the translational response was dominant both ribosome occupancy the fraction of a given gene s transcripts associated with ribosomes and ribosome density the average number of ribosomes bound per unit length of coding sequence were selectively reduced for hundreds of mir targets by the presence of mir changes in protein abundance inferred from the observed changes in mrna abundance and translation profiles closely matched changes directly determined by western analysis for of proteins suggesting that our assays captured most of mir mediated regulation these results suggest that mirnas inhibit translation initiation or stimulate ribosome drop off preferentially near the start site and are not consistent with inhibition of polypeptide elongation or nascent polypeptide degradation contributing significantly to mirna mediated regulation in proliferating cells the observation of concordant changes in mrna abundance and translational rate for hundreds of mir targets is consistent with a functional link between these two regulatory outcomes of mirna targeting and the well documented interrelationship between translation and decay
motivation non coding micrornas mirnas act as regulators of global protein output while their major effect is on protein levels of target genes it has been proven that they also specifically impact on the messenger rna level of targets prominent interest in mirnas strongly motivates the need for increasing the options available to detect their cellular activity results we used the effect of mirnas over their targets for the detection of mirna activity using mrnas expression profiles here we describe the method called t rex from targets reverse expression compare it to other similar applications show its effectiveness and apply it to build activity maps we used six different target predictions from each of four algorithms targetscan pictar diana microt and diana union first we proved the sensitivity and specificity of our technique in mirna over expression and knock out animal models then we used whole transcriptome data from acute myeloid leukemia to show that we could identify critical mirnas in a real life complex clinically relevant dataset finally we studied different cellular conditions to confirm and extend the current knowledge on the role of mirnas in cellular physiology and in cancer availability software is available at http aqua unife it and is free for all users with no requirement
background natural history science is characterised by a single immense goal to document describe and synthesise all facets pertaining to the diversity of life that can only be addressed through a seemingly infinite series of smaller studies the discipline s failure to meaningfully connect these small studies with natural history s goal has made it hard to demonstrate the value of natural history to a wider scientific community digital technologies provide the means to bridge this gap results we describe the system architecture and template design of scratchpads a data publishing framework for groups of people to create their own social networks supporting natural history science scratchpads cater to the particular needs of individual research communities through a common database and system architecture this is flexible and scalable enough to support multiple networks each with its own choice of features visual design and constituent data our data model supports web services on standardised data elements that might be used by related initiatives such as gbif and the encyclopedia of life a scratchpad allows users to organise data around user defined or imported ontologies including biological classifications automated semantic annotation and indexing is applied to all content allowing users to navigate intuitively and curate diverse biological data including content drawn from third party resources a system of archiving citable pages allows stable referencing with unique identifiers and provides credit to contributors through normal citation processes conclusion our framework http scratchpads eu currently serves more than registered users across sites spanning academic amateur and citizen science audiences these users have generated more than nodes of content in the first two years of use the template of our architecture may serve as a model to other research communities developing data publishing frameworks outside research
the new generation of massively parallel dna sequencers combined with the challenge of whole human genome resequencing result in the need for rapid and accurate alignment of billions of short dna sequence reads to a large reference genome speed is obviously of great importance but equally important is maintaining alignment accuracy of short reads in the base range in the presence of errors and true variation
an important aim of synthetic biology is to uncover the design principles of natural biological systems through the rational design of gene and protein circuits here we highlight how the process of engineering biological systems from synthetic promoters to the control of cellcell interactions has contributed to our understanding of how endogenous systems are put together and function synthetic biological devices allow us to grasp intuitively the ranges of behaviour generated by simple biological circuits such as linear cascades and interlocking feedback loops as well as to exert control over natural processes such as gene expression and dynamics
direct reprogramming of somatic cells into induced pluripotent stem ips cells can be achieved by overexpression of and c myc transcription factors but only a minority of donor somatic cells can be reprogrammed to pluripotency here we demonstrate that reprogramming by these transcription factors is a continuous stochastic process where almost all mouse donor cells eventually give rise to ips cells on continued growth and transcription factor expression additional inhibition of the pathway or overexpression of increased the cell division rate and resulted in an accelerated kinetics of ips cell formation that was directly proportional to the increase in cell proliferation in contrast nanog overexpression accelerated reprogramming in a predominantly cell division rate independent manner quantitative analyses define distinct cell division rate dependent and independent modes for accelerating the stochastic course of reprogramming and suggest that the number of cell divisions is a key parameter driving epigenetic reprogramming pluripotency
the signalling pathways controlling both the evolution and development of language in the human brain remain unknown so far the transcription factor forkhead box is the only gene implicated in mendelian forms of human speech and language dysfunction it has been proposed that the amino acid composition in the human variant of has undergone accelerated evolution and this two amino acid change occurred around the time of language emergence in humans however this remains controversial and whether the acquisition of these amino acids in human has any functional consequence in human neurons remains untested here we demonstrate that these two human specific amino acids alter function by conferring differential transcriptional regulation in vitro we extend these observations in vivo to human and chimpanzee brain and use network analysis to identify novel relationships among the differentially expressed genes these data provide experimental support for the functional relevance of changes in that occur on the human lineage highlighting specific pathways with direct consequences for human brain development and disease in the central nervous system cns because has an important role in speech and language in humans the identified targets may have a critical function in the development and evolution of language circuitry humans
background biochemical pathways provide an essential context for understanding comprehensive experimental data and the systematic workings of a cell therefore the availability of online pathway browsers will facilitate post genomic research just as genome browsers have contributed to genomics many pathway maps have been provided online as part of public pathway databases most of these maps however function as the gateway interface to a specific database and the comprehensiveness of their represented entities data mapping capabilities and user interfaces are not always sufficient for generic usage methodology principal findings we have identified five central requirements for a pathway browser availability of large integrated maps showing genes enzymes and metabolites comprehensive search features and data access data mapping for transcriptomic proteomic and metabolomic experiments as well as the ability to edit and annotate pathway maps easy exchange of pathway data and intuitive user experience without the requirement for installation and regular maintenance according to these requirements we have evaluated existing pathway databases and tools and implemented a web based pathway browser named pathway projector as a solution conclusions significance pathway projector provides integrated pathway maps that are based upon the kegg atlas with the addition of nodes for genes and enzymes and is implemented as a scalable zoomable map utilizing the google maps api users can search pathway related data using keywords molecular weights nucleotide sequences and amino acid sequences or as possible routes between compounds in addition experimental data from transcriptomic proteomic and metabolomic analyses can be readily mapped pathway projector is freely available for academic users at http www g language pathwayprojector
the field of synthetic biology holds an inspiring vision for the future it integrates computational analysis biological data and the systems engineering paradigm in the design of new biological machines and systems these biological machines are built from basic biomolecular components analogous to electrical devices and the information flow among these components requires the augmentation of biological insight with the power of a formal approach to information management here we review the informatics challenges in synthetic biology along three dimensions in silico in vitro and in vivo first we describe state of the art of the in silico support of synthetic biology from the specific data exchange formats to the most popular software platforms and algorithms next we cast in vitro synthetic biology in terms of information flow and discuss genetic fidelity in dna manipulation development strategies of biological parts and the regulation of biomolecular networks finally we explore how the engineering chassis can manipulate biological circuitries in vivo to give rise to future artificial bib
collaborative tagging systems pose new challenges to the developers of recommender systems as observed by recent research traditional implementations of classic recommender approaches such as collaborative filtering are not working well in this new context to address these challenges a number of research groups worldwide work on adapting these approaches to the specific nature of collaborative tagging systems in joining this stream of research we have developed and compared three variants of user based collaborative filtering algorithms to provide recommendations of articles on citeulike the first approach classic collaborative filtering ccf uses pearson correlation to calculate similarity between users and a classic adjusted ratings formula to rank the recommendations the second approach neighbor weighted collaborative filtering takes into account the number of raters in the ranking formula of the recommendations the third approach explores an innovative way to form the user neighborhood based on a modified version of the okapi model over users tags our results suggest that both alterations of ccf are beneficial incorporating the number of raters into the algorithms leads to an improvement of precision while tag based can be considered as an alternative to pearson correlation to calculate the similarity between users and neighbors
background gene prediction is an essential step in the annotation of metagenomic sequencing reads since most metagenomic reads cannot be assembled into long contigs specialized statistical gene prediction tools have been developed for short and anonymous dna fragments e g metageneannotator and orphelia while conventional gene prediction methods have been subject to a benchmark study on real sequencing reads with typical errors such a comparison has not been conducted for specialized tools yet their gene prediction accuracy was mostly measured on error free dna fragments results in this study sanger and pyrosequencing reads were simulated on the basis of models that take all types of sequencing errors into account all metagenomic gene prediction tools showed decreasing accuracy with increasing sequencing error rates performance results on an established metagenomic benchmark dataset are also reported in addition we demonstrate that estscan a tool for sequencing error compensation in eukaryotic expressed sequence tags outperforms some metagenomic gene prediction tools on reads with high error rates although it was not designed for the task at hand conclusion this study fills an important gap in metagenomic gene prediction research specialized methods are evaluated and compared with respect to sequencing error robustness results indicate that the integration of error compensating methods into metagenomic gene prediction tools would be beneficial to improve metagenome quality
motivation microarray based gene expression data have been generated widely to study different biological processes and systems gene co expression networks are often used to extract information about groups of genes that are functionally related or co regulated however the structural properties of such co expression networks have not been rigorously studied and fully compared with known biological networks in this article we aim at investigating the structural properties of co expression networks inferred for the species saccharomyces cerevisiae and comparing them with the topological properties of the known well established transcriptional network mips physical network and protein protein interaction ppi network of yeast results these topological comparisons indicate that co expression networks are not distinctly related with either the ppi or the mips physical interaction networks showing important structural differences between them when focusing on a more literal comparison vertex by vertex and edge by edge the conclusion is the same the fact that two genes exhibit a high gene expression correlation degree does not seem to obviously correlate with the existence of a physical binding between the proteins produced by these genes or the existence of a mips physical interaction between the genes the comparison of the yeast regulatory network with inferred yeast co expression networks would suggest however that they could somehow be related conclusions we conclude that the gene expression based co expression networks reflect more on the gene regulatory networks but less on the ppi or mips physical interaction networks supplementary information supplementary data are available at online
the idea of date and party hubs has been influential in the study of protein protein interaction networks date hubs display low co expression with their partners whilst party hubs have high co expression it was proposed that party hubs are local coordinators whereas date hubs are global connectors here we show that the reported importance of date hubs to network connectivity can in fact be attributed to a tiny subset of them crucially these few extremely central hubs do not display particularly low expression correlation undermining the idea of a link between this quantity and hub function the date party distinction was originally motivated by an approximately bimodal distribution of hub co expression we show that this feature is not always robust to methodological changes additionally topological properties of hubs do not in general correlate with co expression however we find significant correlations between interaction centrality and the functional similarity of the interacting proteins we suggest that thinking in terms of a date party dichotomy for hubs in protein interaction networks is not meaningful and it might be more useful to conceive of roles for protein protein interactions rather than for proteins
during the last decade the science of networks has grown into an enormous interdisciplinary endeavor with methods and applications drawn from across the natural social and information sciences one of the most important and prominent ideas from network science is the algorithmic detection of tightly connected groups of nodes known as communities here we develop a formulation to detect communities in a very broad setting by studying general dynamical processes on networks we create a new framework of network quality functions that allows us to study the community structure of arbitrary multislice networks which are combinations of individual networks coupled through additional links that connect each node in one network slice to itself in other slices this new framework allows one for the first time to study community structure in a very general setting that encompasses networks that evolve in time have multiple types of ties multiplexity and have scales
summary it is expected that emerging digital gene expression dge technologies will overtake microarray technologies in the near future for many functional genomics applications one of the fundamental data analysis tasks especially for gene expression studies involves determining whether there is evidence that counts for a transcript or exon are significantly different across experimental conditions edger is a bioconductor software package for examining differential expression of replicated count data an overdispersed poisson model is used to account for both biological and technical variability empirical bayes methods are used to moderate the degree of overdispersion across transcripts improving the reliability of inference the methodology can be used even with the most minimal levels of replication provided at least one phenotype or experimental condition is replicated the software may have other applications beyond sequencing data such as proteome peptide count data availability the package is freely available under the lgpl licence from the bioconductor web site http bioconductor org contact mrobinson wehi edu bioinformatics
jaspar http jaspar genereg net is the leading open access database of matrix profiles describing the dna binding patterns of transcription factors tfs and other proteins interacting with dna in a sequence specific manner its fourth major release is the largest expansion of the core database to date the database now holds non redundant curated profiles the new entries include the first batch of profiles derived from chip seq and chip chip whole genome binding experiments and yeast tf binding profiles the introduction of a yeast division brings the convenience of jaspar to an active research community as binding models are refined by newer data the jaspar database now uses versioning of matrices in this release of the older models were updated to improved versions classification of tf families has been improved by adopting a new dna binding domain nomenclature a curated catalog of mammalian tfs is provided extending the use of the jaspar profiles to additional tfs belonging to the same structural family the changes in the database set the system ready for more rapid acquisition of new high throughput data sources additionally three new special collections provide matrix profile data produced by recent alternative high approaches
we demonstrate the first successful application of exome sequencing to discover the gene for a rare mendelian disorder of unknown cause miller syndrome mim for four affected individuals in three independent kindreds we captured and sequenced coding regions to a mean coverage of and sufficient depth to call variants at approximately of each targeted exome filtering against public snp databases and eight hapmap exomes for genes with two previously unknown variants in each of the four individuals identified a single candidate gene dhodh which encodes a key enzyme in the pyrimidine de novo biosynthesis pathway sanger sequencing confirmed the presence of dhodh mutations in three additional families with miller syndrome exome sequencing of a small number of unrelated affected individuals is a powerful efficient strategy for identifying the genes underlying rare mendelian disorders and will likely transform the genetic analysis of traits
what qualifies a neural representation for a role in subjective experience previous evidence suggests that the duration and intensity of the neural response to a sensory stimulus are factors we introduce another attribute the reproducibility of a pattern of neural activity across different episodes that predicts specific and measurable differences between conscious and nonconscious neural representations independently of duration and intensity we found that conscious neural activation patterns are relatively reproducible when compared with nonconscious neural activation patterns corresponding to the same perceptual content this is not adequately explained by a difference in signal to ratio
pmid conformational flexibility of proteins has been linked to their designated functions slow conformational fluctuations occurring at the microsecond to millisecond time scale in particular have recently attracted considerable interest in connection to the mechanism of enzyme catalysis computational methods are providing valuable insights into the connection between protein structure flexibility and function in this report we present studies on identification and characterization of microsecond flexibility of ubiquitin based on quasi harmonic analysis qha and normal mode analysis nma the results indicate that the slowest qha modes computed from the s molecular dynamics ensemble contribute over of all motions the identified slow movements show over similarity with the conformational fluctuations observed in nuclear magnetic resonance ensemble and also agree with displacements in the set of x ray structures the slowest modes show high flexibility in the and loop regions with functional implications in the mechanism of binding other proteins nma of ubiquitin structures was not able to reproduce the long time scale fluctuations as they were found to strongly depend on the reference structures further conformational fluctuations coupled to the cis trans isomerization reaction catalyzed by the enzyme cyclophilin a cypa occurring at the microsecond to millisecond time scale have also been identified and characterized on the basis of qha of conformations sampled along the reaction pathway the results indicate that qha covers the same conformational landscape as the experimentally observed cypa flexibility overall the identified slow conformational fluctuations in ubiquitin and cypa indicate that the intrinsic flexibility of these proteins is closely linked to their functions
attending to a stimulus enhances its neuronal representation even at the level of primary sensory cortex cross modal modulation can similarly enhancea neuronal representation and this process can also operate at the primary cortical level phase reset of ongoing neuronal oscillatory activity has been shown to be an important element of the underlying modulation of local cortical excitability in both cases we investigated the influence of attention on oscillatory phase reset in primary auditory and visual cortices of macaques performing an intermodal selective attention task in addition to responses driven by preferred modality stimuli we noted that both preferred and nonpreferred modality stimuli could modulate local cortical excitability by phase reset of ongoing oscillatory activity and that this effect was linked to their being attended these findings outline a supramodal mechanism by which attention can control neurophysiological context thus determining the representation of specific sensory content in primary cortex
online gene annotation resources are indispensable for analysis of genomics data however the landscape of these online resources is highly fragmented and scientists often visit dozens of these sites for each gene in a candidate gene list here we introduce biogps http biogps gnf org webcite a centralized gene portal for aggregating distributed gene annotation resources moreover biogps embraces the principle of community intelligence enabling any user to easily and directly contribute to the platform
the ability to represent scientific data and concepts visually is becomingincreasingly important due to the unprecedented exponential growth ofcomputational power during the present digital age the data sets andsimulations scientists in all fields can now create are literally thousands oftimes as large as those created just years ago historically successfulmethods for data visualization can and should be applied to today s huge datasets but new approaches also enabled by technology are needed as well increasingly modular craftsmanship will be applied as relevantfunctionality from the graphically and technically best tools for a job arecombined as needed without low programming
background biological sequences play a major role in molecular and computational biology they are studied as information bearing entities that make up dna rna or proteins the sequence ontology which is part of the obo foundry contains descriptions and definitions of sequences and their properties yet the most basic question about sequences remains unanswered what kind of entity is a biological sequence an answer to this question benefits formal ontologies that use the notion of biological sequences and analyses in computational biology alike results we provide both an ontological analysis of biological sequences and a formal representation that can be used in knowledge based applications and other ontologies we distinguish three distinct kinds of entities that can be referred to as biological sequence chains of molecules syntactic representations such as those in biological databases and the abstract information bearing entities for use in knowledge based applications and inclusion in biomedical ontologies we implemented the developed axiom system for use in automated theorem proving conclusion axioms are necessary to achieve the main goal of ontologies to formally specify the meaning of terms used within a domain the axiom system for the ontology of biological sequences is the first elaborate axiom system for an obo foundry ontology and can serve as starting point for the development of more formal ontologies and ultimately of knowledge applications
motivation micrornas mirnas are short non coding rnas that regulate gene expression by inhibiting target mrna genes their tissue and disease specific expression patterns have immense therapeutic and diagnostic potential to understand these patterns a reliable compilation of mirna and mrna expression data is required to compare multiple tissue types moreover with the appropriate statistical tools such a resource could be interrogated to discover functionally related mirna mrna pairs results we have developed mimirna an online resource that integrates expression data from samples and permits visualization of the expression of human mirnas across different tissues or cell types mimirna incorporates a novel sample classification algorithm exparser that groups identical mirna or mrna experiments from separate sources this enables mimirna to provide reliable expression profiles and to discover functional relations between mirnas and mrnas such as mirna targets additionally mimirna incorporates a decision tree algorithm to discover distinguishing mirna features between two tissue or cell types we validate the efficacy of our resource on independent experimental data and through biologically relevant analyses availability http mimirna centenary org au contact j rasko centenary org au supplementary information supplementary data are available at bioinformatics bioinformatics
summary systems biology markup language sbml is the leading exchange format for mathematical models in systems biology semantic annotations link model elements with external knowledge via unique database identifiers and ontology terms enabling software to check and process models by their biochemical meaning such information is essential for model merging one of the key steps towards the construction of large kinetic models semanticsbml is a tool that helps users to check and edit miriam annotations and sbo terms in sbml models using a large collection of biochemical names and database identifiers it supports modellers in finding the right annotations and in merging existing models initially an element matching is derived from the miriam annotations and conflicting element attributes are categorized and highlighted conflicts can then be resolved automatically or manually allowing the user to control the merging process in detail availability semanticsbml comes as a free software written in python and released under the gpl a debian package a source package for other linux distributions a windows installer and an online version of semanticsbml with limited functionality are available at http www semanticsbml org a preinstalled version can be found on the linux live dvd sb os available at http www sbos eu contact wolfram liebermeister biologie hu berlin desupplementary information supplementary data are available at online
molecular regulation of embryonic stem cell esc fate involves a coordinated interaction between and mechanisms it is unclear how these different molecular regulatory mechanisms interact to regulate changes in stem cell fate here we present a dynamic systems level study of cell fate change in murine escs following a well defined perturbation global changes in histone acetylation chromatin bound rna polymerase ii messenger rna mrna and nuclear protein levels were measured over after downregulation of nanog a key pluripotency our data demonstrate how a single genetic perturbation leads to progressive widespread changes in several molecular regulatory layers and provide a dynamic view of information flow in the epigenome transcriptome and proteome we observe that a large proportion of changes in nuclear protein levels are not accompanied by concordant changes in the expression of corresponding mrnas indicating important roles for translational and post translational regulation of esc fate gene ontology analysis across different molecular layers indicates that although chromatin reconfiguration is important for altering cell fate it is preceded by transcription factor mediated regulatory events the temporal order of gene expression alterations shows the order of the regulatory network reconfiguration and offers further insight into the gene regulatory network our studies extend the conventional systems biology approach to include many molecular species regulatory layers and temporal series and underscore the complexity of the multilayer regulatory mechanisms responsible for changes in protein expression that determine stem fate
over the past few years techniques have been developed that have allowed the study of transcriptomes without bias from previous genome annotations which has led to the discovery of a plethora of unexpected rnas that have no obvious coding capacities there are many different kinds of products that are generated by this pervasive transcription this review focuses on small non coding rnas ncrnas that have been found to be associated with promoters in eukaryotes from animals to yeast after comparing the different classes of such ncrnas described in various studies the review discusses how the models proposed for their origins and their possible functions challenge previous views of the basic transcription process and regulation
the contribution of horizontal gene transfer to evolution has been controversial since it was suggested to be a force driving evolution in the microbial world in this paper i review the current standpoint on horizontal gene transfer in evolutionary thinking and discuss how important horizontal gene transfer is in evolution in the broad sense and particularly in prokaryotic evolution i review recent literature asking first which processes are involved in the evolutionary success of transferred genes and secondly about the extent of horizontal gene transfer towards different evolutionary times moreover i discuss the feasibility of reconstructing ancient phylogenetic relationships in the face of horizontal gene transfer finally i discuss how horizontal gene transfer fits in the current neo darwinian evolutionary paradigm and conclude there is a need for a new evolutionary paradigm that includes horizontal gene transfer as well as other mechanisms in the explanation evolution
high performance scientific computing has recently seen a surge of interest in heterogeneous systems with an emphasis on modern graphics processing units gpus these devices offer tremendous potential for performance and efficiency in important large scale applications of computational science however exploiting this potential can be challenging as one must adapt to the specialized and rapidly evolving computing environment currently exhibited by gpus one way of addressing this challenge is to embrace better techniques and develop tools tailored to their needs this article presents one simple technique gpu run time code generation rtcg and pycuda an open source toolkit that supports this technique in introducing pycuda this article proposes the combination of a dynamic high level scripting language with the massive performance of a gpu as a compelling two tiered computing platform potentially offering significant performance and productivity advantages over conventional single tier static systems it is further observed that compared to competing techniques the effort required to create codes using run time code generation with pycuda grows more gently in response to growing needs the concept of rtcg is simple and easily implemented using existing robust tools nonetheless it is powerful enough to support and encourage the creation of custom application specific tools by its users the premise of the paper is illustrated by a wide range of examples where the technique has been applied with success
gamma oscillations are thought to transiently link distributed cell assemblies that are processing related information a function that is probably important for network processes such as perception attentional selection and memory this binding mechanism requires that spatially distributed cells fire together with millisecond range precision however it is not clear how such coordinated timing is achieved given that the frequency of gamma oscillations varies substantially across space and time from approximately to almost hz here we show that gamma oscillations in the area of the hippocampus split into distinct fast and slow frequency components that differentially couple to inputs from the medial entorhinal cortex an area that provides information about the animal s current position and a hippocampal subfield essential for storage of such information fast gamma oscillations in were synchronized with fast gamma in medial entorhinal cortex and slow gamma oscillations in were coherent with slow gamma in significant proportions of cells in medial entorhinal cortex and were phase locked to fast and slow gamma waves respectively the two types of gamma occurred at different phases of the theta rhythm and mostly on different theta cycles these results point to routeing of information as a possible function of gamma frequency variations in the brain and provide a mechanism for temporal segregation of potentially interfering information from sources
this book presents international practices in the development and use of applied e learning and e teaching in the classroom in order to enhance student experience add value to teaching practices and illuminate best practices in the area of e assessment this book provides insight into e learning and e teaching practices while exploring the roles of academic staff in adoption and application provided publisher
we report an improved draft nucleotide sequence of the gigabase genome of maize an important crop plant and model for biological research over genes were predicted of which were placed on reference chromosomes nearly of the genome is composed of hundreds of families of transposable elements dispersed nonuniformly across the genome these were responsible for the capture and amplification of numerous gene fragments and affect the composition sizes and positions of centromeres we also report on the correlation of methylation poor regions with mu transposon insertions and recombination and copy number variants with insertions and or deletions as well as how uneven gene losses between duplicated regions were involved in returning an ancient allotetraploid to a genetically diploid state these analyses inform and set the stage for further investigations to improve our understanding of the domestication and agricultural improvements maize
abstract asi abs using the scopus dataset a grand matrix of aggregated journal journal citations was constructed this matrix can be compared in terms of the network structures with the matrix contained in the journal citation reports jcr of the institute of scientific information isi because the scopus database contains a larger number of journals and covers the humanities one would expect richer maps however the matrix is in this case sparser than in the case of the isi data this is because of a the larger number of journals covered by scopus and b the historical record of citations older than years contained in the isi database when the data is highly structured as in the case of large journals the maps are comparable although one may have to vary a threshold because of the differences in densities in the case of interdisciplinary journals and journals in the social sciences and humanities the new database does not add a lot to what is possible with the databases
the gene expression atlas http www ebi ac uk gxa is an added value database providing information about gene expression in different cell types organism parts developmental stages disease states sample treatments and other biological experimental conditions the content of this database derives from curation re annotation and statistical analysis of selected data from the arrayexpress archive of functional genomics data a simple interface allows the user to query for differential gene expression either i by gene names or attributes such as gene ontology terms or ii by biological conditions e g diseases organism parts or cell types the gene queries return the conditions where expression has been reported while condition queries return which genes are reported to be expressed in these conditions a combination of both query types is possible the query results are ranked using various statistical measures and by how many independent studies in the database show the particular gene condition association currently the database contains information about more than genes from nine species and almost biological conditions studied in over assays from over studies
the gene ontology go consortium http www geneontology org goc continues to develop maintain and use a set of structured controlled vocabularies for the annotation of genes gene products and sequences the go ontologies are expanding both in content and in structure several new relationship types have been introduced and used along with existing relationships to create links between and within the go domains these improve the representation of biology facilitate querying and allow go developers to systematically check for and correct inconsistencies within the go gene product annotation using go continues to increase both in the number of total annotations and in species coverage go tools such as obo edit an ontology editing tool and amigo the goc ontology browser have seen major improvements in functionality speed and ease use
background ctcf ccctc binding factor is an evolutionarily conserved zinc finger protein involved in diverse functions ranging from negative regulation of myc to chromatin insulation of the beta globin gene cluster to imprinting of the locus the zinc fingers of ctcf are known to differentially contribute to the ctcf dna interaction at different binding sites it is possible that the differences in ctcf dna conformation at different binding sites underlie ctcf s functional diversity if so the ctcf binding sites may belong to distinct classes each compatible with a specific functional role results we have classified approximately ctcf binding sites in t cells into three classes based on their similarity to the well characterized ctcf dna binding motif we have comprehensively characterized these three classes of ctcf sites with respect to several evolutionary genomic epigenomic transcriptomic and functional features we find that the low occupancy sites tend to be cell type specific furthermore while the high occupancy sites associate with repressive histone marks and greater gene co expression within a ctcf flanked block the low occupancy sites associate with active histone marks and higher gene expression we found that the low occupancy sites have greater conservation in their flanking regions compared to high occupancy sites interestingly based on a novel class conservation metric we observed that human low occupancy sites tend to be conserved as low occupancy sites in mouse and vice versa more frequently than expected conclusions our work reveals several key differences among ctcf occupancy based classes and suggests a critical yet distinct functional role played by low sites
bib the ability to sequence the dna of an organism has become one of the most important tools in modern biological research until recently the sequencing of even small model genomes required substantial funds and international collaboration the development of second generation sequencing technology has increased the throughput and reduced the cost of sequence generation by several orders of magnitude these new methods produce vast numbers of relatively short reads usually at the expense of read accuracy since the first commercial second generation sequencing system was produced by technologies and commercialised by roche several other companies including illumina applied biosystems helicos biosciences and pacific biosciences have joined the competition because of the relatively high error rate and lack of assembly tools short read sequence technology has mainly been applied to the re sequencing of genomes however some recent applications have focused on the de novo assembly of these data de novo assembly remains the greatest challenge for dna sequencing and there are specific problems for second generation sequencing which produces short reads with a high error rate however a number of different approaches for short read assembly have been proposed and some have been implemented in working software in this review we compare the current approaches for second generation genome sequencing explore the future direction of this technology and the implications for plant research
background the year is the anniversary of the publication of jean bapteste lamarck s philosophie zoologique and the anniversary of charles darwin s on the origin of species lamarck believed that evolution is driven primarily by non randomly acquired beneficial phenotypic changes in particular those directly affected by the use of organs which lamarck believed to be inheritable in contrast darwin assigned a greater importance to random undirected change that provided material for natural selection the concept the classic lamarckian scheme appears untenable owing to the non existence of mechanisms for direct reverse engineering of adaptive phenotypic characters acquired by an individual during its life span into the genome however various evolutionary phenomena that came to fore in the last few years seem to fit a more broadly interpreted quasi lamarckian paradigm the prokaryotic crispr cas system of defense against mobile elements seems to function via a bona fide lamarckian mechanism namely by integrating small segments of viral or plasmid dna into specific loci in the host prokaryote genome and then utilizing the respective transcripts to destroy the cognate mobile element dna or rna a similar principle seems to be employed in the pirna branch of rna interference which is involved in defense against transposable elements in the animal germ line horizontal gene transfer hgt a dominant evolutionary process at least in prokaryotes appears to be a form of quasi lamarckian inheritance the rate of hgt and the nature of acquired genes depend on the environment of the recipient organism and in some cases the transferred genes confer a selective advantage for growth in that environment meeting the lamarckian criteria various forms of stress induced mutagenesis are tightly regulated and comprise a universal adaptive response to environmental stress in cellular life forms stress induced mutagenesis can be construed as a quasi lamarckian phenomenon because the induced genomic changes although random are triggered by environmental factors and are beneficial to the organism conclusion both darwinian and lamarckian modalities of evolution appear to be important and reflect different aspects of the interaction between populations and the environment reviewers this article was reviewed by juergen brosius valerian dolja and martijn huynen for complete reports see the reviewers section
most function prediction methods that identify cognate ligands from binding site analyses work on the assumption of molecular complementarity these approaches build on the conjectured complementarity of geometrical and physicochemical properties between ligands and binding sites so that similar binding sites will bind similar ligands we found that this assumption does not generally hold for protein ligand interactions and observed that it is not the chemical composition of ligand molecules that dictates the complementarity between protein and ligand molecules but that the ligand s share within the functional mechanism of a protein determines the degree of complementarity here we present for a set of cognate ligands a descriptive analysis and comparison of the physicochemical properties that each ligand experiences in various nonhomologous binding pockets the comparisons in each ligand set reveal large variations in their experienced physicochemical properties suggesting that the same ligand can bind to distinct physicochemical environments in some protein ligand complexes the variation was found to correlate with the electrochemical characteristic of ligand molecules whereas in others it was disclosed as a prerequisite for the biochemical function of the protein to achieve binding proteins were observed to engage in subtle balancing acts between electrostatic and hydrophobic interactions to generate stabilizing free energies of binding for the presented analysis a new method for scoring hydrophobicity from molecular environments was developed showing high correlations with experimental determined desolvation energies the presented results highlight the complexities of molecular recognition and underline the challenges of computational structural biology in developing methods to detect these important subtleties proteins c wiley inc
as dna sequencing outpaces improvements in computer speed there is a critical need to accelerate tasks like alignment and snp calling crossbow is a cloud computing software tool that combines the aligner bowtie and the snp caller soapsnp executing in parallel using hadoop crossbow analyzes data comprising fold coverage of the human genome in three hours using a cpu cluster rented from a cloud computing service for about crossbow is available from http bowtie bio sourceforge net webcite
in tefko saracevic declared ldquothe subject knowledge viewrdquo to be the most fundamental perspective of relevance this paper examines the assumptions in different views of relevance including ldquothe system s viewrdquo and ldquothe user s viewrdquo and offers a reinterpretation of these views the paper finds that what was regarded as the most fundamental view by saracevic in has not since been considered with very few exceptions other views which are based on less fruitful assumptions have dominated the discourse on relevance in information retrieval and information science many authors have reexamined the concept of relevance in information science but have neglected the subject knowledge view hence basic theoretical assumptions seem not to have been properly addressed it is as urgent now as it was in seriously to consider ldquothe subject knowledge viewrdquo of relevance which may also be termed ldquothe epistemological viewrdquo the concept of relevance like other basic concepts is influenced by overall approaches to information science such as the cognitive view and the domain analytic view there is today a trend toward a social paradigm for information science this paper offers an understanding of relevance from such a social point view
efforts to control climate change require the stabilization of atmospheric concentrations this can only be achieved through a drastic reduction of global emissions yet fossil fuel emissions increased by between and in conjunction with increased contributions from emerging economies from the production and international trade of goods and services and from the use of coal as a fuel source in contrast emissions from land use changes were nearly constant between and of each year s emissions remained in the atmosphere on average the rest was absorbed by carbon sinks on land and in the oceans in the past years the fraction of emissions that remains in the atmosphere each year has likely increased from about to and models suggest that this trend was caused by a decrease in the uptake of by the carbon sinks in response to climate change and variability changes in the sinks are highly uncertain but they could have a significant influence on future atmospheric levels it is therefore crucial to reduce uncertainties
stochasticity pervades life at the cellular level cells receive stochastic signals perform detection and transduction with stochastic biochemistry and grow and die in stochastic environments here we review progress in going from the molecular details to the information processing strategies cells use in their decision making such strategies are fundamentally influenced by stochasticity we argue that the cellular decision making can only be probabilistic and occurs at three levels first cells must infer from noisy signals the probable current and anticipated future state of their environment second they must weigh the costs and benefits of each potential response given that future third cells must decide in the presence of other potentially competitive decision makers in this context we discuss cooperative responses where some individuals can appear to sacrifice for the common good we believe that decision making strategies will be conserved with comparatively few strategies being implemented by different biochemical mechanisms in many organisms determining the strategy of a decision making network provides a potentially powerful coarse graining that links systems and evolutionary biology to understand design
summary multiple sequence alignment msa is a central tool in most modern biology studies however despite generations of valuable tools human experts are still able to improve automatically generated msas in an effort to automatically identify the most reliable msa for a given protein family we propose a very simple protocol named aqua for automated quality improvement for multiple sequence alignments our current implementation relies on two alignment programs muscle and mafft one refinement program rascal and one assessment program normd but other programs could be incorporated at any of the three steps availability aqua is implemented in tcl tk and runs in command line on all platforms the source code is available under the gnu gpl license source code readme and supplementary data are available at http www bork embl de docu aqua contact muller embl de bork embl bioinformatics
visual attention can improve behavioral performance by allowing observers to focus on the important information in a complex scene attention also typically increases the firing rates of cortical sensory neurons rate increases improve the signal to noise ratio of individual neurons and this improvement has been assumed to underlie attention related improvements in behavior we recorded dozens of neurons simultaneously in visual area and found that changes in single neurons accounted for only a small fraction of the improvement in the sensitivity of the population instead over of the attentional improvement in the population signal was caused by decreases in the correlations between the trial to trial fluctuations in the responses of pairs of neurons these results suggest that the representation of sensory information in populations of neurons and the way attention affects the sensitivity of the population may only be understood by considering the interactions neurons
background non random patterns of genetic variation exist among individuals in a population owing to a variety of evolutionary factors therefore populations are structured into genetically distinct subpopulations as genotypic datasets become ever larger it is increasingly difficult to correctly estimate the number of subpopulations and assign individuals to them the computationally efficient non parametric chiefly principal components analysis pca based methods are thus becoming increasingly relied upon for population structure analysis current pca based methods can accurately detect structure however the accuracy in resolving subpopulations and assigning individuals to them is wanting when subpopulations are closely related to one another they overlap in pca space and appear as a conglomerate this problem is exacerbated when some subpopulations in the dataset are genetically far removed from others we propose a novel pca based framework which addresses this shortcoming results a novel population structure analysis algorithm called iterative pruning pca ippca was developed which assigns individuals to subpopulations and infers the total number of subpopulations present genotypic data from simulated and real population datasets with different degrees of structure were analyzed for datasets with simple structures the subpopulation assignments of individuals made by ippca were largely consistent with the structure baps and awclust algorithms on the other hand highly structured populations containing many closely related subpopulations could be accurately resolved only by ippca and not by other methods conclusion the algorithm is computationally efficient and not constrained by the dataset complexity this systematic subpopulation assignment approach removes the need for prior population labels which could be advantageous when cryptic stratification is encountered in datasets containing individuals otherwise assumed to belong to a population
background the next generation sequencing technologies provide new options to characterize the transcriptome and to develop affordable tools for functional genomics we describe here an innovative approach for this purpose and demonstrate its potential also for non model species results the method we developed is based on sequencing of cdna fragments from a normalized library constructed from pooled rnas to generate through de novo reads assembly a large catalog of unique transcripts in organisms for which a comprehensive collection of transcripts or the complete genome sequence is not available this virtual transcriptome provides extensive coverage depth and can be used for the setting up of a comprehensive microarray based expression analysis we evaluated the potential of this approach by monitoring gene expression during berry maturation in vitis vinifera as if no other sequence information was available for this species the microarray designed on the berries transcriptome derived from half of a run detected the expression of genes and proved to be more informative than one of the most comprehensive grape microarrays available to date the grapearray developed by the italian french public consortium for grapevine genome characterization which could detect the expression of genes in the same samples conclusion this approach provides a powerful method to rapidly build up an extensive catalog of unique transcripts that can be successfully used to develop a microarray for large scale analysis of gene expression in any species without the need for prior knowledge
the rrna approach is the principal tool to study microbial diversity but it has important biases these include polymerase chain reaction pcr primers bias and relative inefficiency of dna extraction techniques such sources of potential undersampling of microbial diversity are well known but the scale of the undersampling has not been quantified using a marine tidal flat bacterial community as a model we show that even with unlimited sampling and sequencing effort a single combination of pcr primers dna extraction technique enables theoretical recovery of only half of the richness recoverable with three such combinations this shows that different combinations of pcr primers dna extraction techniques recover in principle different species as well as higher taxa the majority of earlier estimates of microbial richness seem to be underestimates the combined use of multiple pcr primer sets multiple dna extraction techniques and deep community sequencing will minimize the biases and recover substantially more species than prior studies but we caution that even this yet to be used approach may still leave an unknown number of species and higher undetected
pnas prompted by the increasing interest in networks in many fields we present an attempt at unifying points of view and analyses of these objects coming from the social sciences statistics probability and physics communities we apply our approach to the newmangirvan modularity widely used for community detection among others our analysis is asymptotic but we show by simulation and application to real examples that the theory is a reasonable guide practice
the rarity of beneficial mutations has frustrated efforts to develop a quantitative theory of adaptation recent models of adaptive walks the sequential substitution of beneficial mutations by selection make two compelling predictions adaptive walks should be short and fitness increases should become exponentially smaller as successive mutations fix we estimated the number and fitness effects of beneficial mutations in each of replicate lineages of aspergillus nidulans evolving for approximately generations at two population sizes using a novel maximum likelihood framework the results of which were confirmed experimentally using sexual crosses we find that adaptive walks do indeed tend to be short and fitness increases become smaller as successive mutations fix moreover we show that these patterns are associated with a decreasing supply of beneficial mutations as the population adapts we also provide empirical distributions of fitness effects among mutations fixed at each step our results provide a first glimpse into the properties of multiple steps in an adaptive walk in asexual populations and lend empirical support to models of adaptation involving selection towards a single optimum phenotype in practical terms our results suggest that the bulk of adaptation is likely to be accomplished within the first steps
the negatome is a collection of protein and domain pairs that are unlikely to be engaged in direct physical interactions the database currently contains experimentally supported non interacting protein pairs derived from two distinct sources by manual curation of literature and by analyzing protein complexes with known structure more stringent lists of non interacting pairs were derived from these two datasets by excluding interactions detected by high throughput approaches additionally non interacting protein domains have been derived from the stringent manual and structural data respectively the negatome is much less biased toward functionally dissimilar proteins than the negative data derived by randomly selecting proteins from different cellular locations it can be used to evaluate protein and domain interactions from new experiments and improve the training of interaction prediction algorithms the negatome database is available at http mips helmholtz muenchen de proj negatome
scientists and clinicians who study genetic alterations and disease have traditionally described phenotypes in natural language the considerable variation in these free text descriptions has posed a hindrance to the important task of identifying candidate genes and models for human diseases and indicates the need for a computationally tractable method to mine data resources for mutant phenotypes in this study we tested the hypothesis that ontological annotation of disease phenotypes will facilitate the discovery of new genotype phenotype relationships within and across species to describe phenotypes using ontologies we used an entity quality eq methodology wherein the affected entity e and how it is affected q are recorded using terms from a variety of ontologies using this eq method we annotated the phenotypes of gene linked human diseases described in online mendelian inheritance in man omim these human annotations were loaded into our ontology based database obd along with other ontology based phenotype descriptions of mutants from various model organism databases phenotypes recorded with this eq method can be computationally compared based on the hierarchy of terms in the ontologies and the frequency of annotation we utilized four similarity metrics to compare phenotypes and developed an ontology of homologous and analogous anatomical structures to compare phenotypes between species using these tools we demonstrate that we can identify through the similarity of the recorded phenotypes other alleles of the same gene other members of a signaling pathway and orthologous genes and pathway members across species we conclude that eq based annotation of phenotypes in conjunction with a cross species ontology and a variety of similarity metrics can identify biologically meaningful similarities between genes by comparing phenotypes alone this annotation and search method provides a novel and efficient means to identify gene candidates and animal models of human disease which may shorten the lengthy path to identification and understanding of the genetic basis of disease
directed evolution circumvents our profound ignorance of how a protein s sequence encodes its function by using iterative rounds of random mutation and artificial selection to discover new and useful proteins proteins can be tuned to adapt to new functions or environments by simple adaptive walks involving small numbers of mutations directed evolution studies have shown how rapidly some proteins can evolve under strong selection pressures and because the entire fossil record of evolutionary intermediates is available for detailed study they have provided new insight into the relationship between sequence and function directed evolution has also shown how mutations that are functionally neutral can set the stage for adaptation
we live in interesting times portents of impending catastrophe pervade the literature calling us to action in the face of unmanageable volumes of scientific data but it isn t so much data generation per se but the systematic burial of the knowledge embodied in those data that poses the problem there is so much information available that we simply no longer know what we know and finding what we want is hard too hard the knowledge we seek is often fragmentary and disconnected spread thinly across thousands of databases and millions of articles in thousands of journals the intellectual energy required to search this array of data archives and the time and money this wastes has led several researchers to challenge the methods by which we traditionally commit newly acquired facts and knowledge to the scientific record we present some of these initiatives here a whirlwind tour of recent projects to transform scholarly publishing paradigms culminating in utopia and the semantic biochemical journal experiment with their promises to provide new ways of interacting with the literature and new and more powerful tools to access and extract the knowledge sequestered within it we ask what advances they make and what obstacles to progress still exist we explore these questions and as you read on we invite you to engage in an experiment with us a real time test of a new technology to rescue data from the dormant pages of published documents we ask you please to read the instructions carefully the time has come you may turn over papers
motivation biomarker discovery is an important topic in biomedical applications of computational biology including applications such as gene and snp selection from high dimensional data surprisingly the stability with respect to sampling variation or robustness of such selection processes has received attention only recently however robustness of biomarkers is an important issue as it may greatly influence subsequent biological validations in addition a more robust set of markers may strengthen the confidence of an expert in the results of a selection method results our first contribution is a general framework for the analysis of the robustness of a biomarker selection algorithm secondly we conducted a large scale analysis of the recently introduced concept of ensemble feature selection where multiple feature selections are combined in order to increase the robustness of the final set of selected features we focus on selection methods that are embedded in the estimation of support vector machines svms svms are powerful classification models that have shown state of the art performance on several diagnosis and prognosis tasks on biological data their feature selection extensions also offered good results for gene selection tasks we show that the robustness of svms for biomarker discovery can be substantially increased by using ensemble feature selection techniques while at the same time improving upon classification performances the proposed methodology is evaluated on four microarray datasets showing increases of up to almost in robustness of the selected biomarkers along with an improvement of in classification performance the stability improvement with ensemble methods is particularly noticeable for small signature sizes a few tens of genes which is most relevant for the design of a diagnosis or prognosis model from a gene signature contact yvan saeys psb ugent be supplementary information supplementary data are available at bioinformatics bioinformatics
while many models of biological object recognition share a common set of broad stroke properties the performance of any one model depends strongly on the choice of parameters in a particular instantiation of that model e g the number of units per layer the size of pooling kernels exponents in normalization operations etc since the number of such parameters explicit or implicit is typically large and the computational cost of evaluating one particular parameter set is high the space of possible model instantiations goes largely unexplored thus when a model fails to approach the abilities of biological visual systems we are left uncertain whether this failure is because we are missing a fundamental idea or because the correct parts have not been tuned correctly assembled at sufficient scale or provided with enough training here we present a high throughput approach to the exploration of such parameter sets leveraging recent advances in stream processing hardware high end nvidia graphic cards and the playstation s ibm cell processor in analogy to high throughput screening approaches in molecular biology and genetics we explored thousands of potential network architectures and parameter instantiations screening those that show promising object recognition performance for further analysis we show that this approach can yield significant reproducible gains in performance across an array of basic object recognition tasks consistently outperforming a variety of state of the art purpose built vision systems from the literature as the scale of available computational power continues to expand we argue that this approach has the potential to greatly accelerate progress in both artificial vision and our understanding of the computational underpinning of vision
transcriptome wide studies in eukaryotes have been instrumental in the characterization of fundamental regulatory mechanisms for more than a decade by contrast in prokaryotes bacteria and archaea whole transcriptome studies have not been performed until recently owing to the general view that microbial gene structures are simple as well as technical difficulties in enriching for mrnas that lack poly a tails deep rna sequencing and tiling array studies are now revolutionizing our understanding of the complexity plasticity and regulation of transcriptomes
the problem of false research findings in medical research has focused much attention in the last few years ioannidis one of the main problems termed as fishing for significance in the present letter is that researchers often consciously or subconsciously report results that are in fact the product of an intensive optimization i e of multiple comparisons such results are typically unlikely to be reproduced in an independent study and have a high probability to be false ioannidis the fishing for significance problem is enhanced by the so called publication bias positive results have a much higher chance to get published than negative results as already acknowledged years ago sterling in a word many false positive results are produced through multiple comparisons and false positives have higher chance to get published than true negatives moreover the difficulty to publish negative results obviously encourages authors to find something positive in their study by performing numerous analyses until one of them yields positive results by chance i e to fish for significance although this issue is by far less acknowledged and publicly admitted than in the medical context the same types of problems occur in biostatistics and research
exchanging and sharing scientific results are essential for researchers in the field of computational modelling biomodels net defines agreed upon standards for model curation a fundamental one miriam minimum information requested in the annotation of models standardises the annotation and curation process of quantitative models in biology to support this standard miriam resources maintains a set of standard data types for annotating models and provides services for manipulating these annotations furthermore biomodels net creates controlled vocabularies such as sbo systems biology ontology which strictly indexes defines and links terms used in systems biology finally biomodels database provides a free centralised publicly accessible database for storing searching and retrieving curated and annotated computational models each resource provides a web interface to submit search retrieve and display its data in addition the biomodels net team provides a set of web services which allows the community to programmatically access the resources a user is then able to perform remote queries such as retrieving a model and resolving all its miriam annotations as well as getting the details about the associated sbo terms these web services use established standards communications rely on soap simple object access protocol messages and the available queries are described in a wsdl web services description language file several libraries are provided in order to simplify the development of client software biomodels net web services make one step further for the researchers to simulate and understand the entirety of a biological system by allowing them to retrieve biological models in their own tool combine queries in workflows and efficiently models
to study basic principles of transcriptome organization in bacteria we analyzed one of the smallest self replicating organisms mycoplasma pneumoniae we combined strand specific tiling arrays complemented by transcriptome sequencing with more than spotted arrays we detected previously undescribed mostly noncoding transcripts of them in antisense configuration to known genes we identified operons of which are polycistronic almost half of the latter show decaying expression in a staircase like manner under various conditions operons could be divided into smaller transcriptional units resulting in many alternative transcripts frequent antisense transcripts alternative transcripts and multiple regulators per gene imply a highly dynamic transcriptome more similar to that of eukaryotes than thought
to understand basic principles of bacterial metabolism organization and regulation but also the impact of genome size we systematically studied one of the smallest bacteria mycoplasma pneumoniae a manually curated metabolic network of reactions catalyzed by enzymes allowed the design of a defined minimal medium with essential nutrients more than growth curves were recorded in the presence of various nutrient concentrations measurements of biomass indicators metabolites and glucose experiments provided information on directionality fluxes and energetics integration with transcription profiling enabled the global analysis of metabolic regulation compared with more complex bacteria the m pneumoniae metabolic network has a more linear topology and contains a higher fraction of multifunctional enzymes general features such as metabolite concentrations cellular energetics adaptability and global gene expression responses are similar science
ecological speciation is considered an adaptive response to selection for local adaptation however besides suitable ecological conditions the process requires assortative mating to protect the nascent species from homogenization by gene flow by means of a simple model we demonstrate that disruptive ecological selection favors the evolution of sexual preferences for ornaments that signal local adaptation such preferences induce assortative mating with respect to ecological characters and enhance the strength of disruptive selection natural and sexual selection thus work in concert to achieve local adaptation and reproductive isolation even in the presence of substantial gene flow the resulting speciation process ensues without the divergence of mating preferences avoiding problems that have plagued previous models of speciation by sexual science
the genome of mycoplasma pneumoniae is among the smallest found in self replicating organisms to study the basic principles of bacterial proteome organization we used tandem affinity purification mass spectrometry tap ms in a proteome wide screen the analysis revealed homomultimeric and heteromultimeric soluble protein complexes of which the majority are novel about a third of the heteromultimeric complexes show higher levels of proteome organization including assembly into larger multiprotein complex entities suggesting sequential steps in biological processes and extensive sharing of components implying protein multifunctionality incorporation of structural models for proteins single particle electron microscopy and cellular electron tomograms provided supporting structural details for this proteome organization the data set provides a blueprint of the minimal cellular machinery required life
we examined how remote enhancers establish physical communication with target promoters to activate gene transcription in response to environmental signals although the natural ifn enhancer is located immediately upstream of the core promoter it also can function as a classical enhancer element conferring virus infection dependent activation of heterologous promoters even when it is placed several kilobases away from these promoters we demonstrated that the remote ifn enhancer loops out the intervening dna to reach the target promoter these chromatin loops depend on sequence specific transcription factors bound to the enhancer and the promoter and thus can explain the specificity observed in enhancerpromoter interactions especially in complex genetic loci transcription factor binding sites scattered between an enhancer and a promoter can work as decoys trapping the enhancer in nonproductive loops thus resembling insulator elements finally replacement of the transcription factor binding sites involved in dna looping with those of a heterologous prokaryotic protein the repressor which is capable of loop formation rescues enhancer function from a distance by re establishing enhancerpromoter formation
large scale protein signalling networks are useful for exploring complex biochemical pathways but do not reveal how pathways respond to specific stimuli such specificity is critical for understanding disease and designing drugs here we describe a computational approachimplemented in the free cno softwarefor turning signalling networks into logical models and calibrating the models against experimental data when a literature derived network of proteins covering the immediate early responses of human cells to seven cytokines was modelled we found that training against experimental data dramatically increased predictive power despite the crudeness of boolean approximations while significantly reducing the number of interactions thus many interactions in literature derived networks do not appear to be functional in the liver cells from which we collected our data at the same time cno identified several new interactions that improved the match of model to data although missing from the starting network these interactions have literature support our approach therefore represents a means to generate predictive cell type specific models of mammalian signalling from generic protein networks
this colloquium reviews statistical models for money wealth and income distributions developed in the econophysics literature since the late by analogy with the boltzmann gibbs distribution of energy in physics it is shown that the probability distribution of money is exponential for certain classes of models with interacting economic agents alternative scenarios are also reviewed data analysis of the empirical distributions of wealth and income reveals a two class distribution the majority of the population belongs to the lower class characterized by the exponential thermal distribution whereas a small fraction of the population in the upper class is characterized by the power law superthermal distribution the lower part is very stable stationary in time whereas the upper part is highly dynamical and out equilibrium
estimates of the total number of bacterial indicate that existing dna sequence databases carry only a tiny fraction of the total amount of dna sequence space represented by this division of life indeed environmental dna samples have been shown to encode many previously unknown classes of and bioinformatics of genomic dna from bacteria commonly identify new noncoding rnas ncrnas such as in rare instances rnas that exhibit more extensive sequence and structural conservation across a wide range of bacteria are given that large structured rnas are known to carry out complex biochemical functions such as protein synthesis and rna processing reactions identifying more rnas of great size and intricate structure is likely to reveal additional biochemical functions that can be achieved by rna we applied an updated computational to discover ncrnas that rival the known large ribozymes in size and structural complexity or that are among the most abundant rnas in bacteria that encode them these rnas would have been difficult or impossible to detect without examining environmental dna sequences indicating that numerous rnas with extraordinary size structural complexity or other exceptional characteristics remain to be discovered in unexplored space
the ability to produce stem cells by induced pluripotency ips reprogramming has rekindled an interest in earlier studies showing that transcription factors can directly convert specialized cells from one lineage to another lineage reprogramming has become a powerful tool to study cell fate choice during differentiation akin to inducing mutations for the discovery of gene functions the lessons learnt provide a rubric for how cells may be manipulated for purposes
background contemporary biological research integrates neighboring scientific domains to answer complex questions in fields such as systems biology and drug discovery this calls for tools that are intuitive to use yet flexible to adapt to new tasks results bioclipse is a free open source workbench with advanced features for the life sciences version constitutes a complete rewrite of bioclipse and delivers a stable scalable integration platform for developers and an intuitive workbench for end users all functionality is available both from the graphical user interface and from a built in novel domain specific language supporting the scientist in interdisciplinary research and reproducible analyses through advanced visualization of the inputs and the results new components for bioclipse include a rewritten editor for chemical structures a table for multiple molecules that supports gigabyte sized files as well as a graphical editor for sequences and alignments conclusion bioclipse is equipped with advanced tools required to carry out complex analysis in the fields of bio and cheminformatics developed as a rich client based on eclipse bioclipse leverages on today s powerful desktop computers for providing a responsive user interface but also takes full advantage of the web and networked web cloud services for more demanding calculations or retrieval of data the fact that bioclipse is based on an advanced and widely used service platform ensures wide extensibility making it easy to add new algorithms visualizations as well as scripting commands the intuitive tools for end users and the extensible architecture make bioclipse ideal for interdisciplinary and integrative research bioclipse is released under the eclipse public license epl a flexible open source license that allows additional plugins to be of any license bioclipse is implemented in java and supported on all major platforms source code and binaries are freely available at http www net
pathway tools is a production quality software environment for creating a type of model organism database called a pathway genome database pgdb a pgdb such as ecocyc integrates the evolving understanding of the genes proteins metabolic network and regulatory network of an organism this article provides an overview of pathway tools capabilities the software performs multiple computational inferences including prediction of metabolic pathways prediction of metabolic pathway hole fillers and prediction of operons it enables interactive editing of pgdbs by db curators it supports web publishing of pgdbs and provides a large number of query and visualization tools the software also supports comparative analyses of pgdbs and provides several systems biology analyses of pgdbs including reachability analysis of metabolic networks and interactive tracing of metabolites through a metabolic network more than pgdbs have been created using pathway tools by scientists around the world many of which are curated dbs for important model organisms those pgdbs can be exchanged using a peer to peer db sharing system called the registry
a new implementation of the adaptive biasing force abf method is described this implementation supports a wide range of collective variables and can be applied to the computation of multidimensional energy profiles it is provided to the community as part of a code that implements several analogous methods including metadynamics abf and metadynamics have not previously been tested side by side on identical systems here numerical tests are carried out on processes including conformational changes in model peptides and translocation of a halide ion across a lipid membrane through a peptide nanotube on the basis of these examples we discuss similarities and differences between the abf and metadynamics schemes both approaches provide enhanced sampling and free energy profiles in quantitative agreement with each other in different applications the method of choice depends on the dimension of the reaction coordinate space the height of the barriers and the relaxation times of degrees of freedom in the orthogonal space which are not explicitly described by the chosen variables
brain function operates through the coordinated activation of neuronal assemblies graph theory predicts that scale free topologies which include hubs superconnected nodes are an effective design to orchestrate synchronization whether hubs are present in neuronal assemblies and coordinate network activity remains unknown using network dynamics imaging online reconstruction of functional connectivity and targeted whole cell recordings in rats and mice we found that developing hippocampal networks follow a scale free topology and we demonstrated the existence of functional hubs perturbation of a single hub influenced the entire network dynamics morphophysiological analysis revealed that hub cells are a subpopulation of gamma aminobutyric acid releasing gabaergic interneurons possessing widespread axonal arborizations these findings establish a central role for gabaergic interneurons in shaping developing networks and help provide a conceptual framework for studying synchrony
next generation sequencing platforms are producing biological sequencing data in unprecedented amounts the partners of the international nucleotide sequencing database collaboration which includes the national center for biotechnology information ncbi the european bioinformatics institute ebi and the dna data bank of japan ddbj have established the sequence read archive sra to provide the scientific community with an archival destination for next generation data sets the sra is now accessible at http www ncbi nlm nih gov traces sra from ncbi at http www ebi ac uk ena from ebi and at http www ddbj nig ac jp sub e html from ddbj users of these resources can obtain data sets deposited in any of the three sra instances links and submission instructions provided
understanding the mechanisms of cell function and drug action is a major endeavor in the pharmaceutical industry drug effects are governed by the intrinsic properties of the drug i e selectivity and potency and the specific signaling transduction network of the host i e normal vs diseased cells here we describe an unbiased phosphoproteomic based approach to identify drug effects by monitoring drug induced topology alterations with our proposed method drug effects are investigated under diverse stimulations of the signaling network starting with a generic pathway made of logical gates we build a cell type specific map by constraining it to fit key phopshoprotein signals under experimental conditions fitting is performed via an integer linear program ilp formulation and solution by standard ilp solvers a procedure that drastically outperforms previous fitting schemes then knowing the cell s topology we monitor the same key phosphoprotein signals under the presence of drug and we re optimize the specific map to reveal drug induced topology alterations to prove our case we make a topology for the hepatocytic cell line and we evaluate the effects of drugs selective inhibitors for the epidermal growth factor receptor egfr and a non selective drug we confirm effects easily predictable from the drugs main target i e egfr inhibitors blocks the egfr pathway but we also uncover unanticipated effects due to either drug promiscuity or the cell s specific topology an interesting finding is that the selective egfr inhibitor gefitinib inhibits signaling downstream the interleukin pathway an effect that cannot be extracted from binding affinity based approaches our method represents an unbiased approach to identify drug effects on small to medium size pathways which is scalable to larger topologies with any type of signaling interventions small molecules rnai etc the method can reveal drug effects on pathways the cornerstone for identifying mechanisms of drug efficacy
we employ a biophysical model that accounts for the non linear relationship between binding energy and the statistics of selected binding sites the model includes the chemical potential of the transcription factor non specific binding affinity of the protein for dna as well as sequence specific parameters that may include non independent contributions of bases to the interaction we obtain maximum likelihood estimates for all of the parameters and compare the results to standard probabilistic methods of parameter estimation on simulated data where the true energy model is known and samples are generated with a variety of parameter values we show that our method returns much more accurate estimates of the true parameters and much better predictions of the selected binding site distributions we also introduce a new high throughput selex ht selex procedure to determine the binding specificity of a transcription factor in which the initial randomized library and the selected sites are sequenced with next generation methods that return hundreds of thousands of sites we show that after a single round of selection our method can estimate binding parameters that give very good fits to the selected site distributions much better than standard motif algorithms
identifying a protein s functional sites is an important step towards characterizing its molecular function numerous structure and sequence based methods have been developed for this problem here we introduce concavity a small molecule binding site prediction algorithm that integrates evolutionary sequence conservation estimates with structure based methods for identifying protein surface cavities in large scale testing on a diverse set of single and multi chain protein structures we show that concavity substantially outperforms existing methods for identifying both ligand binding pockets and individual ligand binding residues as part of our testing we perform one of the first direct comparisons of conservation based and structure based methods we find that the two approaches provide largely complementary information which can be combined to improve upon either approach alone we also demonstrate that concavity has state of the art performance in predicting catalytic sites and drug binding pockets overall the algorithms and analysis presented here significantly improve our ability to identify ligand binding sites and further advance our understanding of the relationship between evolutionary sequence conservation and structural and functional attributes of proteins data source code and prediction visualizations are available on the concavity web site http compbio cs princeton concavity
we present a web resource mem multi experiment matrix for gene expression similarity searches across many datasets mem features large collections of microarray datasets and utilizes rank aggregation to merge information from different datasets into a single global ordering with simultaneous statistical significance estimation unique features of mem include automatic detection characterization and visualization of datasets that includes the strongest coexpression patterns mem is freely available at http biit cs ut mem
motivation the increase in the amount of available protein protein interaction ppi data enables us to develop computational methods for protein complex predictions a protein complex is a group of proteins that interact with each other at the same time and place the protein complex generally corresponds to a cluster in ppi network ppin however clusters correspond not only to protein complexes but also to sets of proteins that interact dynamically with each other as a result conventional graph theoretic clustering methods that disregard interaction dynamics show high false positive rates in protein complex predictions results in this article a method of refining ppin is proposed that uses the structural interface data of protein pairs for protein complex predictions a simultaneous protein interaction network spin is introduced to specify mutually exclusive interactions meis as indicated from the overlapping interfaces and to exclude competition from meis that arise during the detection of protein complexes after constructing spins naive clustering algorithms are applied to the spins for protein complex predictions the evaluation results show that the proposed method outperforms the simple ppin based method in terms of removing false positive proteins in the formation of complexes this shows that excluding competition between meis can be effective for improving prediction accuracy in general computational approaches involving protein interactions availability http code google com p simultaneous pin contact dshan kaist ac kr supplementary information supplementary data are available at bioinformatics bioinformatics
dietary restriction extends healthy lifespan in diverse organisms and reduces it is widely assumed to induce adaptive reallocation of nutrients from reproduction to somatic maintenance aiding survival of food shortages in if this were the case long life under dietary restriction and high fecundity under full feeding would be mutually exclusive through competition for the same limiting nutrients here we report a test of this idea in which we identified the nutrients producing the responses of lifespan and fecundity to dietary restriction in drosophila adding essential amino acids to the dietary restriction condition increased fecundity and decreased lifespan similar to the effects of full feeding with other nutrients having little or no effect however methionine alone was necessary and sufficient to increase fecundity as much as did full feeding but without reducing lifespan reallocation of nutrients therefore does not explain the responses to dietary restriction lifespan was decreased by the addition of amino acids with an interaction between methionine and other essential amino acids having a key role hence an imbalance in dietary amino acids away from the ratio optimal for reproduction shortens lifespan during full feeding and limits fecundity during dietary restriction reduced activity of the insulin insulin like growth factor signalling pathway extends lifespan in diverse and we find that it also protects against the shortening of lifespan with full feeding in other organisms including mammals it may be possible to obtain the benefits to lifespan of dietary restriction without incurring a reduction in fecundity through a suitable balance of nutrients in diet
phenotype ontologies are typically constructed to serve the needs of a particular community such as annotation of genotype phenotype associations in mouse or human here we demonstrate how these ontologies can be improved through assignment of logical definitions using a core ontology of phenotypic qualities and multiple additional ontologies from the open biological ontologies library we also show how these logical definitions can be used for data integration when combined with a unified multi species ontology
after two decades of repository development some conclusions may be drawn as to which type of repository and what kind of service best supports digital scholarly communication and thus the production of new knowledge four types of publication repository may be distinguished namely the subject based repository research repository national repository system and institutional repository two important shifts in the role of repositories may be noted with regard to content a well defined and high quality corpus is essential this implies that repository services are likely to be most successful when constructed with the user and reader uppermost in mind with regard to service high value to specific scholarly communities is essential this implies that repositories are likely to be most useful to scholars when they offer dedicated services supporting the production of new knowledge along these lines challenges and barriers to repository development may be identified in three key dimensions a identification and deposit of content b access and use of services and c preservation of content and sustainability of service an indicative comparison of challenges and barriers in some major world regions such as europe north america and east asia plus australia is offered conclusion
a study was undertaken to research the influences and factors of high reading scores with a population of various first and second graders this study after having been thoroughly conducted among a large number of students seems to suggest that both the sheer number of books and time dedicated to those books and the depth of comprehension are equally significant when it comes to successful reading habits but what also seems to be an essential variable is the teachers interventions and involvement in the process of independent reading http www sciencedirect com science articleurl search d view c google
next generation sequencing has greatly increased the scope and the resolution of transcriptional regulation study rna sequencing rna seq and chip seq experiments are now generating comprehensive data on transcript abundance and on regulator dna interactions we propose an approach for an integrated analysis of these data based on feature extraction of chip seq signals principal component analysis and regression based component selection compared with traditional methods our approach not only offers higher power in predicting gene expression from chip seq data but also provides a way to capture cooperation among regulators in mouse embryonic stem cells escs we find that a remarkably high proportion of variation in gene expression can be explained by the binding signals of transcription factors tfs two groups of tfs are identified whereas the first group myc mycn and zfx act as activators in general the second group nanog and esrrb may serve as either activator or repressor depending on the target the two groups of tfs cooperate tightly to activate genes that are differentially up regulated in escs in the absence of binding by the first group the binding of the second group is associated with genes that are repressed in escs and derepressed upon differentiation
here we integrate the de novo assembly of an asian and an african genome with the ncbi reference human genome as a step toward constructing the human pan genome we identified of novel sequences not present in the reference genome in each of these assemblies most novel sequences are individual or population specific as revealed by their comparison to all available human dna sequence and by pcr validation using the human genome diversity cell line panel we found novel sequences present in patterns consistent with known human migration paths cross species conservation analysis of predicted genes indicated that the novel sequences contain potentially functional coding regions we estimate that a complete human pan genome would contain of novel sequence not present in the extant reference genome the extensive amount of novel sequence contributing to the genetic variation of the pan genome indicates the importance of using complete genome sequencing and de assembly
web has been during the last years one of the most fashionable words for a whole range of evolutions regarding the internet although it was identified by the current analysts as the key technology for the next decade the actors from the educational field do not really know what web means since the author started to explore and use web technologies in her own development improvement she has been intrigued by their potential and especially by the possibility of integrating them in education and in particular in the teaching activity the purpose of this paper is both to promote scholarly inquiry about the need of a new type a pedagogy web based and the development adoption of best practice in teaching and learning with web in higher education he the article main objectives are to introduce theoretical aspects of using web technologies in higher education to present models of integrating web technologies in teaching learning and assessment to identify the potential benefits of these technologies as well as to highlight some of the problematic issues barriers encountered surrounding the pedagogical use of web in higher education to propose an agenda for future research and to develop pedagogy scenarios for sector
when prioritizing hits from a high throughput experiment it is important to correct for random events that falsely appear significant how is this done and what methods should be used imagine that you have just invested a substantial amount of time and money in a shotgun proteomics experiment designed to identify proteins involved in a particular biological process the experiment successfully identifies most of the proteins that you already know to be involved in the process and implicates a more
recent research on changing fears has examined targeting reconsolidation during reconsolidation stored information is rendered labile after being retrieved pharmacological manipulations at this stage result in an inability to retrieve the memories at later times suggesting that they are erased or persistently inhibited unfortunately the use of these pharmacological manipulations in humans can be problematic here we introduce a non invasive technique to target the reconsolidation of fear memories in humans we provide evidence that old fear memories can be updated with non fearful information provided during the reconsolidation window as a consequence fear responses are no longer expressed an effect that lasted at least a year and was selective only to reactivated memories without affecting others these findings demonstrate the adaptive role of reconsolidation as a window of opportunity to rewrite emotional memories and suggest a non invasive technique that can be used safely in humans to prevent the return fear
summary tablet is a lightweight high performance graphical viewer for next generation sequence assemblies and alignments supporting a range of input assembly formats tablet provides high quality visualizations showing data in packed or stacked views allowing instant access and navigation to any region of interest and whole contig overviews and data summaries tablet is both multi core aware and memory efficient allowing it to handle assemblies containing millions of reads even on a bit desktop machine availability tablet is freely available for microsoft windows apple mac os x linux and solaris fully bundled installers can be downloaded from http bioinf scri ac uk tablet in and bit versions contact tablet scri uk
motivation the elucidation of biological concepts enriched with differentially expressed genes has become an integral part of the analysis and interpretation of genomic data of additional importance is the ability to explore networks of relationships among previously defined biological concepts from diverse information sources and to explore results visually from multiple perspectives accomplishing these tasks requires a unified framework for agglomeration of data from various genomic resources novel visualizations and user functionality results we have developed conceptgen a web based gene set enrichment and gene set relation mapping tool that is streamlined and simple to use conceptgen offers over concepts comprising different types of biological knowledge including data not currently available in any other gene set enrichment or gene set relation mapping tool we demonstrate the functionalities of conceptgen using gene expression data modeling tgf beta induced epithelial mesenchymal transition and metabolomics data comparing metastatic versus localized cancers
the red describes a view of nature in which species continually evolve but do not become better adapted it is one of the more distinctive metaphors of evolutionary biology but no test of its claim that speciation occurs at a constant has ever been made against competing models that can predict virtually identical outcomes nor has any mechanism been proposed that could cause the constant rate phenomenon here we use phylogenies of animal plant and fungal taxa to test the constant rate claim against four competing models phylogenetic branch lengths record the amount of time or evolutionary change between successive events of speciation the models predict the distribution of these lengths by specifying how factors combine to bring about speciation or by describing how rates of speciation vary throughout a tree we find that the hypotheses that speciation follows the accumulation of many small events that act either multiplicatively or additively found support in and none of the trees respectively a further of trees hinted that the probability of speciation changes according to the amount of divergence from the ancestral species and suggested speciation rates vary among taxa by comparison of the trees fit the simplest model in which new species emerge from single events each rare but individually sufficient to cause speciation this model predicts a constant rate of speciation and provides a new interpretation of the red queen the metaphor of species losing a race against a deteriorating environment is replaced by a view linking speciation to rare stochastic events that cause reproductive isolation attempts to understand species or why some groups have more or fewer species should look to the size of the catalogue of potential causes of speciation shared by a group of closely related organisms rather than to how those combine
summary motif discovery is an important topic in computational transcriptional regulation studies in the past decade many researchers have contributed to the field and many de novo motif finding tools have been developed each may have a different strength however most of these tools do not have a user friendly interface and their results are not easily comparable we present a software called toolbox of motif discovery tmod for windows operating systems the current version of tmod integrates widely used motif discovery programs mdscan bioprospector alignace gibbs motif sampler meme consensus motifregressor glam motifsampler sesimcmc weeder and ymf tmod provides a unified interface to ease the use of these programs and help users to understand the tuning parameters it allows plug in motif finding programs to run either separately or in a batch mode with predetermined parameters and provides a summary comprising of outputs from multiple programs tmod is developed in c with the support of microsoft foundation classes and cygwin tmod can also be easily expanded to include future algorithms availability tmod is available for download at http www fas harvard edu junliu tmod contact nudt edu cn jliu stat harvard bioinformatics
due to the rapid release of new data from genome sequencing projects the majority of protein sequences in public databases have not been experimentally characterized rather sequences are annotated using computational analysis the level of misannotation and the types of misannotation in large public databases are currently unknown and have not been analyzed in depth we have investigated the misannotation levels for molecular function in four public protein sequence databases uniprotkb swiss prot genbank nr uniprotkb trembl and kegg for a model set of enzyme families for which extensive experimental information is available the manually curated database swiss prot shows the lowest annotation error levels close to for most families the two other protein sequence databases genbank nr and trembl and the protein sequences in the kegg pathways database exhibit similar and surprisingly high levels of misannotation that average across the six superfamilies studied for of the families examined the level of misannotation in one or more of these databases is examination of the nr database over time shows that misannotation has increased from to the types of misannotation that were found fall into several categories most associated with overprediction of molecular function these results suggest that misannotation in enzyme superfamilies containing multiple families that catalyze different reactions is a larger problem than has been recognized strategies are suggested for addressing some of the systematic problems contributing to these high levels misannotation
protein protein interaction networks provide a global picture of cellular function and biological processes some proteins act as hub proteins highly connected to others whereas some others have few interactions the dysfunction of some interactions causes many diseases including cancer proteins interact through their interfaces therefore studying the interface properties of cancer related proteins will help explain their role in the interaction networks similar or overlapping binding sites should be used repeatedly in single interface hub proteins making them promiscuous alternatively multi interface hub proteins make use of several distinct binding sites to bind to different partners we propose a methodology to integrate protein interfaces into cancer interaction networks cispin cancer structural protein interface network the interactions in the human protein interaction network are replaced by interfaces coming from either known or predicted complexes we provide a detailed analysis of cancer related human protein protein interfaces and the topological properties of the cancer network the results reveal that cancer related proteins have smaller more planar more charged and less hydrophobic binding sites than non cancer proteins which may indicate low affinity and high specificity of the cancer related interactions we also classified the genes in cispin according to phenotypes within phenotypes for breast cancer colorectal cancer and leukemia interface properties were found to be discriminating from non cancer interfaces with an accuracy of respectively in addition cancer related proteins tend to interact with their partners through distinct interfaces corresponding mostly to multi interface hubs which comprise of cancer related proteins and constituting the nodes with higher essentiality in the network we illustrate the interface related affinity properties of two cancer related hub proteins a multi interface and a single interface hub the results reveal that affinity of interactions of the multi interface hub tends to be higher than that of the single interface hub these findings might be important in obtaining new targets in cancer as well as finding the details of specific binding regions of putative cancer candidates
the current issue of nucleic acids research includes descriptions of new and updated data resources the accompanying online database collection available at http www oxfordjournals org nar database a now lists carefully selected databases covering various aspects of molecular and cell biology while most data resource descriptions remain very brief the issue includes several longer papers that highlight recent significant developments in such databases as pfam metacyc uniprot elm and pdbe the databases described in the database issue and database collection however are far more than a distinct set of resources they form a network of connected data concepts and shared technology the full content of the database issue is available online at the nucleic acids research web site http nar org
for more than years those engineering genetic material have pursued increasingly challenging targets during that time the tools and resources available to the genetic engineer have grown to encompass new extremes of both scale and precision opening up new opportunities in genome engineering today our capacity to generate larger de novo assemblies of dna is increasing at a rapid pace with concomitant decreases in manufacturing cost we are also witnessing potent demonstrations of the power of merging randomness and selection with engineering approaches targeting large numbers of specific sites within genomes these developments promise genetic engineering with unprecedented levels of design originality and offer new avenues to expand both our understanding of the biological world and the diversity of applications for benefit
asia harbors substantial cultural and linguistic diversity but the geographic structure of genetic variation across the continent remains enigmatic here we report a large scale survey of autosomal variation from a broad geographic sample of asian human populations our results show that genetic ancestry is strongly correlated with linguistic affiliations as well as geography most populations show relatedness within ethnic linguistic groups despite prevalent gene flow among populations more than of east asian ea haplotypes could be found in either southeast asian sea or central south asian csa populations and show clinal structure with haplotype diversity decreasing from south to north furthermore of ea haplotypes were found in sea only and were found in csa only indicating that sea was a major geographic source of ea science
metagenomic studies characterize both the composition and diversity of uncultured viral and microbial communities blast based comparisons have typically been used for such analyses however sampling biases high percentages of unknown sequences and the use of arbitrary thresholds to find significant similarities can decrease the accuracy and validity of estimates here we present genome relative abundance and average size gaas a complete software package that provides improved estimates of community composition and average genome length for metagenomes in both textual and graphical formats gaas implements a novel methodology to control for sampling bias via length normalization to adjust for multiple blast similarities by similarity weighting and to select significant similarities using relative alignment lengths in benchmark tests the gaas method was robust to both high percentages of unknown sequences and to variations in metagenomic sequence read lengths re analysis of the sargasso sea virome using gaas indicated that standard methodologies for metagenomic analysis may dramatically underestimate the abundance and importance of organisms with small genomes in environmental systems using gaas we conducted a meta analysis of microbial and viral average genome lengths in over metagenomes from four biomes to determine whether genome lengths vary consistently between and within biomes and between microbial and viral communities from the same environment significant differences between biomes and within aquatic sub biomes oceans hypersaline systems freshwater and microbialites suggested that average genome length is a fundamental property of environments driven by factors at the sub biome level the behavior of paired viral and microbial metagenomes from the same environment indicated that microbial and viral average genome sizes are independent of each other but indicative of community responses to stressors and conditions
over a years ago william bateson provided through his observations of the transmission of alkaptonuria in first cousin offspring evidence of the application of mendelian genetics to certain human traits and diseases his work was corroborated by archibald garrod archibald ae the incidence of alkaptonuria a study in chemical individuality lancert ii and william farabee farabee wc inheritance of digital malformations in man in papers of the peabody museum of american archaeology and ethnology cambridge mass harvard university who recorded the familial tendencies of inheritance of malformations of human hands and feet these were the pioneers of the hunt for disease genes that would continue through the century and result in the discovery of hundreds of genes that can be associated with different diseases despite many ground breaking discoveries during the last century we are far from having a complete understanding of the intricate network of molecular processes involved in diseases and we are still searching for the cures for most complex diseases in the last few years new genome sequencing and other high throughput experimental techniques have generated vast amounts of molecular and clinical data that contain crucial information with the potential of leading to the next major biomedical discoveries the need to mine visualize and integrate these data has motivated the development of several informatics approaches that can broadly be grouped in the research area of translational bioinformatics this review highlights the latest advances in the field of translational bioinformatics focusing on the advances of computational techniques to search for and classify genes
scientific computing often requires the availability of a massive number of computers for performing large scale experiments traditionally these needs have been addressed by using high performance computing solutions and installed facilities such as clusters and super computers which are difficult to setup maintain and operate cloud computing provides scientists with a completely new model of utilizing the computing infrastructure compute resources storage resources as well as applications can be dynamically provisioned and integrated within the existing infrastructure on a pay per use basis these resources can be released when they are no more needed such services are often offered within the context of a service level agreement sla which ensure the desired quality of service qos aneka an enterprise cloud computing solution harnesses the power of compute resources by relying on private and public clouds and delivers to users the desired qos its flexible and service based infrastructure supports multiple programming paradigms that make aneka address a variety of different scenarios from finance applications to computational science as examples of scientific computing in the cloud we present a preliminary case study on using aneka for the classification of gene expression data and the execution of fmri brain workflow
using next generation sequencing technology alone we have successfully generated and assembled a draft sequence of the giant panda genome the assembled contigs gb cover approximately of the whole genome and the remaining gaps seem to contain carnivore specific repeats and tandem repeats comparisons with the dog and human showed that the panda genome has a lower divergence rate the assessment of panda genes potentially underlying some of its unique traits indicated that its bamboo diet might be more dependent on its gut microbiome than its own genetic composition we also identified more than heterozygous single nucleotide polymorphisms in the diploid genome our data and analyses provide a foundation for promoting mammalian genetic research and demonstrate the feasibility for using next generation sequencing technologies for accurate cost effective and rapid de novo assembly of large genomes
replicate mass spectrometry ms measurements and the use of multiple analytical methods can greatly expand the comprehensiveness of shotgun proteomic profiling of biological however the inherent biases and variations in such data create computational and statistical challenges for quantitative comparative we developed and tested a normalized label free quantitative method termed the normalized spectral index sin which combines three ms abundance features peptide count spectral count and fragment ion tandem ms or ms ms intensity sin largely eliminated variances between replicate ms measurements permitting quantitative reproducibility and highly significant quantification of thousands of proteins detected in replicate ms measurements of the same and distinct samples it accurately predicts protein abundance more often than the five other methods we tested comparative immunoblotting and densitometry further validate our method comparative quantification of complex data sets from multiple shotgun proteomics measurements is relevant for systems biology and discovery
pnas network analysis is currently used in a myriad of contexts from identifying potential drug targets to predicting the spread of epidemics and designing vaccination strategies and from finding friends to uncovering criminal activity despite the promise of the network approach the reliability of network data is a source of great concern in all fields where complex networks are studied here we present a general mathematical and computational framework to deal with the problem of data reliability in complex networks in particular we are able to reliably identify both missing and spurious interactions in noisy network observations remarkably our approach also enables us to obtain from those noisy observations network reconstructions that yield estimates of the true network properties that are more accurate than those provided by the observations themselves our approach has the potential to guide experiments to better characterize network data sets and to drive discoveries
this work investigates personalized social search based on the user s social relations search results are re ranked according to their relations with individuals in the user s social network we study the effectiveness of several social network types for personalization familiarity based network of people related to the user through explicit familiarity connection similarity based network of people similar to the user as reflected by their social activity overall network that provides both relationship types for comparison we also experiment with topic based personalization that is based on the user s related terms aggregated from several social applications we evaluate the contribution of the different personalization strategies by an off line study and by a user survey within our organization in the off line study we apply bookmark based evaluation suggested recently that exploits data gathered from a social bookmarking system to evaluate personalized retrieval in the on line study we analyze the feedback of employees exposed to the alternative personalization approaches our main results show that both in the off line study and in the user survey social network based personalization significantly outperforms non personalized social search additionally as reflected by the user survey all three sn based strategies significantly outperform the topic strategy
background sequence similarity searching is a very important bioinformatics task while basic local alignment search tool blast outperforms exact methods through its use of heuristics the speed of the current blast software is suboptimal for very long queries or database sequences there are also some shortcomings in the user interface of the current command line applications results we describe features and improvements of rewritten blast software and introduce new command line applications long query sequences are broken into chunks for processing in some cases leading to dramatically shorter run times for long database sequences it is possible to retrieve only the relevant parts of the sequence reducing cpu time and memory usage for searches of short queries against databases of contigs or chromosomes the program can now retrieve masking information for database sequences from the blast databases a new modular software library can now access subject sequence data from arbitrary data sources we introduce several new features including strategy files that allow a user to save and reuse their favorite set of options the strategy files can be uploaded to and downloaded from the ncbi blast web site conclusion the new blast command line applications compared to the current blast tools demonstrate substantial speed improvements for long queries as well as chromosome length database sequences we have also improved the user interface of the command applications
pnas we present here a structural and mechanistic description of how a protein changes its fold and function mutation by mutation our approach was to create proteins that are stably folded into different folds have different functions and are very similar in sequence in this simplified sequence space we explore the mutational path from one fold to another we show that an igg binding fold can be transformed into an albumin binding fold via a mutational pathway in which neither function nor native structure is completely lost the stabilities of all mutants along the pathway are evaluated key high resolution structures are determined by nmr and an explanation of the switching mechanism is provided we show that the conformational switch from to structure can occur via a single amino acid substitution on one side of the switch point the fold is populated ph c a single mutation switches the conformation to the fold which is populated ph c we further show that a bifunctional protein exists at the switch point with affinity for both igg albumin
although the quantitative description of biological systems has been going on for centuries recent advances in the measurement of phenomena ranging from metabolism to gene expression to signal transduction have resulted in a new emphasis on biological numeracy this article describes the confluence of two different approaches to biological numbers first an impressive array of quantitative measurements make it possible to develop intuition about biological numbers ranging from how many gigatons of atmospheric carbon are fixed every year in the process of photosynthesis to the number of membrane transporters needed to provide sugars to rapidly dividing escherichia coli cells as a result of the vast array of such quantitative data the bionumbers web site has recently been developed as a repository for biology by the numbers second a complementary and powerful tradition of numerical estimates familiar from the physical sciences and canonized in the so called fermi problems calls for efforts to estimate key biological quantities on the basis of a few foundational facts and simple ideas from physics and chemistry in this article we describe these two approaches and illustrate their synergism in several particularly appealing case studies these case studies reveal the impact that an emphasis on numbers can have on important questions
the conversion of data into knowledge constitutes a great challenge for future biological research the new science of systems biology claims to be able to solve the problem but i contend that this approach will fail because deducing models of function from the behaviour of a complex system is an inverse problem that is impossible to solve in addition one cannot easily escape into high level holistic approaches since the essence of all biological systems is that they are encoded as molecular descriptions in their genes and since genes are molecules and exert their functions through other molecules the molecular explanation must constitute the core of understanding biological systems we then solve the forward problem of computing the behaviour of the system from its components and their interactions i propose that the correct level of abstraction is the cell and provide an outline of cellmap a design for a system to organize information
all cancers carry somatic mutations a subset of these somatic alterations termed driver mutations confer selective growth advantage and are implicated in cancer development whereas the remainder are passengers here we have sequenced the genomes of a malignant melanoma and a lymphoblastoid cell line from the same person providing the first comprehensive catalogue of somatic mutations from an individual cancer the catalogue provides remarkable insights into the forces that have shaped this cancer genome the dominant mutational signature reflects dna damage due to ultraviolet light exposure a known risk factor for malignant melanoma whereas the uneven distribution of mutations across the genome with a lower prevalence in gene footprints indicates that dna repair has been preferentially deployed towards transcribed regions the results illustrate the power of a cancer genome sequence to reveal traces of the dna damage repair mutation and selection processes that were operative years before the cancer symptomatic
cancer is driven by mutation worldwide tobacco smoking is the principal lifestyle exposure that causes cancer exerting carcinogenicity through chemicals that bind and mutate dna using massively parallel sequencing technology we sequenced a small cell lung cancer cell line nci to explore the mutational burden associated with tobacco smoking a total of somatic substitutions were identified including in coding exons multiple mutation signatures testify to the cocktail of carcinogens in tobacco smoke and their proclivities for particular bases and surrounding sequence context effects of transcription coupled repair and a second more general expression linked repair pathway were evident we identified a tandem duplication that duplicates exons of in frame and another two lines carrying fusion genes indicating that may be recurrently rearranged in this disease these findings illustrate the potential for next generation sequencing to provide unprecedented insights into mutational processes cellular repair pathways and gene networks associated cancer
background prediction of protein protein interaction sites is one of the most challenging and intriguing problems in the field of computational biology although much progress has been achieved by using various machine learning methods and a variety of available features the problem is still far from being solved results in this paper an ensemble method is proposed which combines bootstrap resampling technique svm based fusion classifiers and weighted voting strategy to overcome the imbalanced problem and effectively utilize a wide variety of features we evaluate the ensemble classifier using a dataset extracted from polypeptide chains with fold cross validation and get a auc score of with a sensitivity of and a specificity of which are better than that of the existing methods to improve the usefulness of the proposed method two special ensemble classifiers are designed to handle the cases of missing homologues and structural information respectively and the performance is still encouraging the robustness of the ensemble method is also evaluated by effectively classifying interaction sites from surface residues as well as from all residues in proteins moreover we demonstrate the applicability of the proposed method to identify interaction sites from the non structural proteins ns of the influenza a virus which may be utilized as potential drug target sites conclusion our experimental results show that the ensemble classifiers are quite effective in predicting protein interaction sites the sub enclassifiers with resampling technique can alleviate the imbalanced problem and the combination of sub enclassifiers with a wide variety of feature groups can significantly improve performance
the ability to determine the structure of matter in three dimensions has profoundly advanced our understanding of nature traditionally the most widely used schemes for three dimensional structure determination of an object are implemented by acquiring multiple measurements over various sample orientations as in the case of crystallography and or by scanning a series of thin sections through the sample as in confocal here we present a imaging modality termed ankylography derived from the greek words ankylos meaning curved and graphein meaning writing which under certain circumstances enables complete structure determination from a single exposure using a monochromatic incident beam we demonstrate that when the diffraction pattern of a finite object is sampled at a sufficiently fine scale on the ewald sphere the structure of the object is in principle determined by the spherical pattern we confirm the theoretical analysis by performing numerical reconstructions of a sodium silicate glass structure at resolution and a single poliovirus at resolution from spherical diffraction patterns alone using diffraction data from a soft x ray laser we also provide a preliminary demonstration that ankylography is experimentally feasible by obtaining a image of a test object from a single diffraction pattern with further development this approach of obtaining complete structure information from a single view could find broad applications in the physical and sciences
effects of susceptibility variants may depend on from which parent they are inherited although many associations between sequence variants and human traits have been discovered through genome wide associations the impact of parental origin has largely been ignored here we show that for icelanders genotyped using single nucleotide polymorphism snp chips the parental origin of most alleles can be determined for this we used a combination of genealogy and long range phasing we then focused on snps that associate with diseases and are within kilobases of known imprinted genes seven independent snp associations were examined fiveone with breast cancer one with basal cell carcinoma and three with diabeteshave parental origin specific associations these variants are located in two genomic regions and each harbouring a cluster of imprinted genes furthermore we observed a novel association between the snp at and diabetes here the allele that confers risk when paternally inherited is protective when maternally transmitted we identified a differentially methylated ctcf binding site at and demonstrated correlation of with decreased methylation of site
fastq has emerged as a common file format for sharing sequencing read data combining both the sequence and an associated per base quality score despite lacking any formal definition to date and existing in at least three incompatible variants this article defines the fastq format covering the original sanger standard the solexa illumina variants and conversion between them based on publicly available information such as the maq documentation and conventions recently agreed by the open bioinformatics foundation projects biopython bioperl bioruby biojava and emboss being an open access publication it is hoped that this description with the example files provided as supplementary data will serve in future as a reference for this important format
in recent years views of eukaryotic gene expression have been transformed by the finding that enormous diversity can be generated at the rna level advances in technologies for characterizing rna populations are revealing increasingly complete descriptions of rna regulation and complexity for example through alternative splicing alternative polyadenylation and rna editing new biochemical strategies to map proteinrna interactions in vivo are yielding transcriptome wide insights into mechanisms of rna processing these advances combined with bioinformatics and genetic validation are leading to the generation of functional rna maps that reveal the rules underlying rna regulation and networks of biologically coherent transcripts together these are providing new insights into molecular cell biology disease
accurate identification of genetic variants from next generation sequencing ngs data is essential for immediate large scale genomic endeavors such as the genomes project and is crucial for further genetic analysis based on the discoveries the key challenge in single nucleotide polymorphism snp discovery is to distinguish true individual variants occurring at a low frequency from sequencing errors often occurring at frequencies orders of magnitude higher therefore knowledge of the error probabilities of base calls is essential we have developed atlas a computational tool that detects and accounts for systematic sequencing errors caused by context related variables in a logistic regression model learned from training data sets subsequently it estimates the posterior error probability for each substitution through a bayesian formula that integrates prior knowledge of the overall sequencing error probability and the estimated snp rate with the results from the logistic regression model for the given substitutions the estimated posterior snp probability can be used to distinguish true snps from sequencing errors validation results show that atlas achieves a false positive rate of lower than with an approximately or lower false rate
next generation massively parallel dna sequencing technologies provide ultrahigh throughput at a substantially lower unit data cost however the data are very short read length sequences making de novo assembly extremely challenging here we describe a novel method for de novo assembly of large genomes from short read sequences we successfully assembled both the asian and african human genome sequences achieving an contig size of and kilobases kb and scaffold of and kb respectively the development of this de novo short read assembly method creates new opportunities for building reference sequences and carrying out accurate analyses of unexplored genomes in a cost way
background chromatin immunoprecipitation coupled with massively parallel sequencing chip seq is increasingly being applied to study transcriptional regulation on a genome wide scale while numerous algorithms have recently been proposed for analysing the large chip seq datasets their relative merits and potential limitations remain unclear in practical applications results the present study compares the state of the art algorithms for detecting transcription factor binding sites in four diverse chip seq datasets under a variety of practical research settings first we demonstrate how the biological conclusions may change dramatically when the different algorithms are applied the reproducibility across biological replicates is then investigated as an internal validation of the detections finally the predicted binding sites with each method are compared to high scoring binding motifs as well as binding regions confirmed in independent qpcr experiments conclusions in general our results indicate that the optimal choice of the computational approach depends heavily on the dataset under analysis in addition to revealing valuable information to the users of this technology about the characteristics of the binding site detection approaches the systematic evaluation framework provides also a useful reference to the developers of improved algorithms for chip data
coordinated spiking activity in neuronal ensembles in local networks and across multiple cortical areas is thought to provide the neural basis for cognition and adaptive behavior examining such collective dynamics at the level of single neuron spikes has remained however a considerable challenge we found that the spiking history of small and randomly sampled ensembles approximately neurons could predict subsequent single neuron spiking with substantial accuracy in the sensorimotor cortex of humans and nonhuman behaving primates furthermore spiking was better predicted by the ensemble s history than by the ensemble s instantaneous state ising models emphasizing the role of temporal dynamics leading to spiking notably spiking could be predicted not only by local ensemble spiking histories but also by spiking histories in different cortical areas these strong collective dynamics may provide a basis for understanding cognition and adaptive behavior at the level of coordinated spiking in networks
background protein structural domains are evolutionary units whose relationships can be detected over long evolutionary distances the evolutionary history of protein domains including the origin of protein domains the identification of domain loss transfer duplication and combination with other domains to form new proteins and the formation of the entire protein domain repertoire are of great interest methodology principal findings a methodology is presented for providing a parsimonious domain history based on gain loss vertical and horizontal transfer derived from the complete genomic domain assignments of organisms across the tree of life when mapped to species trees the evolutionary history of domains and domain combinations is revealed and the general evolutionary trend of domain and combination is analyzed conclusions significance we show that this approach provides a powerful tool to study how new proteins and functions emerged and to study such processes as horizontal gene transfer among more species
the evolution of cis regulatory elements enhancers of developmentally regulated genes plays a large role in the evolution of animal morphology however the mutational path of enhancer evolution the number origin effect and order of mutations that alter enhancer function has not been elucidated here we localized a suite of substitutions in a modular enhancer of the ebony locus responsible for adaptive melanism in a ugandan drosophila population we show that at least five mutations with varied effects arose recently from a combination of standing variation and new mutations and combined to create an allele of large phenotypic effect we underscore how enhancers are distinct macromolecular entities subject to fundamentally different and generally more relaxed functional constraints relative to protein science
motivation rna seq is a promising new technology for accurately measuring gene expression levels expression estimation with rna seq requires the mapping of relatively short sequencing reads to a reference genome or transcript set because reads are generally shorter than transcripts from which they are derived a single read may map to multiple genes and isoforms complicating expression analyses previous computational methods either discard reads that map to multiple locations or allocate them to genes heuristically results we present a generative statistical model and associated inference methods that handle read mapping uncertainty in a principled manner through simulations parameterized by real rna seq data we show that our method is more accurate than previous methods our improved accuracy is the result of handling read mapping uncertainty with a statistical model and the estimation of gene expression levels as the sum of isoform expression levels unlike previous methods our method is capable of modeling non uniform read distributions simulations with our method indicate that a read length of bases is optimal for gene level expression estimation from mouse and maize rna seq data when sequencing throughput is fixed availability an initial c implementation of our method that was used for the results presented in this article is available at http deweylab biostat wisc edu rsem contact cdewey biostat wisc edusupplementary information supplementary data are available at on
purpose the purpose of this paper is to find out whether ebooks are cannibalizing print books as well as an assessment of factors that are influencing ebook usage ebooks are a hot topic traditional book publishing especially in the academic world is changing at a rapid pace the question on everybody s mind is what direction book publishing will take will print survive in the google generation or is it destined to be totally replaced by ebooks springer publishes over book titles annually which are converted into ebooks almost without exception being the market leader and innovator of a new business model in electronic books in the stm area springer has conducted a study on the implications of the springer ebook collection in comparison to its print book activities design methodology approach the study is based on interviews with both end users and librarians in addition springer has assessed the counter compliant usage statistics from springerlink findings overall springer s ebook usage is already percent of its journal usage while the amount of content compared with journals is only percent taking this success of ebook usage into account springer still believes strongly in the print model and has recently launched mycopy heavily discounted print on demand books from the electronic versions originality value the study shows that print and electronic can exist together and will complement each other s strengths abstract author
it is currently not known how distributed neuronal responses in early visual areas carry stimulus related information we made multielectrode recordings from cat primary visual cortex and applied methods from machine learning in order to analyze the temporal evolution of stimulus related information in the spiking activity of large ensembles of around neurons we used sequences of up to three different visual stimuli letters of the alphabet presented for ms and with intervals of ms or larger most of the information about visual stimuli extractable by sophisticated methods of machine learning i e support vector machines with nonlinear kernel functions was also extractable by simple linear classification such as can be achieved by individual neurons new stimuli did not erase information about previous stimuli the responses to the most recent stimulus contained about equal amounts of information about both this and the preceding stimulus this information was encoded both in the discharge rates response amplitudes of the ensemble of neurons and when using short time constants for integration e g ms in the precise timing of individual spikes or approximately ms and persisted for several ms beyond the offset of stimuli the results indicate that the network from which we recorded is endowed with fading memory and is capable of performing online computations utilizing information about temporally sequential stimuli this result challenges models assuming frame by frame analyses of inputs
the separation of red blood cells from plasma flowing in microchannels is possible by biophysical effects such as the zweifach fung bifurcation law in the present study daughter channels are placed alongside a main channel such that cells and plasma are collected separately the device is aimed to be a versatile but yet very simple module producing high speed and high efficiency plasma separation the resulting lab on a chip is manufactured using biocompatible materials purity efficiency is measured for mussel and human blood suspensions as different parameters such as flow rate and geometries of the parent and daughter channels are varied the issues of blood plasma separation at the microscale are discussed in relation to the different regimes of flow results are compared with those obtained by other researchers in the field of micro separation of blood verlag
the failure mechanisms associated with microelectromechanical systems mems discussed failure modes during fabrication breaks in the suspended parts and occurrence of stiction are some of the major failure modes of these devices mems devices are much more sensitive to the surrounding environment than conventional macro scale devices and thus can be affected with particle contamination due to environmental pollution and vibration to avoid these problems failure mode and effect analysis fmea can help access the criticality of failure modes in mems devices fema also helps in investigating the problems before the design has been completed and before failures can occur in field
sequencing of bacterial and archaeal genomes has revolutionized our understanding of the many roles played by microorganisms there are now nearly completed bacterial and archaeal genomes available most of which were chosen for sequencing on the basis of their physiology as a result the perspective provided by the currently available genomes is limited by a highly biased phylogenetic distribution to explore the value added by choosing microbial genomes for sequencing on the basis of their evolutionary relationships we have sequenced and analysed the genomes of culturable species of bacteria and archaea selected to maximize phylogenetic coverage analysis of these genomes demonstrated pronounced benefits compared to an equivalent set of genomes randomly selected from the existing database in diverse areas including the reconstruction of phylogenetic history the discovery of new protein families and biological properties and the prediction of functions for known genes from other organisms our results strongly support the need for systematic phylogenomic efforts to compile a phylogeny driven genomic encyclopedia of bacteria and archaea in order to derive maximum knowledge from existing microbial genome data as well as from genome sequences come
the inference of transcriptional networks that regulate transitions into physiological or pathological cellular states remains a central challenge in systems biology a mesenchymal phenotype is the hallmark of tumour aggressiveness in human malignant glioma but the regulatory programs responsible for implementing the associated molecular signature are largely unknown here we show that reverse engineering and an unbiased interrogation of a glioma specific regulatory network reveal the transcriptional module that activates expression of mesenchymal genes in malignant glioma two transcription factors c ebp and emerge as synergistic initiators and master regulators of mesenchymal transformation ectopic co expression of c ebp and reprograms neural stem cells along the aberrant mesenchymal lineage whereas elimination of the two factors in glioma cells leads to collapse of the mesenchymal signature and reduces tumour aggressiveness in human glioma expression of c ebp and correlates with mesenchymal differentiation and predicts poor clinical outcome these results show that the activation of a small regulatory module is necessary and sufficient to initiate and maintain an aberrant phenotypic state in cells
multiple somatic rearrangements are often found in cancer genomes however the underlying processes of rearrangement and their contribution to cancer development are poorly characterized here we use a paired end sequencing strategy to identify somatic rearrangements in breast cancer genomes there are more rearrangements in some breast cancers than previously appreciated rearrangements are more frequent over gene footprints and most are intrachromosomal multiple rearrangement architectures are present but tandem duplications are particularly common in some cancers perhaps reflecting a specific defect in dna maintenance short overlapping sequences at most rearrangement junctions indicate that these have been mediated by non homologous end joining dna repair although varying sequence patterns indicate that multiple processes of this type are operative several expressed in frame fusion genes were identified but none was recurrent the study provides a new perspective on cancer genomes highlighting the diversity of somatic rearrangements and their potential contribution to development
this work demonstrates how gene association studies can be analyzed to map a global landscape of genetic interactions among protein complexes and pathways despite the immense potential of gene association studies they have been challenging to analyze because most traits are complex involving the combined effect of mutations at many different genes due to lack of statistical power only the strongest single markers are typically identified here we present an integrative approach that greatly increases power through marker clustering and projection of marker interactions within and across protein complexes applied to a recent gene association study in yeast this approach identifies genetic interactions which map to functional interactions among protein complexes we show that such interactions are analogous to interactions derived through reverse genetic screens and that they provide coverage in areas not yet tested by reverse genetic analysis this work has the potential to transform gene association studies by elevating the analysis from the level of individual markers to global maps of genetic interactions as proof of principle we use synthetic genetic screens to confirm numerous novel genetic interactions for the chromatin complex
bioinformatics programming skills are becoming a necessity across many facets of biology and medicine owed in part to the continuing explosion of biological data aggregation and the complexity and scale of questions now being addressed through modern bioinformatics although many are now receiving formal training in bioinformatics through various university degree and certificate programs this training is often focused strongly on bioinformatics methodology leaving many important and practical aspects of bioinformatics to self education and experience the following set of guidelines distill several key principals of effective bioinformatics programming which the authors learned through insights gained across many years of combined experience developing popular bioinformatics software applications and database systems in both academic and commercial settings successful adoption of these principals will serve both beginner and experienced bioinformaticians alike in career development and pursuit of professional and goals
this tutorial is intended for biologists and computational biologists interested in adding text mining tools to their bioinformatics toolbox as an illustrative example the tutorial examines the relationship between progressive multifocal leukoencephalopathy pml and antibodies recent cases of pml have been associated to the administration of some monoclonal antibodies such as efalizumab those interested in a further introduction to text mining may also want to read other reviews understanding large amounts of text with the aid of a computer is harder than simply equipping a computer with a grammar and a dictionary a computer like a human needs certain specialized knowledge in order to understand text the scientific field that is dedicated to train computers with the right knowledge for this task among other tasks is called natural language processing nlp biomedical text mining henceforth text mining is the subfield that deals with text that comes from biology medicine and chemistry henceforth biomedical text another popular name is bionlp which some practitioners use as synonymous with mining
methods to compute free energy differences between different states of a molecular system are reviewed with the aim of identifying their basic ingredients and their utility when applied in practice to biomolecular systems a free energy calculation is comprised of three basic components i a suitable model or hamiltonian ii a sampling protocol with which one can generate a representative ensemble of molecular configurations and iii an estimator of the free energy difference itself alternative sampling protocols can be distinguished according to whether one or more states are to be sampled in cases where only a single state is considered six alternative techniques could be distinguished i changing the dynamics ii deforming the energy surface iii extending the dimensionality iv perturbing the forces v reducing the number of degrees of freedom and vi multi copy approaches in cases where multiple states are to be sampled the three primary techniques are staging importance sampling and adiabatic decoupling estimators of the free energy can be classified as global methods that either count the number of times a given state is sampled or use energy differences or they can be classified as local methods that either make use of the force or are based on transition probabilities finally this overview of the available techniques and how they can be best used in a practical context is aimed at helping the reader choose the most appropriate combination of approaches for the biomolecular system hamiltonian and free energy difference interest
background the decreasing costs of capillary based sanger sequencing and next generation technologies such as pyrosequencing have prompted an explosion of transcriptome projects in non model species where even shallow sequencing of transcriptomes can now be used to examine a range of research questions this rapid growth in data has outstripped the ability of researchers working on non model species to analyze and mine transcriptome data efficiently results here we present a semi automated platform that processes raw sequence data from sanger or sequencing into a hybrid de novo assembly annotates it and produces gmod compatible output including a seqfeature database suitable for gbrowse users are able to parameterize assembler variables judge assembly quality and determine the optimal assembly for their specific needs we used to process drosophila and bicyclus public sanger est data and then compared them to published data as well as eight new insect transcriptome collections conclusions analysis of such a wide variety of data allows us to understand how these new technologies can assist est project design we determine that assembler parameterization is as essential as standardized methods to judge the output of ests projects further even shallow sequencing using produces sufficient data to be of wide use to the community is an important tool to assist manual curation for gene models an important resource in their own right but especially for species which are due to acquire a genome project using next sequencing
defining the protein profiles of tissues and organs is critical to understanding the unique characteristics of the various cell types in the human body in this study we report on an anatomically comprehensive analysis of protein profiles in human tissues and human cell lines a detailed analysis of over million manually annotated high resolution immunohistochemistry based images showed a high fraction of expressed proteins in most cells and tissues with very few proteins detected in any single cell type similarly confocal microscopy in three human cell lines detected expression of more than of the analyzed proteins despite this ubiquitous expression hierarchical clustering analysis based on global protein expression patterns shows that the analyzed cells can be still subdivided into groups according to the current concepts of histology and cellular differentiation this study suggests that tissue specificity is achieved by precise regulation of protein levels in space and time and that different tissues in the body acquire their unique characteristics by controlling not which proteins are expressed but how much of each produced
background despite the short length of their reads micro read sequencing technologies have shown their usefulness for de novo sequencing however especially in eukaryotic genomes complex repeat patterns are an obstacle to large assemblies principal findings we present a novel heuristic algorithm pebble which uses paired end read information to resolve repeats and scaffold contigs to produce large scale assemblies in simulations we can achieve weighted median scaffold lengths of above mbp in bacteria and above kbp in more complex organisms using real datasets we obtained a kbp in pseudomonas syringae and a unique kbp scaffold of a ferret bac clone we also present an efficient algorithm called rock band for the resolution of repeats in the case of mixed length assemblies where different sequencing platforms are combined to obtain a cost effective assembly conclusions these algorithms extend the utility of short read only assemblies into large complex genomes they have been implemented and made available within the open source velvet short read de assembler
j x the term learning styles refers to the concept that individuals differ in regard to what mode of instruction or study is most effective for them proponents of learning style assessment contend that optimal instruction requires diagnosing individuals learning style and tailoring instruction accordingly assessments of learning style typically ask people to evaluate what sort of information presentation they prefer e g words versus pictures versus speech and or what kind of mental activity they find most engaging or congenial e g analysis versus listening although assessment instruments are extremely diverse the most commonbut not the onlyhypothesis about the instructional relevance of learning styles is the meshing hypothesis according to which instruction is best provided in a format that matches the preferences of the learner e g for a visual learner emphasizing visual presentation of information the learning styles view has acquired great influence within the education field and is frequently encountered at levels ranging from kindergarten to graduate school there is a thriving industry devoted to publishing learning styles tests and guidebooks for teachers and many organizations offer professional development workshops for teachers and educators built around the concept of learning styles the authors of the present review were charged with determining whether these practices are supported by scientific evidence we concluded that any credible validation of learning styles based instruction requires robust documentation of a very particular type of experimental finding with several necessary criteria first students must be divided into groups on the basis of their learning styles and then students from each group must be randomly assigned to receive one of multiple instructional methods next students must then sit for a final test that is the same for all students finally in order to demonstrate that optimal learning requires that students receive instruction tailored to their putative learning style the experiment must reveal a specific type of interaction between learning style and instructional method students with one learning style achieve the best educational outcome when given an instructional method that differs from the instructional method producing the best outcome for students with a different learning style in other words the instructional method that proves most effective for students with one learning style is not the most effective method for students with a different learning style our review of the literature disclosed ample evidence that children and adults will if asked express preferences about how they prefer information to be presented to them there is also plentiful evidence arguing that people differ in the degree to which they have some fairly specific aptitudes for different kinds of thinking and for processing different types of information however we found virtually no evidence for the interaction pattern mentioned above which was judged to be a precondition for validating the educational applications of learning styles although the literature on learning styles is enormous very few studies have even used an experimental methodology capable of testing the validity of learning styles applied to education moreover of those that did use an appropriate method several found results that flatly contradict the popular meshing hypothesis we conclude therefore that at present there is no adequate evidence base to justify incorporating learning styles assessments into general educational practice thus limited education resources would better be devoted to adopting other educational practices that have a strong evidence base of which there are an increasing number however given the lack of methodologically sound studies of learning styles it would be an error to conclude that all possible versions of learning styles have been tested and found wanting many have simply not been tested at all further research on the use of learning styles assessment in instruction may in some cases be warranted but such research needs to be appropriately
node characteristics and behaviors are often correlated with the structure of social networks over time while evidence of this type of assortative mixing and temporal clustering of behaviors among linked nodes is used to support claims of peer influence and social contagion in networks homophily may also explain such evidence here we develop a dynamic matched sample estimation framework to distinguish influence and homophily effects in dynamic networks and we apply this framework to a global instant messaging network of million users using data on the day by day adoption of a mobile service application and users longitudinal behavioral demographic and geographic data we find that previous methods overestimate peer influence in product adoption decisions in this network by and that homophily explains of the perceived behavioral contagion these findings and methods are essential to both our understanding of the mechanisms that drive contagions in networks and our knowledge of how to propagate or combat them in domains as diverse as epidemiology marketing development economics and health
motivation many biological phenomena involve extensive interactions between many of the biological pathways present in cells however extraction of all the inherent biological pathways remains a major challenge in systems biology with the advent of high throughput functional genomic techniques it is now possible to infer biological pathways and pathway organization in a systematic way by integrating disparate biological information results here we propose a novel integrated approach that uses network topology to predict biological pathways we integrated four types of biological evidence protein protein interaction genetic interaction domain domain interaction and semantic similarity of gene ontology terms to generate a functionally associated network this network was then used to develop a new pathway finding algorithm to predict biological pathways in yeast our approach discovered biological pathways and functionally redundant pathway pairs in yeast by comparing our identified pathways to three public pathway databases kegg biocyc and reactome we observed that our approach achieves a maximum positive predictive value of and improves on other predictive approaches this study allows us to reconstruct biological pathways and delineates cellular machinery in a view
more than twelve years have elapsed since the first public release of weka in that time the software has been rewritten entirely from scratch evolved substantially and now accompanies a text on data mining these days weka enjoys widespread acceptance in both academia and business has an active community and has been downloaded more than million times since being placed on source forge in april this paper provides an introduction to the weka workbench reviews the history of the project and in light of the recent stable release briefly discusses what has been added since the last stable version weka in
recent studies have shown evidence for the coevolution of functionally related genes this coevolution is a result of constraints to maintain functional relationships between interacting proteins the studies have focused on the correlation in gene tree branch lengths of proteins that are directly interacting with each other we here hypothesize that the correlation in branch lengths is not limited only to proteins that directly interact but also to proteins that operate within the same pathway using generalized linear models as a basis of identifying correlation we attempted to predict the gene ontology go terms of a gene based on its gene tree branch lengths we applied our method to a dataset consisting of proteins from ten prokaryotic species we found that the degree of accuracy to which we could predict the function of the proteins from their gene tree varied substantially with different go terms in particular our model could accurately predict genes involved in translation and certain ribosomal activities with the area of the receiver operator curve of up to further analysis showed that the similarity between the trees of genes labeled with similar go terms was not limited to genes that physically interacted but also extended to genes functioning within the same pathway we discuss the relevance of our findings as it relates to the use of phylogenetic methods in genomics
to take complete advantage of information on within species polymorphism and divergence from close relatives one needs to know the rate and the molecular spectrum of spontaneous mutations to this end we have searched for de novo spontaneous mutations in the complete nuclear genomes of five arabidopsis thaliana mutation accumulation lines that had been maintained by single seed descent for generations we identified and validated base substitutions and small and large insertions and deletions our results imply a spontaneous mutation rate of x base substitutions per site per generation the majority of which are g c gt a t transitions we explain this very biased spectrum of base substitution mutations as a result of two main processes deamination of methylated cytosines and ultraviolet light induced science
structural variants svs are a major source of human genomic variation however characterizing them at nucleotide resolution remains challenging here we assemble a library of breakpoints at nucleotide resolution from collating and standardizing published svs for each breakpoint we infer its ancestral state through comparison to primate genomes and its mechanism of formation e g nonallelic homologous recombination nahr we characterize breakpoint sequences with respect to genomic landmarks chromosomal location sequence motifs and physical properties finding that the occurrence of insertions and deletions is more balanced than previously reported and that nahr formed breakpoints are associated with relatively rigid stable dna helices finally we demonstrate an approach breakseq for scanning the reads from short read sequenced genomes against our breakpoint library to accurately identify previously overlooked svs which we then validate by pcr as new data become available we expect our breakseq approach will become more sensitive and facilitate rapid sv genotyping of genomes
meiotic recombination events cluster into narrow segments of the genome defined as hotspots here we demonstrate that a major player for hotspot specification is the gene first two mouse strains that differ in hotspot usage are polymorphic for the zinc finger dna binding array of second the human consensus allele is predicted to recognize the mer motif enriched at human hotspots this dna binding specificity is verified by in vitro studies third allelic variants of zinc fingers are significantly associated with variability in genome wide hotspot usage among humans our results provide a molecular basis for the distribution of meiotic recombination in mammals in which the binding of to specific dna sequences targets the initiation of recombination at specific locations in the science
cell membranes display a tremendous complexity of lipids and proteins designed to perform the functions cells require to coordinate these functions the membrane is able to laterally segregate its constituents this capability is based on dynamic liquid liquid immiscibility and underlies the raft concept of membrane subcompartmentalization lipid rafts are fluctuating nanoscale assemblies of sphingolipid cholesterol and proteins that can be stabilized to coalesce forming platforms that function in membrane signaling and trafficking here we review the evidence for how this principle combines the potential for sphingolipid cholesterol self assembly with protein specificity to selectively focus bioactivity
predicting protein structure from primary sequence is one of the ultimate challenges in computational biology given the large amount of available sequence data the analysis of co evolution i e statistical dependency between columns in multiple alignments of protein domain sequences remains one of the most promising avenues for predicting residues that are contacting in the structure a key impediment to this approach is that strong statistical dependencies are also observed for many residue pairs that are distal in the structure using a comprehensive analysis of protein domains with available three dimensional structures we show that co evolving contacts very commonly form chains that percolate through the protein structure inducing indirect statistical dependencies between many distal pairs of residues we characterize the distributions of length and spatial distance traveled by these co evolving contact chains and show that they explain a large fraction of observed statistical dependencies between structurally distal pairs we adapt a recently developed bayesian network model into a rigorous procedure for disentangling direct from indirect statistical dependencies and we demonstrate that this method not only successfully accomplishes this task but also allows contacts with weak statistical dependency to be detected to illustrate how additional information can be incorporated into our method we incorporate a phylogenetic correction and we develop an informative prior that takes into account that the probability for a pair of residues to contact depends strongly on their primary sequence distance and the amount of conservation that the corresponding columns in the multiple alignment exhibit we show that our model including these extensions dramatically improves the accuracy of contact prediction from multiple alignments
networks are ubiquitous in science and have become a focal point for discussion in everyday life formal statistical models for the analysis of network data have emerged as a major topic of interest in diverse areas of study and most of these involve a form of graphical representation probability models on graphs date back to along with empirical studies in social psychology and sociology from the these early works generated an active network community and a substantial literature in the this effort moved into the statistical literature in the late and and the past decade has seen a burgeoning network literature in statistical physics and computer science the growth of the world wide web and the emergence of online networking communities such as facebook myspace and linkedin and a host of more specialized professional network communities has intensified interest in the study of networks and network data our goal in this review is to provide the reader with an entry point to this burgeoning literature we begin with an overview of the historical development of statistical network modeling and then we introduce a number of examples that have been studied in the network literature our subsequent discussion focuses on a number of prominent static and dynamic network models and their interconnections we emphasize formal model descriptions and pay special attention to the interpretation of parameters and their estimation we end with a description of some open problems and challenges for machine learning statistics
background many complementary solutions are available for the identifier mapping problem this creates an opportunity for bioinformatics tool developers tools can be made to flexibly support multiple mapping services or mapping services could be combined to get broader coverage this approach requires an interface layer between tools and mapping services results here we present bridgedb a software framework for gene protein and metabolite identifier mapping this framework provides a standardized interface layer through which bioinformatics tools can be connected to different identifier mapping services this approach makes it easier for tool developers to support identifier mapping mapping services can be combined or merged to support multi omics experiments or to integrate custom microarray annotations bridgedb provides its own ready to go mapping services both in webservice and local database forms however the framework is intended for customization and adaptation to any identifier mapping service bridgedb has already been integrated into several bioinformatics applications conclusion by uncoupling bioinformatics tools from mapping services bridgedb improves capability and flexibility of those tools all described software is open source and available at http www org
assembling the tree of life is a major goal of biology but progress has been hindered by the difficulty and expense of obtaining the orthologous dna required for accurate and fully resolved phylogenies next generation dna sequencing technologies promise to accelerate progress but sequencing the genomes of hundreds of thousands of eukaryotic species remains impractical eukaryotic transcriptomes which are smaller than genomes and biased toward highly expressed genes that tend to be conserved could potentially provide a rich set of phylogenetic characters we sampled the transcriptomes of mosquito species by assembling bp sequence reads into phylogenomic data matrices containing hundreds of thousands of orthologous nucleotides from hundreds of genes analysis of these data matrices yielded robust phylogenetic inferences even with data matrices constructed from surprisingly few sequence reads this approach is more efficient data rich and economical than traditional pcr based and est based methods and provides a scalable strategy for generating phylogenomic data matrices to infer the branches and twigs of the tree life
we study personalized recommendation of social software items including bookmarked web pages blog entries and communities we focus on recommendations that are derived from the user s social network social network information is collected and aggregated across different data sources within our organization at the core of our research is a comparison between recommendations that are based on the user s familiarity network and his her similarity network we also examine the effect of adding explanations to each recommended item that show related people and their relationship to the user and to the item evaluation based on an extensive user survey with participants and a field study including users indicates superiority of the familiarity network as a basis for recommendations in addition an important instant effect of explanations is found interest rate in recommended items increases when explanations provided
context antidepressant medications represent the best established treatment for major depressive disorder but there is little evidence that they have a specific pharmacological effect relative to pill placebo for patients with less severe depression objective to estimate the relative benefit of medication vs placebo across a wide range of initial symptom severity in patients diagnosed with depression data sources pubmed psycinfo and the cochrane library databases were searched from january through march along with references from meta analyses and reviews study selection randomized placebo controlled trials of antidepressants approved by the food and drug administration in the treatment of major or minor depressive disorder were selected studies were included if their authors provided the requisite original data they comprised adult outpatients they included a medication vs placebo comparison for at least weeks they did not exclude patients on the basis of a placebo washout period and they used the hamilton depression rating scale hdrs data from studies patients were included data extraction individual patient level data were obtained from study authors results medication vs placebo differences varied substantially as a function of baseline severity among patients with hdrs scores below cohen d effect sizes for the difference between medication and placebo were estimated to be less than a standard definition of a small effect estimates of the magnitude of the superiority of medication over placebo increased with increases in baseline depression severity and crossed the threshold defined by the national institute for clinical excellence for a clinically significant difference at a baseline hdrs score of conclusions the magnitude of benefit of antidepressant medication compared with placebo increases with severity of depression symptoms and may be minimal or nonexistent on average in patients with mild or moderate symptoms for patients with very severe depression the benefit of medications over placebo substantial
background cluster analysis is an important technique for the exploratory analysis of biological data such data is often high dimensional inherently noisy and contains outliers this makes clustering challenging mixtures are versatile and powerful statistical models which perform robustly for clustering in the presence of noise and have been successfully applied in a wide range of applications results pymix the python mixture package implements algorithms and data structures for clustering with basic and advanced mixture models the advanced models include context specific independence mixtures mixtures of dependence trees and semi supervised learning pymix is licenced under the gnu general public licence gpl pymix has been successfully used for the analysis of biological sequence complex disease and gene expression data conclusions pymix is a useful tool for cluster analysis of biological data due to the general nature of the framework pymix can be applied to a wide range of applications and sets
retroviruses are the only group of viruses known to have left a fossil record in the form of endogenous proviruses and approximately of the human genome is made up of these although many other viruses including non retroviral rna viruses are known to generate dna forms of their own genomes during none has been found as dna in the germline of animals bornaviruses a genus of non segmented negative sense rna virus are unique among rna viruses in that they establish persistent infection in the cell here we show that elements homologous to the nucleoprotein n gene of bornavirus exist in the genomes of several mammalian species including humans non human primates rodents and elephants these sequences have been designated endogenous borna like n ebln elements some of the primate eblns contain an intact open reading frame orf and are expressed as mrna phylogenetic analyses showed that eblns seem to have been generated by different insertional events in each specific animal family furthermore the ebln of a ground squirrel was formed by a recent integration event whereas those in primates must have been formed more than million years ago we also show that the n mrna of a current mammalian bornavirus borna disease virus bdv can form ebln like elements in the genomes of persistently infected cultured cells our results provide the first evidence for endogenization of non retroviral virus derived elements in mammalian genomes and give novel insights not only into generation of endogenous elements but also into a role of bornavirus as a source of genetic novelty in host
starting from first principles and general assumptions newton s law of gravitation is shown to arise naturally and unavoidably in a theory in which space is emergent through a holographic scenario gravity is explained as an entropic force caused by changes in the information associated with the positions of material bodies a relativistic generalization of the presented arguments directly leads to the einstein equations when space is emergent even newton s law of inertia needs to be explained the equivalence principle leads us to conclude that it is actually this law of inertia whose origin entropic
randomness is a fundamental feature of nature and a valuable resource for applications ranging from cryptography and gambling to numerical simulation of physical and biological systems random numbers however are difficult to characterize and their generation must rely on an unpredictable physical inaccuracies in the theoretical modelling of such processes or failures of the devices possibly due to adversarial attacks limit the reliability of random number generators in ways that are difficult to control and detect here inspired by earlier work on non locality and device quantum information processing we show that the non local correlations of entangled quantum particles can be used to certify the presence of genuine randomness it is thereby possible to design a cryptographically secure random number generator that does not require any assumption about the internal working of the device such a strong form of randomness generation is impossible classically and possible in quantum systems only if certified by a bell inequality we carry out a proof of concept demonstration of this proposal in a system of two entangled atoms separated by approximately one metre the observed bell inequality violation featuring near perfect detection efficiency guarantees that new random numbers are generated with per cent confidence our results lay the groundwork for future device independent quantum information experiments and for addressing fundamental issues raised by the intrinsic randomness of theory
the dirac successfully merges quantum mechanics with special relativity it provides a natural description of the electron spin predicts the existence of and is able to reproduce accurately the spectrum of the hydrogen atom the realm of the dirac equationrelativistic quantum mechanicsis considered to be the natural transition to quantum field theory however the dirac equation also predicts some peculiar effects such as kleins and zitterbewegung an unexpected quivering motion of a free relativistic quantum these and other predicted phenomena are key fundamental examples for understanding relativistic quantum effects but are difficult to observe in real particles in recent years there has been increased interest in simulations of relativistic quantum effects using different physical set in which parameter tunability allows access to different physical regimes here we perform a proof of principle quantum simulation of the one dimensional dirac equation using a single trapped set to behave as a free relativistic quantum particle we measure the particle position as a function of time and study zitterbewegung for different initial superpositions of positive and negative energy spinor states as well as the crossover from relativistic to non relativistic dynamics the high level of control of trapped ion experimental parameters makes it possible to simulate textbook examples of relativistic physics
motivation the sequencing of the human genome has made it possible to identify an informative set of million single nucleotide polymorphisms snps across the genome that can be used to carry out genome wide association studies gwass the availability of massive amounts of gwas data has necessitated the development of new biostatistical methods for quality control imputation and analysis issues including multiple testing this work has been successful and has enabled the discovery of new associations that have been replicated in multiple studies however it is now recognized that most snps discovered via gwas have small effects on disease susceptibility and thus may not be suitable for improving health care through genetic testing one likely explanation for the mixed results of gwas is that the current biostatistical analysis paradigm is by design agnostic or unbiased in that it ignores all prior knowledge about disease pathobiology further the linear modeling framework that is employed in gwas often considers only one snp at a time thus ignoring their genomic and environmental context there is now a shift away from the biostatistical approach toward a more holistic approach that recognizes the complexity of the genotype phenotype relationship that is characterized by significant heterogeneity and gene gene and gene environment interaction we argue here that bioinformatics has an important role to play in addressing the complexity of the underlying genetic basis of common human diseases the goal of this review is to identify and discuss those gwas challenges that will require methods
cells respond to their environment by sensing signals and translating them into changes in gene expression in recent years synthetic networks have been designed in both prokaryotic and eukaryotic systems to create new functionalities and for specific applications in this review we discuss the challenges associated with engineering signal transduction pathways furthermore we address advantages and disadvantages of engineering signaling pathways in prokaryotic and eukaryotic cells highlighting recent examples and discuss how progress in synthetic biology might impact biotechnology biomedicine
an important aspect of the functional annotation of enzymes is not only the type of reaction catalysed by an enzyme but also the substrate specificity which can vary widely within the same family in many cases prediction of family membership and even substrate specificity is possible from enzyme sequence alone using a nearest neighbour classification rule however the combination of structural information and sequence information can improve the interpretability and accuracy of predictive models the method presented here active site classification asc automatically extracts the residues lining the active site from one representative three dimensional structure and the corresponding residues from sequences of other members of the family from a set of representatives with known substrate specificity a support vector machine svm can then learn a model of substrate specificity applied to a sequence of unknown specificity the svm can then predict the most likely substrate the models can also be analysed to reveal the underlying structural reasons determining substrate specificities and thus yield valuable insights into mechanisms of enzyme specificity we illustrate the high prediction accuracy achieved on two benchmark data sets and the structural insights gained from asc by a detailed analysis of the family of decarboxylating dehydrogenases the asc web service is available at http asc informatik uni de
perturbation experiments in which a certain gene is knocked out and the expression levels of other genes are observed constitute a fundamental step in uncovering the intricate wiring diagrams in the living cell and elucidating the causal roles of genes in signaling and regulation here we present a novel framework for analyzing large cohorts of gene knockout experiments and their genome wide effects on expression levels we devise clustering like algorithms that identify groups of genes that behave similarly with respect to the knockout data and utilize them to predict knockout effects and to annotate physical interactions between proteins as inhibiting or activating differing from previous approaches our prediction approach does not depend on physical network information the latter is used only for the annotation task consequently it is both more efficient and of wider applicability than previous methods we evaluate our approach using a large scale collection of gene knockout experiments in yeast comparing it to the state of the art spine algorithm in cross validation tests our algorithm exhibits superior prediction accuracy while at the same time increasing the coverage by over fold significant coverage gains are obtained also in the annotation of the network
micrornas mirnas control gene expression in animals and plants like another class of small rnas sirnas they affect gene expression posttranscriptionally while sirnas in addition act in transcriptional gene silencing a role of mirnas in transcriptional regulation has been less clear we show here that in moss physcomitrella patens mutants without a dicer gene maturation of mirnas is normal but cleavage of target rnas is abolished and levels of these transcripts are drastically reduced these mutants accumulate mirna target rna duplexes and show hypermethylation of the genes encoding target rnas leading to gene silencing this pathway occurs also in the wild type upon hormone treatment we propose that initiation of epigenetic silencing by dna methylation depends on the ratio of the mirna and its target rna the moss dicer protein is required for mirna biogenesis the related protein is required for target cleavage but not mirna biogenesis in mutants genes encoding mirna targets are silenced by dna methylation this epigenetic gene silencing is initiated by high mirna to target ratios
gene duplications and their subsequent divergence play an important part in the evolution of novel gene functions several models for the emergence maintenance and evolution of gene copies have been proposed however a clear consensus on how gene duplications are fixed and maintained in genomes is lacking here we present a comprehensive classification of the models that are relevant to all stages of the evolution of gene duplications each model predicts a unique combination of evolutionary dynamics and functional properties setting out these predictions is an important step towards identifying the main mechanisms that are involved in the evolution of duplications
deep sequencing of transcriptome rna seq provides unprecedented opportunity to interrogate plausible mrna splicing patterns by mapping rna seq reads to exon junctions thereafter junction reads in most previous studies exon junctions were detected by using the quantitative information of junction reads the quantitative criterion e g minimum of two junction reads although is straightforward and widely used usually results in high false positive and false negative rates owning to the complexity of transcriptome here we introduced a new metric namely minimal match on either side of exon junction mmes to measure the quality of each junction read and subsequently implemented an empirical statistical model to detect exon junctions when applied to a large dataset reads consisting of mouse brain liver and muscle mrna sequences and using independent transcripts databases as positive control our method was proved to be considerably more accurate than previous ones especially for detecting junctions originated from low abundance transcripts our results were also confirmed by real time rt pcr assay the mmes metric can be used either in this empirical statistical model or in other more sophisticated classifiers such as regression
the oral microbiomeorganisms residing in the oral cavity and their collective genomeare critical components of health and disease the fungal component of the oral microbiota has not been characterized in this study we used a novel multitag pyrosequencing approach to characterize fungi present in the oral cavity of healthy individuals using the pan fungal internal transcribed spacer its primers our results revealed the basal oral mycobiome profile of the enrolled individuals and showed that across all the samples studied the oral cavity contained culturable and non culturable fungal genera among these genera were present in only one person genera were present in two participants and genera were present in three people while genera including non culturable organisms were present in participants candida species were the most frequent isolated from of participants followed by cladosporium aureobasidium saccharomycetales for both aspergillus fusarium and cryptococcus four of these predominant genera are known to be pathogenic in humans the low abundance genera may represent environmental fungi present in the oral cavity and could simply be spores inhaled from the air or material ingested with food among the culturable genera were represented by one species each while genera comprised between and different species the total number of species identified were the number of species in the oral cavity of each individual ranged between and principal component pco analysis of the obtained data set followed by sample clustering and unifrac analysis revealed that white males and asian males clustered differently from each other whereas both asian and white females clustered together this is the first study that identified the basal mycobiome of healthy individuals and provides the basis for a detailed characterization of the oral mycobiome in health disease
biological function of proteins is frequently associated with the formation of complexes with small molecule ligands experimental structure determination of such complexes at atomic resolution however can be time consuming and costly computational methods for structure prediction of protein ligand complexes particularly docking are as yet restricted by their limited consideration of receptor flexibility rendering them not applicable for predicting protein ligand complexes if large conformational changes of the receptor upon ligand binding are involved accurate receptor models in the ligand bound state holo structures however are a prerequisite for successful structure based drug design hence if only an unbound apo structure is available distinct from the ligand bound conformation structure based drug design is severely limited we present a method to predict the structure of protein ligand complexes based solely on the apo structure the ligand and the radius of gyration of the holo structure the method is applied to ten cases in which proteins undergo structural rearrangements of up to backbone rmsd upon ligand binding in all cases receptor models within backbone rmsd to the target were predicted and close to native ligand binding poses were obtained for of cases in the top ranked complex models a protocol is presented that is expected to enable structure modeling of protein ligand complexes and structure based drug design for cases where crystal structures of ligand bound conformations are available
articles whose authors make them open access oa by self archiving them online are cited significantly more than articles accessible only to subscribers some have suggested that this oa advantage may not be causal but just a self selection bias because authors preferentially make higher quality articles oa to test this we compared self selective self archiving with mandatory self archiving for a sample of articles published in journals the oa advantage proved just as high for both logistic regression showed that the advantage is independent of other correlates of citations article age journal impact factor number of co authors references or pages field article type country or institution and greatest for the most highly cited articles the oa advantage is real independent and causal but skewed its size is indeed correlated with quality just as citations themselves are the top of articles receive about of all citations the advantage is greater for the more citeable articles not because of a quality bias from authors self selecting what to make oa but because of a quality advantage from users self selecting what to use and cite freed by oa from the constraints of selective accessibility to subscribers only see accompanying rtf file for responses to feedback four pdf files provide analysis
microbes rely on diverse defense mechanisms that allow them to withstand viral predation and exposure to invading nucleic acid in many bacteria and most archaea clustered regularly interspaced short palindromic repeats crispr form peculiar genetic loci which provide acquired immunity against viruses and plasmids by targeting nucleic acid in a sequence specific manner these hypervariable loci take up genetic material from invasive elements and build up inheritable dna encoded immunity over time conversely viruses have devised mutational escape strategies that allow them to circumvent the crispr cas system albeit at a cost crispr features may be exploited for typing purposes epidemiological studies host virus ecological surveys building specific immunity against undesirable genetic elements and enhancing viral resistance in domesticated science
background a key challenge in systems biology is the reconstruction of an organism s metabolic network from its genome sequence one strategy for addressing this problem is to predict which metabolic pathways from a reference database of known pathways are present in the organism based on the annotated genome of the organism results to quantitatively validate methods for pathway prediction we developed a large gold standard dataset of pathway instances known to be present or absent in curated metabolic pathway databases for six organisms we defined a collection of pathway features whose information content we evaluated with respect to the gold standard feature data were used as input to an extensive collection of machine learning ml methods including naive bayes decision trees and logistic regression together with feature selection and ensemble methods we compared the ml methods to the previous pathologic algorithm for pathway prediction using the gold standard dataset we found that ml based prediction methods can match the performance of the pathologic algorithm pathologic achieved an accuracy of and an f measure of the ml based prediction methods achieved accuracy as high as and f measure as high as the ml based methods output a probability for each predicted pathway whereas pathologic does not which provides more information to the user and facilitates filtering of predicted pathways conclusions ml methods for pathway prediction perform as well as existing methods and have qualitative advantages in terms of extensibility tunability and explainability more advanced prediction methods and or more sophisticated input features may improve the performance of ml methods however pathway prediction performance appears to be limited largely by the ability to correctly match enzymes to the reactions they catalyze based on annotations
advances in genome analysis network biology and computational chemistry have the potential to revolutionize drug discovery by combining system level identification of drug targets with the atomistic modeling of small molecules capable of modulating their activity to demonstrate the effectiveness of such a discovery pipeline we deduced common antibiotic targets in escherichia coli and staphylococcus aureus by identifying shared tissue specific or uniformly essential metabolic reactions in their metabolic networks we then predicted through virtual screening dozens of potential inhibitors for several enzymes of these reactions and showed experimentally that a subset of these inhibited both enzyme activities in vitro and bacterial cell viability this blueprint is applicable for any sequenced organism with high quality metabolic reconstruction and suggests a general strategy for strain specific therapy
when embryonic stem cells escs differentiate they must both silence the esc self renewal program and activate new tissue specific programs in the absence of a protein required for microrna mirna biogenesis mouse escs are unable to silence self renewal here we show that the introduction of let mirnasa family of mirnas highly expressed in somatic cellscan suppress self renewal in but not wild type escs introduction of esc cell cycle regulating escc mirnas into the escs blocks the capacity of let to suppress self renewal profiling and bioinformatic analyses show that let inhibits whereas escc mirnas indirectly activate numerous self renewal genes furthermore inhibition of the let family promotes de differentiation of somatic cells to induced pluripotent stem cells together these findings show how the escc and let mirnas act through common pathways to alternatively stabilize the self renewing versus differentiated fates
the advent of high throughput sequencing hts technologies is enabling sequencing of human genomes at a significantly lower cost the availability of these genomes is hoped to enable novel medical diagnostics and treatment specific to the individual thus launching the era of personalized medicine the data currently generated by hts machines require extensive computational analysis in order to identify genomic variants present in the sequenced individual in this paper we overview hts technologies and discuss several of the plethora of algorithms and tools designed to analyze hts data including algorithms for read mapping as well as methods for identification of single nucleotide polymorphisms insertions deletions and large scale structural variants and copy number variants from mappings
coupling chromatin immunoprecipitation chip with recently developed massively parallel sequencing technologies has enabled genome wide detection of protein dna interactions with unprecedented sensitivity and specificity this new technology chip seq presents opportunities for in depth analysis of transcription regulation in this study we explore the value of using chip seq data to better detect and refine transcription factor binding sites tfbs we introduce a novel computational algorithm named hybrid motif sampler hms specifically designed for tfbs motif discovery in chip seq data we propose a bayesian model that incorporates sequencing depth information to aid motif identification our model also allows intra motif dependency to describe more accurately the underlying motif pattern our algorithm combines stochastic sampling and deterministic greedy search steps into a novel hybrid iterative scheme this combination accelerates the computation process simulation studies demonstrate favorable performance of hms compared to other existing methods when applying hms to real chip seq datasets we find that i the accuracy of existing tfbs motif patterns can be significantly improved and ii there is significant intra motif dependency inside all the tfbs motifs we tested modeling these dependencies further improves the accuracy of these tfbs motif patterns these findings may offer new biological insights into the mechanisms of transcription regulation
genome wide association studies gwas have rapidly become a standard method for disease gene discovery a substantial number of recent gwas indicate that for most disorders only a few common variants are implicated and the associated snps explain only a small fraction of the genetic risk this review is written from the viewpoint that findings from the gwas provide preliminary genetic information that is available for additional analysis by statistical procedures that accumulate evidence and that these secondary analyses are very likely to provide valuable information that will help prioritize the strongest constellations of results we review and discuss three analytic methods to combine preliminary gwas statistics to identify genes alleles and pathways for deeper investigations meta analysis seeks to pool information from multiple gwas to increase the chances of finding true positives among the false positives and providesa way to combine associations across gwas even when the original data are unavailable testing for epistasis within a single gwas study can identify the stronger results that are revealed when genes interact pathway analysis of gwas results is used to prioritize genes and pathways within a biological context followinga gwas association results can be assigned to pathways and tested in aggregate with computational tools and pathway databases reviews of published methods with recommendations for their application are provided within the framework for approach
we have developed netpath as a resource of curated human signaling pathways as an initial step netpath provides detailed maps of a number of immune signaling pathways which include approximately reactions annotated from the literature and more than instances of transcriptionally regulated genes all linked to over published articles we anticipate netpath to become a consolidated resource for human signaling pathways that should enable systems approaches
gr population genetics has evolved from a theory driven field with little empirical data into a data driven discipline in which genome scale data sets test the limits of available models and computational analysis methods in humans and a few model organisms analyses of whole genome sequence polymorphism data are currently under way and in light of the falling costs of next generation sequencing technologies such studies will soon become common in many other organisms as well here we assess the challenges to analyzing whole genome sequence polymorphism data and we discuss the potential of these data to yield new insights concerning population history and the genomic prevalence of selection
background many bioinformatics analyses ranging from gene clustering to phylogenetics produce hierarchical trees as their main result these are used to represent the relationships among different biological entities thus facilitating their analysis and interpretation a number of standalone programs are available that focus on tree visualization or that perform specific analyses on them however such applications are rarely suitable for large scale surveys in which a higher level of automation is required currently many genome wide analyses rely on tree like data representation and hence there is a growing need for scalable tools to handle tree structures at large scale results here we present the environment for tree exploration ete a python programming toolkit that assists in the automated manipulation analysis and visualization of hierarchical trees ete libraries provide a broad set of tree handling options as well as specific methods to analyze phylogenetic and clustering trees among other features ete allows for the independent analysis of tree partitions has support for the extended newick format provides an integrated node annotation system and permits to link trees to external data such as multiple sequence alignments or numerical arrays in addition ete implements a number of built in analytical tools including phylogeny based orthology prediction and cluster validation techniques finally ete s programmable tree drawing engine can be used to automate the graphical rendering of trees with customized node specific visualizations conclusions ete provides a complete set of methods to manipulate tree data structures that extends current functionality in other bioinformatic toolkits of a more general purpose ete is free software and can be downloaded from http ete org
book search gbs initiative once promised to test the bounds of fair use as the company started scanning millions of in copyright books from the collections of major research libraries the initial goal of this scanning was to make indexes of the books contents and to provide short snippets of book contents in response to pertinent search queries the authors guild and five trade publishers sued google in the fall of charging that this scanning activity was copyright infringement google defended by claiming fair use rather than litigating this important issue however the parties devised a radical plan to restructure the market for digital books which was announced on october by means of a class action settlement of the lawsuits approval of this settlement would give google and google alone a license to commercialize all out of print books and to make up to per cent of their contents available in response to search queries unless rights holders expressly forbade this this article discusses the glowingly optimistic predictions about the future of books in cyberspace promulgated by proponents of the gbs settlement and contrasts them with six categories of serious reservations that have emerged about the settlement these more pessimistic views of gbs are reflected in the hundreds objections and numerous amicus curiae briefs filed with the court responsible for determining whether to approve the settlement gbs poses risks for publishers academic authors and libraries professional writers and readers as well as for competition and innovation in several markets and for the cultural ecology of knowledge serious concerns have also been expressed about the gbs settlement as an abuse of the class action process because it usurps legislative prerogatives the article considers what might happen to the future of books in cyberspace if the gbs deal is not approved and recommends that regardless of whether the gbs settlement is approved a consortium of research libraries ought to develop a digital database of books from their collections that would enhance access to books without posing the many risks to the public interest that the gbs deal created
background high throughput sequencing technologies such as the illumina genome analyzer are powerful new tools for investigating a wide range of biological and medical questions statistical and computational methods are key for drawing meaningful and accurate conclusions from the massive and complex datasets generated by the sequencers we provide a detailed evaluation of statistical methods for normalization and differential expression de analysis of illumina transcriptome sequencing mrna seq data results we compare statistical methods for detecting genes that are significantly de between two types of biological samples and find that there are substantial differences in how the test statistics handle low count genes we evaluate how de results are affected by features of the sequencing platform such as varying gene lengths base calling calibration method with and without phi x control lane and flow cell library preparation effects we investigate the impact of the read count normalization method on de results and show that the standard approach of scaling by total lane counts e g rpkm can bias estimates of de we propose more general quantile based normalization procedures and demonstrate an improvement in de detection conclusions our results have significant practical and methodological implications for the design and analysis of mrna seq experiments they highlight the importance of appropriate statistical methods for normalization and de inference to account for features of the sequencing platform that could impact the accuracy of results they also reveal the need for further research in the development of statistical and computational methods for seq
although mutation provides the fuel for phenotypic evolution it also imposes a substantial burden on fitness through the production of predominantly deleterious alleles a matter of concern from a human health perspective here recently established databases on de novo mutations for monogenic disorders are used to estimate the rate and molecular spectrum of spontaneously arising mutations and to derive a number of inferences with respect to eukaryotic genome evolution although the human per generation mutation rate is exceptionally high on a per cell division basis the human germline mutation rate is lower than that recorded for any other species comparison with data from other species demonstrates a universal mutational bias toward a t composition and leads to the hypothesis that genome wide nucleotide composition generally evolves to the point at which the power of selection in favor of g c is approximately balanced by the power of random genetic drift such that variation in equilibrium genome wide nucleotide composition is largely defined by variation in mutation biases quantification of the hazards associated with introns reveals that mutations at key splice site residues are a major source of human mortality finally a consideration of the long term consequences of current human behavior for deleterious mutation accumulation leads to the conclusion that a substantial reduction in human fitness can be expected over the next few centuries in industrialized societies unless novel means of genetic intervention developed
soybean glycine max is one of the most important crop plants for seed protein and oil content and for its capacity to fix atmospheric nitrogen through symbioses with soil borne microorganisms we sequenced the gigabase genome by a whole genome shotgun approach and integrated it with physical and high density genetic maps to create a chromosome scale draft sequence assembly we predict protein coding genes more than arabidopsis and similar to the poplar genome which like soybean is an ancient polyploid palaeopolyploid about of the predicted genes occur in chromosome ends which comprise less than one half of the genome but account for nearly all of the genetic recombination genome duplications occurred at approximately and million years ago resulting in a highly duplicated genome with nearly of the genes present in multiple copies the two duplication events were followed by gene diversification and loss and numerous chromosome rearrangements an accurate soybean genome sequence will facilitate the identification of the genetic basis of many soybean traits and accelerate the creation of improved varieties
summary bamview is an interactive java application for visualising the large amounts of data stored for sequence reads which are aligned against a reference genome sequence it supports the bam binary alignment map format it can be used in a number of contexts including snp calling and structural annotation bamview has also been integrated into artemis so that the reads can be viewed in the context of the nucleotide sequence and genomic features availability bamview and artemis are freely available under a gpl licence for download for macosx unix and windows at http bamview sourceforge net contact artemis sanger uk
the human y chromosome began to evolve from an autosome hundreds of millions of years ago acquiring a sex determining function and undergoing a series of inversions that suppressed crossing over with the x little is known about the recent evolution of the y chromosome because only the human y chromosome has been fully sequenced prevailing theories hold that y chromosomes evolve by gene loss the pace of which slows over time eventually leading to a paucity of genes and these theories have been buttressed by partial sequence data from newly emergent plant and animal y but they have not been tested in older highly evolved y chromosomes such as that of humans here we finished sequencing of the male specific region of the y chromosome msy in our closest living relative the chimpanzee achieving levels of accuracy and completion previously reached for the human msy by comparing the msys of the two species we show that they differ radically in sequence structure and gene content indicating rapid evolution during the past years the chimpanzee msy contains twice as many massive palindromes as the human msy yet it has lost large fractions of the msy protein coding genes and gene families present in the last common ancestor we suggest that the extraordinary divergence of the chimpanzee and human msys was driven by four synergistic factors the prominent role of the msy in sperm production genetic hitchhiking effects in the absence of meiotic crossing over frequent ectopic recombination within the msy and species differences in mating behaviour although genetic decay may be the principal dynamic in the evolution of newly emergent y chromosomes wholesale renovation is the paramount theme in the continuing evolution of chimpanzee human and perhaps other msys
background micrornas mirnas are endogenous small rnas that play a key role in post transcriptional regulation of gene expression in animals and plants the number of known mirnas has increased rapidly over the years the current release version of mirbase the central online repository for mirna annotation comprises over mirna precursors from different species furthermore a large number of decentralized online resources are now available each contributing with important mirna annotation and information results we have developed a software framework designated here as mirmaid with the goal of integrating mirna data resources in a uniform web service interface that can be accessed and queried by researchers and most importantly by computers mirmaid is built around data from mirbase and is designed to follow the official mirbase data releases it exposes mirbase data as inter connected web services third party mirna data resources can be modularly integrated as mirmaid plugins or they can loosely couple with mirmaid as individual entities in the world wide web mirmaid is available as a public web service but is also easily installed as a local application the software framework is freely available under the lgpl open source license for academic and commercial use conclusion mirmaid is an intuitive and modular software platform designed to unify mirbase and independent mirna data resources it enables mirna researchers to computationally address complex questions involving the multitude of mirna data resources furthermore mirmaid constitutes a basic framework for further programming in which microrna interested bioinformaticians can readily develop their own tools and sources
a fundamental challenge in human health is the identification of disease causing genes recently several studies have tackled this challenge via a network based approach motivated by the observation that genes causing the same or similar diseases tend to lie close to one another in a network of protein protein or functional interactions however most of these approaches use only local network information in the inference process and are restricted to inferring single gene associations here we provide a global network based method for prioritizing disease genes and inferring protein complex associations which we call prince the method is based on formulating constraints on the prioritization function that relate to its smoothness over the network and usage of prior information we exploit this function to predict not only genes but also protein complex associations with a disease of interest we test our method on gene disease association data evaluating both the prioritization achieved and the protein complexes inferred we show that our method outperforms extant approaches in both tasks using data on diseases from the omim knowledgebase our method is able in a cross validation setting to rank the true causal gene first for of the diseases and infer disease related complexes that are highly coherent in terms of the function expression and conservation of their member proteins importantly we apply our method to study three multi factorial diseases for which some causal genes have been found already prostate cancer alzheimer and type diabetes mellitus prince s predictions for these diseases highly match the known literature suggesting several novel causal genes and protein complexes for investigation
emerging evidence indicates that gene products implicated in human cancers often cluster together in hot spots in protein protein interaction ppi networks additionally small sub networks within ppi networks that demonstrate synergistic differential expression with respect to tumorigenic phenotypes were recently shown to be more accurate classifiers of disease progression when compared to single targets identified by traditional approaches however many of these studies rely exclusively on mrna expression data a useful but limited measure of cellular activity proteomic profiling experiments provide information at the post translational level yet they generally screen only a limited fraction of the proteome here we demonstrate that integration of these complementary data sources with a proteomics first approach can enhance the discovery of candidate sub networks in cancer that are well suited for mechanistic validation in disease we propose that small changes in the mrna expression of multiple genes in the neighborhood of a protein hub can be synergistically associated with significant changes in the activity of that protein and its network neighbors further we hypothesize that proteomic targets with significant fold change between phenotype and control may be used to seed a search for small ppi sub networks that are functionally associated with these targets to test this hypothesis we select proteomic targets having significant expression changes in human colorectal cancer crc from two independent d gel based screens then we use random walk based models of network crosstalk and develop novel reference models to identify sub networks that are statistically significant in terms of their functional association with these proteomic targets subsequently using an information theoretic measure we evaluate synergistic changes in the activity of identified sub networks based on genome wide screens of mrna expression in crc cross classification experiments to predict disease class show excellent performance using only a few sub networks underwriting the strength of the proposed approach in discovering relevant and reproducible networks
the human genome contains hundreds of regions whose patterns of genetic variation indicate recent positive natural selection yet for most the underlying gene and the advantageous mutation remain unknown we developed a method composite of multiple signals cms that combines tests for multiple signals of selection and increases resolution by up to fold by applying cms to candidate regions from the international haplotype map we localized population specific selective signals to kilobases median identifying known and novel causal variants cms can not just identify individual loci but implicates precise variants selected evolution
researchers from diverse backgrounds are converging on the view that human evolution has been shaped by geneculture interactions theoretical biologists have used population genetic models to demonstrate that cultural processes can have a profound effect on human evolution and anthropologists are investigating cultural practices that modify current selection these findings are supported by recent analyses of human genetic variation which reveal that hundreds of genes have been subject to recent positive selection often in response to human activities here we collate these data highlighting the considerable potential for cross disciplinary exchange to provide novel insights into how culture has shaped the genome
the molecular mechanisms underlying major phenotypic changes that have evolved repeatedly in nature are generally unknown pelvic loss in different natural populations of threespine stickleback fish has occurred through regulatory mutations deleting a tissue specific enhancer of the pituitary homeobox transcription factor gene the high prevalence of deletion mutations at may be influenced by inherent structural features of the locus although null mutations are lethal in laboratory animals regulatory mutations show molecular signatures of positive selection in pelvic reduced populations these studies illustrate how major expression and morphological changes can arise from single mutational leaps in natural populations producing new adaptive alleles via recurrent regulatory alterations in a key developmental control science
context the prevalence of obesity increased in the united states between and and again between and objective to examine trends in obesity from through and the current prevalence of obesity and overweight for design setting and participants analysis of height and weight measurements from adult men and women aged years or older obtained in as part of the national health and nutrition examination survey nhanes a nationally representative sample of the us population data from the nhanes obtained in were compared with results obtained from through main outcome measure estimates of the prevalence of overweight and obesity in adults overweight was defined as a body mass index bmi of to obesity was defined as a bmi of or higher results in the age adjusted prevalence of obesity was confidence interval ci overall ci among men and ci among women the corresponding prevalence estimates for overweight and obesity combined bmi ge were ci ci and ci obesity prevalence varied by age group and by racial and ethnic group for both men and women over the year period obesity showed no significant trend among women adjusted odds ratio aor for vs ci for men there was a significant linear trend aor for vs ci however the most recent data points did not differ significantly from each other conclusions in the prevalence of obesity was among adult men and among adult women the increases in the prevalence of obesity previously observed do not appear to be continuing at the same rate over the past years particularly for women and possibly men
background high throughput automated sequencing has enabled an exponential growth rate of sequencing data this requires increasing sequence quality and reliability in order to avoid database contamination with artefactual sequences the arrival of pyrosequencing enhances this problem and necessitates customisable pre processing algorithms results seqtrim has been implemented both as a web and as a standalone command line application already published and newly designed algorithms have been included to identify sequence inserts to remove low quality vector adaptor low complexity and contaminant sequences and to detect chimeric reads the availability of several input and output formats allows its inclusion in sequence processing workflows due to its specific algorithms seqtrim outperforms other pre processors implemented as web services or standalone applications it performs equally well with sequences from est libraries ssh libraries genomic dna libraries and pyrosequencing reads and does not lead to over trimming conclusions seqtrim is an efficient pipeline designed for pre processing of any type of sequence read including next generation sequencing it is easily configurable and provides a friendly interface that allows users to know what happened with sequences at every pre processing stage and to verify pre processing of an individual sequence if desired the recommended pipeline reveals more information about each sequence than previously described pre processors and can discard more sequencing or artefacts
we demonstrate subassembly an in vitro library construction method that extends the utility of short read sequencing platforms to applications requiring long accurate reads a long dna fragment library is converted to a population of nested sublibraries and a tag sequence directs grouping of short reads derived from the same long fragment enabling localized assembly of long fragment sequences subassembly may facilitate accurate de novo genome assembly and sequencing
in recent years micrornas have been shown to play important roles in physiological as well as malignant processes the phenomir database http mips helmholtz muenchen de phenomir provides data from studies that investigate deregulation of microrna expression in diseases and biological processes as a systematic manually curated resource using the phenomir dataset we could demonstrate that depending on disease type independent information from cell culture studies contrasts with conclusions drawn from studies
the engineering of genetic circuits with predictive functionality in living cells represents a defining focus of the expanding field of synthetic biology this focus was elegantly set in motion a decade ago with the design and construction of a genetic toggle switch and an oscillator with subsequent highlights that have included circuits capable of pattern generation noise shaping edge detection and event counting here we describe an engineered gene network with global intercellular coupling that is capable of generating synchronized oscillations in a growing population of cells using microfluidic devices tailored for cellular populations at differing length scales we investigate the collective synchronization properties along with spatiotemporal waves occurring at millimetre scales we use computational modelling to describe quantitatively the observed dependence of the period and amplitude of the bulk oscillations on the flow rate the synchronized genetic clock sets the stage for the use of microbes in the creation of a macroscopic biosensor with an oscillatory output furthermore it provides a specific model system for the generation of a mechanistic description of emergent coordinated behaviour at the level
robustness seems to be the opposite of evolvability if phenotypes are robust against mutation we might expect that a population will have difficulty adapting to an environmental change as several studies have however other studies contend that robust organisms are more a quantitative understanding of the relationship between robustness and evolvability will help resolve these conflicting reports and will clarify outstanding problems in molecular and experimental evolution evolutionary developmental biology and protein engineering here we demonstrate using a general population genetics model that mutational robustness can either impede or facilitate adaptation depending on the population size the mutation rate and the structure of the fitness landscape in particular neutral diversity in a robust population can accelerate adaptation as long as the number of phenotypes accessible to an individual by mutation is smaller than the total number of phenotypes in the fitness landscape these results provide a quantitative resolution to a significant ambiguity in theory
a free energy principle has been proposed recently that accounts for action perception and learning this review looks at some key brain theories in the biological for example neural darwinism and physical for example information theory and optimal control theory sciences from the free energy perspective crucially one key theme runs through each of these theories optimization furthermore if we look closely at what is optimized the same quantity keeps emerging namely value expected reward expected utility or its complement surprise prediction error expected cost this is the quantity that is optimized under the free energy principle which suggests that several global brain theories might be unified within a free framework
motivation many programs for aligning short sequencing reads to a reference genome have been developed in the last years most of them are very efficient for short reads but inefficient or not applicable for reads bp because the algorithms are heavily and specifically tuned for short queries with low sequencing error rate however some sequencing platforms already produce longer reads and others are expected to become available soon for longer reads hashing based software such as blat and remain the only choices nonetheless these methods are substantially slower than short read aligners in terms of aligned bases per unit time results we designed and implemented a new algorithm burrows wheeler aligner s smith waterman alignment bwa sw to align long sequences up to mb against a large sequence database e g the human genome with a few gigabytes of memory the algorithm is as accurate as more accurate than blat and is several to tens of times faster than both availability http bio bwa net
grid cells in the entorhinal cortex of freely moving rats provide a strikingly periodic representation of self which is indicative of very specific computational however the existence of grid cells in humans and their distribution throughout the brain are unknown here we show that the preferred firing directions of directionally modulated grid cells in rat entorhinal cortex are aligned with the grids and that the spatial organization of grid cell firing is more strongly apparent at faster than slower running speeds because the grids are also aligned with each we predicted a macroscopic signal visible to functional magnetic resonance imaging fmri in humans we then looked for this signal as participants explored a virtual reality environment mimicking the rats foraging task fmri activation and adaptation showing a speed modulated six fold rotational symmetry in running direction the signal was found in a network of entorhinal subicular posterior and medial parietal lateral temporal and medial prefrontal areas the effect was strongest in right entorhinal cortex and the coherence of the directional signal across entorhinal cortex correlated with spatial memory performance our study illustrates the potential power of combining single unit electrophysiology with fmri in systems neuroscience our results provide evidence for grid cell like representations in humans and implicate a specific type of neural representation in a network of regions which supports spatial cognition and also memory
network reconstructions are a common denominator in systems biology bottomup metabolic network reconstructions have been developed over the last years these reconstructions represent structured knowledge bases that abstract pertinent information on the biochemical transformations taking place within specific target organisms the conversion of a reconstruction into a mathematical format facilitates a myriad of computational biological studies including evaluation of network content hypothesis testing and generation analysis of phenotypic characteristics and metabolic engineering to date genome scale metabolic reconstructions for more than organisms have been published and this number is expected to increase rapidly however these reconstructions differ in quality and coverage that may minimize their predictive potential and use as knowledge bases here we present a comprehensive protocol describing each step necessary to build a high quality genome scale metabolic reconstruction as well as the common trials and tribulations therefore this protocol provides a helpful manual for all stages of the process
a genome scale genetic interaction map was constructed by examining million gene gene pairs for synthetic genetic interactions generating quantitative genetic interaction profiles for approximately of all genes in the budding yeast saccharomyces cerevisiae a network based on genetic interaction profiles reveals a functional map of the cell in which genes of similar biological processes cluster together in coherent subsets and highly correlated profiles delineate specific pathways to define gene function the global network identifies functional cross connections between all bioprocesses mapping a cellular wiring diagram of pleiotropy genetic interaction degree correlated with a number of different gene attributes which may be informative about genetic network hubs in other organisms we also demonstrate that extensive and unbiased mapping of the genetic landscape provides a key for interpretation of chemical genetic interactions and drug identification
summary pegas population and evolutionary genetics analysis system is a new package for the analysis of population genetic data it is written in r and is integrated with two other existing r packages ape and adegenet pegas provides functions for standard population genetic methods as well as low level functions for developing new methods the flexible and efficient graphical capabilities of r are used for plotting haplotype networks as well as for other functionalities pegas emphasizes the need to further develop an integrated modular approach for software dedicated to the analysis of population genetic data availability pegas is distributed through the comprehensive r archive network cran http cran r project org web packages pegas index html further information may be found at http ape mpl ird pegas
transport networks are ubiquitous in both social and biological systems robust network performance involves a complex trade off involving cost transport efficiency and fault tolerance biological networks have been honed by many cycles of evolutionary selection pressure and are likely to yield reasonable solutions to such combinatorial optimization problems furthermore they develop without centralized control and may represent a readily scalable solution for growing networks in general we show that the slime mold physarum polycephalum forms networks with comparable efficiency fault tolerance and cost to those of real world infrastructure networks in this case the tokyo rail system the core mechanisms needed for adaptive network formation can be captured in a biologically inspired mathematical model that may be useful to guide network construction in other science
current methods for differentiating isolates of predominant lineages of pathogenic bacteria often do not provide sufficient resolution to define precise relationships here we describe a high throughput genomics approach that provides a high resolution view of the epidemiology and microevolution of a dominant strain of methicillin resistant staphylococcus aureus mrsa this approach reveals the global geographic structure within the lineage its intercontinental transmission through four decades and the potential to trace person to person transmission within a hospital environment the ability to interrogate and resolve bacterial populations is applicable to a range of infectious diseases as well as ecology
next generation sequencing technologies are revolutionizing genomics research it is now possible to generate gigabase pairs of dna sequence within a week without time consuming cloning or massive infrastructure this technology has recently been applied to the development of rna seq techniques for sequencing cdna from various organisms with the goal of characterizing entire transcriptomes these methods provide unprecedented resolution and depth of data enabling simultaneous quantification of gene expression discovery of novel transcripts and exons and measurement of splicing efficiency we present here a validated protocol for nonstrand specific transcriptome sequencing via rna seq describing the library preparation process and outlining the bioinformatic analysis procedure while sample preparation and sequencing take a fairly short period of time weeks the downstream analysis is by far the most challenging and time consuming aspect and can take weeks to months depending on the objectives
the clustering of transcription factor binding sites in developmental enhancers and the apparent preferential conservation of clustered sites have been widely interpreted as proof that spatially constrained physical interactions between transcription factors are required for regulatory function however we show here that selection on the composition of enhancers alone and not their internal structure leads to the accumulation of clustered sites with evolutionary dynamics that suggest they are preferentially conserved we simulated the evolution of idealized enhancers from drosophila melanogaster constrained to contain only a minimum number of binding sites for one or more factors under this constraint mutations that destroy an existing binding site are tolerated only if a compensating site has emerged elsewhere in the enhancer overlapping sites such as those frequently observed for the activator bicoid and repressor krppel had significantly longer evolutionary half lives than isolated sites for the same factors this leads to a substantially higher density of overlapping sites than expected by chance and the appearance that such sites are preferentially conserved because d melanogaster like many other species has a bias for deletions over insertions sites tended to become closer together over time leading to an overall clustering of sites in the absence of any selection for clustered sites since this effect is strongest for the oldest sites clustered sites also incorrectly appear to be preferentially conserved following speciation sites tend to be closer together in all descendent species than in their common ancestors violating the common assumption that shared features of species genomes reflect their ancestral state finally we show that selection on binding site composition alone recapitulates the observed number of overlapping and closely neighboring sites in real d melanogaster enhancers thus this study calls into question the common practice of inferring cis regulatory grammars from the organization and evolutionary dynamics of enhancers
background the clustered heat map is the most popular means of visualizing genomic data it compactly displays a large amount of data in an intuitive format that facilitates the detection of hidden structures and relations in the data however it is hampered by its use of cluster analysis which does not always respect the intrinsic relations in the data often requiring non standardized reordering of rows columns to be performed post clustering this sometimes leads to uninformative and or misleading conclusions often it is more informative to use dimension reduction algorithms such as principal component analysis and multi dimensional scaling which respect the topology inherent in the data yet despite their proven utility in the analysis of biological data they are not as widely used this is at least partially due to the lack of user friendly visualization methods with the visceral impact of the heat map results neatmap is an r package designed to meet this need neatmap offers a variety of novel plots in and dimensions to be used in conjunction with these dimension reduction techniques like the heat map but unlike traditional displays of such results it allows the entire dataset to be displayed while visualizing relations between elements it also allows superimposition of cluster analysis results for mutual validation neatmap is shown to be more informative than the traditional heat map with the help of two well known microarray datasets conclusions neatmap thus preserves many of the strengths of the clustered heat map while addressing some of its deficiencies it is hoped that neatmap will spur the adoption of non clustering dimension algorithms
we present an integrated method called chromia for the genome wide identification of functional target loci of transcription factors designed to capture the characteristic patterns of transcription factor binding motif occurrences and the histone profiles associated with regulatory elements such as promoters and enhancers chromia significantly outperforms other methods in the identification of transcription factor binding sites in mouse embryonic stem cells evaluated by both binding chip seq and functional rna interference experiments
a model for epidemic spreading on rewiring networks is introduced and analyzed for the case of scale free steady state networks it is found that contrary to what one would have naively expected the rewiring process typically tends to suppress epidemic spreading in particular it is found that as in static networks under a mean field approximation rewiring networks with degree distribution exponent exhibit a threshold in the infection rate below which epidemics die out in the steady state however the threshold is higher in the rewiring case for no such threshold exists but for small infection rate the steady state density of infected nodes prevalence is smaller for networks
we investigated the collaborative writing actions carried out by open university of israel graduate students as they built a wiki glossary of key course concepts these actions were analysed using a taxonomy of collaborative writing actions i e adding editing and deleting information in order to find out what students do and what they do not do when writing collaboratively two main findings were reported in accord with previous research students most frequently add content to a wiki rather than delete existing text and contrary to previous research students modify existing texts to a greater extent than previously reported these findings may help teachers design collaborative learning activities teachers should be aware of the difficulties faced by students when writing collaboratively and should design collaborative learning activities in ways that overcome or circumvent these difficulties contains tables figures
microbial minimal generation times range from a few minutes to several weeks they are evolutionarily determined by variables such as environment stability nutrient availability and community diversity selection for fast growth adaptively imprints genomes resulting in gene amplification adapted chromosomal organization and biased codon usage we found that these growth related traits in species of bacteria and archaea are highly correlated suggesting they all result from growth optimization while modeling their association with maximal growth rates in view of synthetic biology applications we observed that codon usage biases are better correlates of growth rates than any other trait including rrna copy number systematic deviations to our model reveal two distinct evolutionary processes first genome organization shows more evolutionary inertia than growth rates this results in over representation of growth related traits in fast degrading genomes second selection for these traits depends on optimal growth temperature for similar generation times purifying selection is stronger in psychrophiles intermediate in mesophiles and lower in thermophiles using this information we created a predictor of maximal growth rate adapted to small genome fragments we applied it to three metagenomic environmental samples to show that a transiently rich environment as the human gut selects for fast growers that a toxic environment as the acid mine biofilm selects for low growth rates whereas a diverse environment like the soil shows all ranges of growth rates we also demonstrate that microbial colonizers of babies gut grow faster than stabilized human adults gut communities in conclusion we show that one can predict maximal growth rates from sequence data alone and we propose that such information can be used to facilitate the manipulation of generation times our predictor allows inferring growth rates in the vast majority of uncultivable prokaryotes and paves the way to the understanding of community dynamics from data
genome wide association studies gwas have now identified at least common variants that appear associated with common diseases or related traits http www genome gov gwastudies hundreds of which have been convincingly replicated it is generally thought that the associated markers reflect the effect of a nearby common minor allele frequency causal site which is associated with the marker leading to extensive resequencing efforts to find causal sites we propose as an alternative explanation that variants much less common than the associated one may create synthetic associations by occurring stochastically more often in association with one of the alleles at the common site versus the other allele although synthetic associations are an obvious theoretical possibility they have never been systematically explored as a possible explanation for gwas findings here we use simple computer simulations to show the conditions under which such synthetic associations will arise and how they may be recognized we show that they are not only possible but inevitable and that under simple but reasonable genetic models they are likely to account for or contribute to many of the recently identified signals reported in genome wide association studies we also illustrate the behavior of synthetic associations in real datasets by showing that rare causal mutations responsible for both hearing loss and sickle cell anemia create genome wide significant synthetic associations in the latter case extending over a mb interval encompassing scores of blocks of associated variants in conclusion uncommon or rare genetic variants can easily create synthetic associations that are credited to common variants and this possibility requires careful consideration in the interpretation and follow up of signals
abridged the year wmap data and improved astrophysical data rigorously tests the standard cosmological model and its extensions by combining wmap with the latest distance measurements from the baryon acoustic oscillations bao and the hubble constant measurement we determine the parameters of the simplest lcdm model the power law index of the primordial power spectrum is a measurement that excludes the scale invariant spectrum by more than sigma the other parameters including those beyond the minimal set are also improved from the year results notable examples of improved parameters are the total mass of neutrinos sum textless and the effective number of neutrino species which benefit from better determinations of the third peak and we detect the effect of primordial helium on the temperature power spectrum and provide a new test of big bang nucleosynthesis we detect and show on the map for the first time the tangential and radial polarization patterns around hot and cold spots of temperature fluctuations an important test of physical processes at z and the dominance of adiabatic scalar fluctuations with the year tb power spectrum the limit on a rotation of the polarization plane due to potential parity violating effects has improved to delta alpha stat syst degrees we report a significant detection of the sz effect at the locations of known clusters and show that the current simulations and analytical calculations overestimate the gas pressure and do not reproduce the observed gas pressure in clusters of galaxies this result is consistent with the lower than expected sz power spectrum recently measured by the collaboration
background with the continued development of new computational tools for multiple sequence alignment it is necessary today to develop benchmarks that aid the selection of the most effective tools simulation based benchmarks have been proposed to meet this necessity especially for non coding sequences however it is not clear if such benchmarks truly represent real sequence data from any given group of species in terms of the difficulty of alignment tasks results we find that the conventional simulation approach which relies on empirically estimated values for various parameters such as substitution rate or insertion deletion rates is unable to generate synthetic sequences reflecting the broad genomic variation in conservation levels we tackle this problem with a new method for simulating non coding sequence evolution by relying on genome wide distributions of evolutionary parameters rather than their averages we then generate synthetic data sets to mimic orthologous sequences from the drosophila group of species and show that these data sets truly represent the variability observed in genomic data in terms of the difficulty of the alignment task this allows us to make significant progress towards estimating the alignment accuracy of current tools in an absolute sense going beyond only a relative assessment of different tools we evaluate six widely used multiple alignment tools in the context of drosophila non coding sequences and find the accuracy to be significantly different from previously reported values interestingly the performance of most tools degrades more rapidly when there are more insertions than deletions in the data set suggesting an asymmetric handling of insertions and deletions even though none of the evaluated tools explicitly distinguishes these two types of events we also examine the accuracy of two existing tools for annotating insertions versus deletions and find their performance to be close to optimal in drosophila non coding sequences if provided with the true alignments conclusion we have developed a method to generate benchmarks for multiple alignments of drosophila non coding sequences and shown it to be more realistic than traditional benchmarks apart from helping to select the most effective tools these benchmarks will help practitioners of comparative genomics deal with the effects of alignment errors by providing accurate estimates of the extent of errors
background the analysis and usage of biological data is hindered by the spread of information across multiple repositories and the difficulties posed by different nomenclature systems and storage formats in particular there is an important need for data unification in the study and use of protein protein interactions without good integration strategies it is difficult to analyze the whole set of available data and its properties results we introduce biana biologic interactions and network analysis a tool for biological information integration and network management biana is a python framework designed to achieve two major goals i the integration of multiple sources of biological information including biological entities and their relationships and ii the management of biological information as a network where entities are nodes and relationships are edges moreover biana uses properties of proteins and genes to infer latent biomolecular relationships by transferring edges to entities sharing similar properties biana is also provided as a plugin for cytoscape which allows users to visualize and interactively manage the data a web interface to biana providing basic functionalities is also available the software can be downloaded under gnu gpl license from http sbi imim es web biana php conclusions biana s approach to data unification solves many of the nomenclature issues common to systems dealing with biological data biana can easily be extended to handle new specific data repositories and new specific data types the unification protocol allows biana to be a flexible tool suitable for different user requirements non expert users can use a suggested unification protocol while expert users can define their own specific rules
humans have engaged in endurance running for millions of but the modern running shoe was not invented until the for most of human evolutionary history runners were either barefoot or wore minimal footwear such as sandals or moccasins with smaller heels and little cushioning relative to modern running shoes we wondered how runners coped with the impact caused by the foot colliding with the ground before the invention of the modern shoe here we show that habitually barefoot endurance runners often land on the fore foot fore foot strike before bringing down the heel but they sometimes land with a flat foot mid foot strike or less often on the heel rear foot strike in contrast habitually shod runners mostly rear foot strike facilitated by the elevated and cushioned heel of the modern running shoe kinematic and kinetic analyses show that even on hard surfaces barefoot runners who fore foot strike generate smaller collision forces than shod rear foot strikers this difference results primarily from a more plantarflexed foot at landing and more ankle compliance during impact decreasing the effective mass of the body that collides with the ground fore foot and mid foot strike gaits were probably more common when humans ran barefoot or in minimal shoes and may protect the feet and lower limbs from some of the impact related injuries now experienced by a high percentage runners
cellular differentiation and lineage commitment are considered to be robust and irreversible processes during development recent work has shown that mouse and human fibroblasts can be reprogrammed to a pluripotent state with a combination of four transcription factors this raised the question of whether transcription factors could directly induce other defined somatic cell fates and not only an undifferentiated state we hypothesized that combinatorial expression of neural lineage specific transcription factors could directly convert fibroblasts into neurons starting from a pool of nineteen candidate genes we identified a combination of only three factors also called and that suffice to rapidly and efficiently convert mouse embryonic and postnatal fibroblasts into functional neurons in vitro these induced neuronal in cells express multiple neuron specific proteins generate action potentials and form functional synapses generation of in cells from non neural lineages could have important implications for studies of neural development neurological disease modelling and medicine
during the last quarter of the twentieth century our knowledge about human genetic variation was limited mainly to the heterochromatin polymorphisms large enough to be visible in the light microscope and the single nucleotide polymorphisms snps identified by traditional pcr based dna sequencing in the past five years the rapid development and expanded use of microarray technologies including oligonucleotide array comparative genomic hybridization and snp genotyping arrays as well as next generation sequencing with paired end methods has enabled a whole genome analysis with essentially unlimited resolution the discovery of submicroscopic copy number variations cnvs present in our genomes has changed dramatically our perspective on dna structural variation and disease it is now thought that cnvs encompass more total nucleotides and arise more frequently than snps cnvs to a larger extent than snps have been shown to be responsible for human evolution genetic diversity between individuals and a rapidly increasing number of traits or susceptibility to traits such conditions have been referred to as genomic disorders in addition to well known sporadic chromosomal microdeletion syndromes and mendelian diseases many common complex traits including autism and schizophrenia can result from cnvs both recombination and replication based mechanisms for cnv formation have described
background advances in bioinformatic techniques and analyses have led to the availability of genome scale metabolic reconstructions the size and complexity of such networks often means that their potential behaviour can only be analysed with constraint based methods whilst requiring minimal experimental data such methods are unable to give insight into cellular substrate concentrations instead the long term goal of systems biology is to use kinetic modelling to characterize fully the mechanics of each enzymatic reaction and to combine such knowledge to predict system behaviour results we describe a method for building a parameterized genome scale kinetic model of a metabolic network simplified linlog kinetics are used and the parameters are extracted from a kinetic model repository we demonstrate our methodology by applying it to yeast metabolism the resultant model has metabolic reactions involving metabolites and whilst approximative has considerably broader remit than any existing models of its type control analysis is used to identify key steps within the system conclusions our modelling framework may be considered a stepping stone toward the long term goal of a fully parameterized model of yeast metabolism the model is available in sbml format from the biomodels database biomodels id and at http www mcisb org genomescale
the hypothesis that folding robustness is the primary determinant of the evolution rate of proteins is explored using a coarse grained off lattice model the simplicity of the model allows rapid computation of the folding probability of a sequence to any folded conformation for each robust folder the network of sequences that share its native structure is identified the fitness of a sequence is postulated to be a simple function of the number of misfolded molecules that have to be produced to reach a characteristic protein abundance after fixation probabilities of mutants are computed under a simple population dynamics model a markov chain on the fold network is constructed and the fold averaged evolution rate is computed the distribution of the logarithm of the evolution rates across distinct networks exhibits a peak with a long tail on the low rate side and resembles the universal empirical distribution of the evolutionary rates more closely than either distribution resembles the log normal distribution the results suggest that the universal distribution of the evolutionary rates of protein coding genes is a direct consequence of the basic physics of folding
we have not yet reached a point at which routine sequencing of large numbers of whole eukaryotic genomes is feasible and so it is often necessary to select genomic regions of interest and to enrich these regions before sequencing there are several enrichment approaches each with unique advantages and disadvantages here we describe our experiences with the leading target enrichment technologies the optimizations that we have performed and typical results that can be obtained using each we also provide detailed protocols for each technology so that end users can find the best compromise between sensitivity specificity and uniformity for their project
a major challenge in the analysis of gene expression microarray data is to extract meaningful biological knowledge out of the huge volume of raw data expander expression analyzer and displayer is an integrated software platform for the analysis of gene expression data which is freely available for academic use it is designed to support all the stages of microarray data analysis from raw data normalization to inference of transcriptional regulatory networks the microarray analysis described in this protocol starts with importing the data into expander and is followed by normalization and filtering then clustering and network based analyses are performed the gene groups identified are tested for enrichment in function based on gene ontology co regulation using transcription factor and microrna target predictions or co location the results of each analysis step can be visualized in a number of ways the complete protocol can be executed h
correlated spiking is often observed in cortical circuits but its functional role is controversial it is believed that correlations are a consequence of shared inputs between nearby neurons and could severely constrain information decoding here we show theoretically that recurrent neural networks can generate an asynchronous state characterized by arbitrarily low mean spiking correlations despite substantial amounts of shared input in this state spontaneous fluctuations in the activity of excitatory and inhibitory populations accurately track each other generating negative correlations in synaptic currents which cancel the effect of shared input near zero mean correlations were seen experimentally in recordings from rodent neocortex in vivo our results suggest a reexamination of the sources underlying observed correlations and their functional consequences for information science
motivation testing for correlations between different sets of genomic features is a fundamental task in genomics research however searching for overlaps between features with existing web based methods is complicated by the massive datasets that are routinely produced with current sequencing technologies fast and flexible tools are therefore required to ask complex questions of these data in an efficient manner results this article introduces a new software suite for the comparison manipulation and annotation of genomic features in browser extensible data bed and general feature format gff format bedtools also supports the comparison of sequence alignments in bam format to both bed and gff features the tools are extremely efficient and allow the user to compare large datasets e g next generation sequencing data with both public and custom genome annotation tracks bedtools can be combined with one another as well as with standard unix commands thus facilitating routine genomics tasks as well as pipelines that can quickly answer intricate questions of large genomic datasets availability and implementation bedtools was written in c source code and a comprehensive user manual are freely available at http code google com p bedtoolscontact aaronquinlan gmail com virginia edusupplementary information supplementary data are available at online
it is widely believed that the modular organization of cellular function is reflected in a modular structure of molecular networks a common view is that a module in a network is a cohesively linked group of nodes densely connected internally and sparsely interacting with the rest of the network many algorithms try to identify functional modules in protein interaction networks pin by searching for such cohesive groups of proteins here we present an alternative approach independent of any prior definition of what actually constitutes a module in a self consistent manner proteins are grouped into functional roles if they interact in similar ways with other proteins according to their functional roles such grouping may well result in cohesive modules again but only if the network structure actually supports this we applied our method to the pin from the human protein reference database hprd and found that a representation of the network in terms of cohesive modules at least on a global scale does not optimally represent the network s structure because it focuses on finding independent groups of proteins in contrast a decomposition into functional roles is able to depict the structure much better as it also takes into account the interdependencies between roles and even allows groupings based on the absence of interactions between proteins in the same functional role this for example is the case for transmembrane proteins which could never be recognized as a cohesive group of nodes in a pin when mapping experimental methods onto the groups we identified profound differences in the coverage suggesting that our method is able to capture experimental bias in the data too for example yeast two hybrid data were highly overrepresented in one particular group thus there is more structure in protein interaction networks than cohesive modules alone and we believe this finding can significantly improve automated function algorithms
a wide range of research areas in molecular biology and medical biochemistry require a reliable enzyme classification system e g drug design metabolic network reconstruction and system biology when research scientists in the above mentioned areas wish to unambiguously refer to an enzyme and its function the ec number introduced by the nomenclature committee of the international union of biochemistry and molecular biology iubmb is used however each and every one of these applications is critically dependent upon the consistency and reliability of the underlying data for success we have developed tools for the validation of the ec number classification scheme in this paper we present validated data of enzymatic reactions including sub subclasses of the ec classification system over agreement was found between our assignment and the ec classification for i e only reactions we found that their assignment was inconsistent with the rules of the nomenclature committee they have to be transferred to other sub subclasses we demonstrate that our validation results can be used to initiate corrections and improvements to the ec number scheme
new dna sequencing technologies have achieved breakthroughs in throughput at the expense of higher error rates the primary way of interpreting biological sequences is via alignment but standard alignment methods assume the sequences are accurate here we describe how to incorporate the per base error probabilities reported by sequencers into alignment unlike existing tools for dna read mapping our method models both sequencer errors and real sequence differences this approach consistently improves mapping accuracy even when the rate of real sequence difference is only furthermore when mapping drosophila melanogaster reads to the drosophila simulans genome it increased the amount of correctly mapped reads from to this approach enables more effective use of dna reads from organisms that lack reference genomes are extinct or are polymorphic
changes in gene expression have been proposed to underlie many or even most adaptive differences between species despite the increasing acceptance of this view only a handful of cases of adaptive gene expression evolution have been demonstrated to address this discrepancy we introduce a simple test for lineage specific selection on gene expression applying the test to genome wide gene expression data from the budding yeast saccharomyces cerevisiae we find that hundreds of gene expression levels have been subject to lineage specific selection comparing these findings with independent population genetic evidence of selective sweeps suggests that this lineage specific selection has resulted in recent sweeps at over a hundred genes most of which led to increased transcript levels examination of the implicated genes revealed a specific biochemical pathway ergosterol biosynthesis where the expression of multiple genes has been subject to selection for reduced levels in sum these results suggest that adaptive evolution of gene expression is common in yeast that regulatory adaptation can occur at the level of entire pathways and that similar genome wide scans may be possible in other species humans
summarybackgroundresults of intervention studies in patients with type diabetes have led to concerns about the safety of aiming for normal blood glucose concentrations we assessed survival as a function of in people with type diabetes methodstwo cohorts of patients aged years and older with type diabetes were generated from the uk general practice research database from november to november we identified patients whose treatment had been intensified from oral monotherapy to combination therapy with oral blood glucose lowering agents and who had changed to regimens that included insulin those with diabetes secondary to other causes were excluded all cause mortality was the primary outcome age sex smoking status cholesterol cardiovascular risk and general morbidity were identified as important confounding factors and cox survival models were adjusted for these factors accordingly findingsfor combined cohorts compared with the glycated haemoglobin decile with the lowest hazard median iqr the adjusted hazard ratio hr of all cause mortality in the lowest decile was ci and in the highest decile median iqr was ci results showed a general u shaped association with the lowest hr at an of about hr for all cause mortality in people given insulin based regimens deaths versus those given combination oral agents was ci interpretationlow and high mean values were associated with increased all cause mortality and cardiac events if confirmed diabetes guidelines might need revision to include a minimum value fundingeli lilly company
networks portray a multitude of interactions through which people meet ideas are spread and infectious diseases propagate within a society identifying the most efficient spreaders in a network is an important step to optimize the use of available resources and ensure the more efficient spread of information here we show that in contrast to common belief the most influential spreaders in a social network do not correspond to the best connected people or to the most central people high betweenness centrality instead we find i the most efficient spreaders are those located within the core of the network as identified by the k shell decomposition analysis ii when multiple spreaders are considered simultaneously the distance between them becomes the crucial parameter that determines the extend of the spreading furthermore we find that in the case of infections that do not confer immunity on recovered individuals the infection persists in the high k shell layers of the network under conditions where hubs may not be able to preserve the infection our analysis provides a plausible route for an optimal design of efficient strategies
the spectacular escalation in complexity in early bilaterian evolution correlates with a strong increase in the number of to explore the link between the birth of ancient micrornas and body plan evolution we set out to determine the ancient sites of activity of conserved bilaterian microrna families in a comparative approach we reason that any specific localization shared between protostomes and deuterostomes the two major superphyla of bilaterian animals should probably reflect an ancient specificity of that microrna in their last common ancestor here we investigate the expression of conserved bilaterian micrornas in platynereis dumerilii a protostome retaining ancestral bilaterian in capitella another marine annelid in the sea urchin strongylocentrotus a deuterostome and in sea anemone nematostella representing an outgroup to the bilaterians our comparative data indicate that the oldest known animal microrna mir and the related mir and let were initially active in neurosecretory cells located around the mouth other sets of ancient micrornas were first present in locomotor ciliated cells specific brain centres or more broadly one of four major organ systems central nervous system sensory tissue musculature and gut these findings reveal that microrna evolution and the establishment of tissue identities were closely coupled in bilaterian evolution also they outline a minimum set of cell types and tissues that existed in the ancestor
gr we have developed a novel approach for using massively parallel short read sequencing to generate fast and inexpensive de novo genomic assemblies comparable to those generated by capillary based methods the ultrashort base sequences generated by this technology pose specific biological and computational challenges for de novo assembly of large genomes to account for this we devised a method for experimentally partitioning the genome using reduced representation rr libraries prior to assembly we use two restriction enzymes independently to create a series of overlapping fragment libraries each containing a tractable subset of the genome together these libraries allow us to reassemble the entire genome without the need of a reference sequence as proof of concept we applied this approach to sequence and assembled the majority of the mb drosophila melanogaster genome we subsequently demonstrate the accuracy of our assembly method with meaningful comparisons against the current available d melanogaster reference genome the ease of assembly and accuracy for comparative genomics suggest that our approach will scale to future mammalian genome sequencing efforts saving both time and money without quality
the divergence accumulated during the evolution of protein families translates into their internal organization as subfamilies and it is directly reflected in the characteristic patterns of differentially conserved residues these specifically conserved positions in protein subfamilies are known as specificity determining positions sdps previous studies have limited their analysis to the study of the relationship between these positions and ligand binding specificity demonstrating significant yet limited predictive capacity we have systematically extended this observation to include the role of differential protein interactions in the segregation of protein subfamilies and explored in detail the structural distribution of sdps at protein interfaces our results show the extensive influence of protein interactions in the evolution of protein families and the widespread association of sdps with protein interfaces the combined analysis of sdps in interfaces and ligand binding sites provides a more complete picture of the organization of protein families constituting the necessary framework for a large scale analysis of the evolution of function
pnas histones are frequently decorated with covalent modifications these histone modifications are thought to be involved in various chromatin dependent processes including transcription to elucidate the relationship between histone modifications and transcription we derived quantitative models to predict the expression level of genes from histone modification levels we found that histone modification levels and gene expression are very well correlated moreover we show that only a small number of histone modifications are necessary to accurately predict gene expression we show that different sets of histone modifications are necessary to predict gene expression driven by high cpg content promoters hcps or low cpg content promoters lcps quantitative models involving and are the most predictive of the expression levels in lcps whereas hcps require and finally we show that the connections between histone modifications and gene expression seem to be general as we were able to predict gene expression levels of one cell type using a model trained on another er
motivation metagenomics is the study of genetic material recovered directly from environmental samples taxonomic and functional differences between metagenomic samples can highlight the influence of ecological factors on patterns of microbial life in a wide range of habitats statistical hypothesis tests can help us distinguish ecological influences from sampling artifacts but knowledge of only the p value from a statistical hypothesis test is insufficient to make inferences about biological relevance current reporting practices for pairwise comparative metagenomics are inadequate and better tools are needed for comparative metagenomic analysis results we have developed a new software package stamp for comparative metagenomics that supports best practices in analysis and reporting examination of a pair of iron mine metagenomes demonstrates that deeper biological insights can be gained using statistical techniques available in our software an analysis of the functional potential of candidatus accumulibacter phosphatis in two enhanced biological phosphorus removal metagenomes identified several subsystems that differ between the a phosphatis stains in these related communities including phosphate metabolism secretion and metal transport availability python source code and binaries are freely available from our website at http kiwi cs dal ca software stampcontact beiko cs dal casupplementary information supplementary data are available at online
background the differential diagnosis of disorders of consciousness is challenging the rate of misdiagnosis is approximately and new methods are required to complement bedside testing particularly if the patient s capacity to show behavioral signs of awareness is diminished methods at two major referral centers in cambridge united kingdom and liege belgium we performed a study involving patients with disorders of consciousness we used functional magnetic resonance imaging mri to assess each patient s ability to generate willful neuroanatomically specific blood oxygenation level dependent responses during two established mental imagery tasks a technique was then developed to determine whether such tasks could be used to communicate yes or no answers to simple questions results of the patients enrolled in the study were able to willfully modulate their brain activity in three of these patients additional bedside testing revealed some sign of awareness but in the other two patients no voluntary behavior could be detected by means of clinical assessment one patient was able to use our technique to answer yes or no to questions during functional mri however it remained impossible to establish any form of communication at the bedside conclusions these results show that a small proportion of patients in a vegetative or minimally conscious state have brain activation reflecting some awareness and cognition careful clinical examination will result in reclassification of the state of consciousness in some of these patients this technique may be useful in establishing basic communication with patients who appear to be unresponsive copyright massachusetts society
pnas fast identification of protein structures that are similar to a specified query structure in the entire protein data bank pdb is fundamental in structure and function prediction we present fragbag an ultrafast and accurate method for comparing protein structures we describe a protein structure by the collection of its overlapping short contiguous backbone segments and discretize this set using a library of fragments then we succinctly represent the protein as a bags of fragmentsa vector that counts the number of occurrences of each fragmentand measure the similarity between two structures by the similarity between their vectors our representation has two additional benefits it can be used to construct an inverted index for implementing a fast structural search engine of the entire pdb and one can specify a structure as a collection of substructures without combining them into a single structure this is valuable for structure prediction when there are reliable predictions only of parts of the protein we use receiver operating characteristic curve analysis to quantify the success of fragbag in identifying neighbor candidate sets in a dataset of over structures the gold standard is the set of neighbors found by six state of the art structural aligners our best fragbag library finds more accurate candidate sets than the three other filter methods the sgm pride and a method by zotenko et al more interestingly fragbag performs on a par with the computationally expensive yet highly trusted structural aligners structal ce
we present goseq an application for performing gene ontology go analysis on rna seq data go analysis is widely used to reduce complexity and highlight biological processes in genome wide expression studies but standard methods give biased results on rna seq data due to over detection of differential expression for long and highly expressed transcripts application of goseq to a prostate cancer data set shows that goseq dramatically changes the results highlighting categories more consistent with the biology
proteinprotein interactions are challenging targets for modulation by small molecules here we propose an approach that harnesses the increasing structural coverage of protein complexes to identify small molecules that may target protein interactions specifically we identify ligand and protein binding sites that overlap upon alignment of homologous proteins of the protein structure families observed to bind proteins also bind small molecules da and exhibit a statistically significant p overlap between ligand and protein binding positions these bi functional positions which bind both ligands and proteins are particularly enriched in tyrosine and tryptophan residues similar to energetic hotspots described previously and are significantly less conserved than mono functional and solvent exposed positions homology transfer identifies ligands whose binding sites overlap at least of the protein interface for of domaindomain and of domainpeptide mediated interactions the analysis recovered known small molecule modulators of protein interactions as well as predicted new interaction targets based on the sequence similarity of ligand binding sites we illustrate the predictive utility of the method by suggesting structural mechanisms for the effects of sanglifehrin a on hiv virion production bepridil on the cellular entry of anthrax edema factor and fusicoccin on vertebrate developmental pathways the results available at http pibase janelia org represent a comprehensive collection of structurally characterized modulators of protein interactions and suggest that homologous structures are a useful resource for the rational design of modulators
current work in elucidating relationships between diseases has largely been based on pre existing knowledge of disease genes consequently these studies are limited in their discovery of new and unknown disease relationships we present the first quantitative framework to compare and contrast diseases by an integrated analysis of disease related mrna expression data and the human protein interaction network we identified functional modules in the human protein network and provided a quantitative metric to record their responses in diseases leading to significant similarities between diseases fourteen of the significant disease correlations also shared common drugs supporting the hypothesis that similar diseases can be treated by the same drugs allowing us to make predictions for new uses of existing drugs finally we also identified modules that were dysregulated in at least half of the diseases representing a common disease state signature these modules were significantly enriched for genes that are known to be drug targets interestingly drugs known to target these genes proteins are already known to treat significantly more diseases than drugs targeting other genes proteins highlighting the importance of these core modules as prime opportunities
conventional protein structure determination from nuclear magnetic resonance data relies heavily on side chain proton to proton distances the necessary side chain resonance assignment however is labor intensive and prone to error here we show that structures can be accurately determined without nuclear magnetic resonance nmr information on the side chains for proteins up to kilodaltons by incorporating backbone chemical shifts residual dipolar couplings and amide proton distances into the rosetta protein structure modeling methodology these data which are too sparse for conventional methods serve only to guide conformational search toward the lowest energy conformations in the folding landscape the details of the computed models are determined by the physical chemistry implicit in the rosetta all atom energy function the new method is not hindered by the deuteration required to suppress nuclear relaxation processes for proteins greater than kilodaltons and should enable routine nmr structure determination for proteins
motivation short sequence motifs are an important class of models in molecular biology used most commonly for describing transcription factor binding site specificity patterns high throughput methods have been recently developed for detecting regulatory factor binding sites in vivo and in vitro and consequently high quality binding site motif data are becoming available for increasing number of organisms and regulatory factors development of intuitive tools for the study of sequence motifs is therefore important imotifs is a graphical motif analysis environment that allows visualisation of annotated sequence motifs and scored motif hits in sequences it also offers motif inference with the sensitive nested mica algorithm as well as overrepresentation and pairwise motif matching capabilities all of the analysis functionality is provided without the need to convert between file formats or learn different command line interfaces the application includes a bundled and graphically integrated version of the nestedmica motif inference suite that has no outside dependencies problems associated with local deployment of software are therefore avoided availability imotifs is licensed with the gnu lesser general public license lgpl the software and its source is available at http wiki github com imotifs and can be run on mac os x leopard intel powerpc we also provide a cross platform linux os x windows lgpl licensed library libxms for the perl ruby r and objective c programming languages for input output of xms formatted annotated sequence motif set files contact matias piipari matias piipari gmail com support queries should be directed to the imotifs mailing list imotifs com
dna methylation is a critical epigenetic regulator in mammalian development here we present a whole genome comparative view of dna methylation using bisulfite sequencing of three cultured cell types representing progressive stages of differentiation human embryonic stem cells hescs a fibroblastic differentiated derivative of the hescs and neonatal fibroblasts as a reference we compared our maps with a methylome map of a fully differentiated adult cell type mature peripheral blood mononuclear cells monocytes we observed many notable common and cell type specific features among all cell types promoter hypomethylation both cg and ca and higher levels of gene body methylation were positively correlated with transcription in all cell types exons were more highly methylated than introns and sharp transitions of methylation occurred at exonintron boundaries suggesting a role for differential methylation in transcript splicing developmental stage was reflected in both the level of global methylation and extent of non cpg methylation with hesc highest fibroblasts intermediate and monocytes lowest differentiation associated differential methylation profiles were observed for developmentally regulated genes including the hox clusters other homeobox transcription factors and pluripotence associated genes such as and our results highlight the value of high resolution methylation maps in conjunction with other systems level analyses for investigation of previously undetectable developmental mechanisms
among primates genome wide analysis of recent positive selection is currently limited to the human species because it requires extensive sampling of genotypic data from many individuals the extent to which genes positively selected in human also present adaptive changes in other primates therefore remains unknown this question is important because a gene that has been positively selected independently in the human and in other primate lineages may be less likely to be involved in human specific phenotypic changes such as dietary habits or cognitive abilities to answer this question we analysed heterozygous single nucleotide polymorphisms snps in the genomes of single human chimpanzee orangutan and macaque individuals using a new method aiming to identify selective sweeps genome wide we found an unexpectedly high number of orthologous genes exhibiting signatures of a selective sweep simultaneously in several primate species suggesting the presence of hotspots of positive selection a similar significant excess is evident when comparing genes positively selected during recent human evolution with genes subjected to positive selection in their coding sequence in other primate lineages and identified using a different test these findings are further supported by comparing several published human genome scans for positive selection with our findings in non human primate genomes we thus provide extensive evidence that the co occurrence of positive selection in humans and in other primates at the same genetic loci can be measured with only four species an indication that it may be a widespread phenomenon the identification of positive selection in humans alongside other primates is a powerful tool to outline those genes that were selected uniquely during recent evolution
accurate profiling of minute quantities of rna in a global manner can enable key advances in many scientific and clinical disciplines here we present low quantity rna sequencing lq rnaseq a high throughput sequencing based technique allowing whole transcriptome surveys from subnanogram rna quantities in an amplification ligation free manner lq rnaseq involves first strand cdna synthesis from rna templates followed by polya tailing of the single stranded cdna products and direct single molecule sequencing we applied lq rnaseq to profile s cerevisiae polya transcripts demonstrate the reproducibility of the approach across different sample preparations and independent instrument runs and establish the absolute quantitative power of this method through comparisons with other reported transcript profiling techniques and through utilization of rna spike in experiments we demonstrate the practical application of this approach to define the transcriptional landscape of mouse embryonic and induced pluripotent stem cells observing transcriptional differences including over genes exhibiting differential expression between these otherwise very similar stem cell populations this amplification independent technology which utilizes small quantities of nucleic acid and provides quantitative measurements of cellular transcripts enables global gene expression measurements from minute amounts of materials and offers broad utility in both basic research and translational biology for characterization of cells
background genome sequence alignments form the basis of much research genome alignment depends on various mundane but critical choices such as how to mask repeats and which score parameters to use surprisingly there has been no large scale assessment of these choices using real genomic data moreover rigorous procedures to control the rate of spurious alignment have not been employed results we have assessed combinations of score parameters for alignment of animal plant and fungal genomes as our gold standard of accuracy we used genome alignments implied by multiple alignments of proteins and of structural rnas we found the hoxd scoring schemes underlying alignments in the ucsc genome database to be far from optimal and suggest better parameters higher values of the x drop parameter are not always better e values accurately indicate the rate of spurious alignment but only if tandem repeats are masked in a non standard way finally we show that gamma centroid probabilistic alignment can find highly reliable subsets of aligned bases conclusions these results enable more accurate genome alignment with reliability measures for local alignments and for individual aligned bases this study was made possible by our new software last which can align vertebrate genomes in a few hours http last jp
the explosive growth in our knowledge of genomes proteomes and metabolomes is driving ever increasing fundamental understanding of the biochemistry of life enabling qualitatively new studies of complex biological systems and their evolution this knowledge also drives modern biotechnologies such as molecular engineering and synthetic biology which have enormous potential to address urgent problems including developing potent new drugs and providing environmentally friendly energy many of these studies however are ultimately limited by their need for even higher throughput measurements of biochemical reactions we present a general ultrahigh throughput screening platform using drop based microfluidics that overcomes these limitations and revolutionizes both the scale and speed of screening we use aqueous drops dispersed in oil as picoliter volume reaction vessels and screen them at rates of thousands per second to demonstrate its power we apply the system to directed evolution identifying new mutants of the enzyme horseradish peroxidase exhibiting catalytic rates more than times faster than their parent which is already a very efficient enzyme we exploit the ultrahigh throughput to use an initial purifying selection that removes inactive mutants we identify approximately variants comparable in activity to the parent from an initial population of approximately after a second generation of mutagenesis and high stringency screening we identify several significantly improved mutants some approaching diffusion limited efficiency in total we screen approximately individual enzyme reactions in only h using microl of total reagent volume compared to state of the art robotic screening systems we perform the entire assay with a fold increase in speed and a million fold reduction cost
background given the availability of full genome sequences mapping gene gains duplications and losses during evolution should theoretically be straightforward however this endeavor suffers from overemphasis on detecting conserved genome features which in turn has led to sequencing multiple eutherian genomes with low coverage rather than fewer genomes with high coverage and more even distribution in the phylogeny although limitations associated with analysis of low coverage genomes are recognized they have not been quantified results here using recently developed comparative genomic application systems we evaluate the impact of low coverage genomes on inferences pertaining to gene gains and losses when analyzing eukaryote genome evolution through gene duplication we demonstrate that when performing inference of genome content evolution low coverage genomes generate not only a massive number of false gene losses but also striking artifacts in gene duplication inference especially at the most recent common ancestor of low coverage genomes we show that the artifactual gains are caused by the low coverage of genome sequence per se rather than by the increased taxon sampling in a biased portion of the species tree conclusions we argue that it will remain difficult to differentiate artifacts from true changes in modes and tempo of genome evolution until there is better homogeneity in both taxon sampling and high coverage sequencing this is important for broadening the utility of full genome data to the community of evolutionary biologists whose interests go well beyond widely conserved physiologies and developmental patterns as they seek to understand the generative mechanisms underlying diversity
motivation several recent studies have demonstrated the effectiveness of resequencing and single nucleotide variant snv detection by deep short read sequencing platforms while several reliable algorithms are available for automated snv detection the automated detection of microindels in deep short read data presents a new bioinformatics challenge results we systematically analyzed how the short read mapping tools maq bowtie burrows wheeler alignment tool bwa novoalign and razers perform on simulated datasets that contain indels and evaluated how indels affect error rates in snv detection we implemented a simple algorithm to compute the equivalent indel region eir which can be used to process the alignments produced by the mapping tools in order to perform indel calling using simulated data that contains indels we demonstrate that indel detection works well on short read data the detection rate for microindels bp is our study provides insights into systematic errors in snv detection that is based on ungapped short sequence read alignments gapped alignments of short sequence reads can be used to reduce this error and to detect microindels in simulated short read data a comparison with microindels automatically identified on the abi sanger and roche platform indicates that microindel detection from short sequence reads identifies both overlapping and distinct indels contact peter krawitz googlemail com peter robinson charite desupplementary information supplementary data are available at online
we report here the genome sequence of an ancient human obtained from year old permafrost preserved hair the genome represents a male individual from the first known culture to settle in greenland sequenced to an average depth of we recover of the diploid genome an amount close to the practical limit of current sequencing technologies we identify high confidence single nucleotide polymorphisms snps of which have not been reported previously we estimate raw read contamination to be no higher than we use functional snp assessment to assign possible phenotypic characteristics of the individual that belonged to a culture whose location has yielded only trace human remains we compare the high confidence snps to those of contemporary populations to find the populations most closely related to the individual this provides evidence for a migration from siberia into the new world some years ago independent of that giving rise to the modern native americans inuit
motivation next generation sequencing captures sequence differences in reads relative to a reference genome or transcriptome including splicing events and complex variants involving multiple mismatches and long indels we present computational methods for fast detection of complex variants and splicing in short reads based on a successively constrained search process of merging and filtering position lists from a genomic index our methods are implemented in gsnap genomic short read nucleotide alignment program which can align both single and paired end reads as short as nt and of arbitrarily long length it can detect short and long distance splicing including interchromosomal splicing in individual reads using probabilistic models or a database of known splice sites our program also permits snp tolerant alignment to a reference space of all possible combinations of major and minor alleles and can align reads from bisulfite treated dna for the study of methylation state results in comparison testing gsnap has speeds comparable to existing programs especially in reads of or nt and is fastest in detecting complex variants with four or more mismatches or insertions of nt and deletions of nt although snp tolerance does not increase alignment yield substantially it affects alignment results in of transcriptional reads typically by revealing alternate genomic mappings for a read simulations of bisulfite converted dna show a decrease in identifying genomic positions uniquely in of nt reads and of nt reads availability source code in c and utility programs in perl are freely available for download as part of the gmap package at http share gene gmap
motivation transcription factors tfs are crucial during the lifetime of the cell their functional roles are defined by the genes they regulate uncovering these roles not only sheds light on the tf at hand but puts it into the context of the complete regulatory network results here we present an alignment and threshold free comparative genomics approach for assigning functional roles to dna regulatory motifs we incorporate our approach into the gomo algorithm a computational tool for detecting associations between a user specified dna regulatory motif expressed as a position weight matrix pwm and gene ontology go terms incorporating multiple species into the analysis significantly improves gomo s ability to identify go terms associated with the regulatory targets of tfs including three comparative species in the process of predicting tf roles in saccharomyces cerevisiae and homo sapiens increases the number of significant predictions by and respectively the predicted go terms are also more specific yielding deeper biological insight into the role of the tf adjusting motif binding affinity scores for individual sequence composition proves to be essential for avoiding false positive associations we describe a novel dna sequence scoring algorithm that compensates a thermodynamic measure of dna binding affinity for individual sequence base composition gomo s prediction accuracy proves to be relatively insensitive to how promoters are defined because gomo uses a threshold free form of gene set analysis there are no free parameters to tune biologists can investigate the potential roles of dna regulatory motifs of interest using gomo via the web http meme nbcr net contact t bailey uq edu ausupplementary information supplementary data are available at online
background the task of recognizing and identifying species names in biomedical literature has recently been regarded as critical for a number of applications in text and data mining including gene name recognition species specific document retrieval and semantic enrichment of biomedical articles results in this paper we describe an open source species name recognition and normalization software system linnaeus and evaluate its performance relative to several automatically generated biomedical corpora as well as a novel corpus of full text documents manually annotated for species mentions linnaeus uses a dictionary based approach implemented as an efficient deterministic finite state automaton to identify species names and a set of heuristics to resolve ambiguous mentions when compared against our manually annotated corpus linnaeus performs with recall and precision at the mention level and recall and precision at the document level our system successfully solves the problem of disambiguating uncertain species mentions with of all mentions in pubmed central full text documents resolved to unambiguous ncbi taxonomy identifiers conclusions linnaeus is an open source stand alone software system capable of recognizing and normalizing species name mentions with speed and accuracy and can therefore be integrated into a range of bioinformatics and text mining applications the software and manually annotated corpus can be downloaded freely at http linnaeus net
next generation sequencing technologies have made it possible to sequence targeted regions of the human genome in hundreds of individuals deep sequencing represents a powerful approach for the discovery of the complete spectrum of dna sequence variants in functionally important genomic intervals current methods for single nucleotide polymorphism snp detection are designed to detect snps from single individual sequence data sets here we describe a novel method snip seq single nucleotide polymorphism identification from population sequence data that leverages sequence data from a population of individuals to detect snps and assign genotypes to individuals to evaluate our method we utilized sequence data from a kilobase kb region on chromosome of the human genome this region was sequenced in individuals five sequenced in duplicate using the illumina ga platform using this data set we demonstrate that our method is highly accurate for detecting variants and can filter out false snps that are attributable to sequencing errors the concordance of sequencing based genotype assignments between duplicate samples was the kb region was independently sequenced to a high depth of coverage using two sequence pools containing the individuals many of the novel snps identified by snip seq from the individual sequencing were validated by the pooled sequencing data and were subsequently confirmed by sanger sequencing we estimate that snip seq achieves a low false positive rate of approximately improving upon the higher false positive rate for existing methods that do not utilize population sequence data collectively these results suggest that analysis of population sequencing data is a powerful approach for the accurate detection of snps and the assignment of genotypes to samples
the eb eye is a fast and efficient search engine that provides easy and uniform access to the biological data resources hosted at the embl ebi currently users can access information from more than distinct datasets covering some million entries the data resources represented in the eb eye include nucleotide and protein sequences at both the genomic and proteomic levels structures ranging from chemicals to macro molecular complexes gene expression experiments binary level molecular interactions as well as reaction maps and pathway models functional classifications biological ontologies and comprehensive literature libraries covering the biomedical sciences and related intellectual property the eb eye can be accessed over the web or programmatically using a soap web services interface this allows its search and retrieval capabilities to be exploited in workflows and analytical pipe lines the eb eye is a novel alternative to existing biological search and retrieval engines in this article we describe in detail how to exploit its powerful bib
alternative splicing of pre mrna is a prominent mechanism to generate protein diversity yet its regulation is poorly understood we demonstrated a direct role for histone modifications in alternative splicing we found distinctive histone modification signatures that correlate with the splicing outcome in a set of human genes and modulation of histone modifications causes splice site switching histone marks affect splicing outcome by influencing the recruitment of splicing regulators via a chromatin binding protein these results outline an adaptor system for the reading of histone marks by the pre mrna machinery
the in vivo genetically programmed incorporation of designer amino acids allows the properties of proteins to be tailored with molecular the methanococcus jannaschii tyrosyl transfer rna synthetasetrnacua mjtyrrstrnacua and the methanosarcina barkeri pyrrolysyl trna synthetasetrnacua mbpylrstrnacua orthogonal pairs have been evolved to incorporate a range of unnatural amino acids in response to the amber codon in escherichia however the potential of synthetic genetic code expansion is generally limited to the low efficiency incorporation of a single type of unnatural amino acid at a time because every triplet codon in the universal genetic code is used in encoding the synthesis of the proteome to encode efficiently many distinct unnatural amino acids into proteins we require blank codons and mutually orthogonal aminoacyl trna synthetasetrna pairs that recognize unnatural amino acids and decode the new codons here we synthetically evolve an orthogonal ribo that efficiently decodes a series of quadruplet codons and the amber codon providing several blank codons on an orthogonal messenger rna which it specifically by creating mutually orthogonal aminoacyl trna synthetasetrna pairs and combining them with ribo we direct the incorporation of distinct unnatural amino acids in response to two of the new blank codons on the orthogonal mrna using this code we genetically direct the formation of a specific redox insensitive nanoscale protein cross link by the bio orthogonal cycloaddition of encoded azide and alkyne containing amino because the synthetasetrna pairs used have been evolved to incorporate numerous unnatural amino it will be possible to encode more than unnatural amino acid combinations using this approach as ribo independently decodes a series of quadruplet codons this work provides foundational technologies for the encoded synthesis and synthetic evolution of unnatural polymers cells
motivation phybase is an r package for phylogenetic analysis using species trees it provides functions to read write manipulate simulate estimate summarize and plot species trees which contain not only the topology and branch lengths but also population sizes availability the phybase package is available at the r repository the manual and supporting materials including source code sample r code and sample data files for the species tree analysis are available at http stat osu edu liuliang research phybase html contact lliu desu bioinformatics
abstract cermit is a computationally efficient motif discovery tool based on analyzing genome wide quantitative regulatory evidence instead of pre selecting promising candidate sequences it utilizes information across all sequence regions to search for high scoring motifs we apply cermit on a range of direct binding and overexpression data sets it substantially outperforms state of the art approaches on curated chip chip datasets and easily scales to current mammalian chip seq experiments with data on thousands of non regions
we provide a full quantum mechanical analysis of a weak energy measurement of a driven mechanical resonator we demonstrate that measurements too weak to resolve individual mechanical fock states can nonetheless be used to unambiguously detect the non classical energy fluctuations of the driven mechanical resonator i e phonon shot noise we also show that the third moment of the oscillator s energy fluctuations provides a far more sensitive probe of quantum effects than the second moment and that measuring the third moment via the phase shift of light in an optomechanical setup directly yields the type of operator ordering postulated in the theory of full statistics
how do observers search through familiar scenes a novel panoramic search method is used to study the interaction of memory and vision in natural search behavior in panoramic search observers see part of an unchanging scene larger than their current field of view a target object can be visible present in the display but hidden from view or absent visual search efficiency does not change after hundreds of trials through an unchanging scene experiment memory search in contrast begins inefficiently but becomes efficient with practice given a choice between vision and memory observers choose vision experiments and however if forced to use their memory on some trials they learn to use memory on all trials even when reliable visual information remains available experiment the results suggest that observers make a pragmatic choice between vision and memory with a strong bias toward visual search even for stimuli
the best selling pulitzer prize winning classic hailed by the new york times book review as a masterwork the novel astonishes with its inventiveness it is nothing less than a grand comic fugue a confederacy of dunces is an american comic masterpiece john kennedy toole s hero one ignatius j reilly is huge obese fractious fastidious a latter day gargantua a don quixote of the french quarter his story bursts with wholly original characters denizens of new orleans lower depths incredibly true to life dialogue and the zaniest series of high and low comic adventures henry kisor chicago times
background the increasing availability and diversity of omics data in the post genomic era offers new perspectives in most areas of biomedical research graph based biological networks models capture the topology of the functional relationships between molecular entities such as gene protein and small compounds and provide a suitable framework for integrating and analyzing omics data the development of software tools capable of integrating data from different sources and to provide flexible methods to reconstruct represent and analyze topological networks is an active field of research in bioinformatics results bisogenet is a multi tier application for visualization and analysis of biomolecular relationships the system consists of three tiers in the data tier an in house database stores genomics information protein protein interactions protein dna interactions gene ontology and metabolic pathways in the middle tier a global network is created at server startup representing the whole data on bioentities and their relationships retrieved from the database the client tier is a cytoscape plugin which manages user input communication with the web service visualization and analysis of the resulting network conclusion bisogenet is able to build and visualize biological networks in a fast and user friendly manner a feature of bisogenet is the possibility to include coding relations to distinguish between genes and their products this feature could be instrumental to achieve a finer grain representation of the bioentities and their relationships the client application includes network analysis tools and interactive network expansion capabilities in addition an option is provided to allow other networks to be converted to bisogenet this feature facilitates the integration of our software with other tools available in the cytoscape platform bisogenet is available at http bio cigb edu cu cytoscape
the cancer genome is moulded by the dual processes of somatic mutation and selection homozygous deletions in cancer genomes occur over recessive cancer genes where they can confer selective growth advantage and over fragile sites where they are thought to reflect an increased local rate of dna breakage however most homozygous deletions in cancer genomes are unexplained here we identified somatic homozygous deletions in cancer cell lines these overlie of protein coding genes that therefore are not mandatory for survival of human cells we derived structural signatures that distinguish between homozygous deletions over recessive cancer genes and fragile sites application to clusters of unexplained homozygous deletions suggests that many are in regions of inherent fragility whereas a small subset overlies recessive cancer genes the results illustrate how structural signatures can be used to distinguish between the influences of mutation and selection in cancer genomes the extensive copy number genotyping sequence and expression data available for this large series of publicly available cancer cell lines renders them informative reagents for future studies of cancer biology and discovery
a powerful way to discover key genes with causal roles in oncogenesis is to identify genomic regions that undergo frequent alteration in human cancers here we present high resolution analyses of somatic copy number alterations scnas from cancer specimens belonging largely to histological types we identify regions of focal scna that are altered at significant frequency across several cancer types of which cannot be explained by the presence of a known cancer target gene located within these regions several gene families are enriched among these regions of focal scna including the family of apoptosis regulators and the nf pathway we show that cancer cells containing amplifications surrounding the and anti apoptotic genes depend on the expression of these genes for survival finally we demonstrate that a large majority of scnas identified in individual cancer types are present in several types
the genetic structure of the indigenous hunter gatherer peoples of southern africa the oldest known lineage of modern human is important for understanding human diversity studies based on and small sets of nuclear have shown that these hunter gatherers known as khoisan san or bushmen are genetically divergent from other however until now fully sequenced human genomes have been limited to recently diverged here we present the complete genome sequences of an indigenous hunter gatherer from the kalahari desert and a bantu from southern africa as well as protein coding regions from an additional three hunter gatherers from disparate regions of the kalahari we characterize the extent of whole genome and exome diversity among the five men reporting million novel dna differences genome wide including novel amino acid variants in terms of nucleotide substitutions the bushmen seem to be on average more different from each other than for example a european and an asian observed genomic differences between the hunter gatherers and others may help to pinpoint genetic adaptations to an agricultural lifestyle adding the described variants to current databases will facilitate inclusion of southern africans in medical research efforts particularly when family and medical histories can be correlated with genome wide doi
the phenotypic differences between individual organisms can often be ascribed to underlying genetic and environmental variation however even genetically identical organisms in homogeneous environments vary indicating that randomness in developmental processes such as gene expression may also generate diversity to examine the consequences of gene expression variability in multicellular organisms we studied intestinal specification in the nematode caenorhabditis elegans in which wild type cell fate is invariant and controlled by a small transcriptional network mutations in elements of this network can have indeterminate effects some mutant embryos fail to develop intestinal cells whereas others produce intestinal precursors by counting transcripts of the genes in this network in individual embryos we show that the expression of an otherwise redundant gene becomes highly variable in the mutants and that this variation is subjected to a threshold producing an on off expression pattern of the master regulatory gene of intestinal differentiation our results demonstrate that mutations in developmental networks can expose otherwise buffered stochastic variability in gene expression leading to pronounced variation
genome sequencing of helicobacter pylori has revealed the potential proteins and genetic diversity of this prevalent human pathogen yet little is known about its transcriptional organization and noncoding rna output massively parallel cdna sequencing rna seq has been revolutionizing global transcriptomic analysis here using a novel differential approach drna seq selective for the end of primary transcripts we present a genome wide map of h pylori transcriptional start sites and operons we discovered hundreds of transcriptional start sites within operons and opposite to annotated genes indicating that complexity of gene expression from the small h pylori genome is increased by uncoupling of polycistrons and by genome wide antisense transcription we also discovered an unexpected number of approximately small rnas including the epsilon subdivision counterpart of the regulatory rna and associated rna products and potential regulators of cis and trans encoded target messenger rnas our approach establishes a paradigm for mapping and annotating the primary transcriptomes of many species
local adaptations within species are often governed by several interacting genes scattered throughout the genome single locus models of selection cannot explain the maintenance of such complex variation because recombination separates co adapted alleles here we report a previously unrecognized type of intraspecific multi locus genetic variation that has been maintained over a vast period the galactose gal utilization gene network of saccharomyces kudriavzevii a relative of brewers yeast exists in two distinct states a functional gene network in portuguese strains and in japanese strains a non functional gene network of allelic pseudogenes genome sequencing of all available s kudriavzevii strains revealed that none of the functional gal genes were acquired from other species rather these polymorphisms have been maintained for nearly the entire history of the species despite more recent gene flow genome wide experimental evidence suggests that inactivation of the and regulatory genes facilitated the origin and long term maintenance of the two gene network states this striking example of a balanced unlinked gene network polymorphism introduces a remarkable type of intraspecific variation that may widespread
sequence directed genetic interference pathways control gene expression and preserve genome integrity in all kingdoms of life the importance of such pathways is highlighted by the extensive study of rna interference rnai and related processes in eukaryotes in many bacteria and most archaea clustered regularly interspaced short palindromic repeats crisprs are involved in a more recently discovered interference pathway that protects cells from bacteriophages and conjugative plasmids crispr sequences provide an adaptive heritable record of past infections and express crispr rnas small rnas that target invasive nucleic acids here we review the mechanisms of crispr interference and its roles in microbial physiology and evolution we also discuss potential applications of this novel pathway
a range of applications from predicting the spread of human and electronic viruses to city planning and resource management in mobile communications depend on our ability to foresee the whereabouts and mobility of individuals raising a fundamental question to what degree is human behavior predictable here we explore the limits of predictability in human dynamics by studying the mobility patterns of anonymized mobile phone users by measuring the entropy of each individuals trajectory we find a potential predictability in user mobility across the whole user base despite the significant differences in the travel patterns we find a remarkable lack of variability in predictability which is largely independent of the distance users cover on a basis
protein lysine acetylation has emerged as a key posttranslational modification in cellular regulation in particular through the modification of histones and nuclear transcription regulators we show that lysine acetylation is a prevalent modification in enzymes that catalyze intermediate metabolism virtually every enzyme in glycolysis gluconeogenesis the tricarboxylic acid tca cycle the urea cycle fatty acid metabolism and glycogen metabolism was found to be acetylated in human liver tissue the concentration of metabolic fuels such as glucose amino acids and fatty acids influenced the acetylation status of metabolic enzymes acetylation activated enoyl coenzyme a hydratase hydroxyacyl coenzyme a dehydrogenase in fatty acid oxidation and malate dehydrogenase in the tca cycle inhibited argininosuccinate lyase in the urea cycle and destabilized phosphoenolpyruvate carboxykinase in gluconeogenesis our study reveals that acetylation plays a major role in metabolic science
background it has been long well known that genes do not act alone rather groups of genes act in consort during a biological process consequently the expression levels of genes are dependent on each other experimental techniques to detect such interacting pairs of genes have been in place for quite some time with the advent of microarray technology newer computational techniques to detect such interaction or association between gene expressions are being proposed which lead to an association network while most microarray analyses look for genes that are differentially expressed it is of potentially greater significance to identify how entire association network structures change between two or more biological settings say normal versus diseased cell types results we provide a recipe for conducting a differential analysis of networks constructed from microarray data under two experimental settings at the core of our approach lies a connectivity score that represents the strength of genetic association or interaction between two genes we use this score to propose formal statistical tests for each of following queries i whether the overall modular structures of the two networks are different ii whether the connectivity of a particular set of interesting genes has changed between the two networks and iii whether the connectivity of a given single gene has changed between the two networks a number of examples of this score is provided we carried out our method on two types of simulated data gaussian networks and networks based on differential equations we show that for appropriate choices of the connectivity scores and tuning parameters our method works well on simulated data we also analyze a real data set involving normal versus heavy mice and identify an interesting set of genes that may play key roles in obesity conclusions examining changes in network structure can provide valuable information about the underlying biochemical pathways differential network analysis with appropriate connectivity scores is a useful tool in exploring changes in network structures under different biological conditions an r package of our tests can be downloaded from the supplementary website http www somnathdatta org dna
alternative splicing polyadenylation of pre messenger rna molecules and differential promoter usage can produce a variety of transcript isoforms whose respective expression levels are regulated in time and space thus contributing specific biological functions however the repertoire of mammalian alternative transcripts and their regulation are still poorly understood second generation sequencing is now opening unprecedented routes to address the analysis of entire transcriptomes here we developed methods that allow the prediction and quantification of alternative isoforms derived solely from exon expression levels in rna seq data these are based on an explicit statistical model and enable the prediction of alternative isoforms within or between conditions using any known gene annotation as well as the relative quantification of known transcript structures applying these methods to a human rna seq dataset we validated a significant fraction of the predictions by rt pcr data further showed that these predictions correlated well with information originating from junction reads a direct comparison with exon arrays indicated improved performances of rna seq over microarrays in the prediction of skipped exons altogether the set of methods presented here comprehensively addresses multiple aspects of alternative isoform analysis the software is available as an open source r package called solas at http cmb molgen mpg de nar
motivation an important class of protein interactions involves the binding of a protein s domain to a short linear motif slim on its interacting partner extracting such motifs either experimentally or computationally is challenging because of their weak binding and high degree of degeneracy recent rapid increase of available protein structures provides an excellent opportunity to study slims directly from their structures results using domain interface extraction diet we characterized distinct slims from the protein data bank pdb of which are validated in varying degrees have literature validation are supported by at least one domain peptide structural instance and another have overrepresentation in high throughput ppi data we further observed that the lacklustre coverage of existing computational slim detection methods could be due to the common assumption that most slims occur outside globular domain regions of slim that we reported are actually found on domain domain interface some of them are implicated in autoimmune and neurodegenerative diseases we suggest that these slims would be useful for designing inhibitors against the pathogenic protein complexes underlying these diseases our findings show that structure based slim detection algorithms can provide a more complete coverage of slim mediated protein interactions than current sequence based approaches contact ksung comp nus edu sg supplementary information supplementary data are available at bioinformatics bioinformatics
we have used multiplexed high throughput sequencing to characterize changes in small rna populations that occur during viral infection in animal cells small rna based mechanisms such as rna interference rnai have been shown in plant and invertebrate systems to play a key role in host responses to viral infection although homologs of the key rnai effector pathways are present in mammalian cells and can launch an rnai mediated degradation of experimentally targeted mrnas any role for such responses in mammalian host virus interactions remains to be characterized six different viruses were examined in experimentally susceptible and resistant host systems we identified virus derived small rnas vsrnas from all six viruses with total abundance varying from vanishingly rare less than of cellular small rna to highly abundant comparable to abundant micro rnas mirnas in addition to the appearance of vsrnas during infection we saw a number of specific changes in host mirna profiles for several infection models investigated in more detail the rnai and interferon pathways modulated the abundance of vsrnas we also found evidence for populations of vsrnas that exist as duplexed sirnas with zero to three nucleotide overhangs using populations of cells carrying a hepatitis c replicon we observed strand selective loading of sirnas onto argonaute complexes these experiments define vsrnas as one possible component of the interplay between animal viruses and hosts
the unprecedented ability of nanometallic that is plasmonic structures to concentrate light into deep subwavelength volumes has propelled their use in a vast array of nanophotonics technologies and research endeavours plasmonic light concentrators can elegantly interface diffraction limited dielectric optical components with nanophotonic structures passive and active plasmonic devices provide new pathways to generate guide modulate and detect light with structures that are similar in size to state of the art electronic devices with the ability to produce highly confined optical fields the conventional rules for lightmatter interactions need to be re examined and researchers are venturing into new regimes of optical physics in this review we will discuss the basic concepts behind plasmonics enabled light concentration and manipulation make an attempt to capture the wide range of activities and excitement in this area and speculate on possible directions
motivation a typical approach for the interpretation of high throughput experiments such as gene expression microarrays is to produce groups of genes based on certain criteria e g genes that are differentially expressed to gain more mechanistic insights into the underlying biology overrepresentation analysis ora is often conducted to investigate whether gene sets associated with particular biological functions for example as represented by gene ontology go annotations are statistically overrepresented in the identified gene groups however the standard ora which is based on the hypergeometric test analyzes each go term in isolation and does not take into account the dependence structure of the go term hierarchy results we have developed a bayesian approach go bayes to measure overrepresentation of go terms that incorporates the go dependence structure by taking into account evidence not only from individual go terms but also from their related terms i e parents children siblings etc the bayesian framework borrows information across related go terms to strengthen the detection of overrepresentation signals as a result this method tends to identify sets of closely related go terms rather than individual isolated go terms the advantage of the go bayes approach is demonstrated with a simulation study and an example
we present a modeling framework for dynamical and bursty contact networks made of agents in social interaction we consider agents behavior at short time scales in which the contact network is formed by disconnected cliques of different sizes at each time a random agent can make a transition from being isolated to being part of a group or vice versa different distributions of contact times and inter contact times between individuals are obtained by considering transition probabilities with memory effects i e the transition probabilities for each agent depend both on its state isolated or interacting and on the time elapsed since the last change of state the model lends itself to analytical and numerical investigations the modeling framework can be easily extended and paves the way for systematic investigations of dynamical processes occurring on rapidly evolving dynamical networks such as the propagation of an information or spreading diseases
over the last three decades promotion of information literacy has become one of the main goals of librarians and academics as the emergence of information technologies has raised new challenges and roles for users information literacy has shifted from the concept of simple training to the provision of the skills and competencies that are critical to the improved use of information a terminological conceptual and statistical analysis of the main subjects related to information literacy as well as its evolution over the last years is provided with the aim of illustrating how information literacy has been progressively incorporated into the library and fields
aphids are important agricultural pests and also biological models for studies of insect plant interactions symbiosis virus vectoring and the developmental causes of extreme phenotypic plasticity here we present the mb draft genome assembly of the pea aphid acyrthosiphon pisum this first published whole genome sequence of a basal hemimetabolous insect provides an outgroup to the multiple published genomes of holometabolous insects pea aphids are host plant specialists they can reproduce both sexually and asexually and they have coevolved with an obligate bacterial symbiont here we highlight findings from whole genome analysis that may be related to these unusual biological features these findings include discovery of extensive gene duplication in more than gene families as well as loss of evolutionarily conserved genes gene family expansions relative to other published genomes include genes involved in chromatin modification mirna synthesis and sugar transport gene losses include genes central to the imd immune pathway selenoprotein utilization purine salvage and the entire urea cycle the pea aphid genome reveals that only a limited number of genes have been acquired from bacteria thus the reduced gene count of buchnera does not reflect gene transfer to the host genome the inventory of metabolic genes in the pea aphid genome suggests that there is extensive metabolite exchange between the aphid and buchnera including sharing of amino acid biosynthesis between the aphid and buchnera the pea aphid genome provides a foundation for post genomic studies of fundamental biological questions and applied problems
biological signalling networks allow living organisms to issue an integrated response to current conditions and make limited predictions about future environmental changes small scale dynamic models of signalling cascades including mitogen activated protein kinase cascades have been developed to generate hypotheses about signal transduction owing to technical limitations these models and the hypotheses they generate have focused on a limited subset of signalling molecules now that we can simultaneously measure a substantial portion of the molecular components of a cell we can begin to develop and test systems level models of cellular signalling and regulatory processes therefore gaining insights into the thought processes of cell
neural responses are typically characterized by computing the mean firing rate but response variability can exist across trials many studies have examined the effect of a stimulus on the mean response but few have examined the effect on response variability we measured neural variability in extracellularly recorded datasets and one intracellularly recorded dataset from seven areas spanning the four cortical lobes in monkeys and cats in every case stimulus onset caused a decline in neural variability this occurred even when the stimulus produced little change in mean firing rate the variability decline was observed in membrane potential recordings in the spiking of individual neurons and in correlated spiking variability measured with implanted electrode arrays the variability decline was observed for all stimuli tested regardless of whether the animal was awake behaving or anaesthetized this widespread variability decline suggests a rather general property of cortex that its state is stabilized by input
background in the current era of scientific research efficient communication of information is paramount as such the nature of scholarly and scientific communication is changing cyberinfrastructure is now absolutely necessary and new media are allowing information and knowledge to be more interactive and immediate one approach to making knowledge more accessible is the addition of machine readable semantic data to scholarly articles results the word add in presented here will assist authors in this effort by automatically recognizing and highlighting words or phrases that are likely information rich allowing authors to associate semantic data with those words or phrases and to embed that data in the document as xml the add in and source code are publicly available at http www codeplex com ucsdbiolit conclusions the word add in for ontology term recognition makes it possible for an author to add semantic data to a document as it is being written and it encodes these data using xml tags that are effectively a standard in life sciences literature allowing authors to mark up their own work will help increase the amount and quality of machine readable metadata
motivation research in systems biology is carried out through a combination of experiments and models several data standards have been adopted for representing models sbml and various types of relevant experimental data such as fuge and those of the proteomics standards initiative however until now there has been no standard way to associate a model and its entities to the corresponding data sets or vice versa such a standard would provide a means to represent computational simulation results as well as to frame experimental data in the context of a particular model target applications include model driven data analysis parameter estimation and sharing and archiving model simulations results we propose the systems biology results markup language sbrml an xml based language which associates a model with several data sets each data set is represented as a series of values associated with model variables and their corresponding parameter values sbrml provides a flexible way of indexing the results to model parameter values which supports both spreadsheetlike data and multidimensional data cubes we present and discuss several examples of sbrml usage in applications such as enzyme kinetics microarray gene expression and various types of simulation results availability and implementation the xml schema file for sbrml is available at http www comp sys bio org sbrml under the academic free license afl contact pedro mendes manchester uk
the red queen hypothesis proposes that coevolution of interacting species such as hosts and parasites should drive molecular evolution through continual natural selection for adaptation and counter although the divergence observed at some host and parasite genes is consistent with this the long time periods typically required to study coevolution have so far prevented any direct empirical test here we show using experimental populations of the bacterium pseudomonas fluorescens and its viral parasite phage refs that the rate of molecular evolution in the phage was far higher when both bacterium and phage coevolved with each other than when phage evolved against a constant host genotype coevolution also resulted in far greater genetic divergence between replicate populations which was correlated with the range of hosts that coevolved phage were able to infect consistent with this the most rapidly evolving phage genes under coevolution were those involved in host infection these results demonstrate at both the genomic and phenotypic level that antagonistic coevolution is a cause of rapid and divergent evolution and is likely to be a major driver of evolutionary change species
motivation understanding the association between genetic diseases and their causal genes is an important problem concerning human health with the recent influx of high throughput data describing interactions between gene products scientists have been provided a new avenue through which these associations can be inferred despite the recent interest in this problem however there is little understanding of the relative benefits and drawbacks underlying the proposed techniques results we assessed the utility of physical protein interactions for determining gene disease associations by examining the performance of seven recently developed computational methods plus several of their variants we found that random walk approaches individually outperform clustering and neighborhood approaches although most methods make predictions not made by any other method we show how combining these methods into a consensus method yields pareto optimal performance we also quantified how a diffuse topological distribution of disease related proteins negatively affects prediction quality and are thus able to identify diseases especially amenable to network based predictions and others for which additional information sources are absolutely required availability the predictions made by each algorithm considered are available online at http www cbcb umd edu diseasenet contact carlk cs umd edu supplementary information supplementary data are available at bioinformatics bioinformatics
many theoretical advances have been made in applying probabilistic inference methods to improve the power of sequence homology searches yet the blast suite of programs is still the workhorse for most of the field the main reason for this is practical blast s programs are about fold faster than the fastest competing implementations of probabilistic inference methods i describe recent work on the hmmer software suite for protein sequence analysis which implements probabilistic inference using profile hidden markov models our aim in is to achieve blast s speed while further improving the power of probabilistic inference based methods implements a new probabilistic model of local sequence alignment and a new heuristic acceleration algorithm combined with efficient vector parallel implementations on modern processors these improvements synergize uses more powerful log odds likelihood scores scores summed over alignment uncertainty rather than scoring a single optimal alignment it calculates accurate expectation values e values for those scores without simulation using a generalization of karlin altschul theory it computes posterior distributions over the ensemble of possible alignments and returns posterior probabilities confidences in each aligned residue and it does all this at an overall speed comparable to blast the hmmer project aims to usher in a new generation of more powerful homology search tools based on probabilistic methods
motivation increasing quantity and quality of data in transcriptomics and interactomics create the need for integrative approaches to network analysis here we present a comprehensive r package for the analysis of biological networks including an exact and a heuristic approach to identify functional modules results the bionet package provides an extensive framework for integrated network analysis in r this includes the statistics for the integration of transcriptomic and functional data with biological networks the scoring of nodes as well as methods for network search and visualization availability the bionet package and a tutorial are available from http bionet bioapps biozentrum uni de
p metagenomics is a discipline that enables the genomic study of uncultured microorganisms faster cheaper sequencing technologies and the ability to sequence uncultured microbes sampled directly from their habitats are expanding and transforming our view of the microbial world distilling meaningful information from the millions of new genomic sequences presents a serious challenge to bioinformaticians in cultured microbes the genomic data come from a single clone making sequence assembly and annotation tractable in metagenomics the data come from heterogeneous microbial communities sometimes containing more than species with the sequence data being noisy and partial from sampling to assembly to gene calling and function prediction bioinformatics faces new demands in interpreting voluminous noisy and often partial sequence data although metagenomics is a relative newcomer to science the past few years have seen an explosion in computational methods applied to metagenomic based research it is therefore not within the scope of this article to provide an exhaustive review rather we provide here a concise yet comprehensive introduction to the current computational requirements presented by metagenomics and review the recent progress made we also note whether there is software that implements any of the methods presented here and briefly review its utility nevertheless it would be useful if readers of this article would avail themselves of the comment section provided by this journal and relate their own experiences finally the last section of this article provides a few representative studies illustrating different facets of recent scientific discoveries made using p
transcriptional positive feedback loops are widely associated with bistability characterized by two stable expression states that allow cells to respond to analog signals in a digital manner using a synthetic system in budding yeast we show that positive feedback involving a promoter with multiple transcription factor tf binding sites can induce a steady state bimodal response without cooperative binding of the tf deterministic models of this system do not predict bistability rather the bimodal response requires a short lived tf and stochastic fluctuations in the tf s expression multiple binding sites provide these fluctuations because many promoters possess multiple binding sites and many tfs are unstable positive feedback loops in gene regulatory networks may exhibit bimodal responses but not necessarily because of deterministic bistability as is commonly science
although it is being successfully implemented for exploration of the genome discovery science has eluded the functional neuroimaging community the core challenge remains the development of common paradigms for interrogating the myriad functional systems in the brain without the constraints of a priori hypotheses resting state functional mri r fmri constitutes a candidate approach capable of addressing this challenge imaging the brain during rest reveals large amplitude spontaneous low frequency hz fluctuations in the fmri signal that are temporally correlated across functionally related areas referred to as functional connectivity these correlations yield detailed maps of complex neural systems collectively constituting an individual s functional connectome reproducibility across datasets and individuals suggests the functional connectome has a common architecture yet each individual s functional connectome exhibits unique features with stable meaningful interindividual differences in connectivity patterns and strengths comprehensive mapping of the functional connectome and its subsequent exploitation to discern genetic influences and brain behavior relationships will require multicenter collaborative datasets here we initiate this endeavor by gathering r fmri data from volunteers collected independently at international centers we demonstrate a universal architecture of positive and negative functional connections as well as consistent loci of inter individual variability age and sex emerged as significant determinants these results demonstrate that independent r fmri datasets can be aggregated and shared high throughput r fmri can provide quantitative phenotypes for molecular genetic studies and biomarkers of developmental and pathological processes in the brain to initiate discovery science of brain function the functional connectomes project dataset is freely accessible at www nitrc projects
biomedical informatics involves a core set of methodologies that can provide a foundation for crossing the translational barriers associated with translational medicine to this end the fundamental aspects of biomedical informatics e g bioinformatics imaging informatics clinical informatics and public health informatics may be essential in helping improve the ability to bring basic research findings to the bedside evaluate the efficacy of interventions across communities and enable the assessment of the eventual impact of translational medicine innovations on health policies here a brief description is provided for a selection of key biomedical informatics topics decision support natural language processing standards information retrieval and electronic health records and their relevance to translational medicine based on contributions and advancements in each of these topic areas the article proposes that biomedical informatics practitioners biomedical informaticians can be essential members of translational teams
pnas recommender systems use data on past user preferences to predict possible future likes and interests a key challenge is that while the most useful individual recommendations are to be found among diverse niche objects the most reliably accurate results are obtained by methods that recommend objects based on user or object similarity in this paper we introduce a new algorithm specifically to address the challenge of diversity and show how it can be used to resolve this apparent dilemma when combined in an elegant hybrid with an accuracy focused algorithm by tuning the hybrid appropriately we are able to obtain without relying on any semantic or context specific information simultaneous gains in both accuracy and diversity recommendations
motivation micrornas mirnas are involved in an abundant class of post transcriptional regulation activated through binding to the untranslated region utr of mrnas the current wealth of mammalian mirna genes results mostly from genomic duplication events many of these events are located within introns of transcriptional units in order to better understand the genomic expansion of mirna genes we investigated the distribution of intronic mirnas results we observe that mirna genes are hosted within introns of short genes much larger than expected by chance implementation we explore several explanations for this phenomenon and conclude that mirna integration into short genes might be evolutionary favorable due to interaction with the pre mrna splicing mechanism contact nshomron post tau ac il supplementary information supplementary data are available at bioinformatics bioinformatics
this resource letter provides a guide to the literature on quantum chromodynamics qcd the relativistic quantum field theory of the strong interactions journal articles books and other documents are cited for the following topics quarks and color the parton model yang mills theory experimental evidence for color qcd as a color gauge theory asymptotic freedom qcd for heavy hadrons qcd on the lattice the qcd vacuum pictures of quark confinement early and modern applications of perturbative qcd the determination of the strong coupling and quark masses qcd and the hadron spectrum hadron decays the quark gluon plasma the strong nuclear interaction and qcd s role in nuclear physics the letter e after an item indicates elementary level or material of general interest to persons becoming informed in the field the letter i for intermediate level indicates material of a somewhat more specialized nature and the letter a indicates rather specialized or material
twitter a microblogging service less than three years old com mands more than million users as of july and is growing fast twitter users tweet about any topic within the character limit and follow others to receive their tweets the goal of this paper is to study the topological characteristics of twitter and its power as a new medium of information sharing we have crawled the entire twitter site and obtained million user proles billion social relations trending topics and million tweets in its follower following topology analysis we have found a non power law follower distribution a short effec tive diameter and low reciprocity which all mark a deviation from known characteristics of human social networks in order to identify inuentials on twitter we have ranked users by the number of followers and by pagerank and found two rankings to be sim ilar ranking by retweets differs from the previous two rankings indicating a gap in inuence inferred from the number of followers and that from the popularity of ones tweets we have analyzed the tweets of top trending topics and reported on their temporal behav ior and user participation we have classied the trending topics based on the active period and the tweets and show that the ma jority over of topics are headline news or persistent news in nature a closer look at retweets reveals that any retweeted tweet is to reach an average of users no matter what the number of followers is of the original tweet once retweeted a tweet gets retweeted almost instantly on next hops signifying fast diffusion of information after the retweet to the best of our knowledge this work is the rst quantitative study on the entire twittersphere and information diffusion it
social reference managers automate repetitive and tedious tasks such as literature management offering an alternative to search engines and traditional databases for social mediation and scientific discovery in this study we reflect upon the implications of social tagging processes for personal bibliographic management in the environment and we study two of the most famous applications although still little known and employed in spain citeulike connotea
motivation detection of single nucleotide polymorphisms snps has been a major application in processing second generation sequencing sgs data in principle snps are called on single base differences between a reference genome and a sequence generated from sgs short reads of a sample genome however this exercise is far from trivial several parameters related to sequencing quality and or reference genome properties play essential effect on the accuracy of called snps especially at shallow coverage data in this work we present slider ii an alignment and snp calling approach that demonstrates improved algorithmic approaches enabling larger number of called snps with lower false positive rate in addition to the regular alignment and snp calling as an optional feature slider ii is capable of utilizing information about known snps of a target genome as priors in the alignment and snps calling to enhance it s capability of detecting these known snps and novel snps and mutations in vicinity
endocytosis is a complex process fulfilling many cellular and developmental functions understanding how it is regulated and integrated with other cellular processes requires a comprehensive analysis of its molecular constituents and general design principles here we developed a new strategy to phenotypically profile the human genome with respect to transferrin tf and epidermal growth factor egf endocytosis by combining rna interference automated high resolution confocal microscopy quantitative multiparametric image analysis and high performance computing we identified several novel components of endocytic trafficking including genes implicated in human diseases we found that signalling pathways such as wnt integrin cell adhesion transforming growth factor tgf and notch regulate the endocytic system and identified new genes involved in cargo sorting to a subset of signalling endosomes a systems analysis by bayesian networks further showed that the number size concentration of cargo and intracellular position of endosomes are not determined randomly but are subject to specific regulation thus uncovering novel properties of the system
summary there is a large amount of tools for interactive display of phylogenetic trees however there is a shortage of tools for the automation of tree rendering scripting phylogenetic graphics would enable the saving of graphical analyses involving numerous and complex tree handling operations and would allow the automation of repetitive tasks scriptree is a tool intended to fill this gap it is an interpreter to be used in batch mode phylogenetic graphics instructions related to tree rendering as well as tree annotation are stored in a text file and processed in a sequential way availability scriptree can be used online or downloaded at www scriptree org under the gpl license implementation scriptree written in tcl tk is a cross platform application available for windows and unix like systems including os x it can be used either as a stand alone package or included in a bioinformatic pipeline and linked to a http server contact chevenet ird bioinformatics
we have produced an evolutionary model for promoters analogous to the commonly used synonymous nonsynonymous mutation models for protein coding sequences although our model called sunflower relies on some simple assumptions it captures enough of the biology of transcription factor action to show clear correlation with other biological features sunflower predicts a binding profile of transcription factors to dna sequences in which different factors compete for the same potential binding sites the parametrized model simultaneously estimates a continuous measurement of binding occupancy across the genomic sequence for each factor we can then introduce a localized mutation rerun the binding model and record the difference in binding profiles a single mutation can alter interactions both upstream and downstream of its position due to potential overlapping binding sites and our statistic captures this domino effect over evolutionary time we observe a clear excess of low scoring mutations fixed in promoters consistent with most changes being neutral however this is not consistent across all promoters and some promoters show more rapid divergence this divergence often occurs in the presence of relatively constant protein coding divergence interestingly different classes of promoters show different sensitivity to mutations with phosphorylation related genes having promoters inherently more sensitive to mutations than immune genes although there have previously been a number of models attempting to handle transcription factor binding sunflower provides a richer biological model incorporating weak binding sites and the possibility of competition the results show the first clear correlations between such a model and processes
we show how to apply the method of temperature accelerated molecular dynamics tamd in collective variables maragliano l vanden eijnden e chem phys lett to sample the conformational space of multidomain proteins in all atom explicitly solvated molecular dynamics simulations the method allows the system to hyperthermally explore the free energy surface in a set of collective variables computed at the physical temperature as collective variables we pick cartesian coordinates of centers of contiguous subdomains the method is applied to the groel subunit a kda three domain protein and hiv for groel the method induces in about conformational changes that recapitulate the tr transition and are not observed in unaccelerated molecular dynamics the apical domain is displaced by with a twist of relative to the equatorial domain and the root mean squared deviation relative to the r conformer is reduced from to representing fairly high predictive capability for the method predicts both counterrotation of inner and outer domains and disruption of the so called bridging sheet in particular tamd on initially in the bound conformation visits conformations that deviate by from the conformer in complex with antibody again reflecting good predictive capability tamd generates plausible all atom models of the so far structurally uncharacterized unliganded conformation of hiv which may prove useful in the development of inhibitors and immunogens the fictitious temperature employed also gives a rough estimate of mol for the free energy barrier between conformers in cases
there has long been interest in understanding the genetic basis of human adaptation to what extent are phenotypic differences among human populations driven by natural selection with the recent arrival of large genome wide data sets on human variation there is now unprecedented opportunity for progress on this type of question several lines of evidence argue for an important role of positive selection in shaping human variation and differences among populations these include studies of comparative morphology and physiology as well as population genetic studies of candidate loci and genome wide data however the data also suggest that it is unusual for strong selection to drive new mutations rapidly to fixation in particular populations the hard sweep model we argue instead for alternatives to the hard sweep model in particular polygenic adaptation could allow rapid adaptation while not producing classical signatures of selective sweeps we close by discussing some of the likely opportunities for progress in field
motivation clustering algorithms play an important role in the analysis of biological networks and can be used to uncover functional modules and obtain hints about cellular organization while most available clustering algorithms work well on biological networks of moderate size such as the yeast protein physical interaction network they either fail or are too slow in practice for larger networks such as functional networks for higher eukaryotes since an increasing number of larger biological networks are being determined the limitations of current clustering approaches curtail the types of biological network analyses that can be performed results we present a fast local network clustering algorithm spici spici runs in time o v log v e and space o e where v and e are the number of vertices and edges in the network respectively we evaluate spici s performance on several existing protein interaction networks of varying size and compare spici to nine previous approaches for clustering biological networks we show that spici is typically several orders of magnitude faster than previous approaches and is the only one that can successfully cluster all test networks within very short time we demonstrate that spici has state of the art performance with respect to the quality of the clusters it uncovers as judged by its ability to recapitulate protein complexes and functional modules finally we demonstrate the power of our fast network clustering algorithm by applying spici across hundreds of large context specific human networks and identifying modules specific for single conditions availability source code is available under the gnu public license at http compbio cs princeton spici
navigating through the ever changing information space is becoming increasingly difficult social navigation support is a technique for guiding users to interesting and relevant information by leveraging the browsing behavior of past users effect of social navigation support on users information seeking behavior has been studied mostly from conceptual basis or under natural experiments in the current work we have designed and conducted a controlled experiment to investigate the effect of social navigation support through a multifaceted method this paper reports on the design of the study and the result of log data subjective evaluation and eye movement analysis
the fine detail provided by sequencing based transcriptome surveys suggests that rna seq is likely to become the platform of choice for interrogating steady state rna in order to discover biologically important changes in expression we show that normalization continues to be an essential step in the analysis we outline a simple and effective method for performing normalization and show dramatically improved results for inferring differential expression in simulated and publicly available sets
to understand the impact of gut microbes on human health and well being it is crucial to assess their genetic potential here we describe the illumina based metagenomic sequencing assembly and characterization of million non redundant microbial genes derived from gigabases of sequence from faecal samples of european individuals the gene set approximately times larger than the human gene complement contains an overwhelming majority of the prevalent more frequent microbial genes of the cohort and probably includes a large proportion of the prevalent human intestinal microbial genes the genes are largely shared among individuals of the cohort over of the genes are bacterial indicating that the entire cohort harbours between and prevalent bacterial species and each individual at least such species which are also largely shared we define and describe the minimal gut metagenome and the minimal gut bacterial genome in terms of functions present in all individuals and most respectively
this work investigates statistical prevalence and overall physical origins of changes in charge states of receptor proteins upon ligand binding these changes are explored as a function of the ligand type small molecule protein and nucleic acid and distance from the binding region standard continuum solvent methodology is used to compute on an equal footing pk changes upon ligand binding for a total of ionizable residues in protein protein protein small molecule and nucleic acid high resolution complexes the size of the data set combined with an extensive error and sensitivity analysis allows us to make statistically justified and conservative conclusions in of all protein small molecule of all protein protein and of all protein nucleic acid complexes there exists at least one ionizable residue that changes its charge state upon ligand binding at physiological conditions ph considering the most biologically relevant ph range of the number of ionizable residues that experience substantial pk changes pk due to ligand binding is appreciable on average of all ionizable residues in protein small molecule complexes in protein protein and in protein nucleic acid complexes experience a substantial pk change upon ligand binding these changes are safely above the statistical false positive noise level most of the changes occur in the immediate binding interface region where approximately one out of five ionizable residues experiences substantial pk change regardless of the ligand type however the physical origins of the change differ between the types in protein nucleic acid complexes the pk values of interface residues are predominantly affected by electrostatic effects whereas in protein protein and protein small molecule complexes structural changes due to the induced fit effect play an equally important role in protein protein and protein nucleic acid complexes there is a statistically significant number of substantial pk perturbations mostly due to the induced fit structural changes in regions far from the interface
motivation the automatic analysis of scientific literature can support authors in writing their manuscripts implementation papermaker is a novel it solution that receives a scientific manuscript via a web interface automatically analyses the publication evaluates consistency parameters and interactively delivers feedback to the author it analyses the proper use of acronyms and their definitions and the use of specialized terminology it provides gene ontology go and medline subject headings mesh categorization of text passages the retrieval of relevant publications from public scientific literature repositories and the identification of missing or unused references result the author receives a summary of findings the manuscript in its corrected form and a digital abstract containing the go and mesh annotations in the nlm pubmed format availability http www ebi ac uk rebholz srv papermaker contact rebholz ebi ac bioinformatics
motivation describing biological sample variables with ontologies is complex due to the cross domain nature of experiments ontologies provide annotation solutions however for cross domain investigations multiple ontologies are needed to represent the data these are subject to rapid change are often not interoperable and present complexities that are a barrier to biological resource users results we present the experimental factor ontology efo designed to meet cross domain application focused use cases for gene expression data we describe our methodology and open source tools used to create the ontology these include tools for creating ontology mappings ontology views detecting ontology changes and using ontologies in interfaces to enhance querying the application of reference ontologies to data is a key problem and this work presents guidelines on how community ontologies can be presented in an application ontology in a data driven way availability http www ebi ac uk efo contact malone ebi uk
combinatorial interactions among transcription factors are critical to directing tissue specific gene expression to build a global atlas of these combinations we have screened for physical interactions among the majority of human and mouse dna binding transcription factors tfs the complete networks contain human and mouse interactions analysis of the networks reveals that highly connected tfs are broadly expressed across tissues and that roughly half of the measured interactions are conserved between mouse and human the data highlight the importance of tf combinations for determining cell fate and they lead to the identification of a complex expressed during development of immunity the availability of large tf combinatorial networks in both human and mouse will provide many opportunities to study gene regulation tissue differentiation and mammalian evolution an atlas of human and mouse transcription factor interactions quantification of transcription factor expression across human and mouse tissues a network of transcription factors predicts tissue type specification forms a repressor complex that controls differentiation
mapreduce is a popular framework for data intensive distributed computing of batch jobs to simplify fault tolerance many implementations of mapreduce materialize the entire output of each map and reduce task before it can be consumed in this paper we propose a modied mapreduce architecture that allows data to be pipelined between operators this extends the mapreduce programming model beyond batch processing and can reduce completion times and improve system utilization for batch jobs as well we present a modied version of the hadoop mapreduce framework that supports on line aggregation which allows users to see early returns from a job as it is being computed our hadoop online prototype hop also supports continuous queries which enable mapreduce programs to be written for applications such as event monitoring and stream processing hop retains the fault tolerance properties of hadoop and can run unmodied user dened programs
summary assessment of phylogenetic diversity is a key element to the analysis of microbial communities tools are needed to handle next generation sequencing data and to cope with the computational complexity of large scale studies here we present treephyler a tool for fast taxonomic profiling of metagenomes treephyler was evaluated on real metagenome to assess its performance in comparison to previous approaches for taxonomic profiling results indicate that treephyler is in terms of speed and accuracy prepared for next generation sequencing techniques and large scale analysis availability treephyler is implemented in perl it is portable to all platforms and applicable to both nucleotide and protein input data treephyler is freely available for download at http www gobics de fabian treephyler phpcontact fschrei de
we suggest a novel discretisation of the momentum equation for smoothed particle hydrodynamics sph and show that it significantly improves the accuracy of the obtained solutions our new formulation which we refer to as relative pressure sph rpsph evaluates the pressure force in respect to the local pressure it respects newtons first law of motion and applies forces to particles only when there is a net force acting upon them this is in contrast to standard sph which explicitly uses newtons third law of motion continuously applying equal but opposite forces between particles rpsph does not show the unphysical particle noise the clumping or banding instability unphysical surface tension and unphysical scattering of different mass particles found for standard sph at the same time it uses fewer computational operations and only changes a single line in existing sph codes we demonstrate its performance on isobaric uniform density distributions uniform density shearing flows the kelvin helmholtz and rayleigh taylor instabilities the sod shock tube the sedov taylor blast wave and a cosmological integration of the santa barbara galaxy cluster formation test rpsph is an improvement these cases the improvements come at the cost of giving up exact momentum conservation of the scheme consequently one can also obtain unphysical solutions particularly at resolutions
high throughput studies of biological systems are rapidly accumulating a wealth of omics scale data visualization is a key aspect of both the analysis and understanding of these data and users now have many visualization methods and tools to choose from the challenge is to create clear meaningful and integrated visualizations that give biological insight without being overwhelmed by the intrinsic complexity of the data in this review we discuss how visualization tools are being used to help interpret protein interaction gene expression and metabolic profile data and we highlight emerging directions
a central challenge in computational modeling of biological systems is the determination of the model parameters typically only a fraction of the parameters such as kinetic rate constants are experimentally measured while the rest are often fitted the fitting process is usually based on experimental time course measurements of observables which are used to assign parameter values that minimize some measure of the error between these measurements and the corresponding model prediction the measurements which can come from immunoblotting assays fluorescent markers etc tend to be very noisy and taken at a limited number of time points in this work we present a new approach to the problem of parameter selection of biological models we show how one can use a dynamic recursive estimator known as extended kalman filter to arrive at estimates of the model parameters the proposed method follows first we use a variation of the kalman filter that is particularly well suited to biological applications to obtain a first guess for the unknown parameters secondly we employ an a posteriori identifiability test to check the reliability of the estimates finally we solve an optimization problem to refine the first guess in case it should not be accurate enough the final estimates are guaranteed to be statistically consistent with the measurements furthermore we show how the same tools can be used to discriminate among alternate models of the same biological process we demonstrate these ideas by applying our methods to two examples namely a model of the heat shock response in e coli and a model of a synthetic gene regulation system the methods presented are quite general and may be applied to a wide class of biological systems where noisy measurements are used for parameter estimation or selection
motivation next generation sequencing ngs has enabled whole genome and transcriptome single nucleotide variant snv discovery in cancer ngs produces millions of short sequence reads that once aligned to a reference genome sequence can be interpreted for the presence of snvs although tools exist for snv discovery from ngs data none are specifically suited to work with data from tumors where altered ploidy and tumor cellularity impact the statistical expectations of snv discovery results we developed three implementations of a probabilistic binomial mixture model called snvmix designed to infer snvs from ngs data from tumors to address this problem the first models allelic counts as observations and infers snvs and model parameters using an expectation maximization em algorithm and is therefore capable of adjusting to deviation of allelic frequencies inherent in genomically unstable tumor genomes the second models nucleotide and mapping qualities of the reads by probabilistically weighting the contribution of a read nucleotide to the inference of a snv based on the confidence we have in the base call and the read alignment the third combines filtering out low quality data in addition to probabilistic weighting of the qualities we quantitatively evaluated these approaches on ovarian cancer rnaseq datasets with matched genotyping arrays and a human breast cancer genome sequenced to haploid coverage with ground truth data and show systematically that the snvmix models outperform competing approaches availability software and data are available at http compbio bccrc cacontact sshah bccrc casupplemantary information supplementary data are available at online
summary here we report the development of a filtering framework designed for efficient identification of both polyclonal and independent errors within solid sequence data the filtering utilizes the quality values reported by solid s primary analysis for the identification of the two different types of errors the filtering framework facilitates the passage of high quality data into a variety of functional genomics applications including de novo assemblers and sequence matching programs for snp calling improving the output quality and reducing resources necessary for analysis availability this error analysis framework is written in perl and runs on mac os and linux unix systems the filter documentation and sample excel files for quality analysis are available at http hts rutgers edu filter and are distributed as open source software under the contact tmichael waksman rutgers edu supplementary information supplementary data is available at bioinformatics bioinformatics
metabolic syndrome is a group of obesity related metabolic abnormalities that increase an individual s risk of developing type diabetes and cardiovascular disease here we show that mice genetically deficient in toll like receptor a component of the innate immune system that is expressed in the gut mucosa and that helps defend against infection exhibit hyperphagia and develop hallmark features of metabolic syndrome including hyperlipidemia hypertension insulin resistance and increased adiposity these metabolic changes correlated with changes in the composition of the gut microbiota and transfer of the gut microbiota from deficient mice to wild type germ free mice conferred many features of metabolic syndrome to the recipients food restriction prevented obesity but not insulin resistance in the deficient mice these results support the emerging view that the gut microbiota contributes to metabolic disease and suggest that malfunction of the innate immune system may promote the development of metabolic science
a longstanding question in molecular biology is the extent to which the behavior of macromolecules observed in vitro accurately reflects their behavior in vivo a number of sophisticated experimental techniques now allow the behavior of individual types of macromolecule to be studied directly in vivo none however allow a wide range of molecule types to be observed simultaneously in order to tackle this issue we have adopted a computational perspective and having selected the model prokaryote escherichia coli as a test system have assembled an atomically detailed model of its cytoplasmic environment that includes of the most abundant types of macromolecules at experimentally measured concentrations brownian dynamics bd simulations of the cytoplasm model have been calibrated to reproduce the translational diffusion coefficients of green fluorescent protein gfp observed in vivo and snapshots of the simulation trajectories have been used to compute the cytoplasm s effects on the thermodynamics of protein folding association and aggregation events the simulation model successfully describes the relative thermodynamic stabilities of proteins measured in e coli and shows that effects additional to the commonly cited crowding effect must be included in attempts to understand macromolecular behavior vivo
the development of detailed coherent models of complex biological systems is recognized as a key requirement for integrating the increasing amount of experimental data in addition in silico simulation of bio chemical models provides an easy way to test different experimental conditions helping in the discovery of the dynamics that regulate biological systems however the computational power required by these simulations often exceeds that available on common desktop computers and thus expensive high performance computing solutions are required an emerging alternative is represented by general purpose scientific computing on graphics processing units gpgpu which offers the power of a small computer cluster at a cost of computing with a gpu requires the development of specific algorithms since the programming paradigm substantially differs from traditional cpu based computing in this paper we review some recent efforts in exploiting the processing power of gpus for the simulation of biological bib
abstract background unitary pseudogenes are a class of unprocessed pseudogenes without functioning counterparts in the genome they constitute only a small fraction of annotated pseudogenes in the human genome however as they represent distinct functional losses over time they shed light on the unique features of humans in primate evolution results we present a pipeline to detect human unitary pseudogenes through analyzing the global inventory of orthologs between the human genome and its mammalian relatives we focus on gene losses along the human lineage after the divergence from rodents million years ago in total we identify unitary pseudogenes including previously annotated ones and many novel ones by comparing these to their functioning ortholog in other mammals we can approximately date the creation of each unitary pseudogene that is the gene death date and show that for our group of unitary pseudogenes the functional genes became disabled at a fairly uniform rate throughout primate evolution not all at once and correlated for instance with the alu burst furthermore we identify unitary pseudogenes that are polymorphic they have both nonfunctional and functional alleles currently segregating in the human population comparing them with their orthologs in other primates we find that two of them are in fact pseudogenes in non human primates suggesting that they represent cases of a gene being resurrected in the human lineage conclusion this analysis of unitary pseudogenes provides insights into the evolutionary constraints faced by different organisms and the timescales of functional gene loss humans
as our ability to generate sequencing data continues to increase data analysis is replacing data generation as the rate limiting step in genomics studies here we provide a guide to genomic data visualization tools that facilitate analysis tasks by enabling researchers to explore interpret and manipulate their data and in some cases perform on the fly computations we will discuss graphical methods designed for the analysis of de novo sequencing assemblies and read alignments genome browsing and comparative genomics highlighting the strengths and limitations of these approaches and the ahead
recently hidden markov models have been applied to numerous problems in genomics here we introduce an explicit population genetics hidden markov model popgenhmm that uses single nucleotide polymorphism snp frequency data to identify genomic regions that have experienced recent selection our popgenhmm assumes that snp frequencies are emitted independently following diffusion approximation expectations but that neighboring snp frequencies are partially correlated by selective state we give results from the training and application of our popgenhmm to a set of early release data from the drosophila population genomics project dpgp org that consists of approximately mb of resequencing from north american drosophila melanogaster lines these results demonstrate the potential utility of our model making predictions based on the site frequency spectrum sfs for regions of the genome that represent elements
the mammalian gut is an attractive model for exploring the general question of how habitat impacts the evolution of gene content therefore we have characterized the relationship between s rrna gene sequence similarity and overall levels of gene conservation in four groups of species gut specialists and cosmopolitans each of which can be divided into pathogens and non pathogens at short phylogenetic distances specialist or cosmopolitan bacteria found in the gut share fewer genes than is typical for genomes that come from non gut environments but at longer phylogenetic distances gut bacteria are more similar to each other than are genomes at equivalent evolutionary distances from non gut environments suggesting a pattern of short term specialization but long term convergence moreover this pattern is observed in both pathogens and non pathogens and can even be seen in the plasmids carried by gut bacteria this observation is consistent with the finding that despite considerable interpersonal variation in species content there is surprising functional convergence in the microbiome of different humans finally we observe that even within bacterial species or genera rrna divergence provides useful information about average conservation of gene content the results described here should be useful for guiding strain selection to maximize novel gene discovery in large scale genome sequencing projects while the approach could be applied in studies seeking to understand the effects of habitat adaptation on genome evolution across other body habitats or types
chromatin plays a central role in eukaryotic gene regulation we performed genome wide mapping of epigenetically marked nucleosomes to determine their position both near transcription start sites and at distal regulatory elements including enhancers in prostate cancer cells where androgen receptor binds primarily to enhancers we found that androgen treatment dismisses a central nucleosome present at androgen receptor binding sites that is flanked by a pair of marked nucleosomes a new quantitative model built on the behavior of such nucleosome pairs correctly identified regions bound by the regulators of the immediate androgen response including androgen receptor and more importantly this model also correctly predicted previously unidentified binding sites for other transcription factors present after prolonged androgen stimulation including and therefore quantitative modeling of enhancer structure provides a powerful predictive method to infer the identity of transcription factors involved in cellular responses to stimuli
gene expression variation between species is a major contributor to phenotypic diversity yet the underlying flexibility of transcriptional regulatory networks remains largely unexplored transcription of the ribosomal regulon is a critical task for all cells in s cerevisiae the transcription factors and form a multi subunit complex that controls ribosomal gene expression while in c albicans this regulation is under the control of and here we analyzed using full genome transcription factor mapping the roles in both s cerevisiae and c albicans of each orthologous component of this complete set of regulators we observe dramatic changes in the binding profiles of the generalist regulators and while the dimer is the only component involved in ribosomal regulation in both fungi it activates ribosomal protein genes and rdna expression in a dependent manner in c albicans and a dependent manner in s cerevisiae we show that the transcriptional regulatory network governing the ribosomal expression program of two related yeast species has been massively reshaped in cis and trans changes occurred in transcription factor wiring with cellular functions movements in transcription factor hierarchies dna binding specificity and regulatory complexes assembly to promote global changes in the architecture of the fungal transcriptional network
background dna microarrays have become the standard method for large scale analyses of gene expression and epigenomics the increasing complexity and inherent noisiness of the generated data makes visual data exploration ever more important fast deployment of new methods as well as a combination of predefined easy to apply methods with programmer s access to the data are important requirements for any analysis framework mayday is an open source platform with emphasis on visual data exploration and analysis many built in methods for clustering machine learning and classification are provided for dissecting complex datasets plugins can easily be written to extend mayday s functionality in a large number of ways as java program mayday is platform independent and can be used as java webstart application without any installation mayday can import data from several file formats database connectivity is included for efficient data organization numerous interactive visualization tools including box plots profile plots principal component plots and a heatmap are available can be enhanced with metadata and exported as publication quality vector files results we have rewritten large parts of mayday s core to make it more efficient and ready for future developments among the large number of new plugins are an automated processing framework dynamic filtering new and efficient clustering methods a machine learning module and database connectivity extensive manual data analysis can be done using an inbuilt r terminal and an integrated sql querying interface our visualization framework has become more powerful new plot types have been added and existing plots improved conclusions we present a major extension of mayday a very versatile open source framework for efficient micro array data analysis designed for biologists and bioinformaticians most everyday tasks are already covered the large number of available plugins as well as the extension possibilities using compiled plugins and ad hoc scripting allow for the rapid adaption of mayday also to very specialized data exploration mayday is available at http microarray org
motivation chip chip and chip seq technologies provide genome wide measurements of various types of chromatin marks at an unprecedented resolution with chip samples collected from different tissue types and or individuals we can now begin to characterize stochastic or systematic changes in epigenetic patterns during development intra individual or at the population level inter individual this requires statistical methods that permit a simultaneous comparison of multiple chip samples on a global as well as locus specific scale current analytical approaches are mainly geared toward single sample investigations and therefore have limited applicability in this comparative setting this shortcoming presents a bottleneck in biological interpretations of multiple sample data results to address this limitation we introduce a parametric classification approach for the simultaneous analysis of two or more chip samples we consider several competing models that reflect alternative biological assumptions about the global distribution of the data inferences about locus specific and genome wide chromatin differences are reached through the estimation of multivariate mixtures parameter estimates are obtained using an incremental version of the expectationmaximization algorithm iem we demonstrate efficient scalability and application to three very diverse chip chip and chip seq experiments the proposed approach is evaluated against several published chip chip and chip seq software packages we recommend its use as a first pass algorithm to identify candidate regions in the epigenome possibly followed by some type of second pass algorithm to fine tune detected peaks in accordance with biological or technological criteria availability r source code is available at http gbic biol rug nl supplementary chromatinprofiles access to chip seq data geo repository f johannes rug nlsupplementary information supplementary data are available at online
we describe cell typespecific significance analysis of microarrays cssam for analyzing differential gene expression for each cell type in a biological sample from microarray data and relative cell type frequencies first we validated cssam with predesigned mixtures and then applied it to whole blood gene expression datasets from stable post transplant kidney transplant recipients and those experiencing acute transplant rejection which revealed hundreds of differentially expressed genes that were undetectable
mixed linear model mlm methods have proven useful in controlling for population structure and relatedness within genome wide association studies however mlm based methods can be computationally challenging for large datasets we report a compression approach called compressed mlm that decreases the effective sample size of such datasets by clustering individuals into groups we also present a complementary approach population parameters previously determined that eliminates the need to re compute variance components we applied these two methods both independently and combined in selected genetic association datasets from human dog and maize the joint implementation of these two methods markedly reduced computing time and either maintained or improved statistical power we used simulations to demonstrate the usefulness in controlling for substructure in genetic association datasets for a range of species and genetic architectures we have made these methods available within an implementation of the software tassel
pnas coexpression of genes within a functional module can be conserved at great evolutionary distances whereas the associated regulatory mechanisms can substantially diverge for example ribosomal protein rp genes are tightly coexpressed in but the and factors associated with them are surprisingly diverged across fungi little is known however about the functional impact of such changes on actual expression levels or about the selective pressures that affect them here we address this question in the context of the evolution of the regulation of rp gene expression by using a comparative genomics approach together with cross species functional assays we show that an activator and a repressor that control rp gene regulation in normal and stress conditions in are derived from the duplication and subsequent specialization of a single ancestral protein we provide evidence that this regulatory innovation coincides with the duplication of rp genes in a whole genome duplication wgd event and may have been important for tighter control of higher levels of rp transcripts we find that subsequent loss of the derived repressor led to the loss of a stress dependent repression of rps in the fungal pathogen our comparative computational and experimental approach shows how gene duplication can constrain and drive regulatory evolution and provides a general strategy for reconstructing the evolutionary trajectory of gene regulation species
gr global studies of transcript structure and abundance in cancer cells enable the systematic discovery of aberrations that contribute to carcinogenesis including gene fusions alternative splice isoforms and somatic mutations we developed a systematic approach to characterize the spectrum of cancer associated mrna alterations through integration of transcriptomic and structural genomic data and we applied this approach to generate new insights into melanoma biology using paired end massively parallel sequencing of cdna rna seq together with analyses of high resolution chromosomal copy number data we identified novel melanoma gene fusions produced by underlying genomic rearrangements as well as novel readthrough transcripts we mapped these chimeric transcripts to base pair resolution and traced them to their genomic origins using matched chromosomal copy number information we also used these data to discover and validate base pair mutations that accumulated in these melanomas revealing a surprisingly high rate of somatic mutation and lending support to the notion that point mutations constitute the major driver of melanoma progression taken together these results may indicate new avenues for target discovery in melanoma while also providing a template for large scale transcriptome studies across many types
the emergence of next generation sequencing platforms led to resurgence of research in whole genome shotgun assembly algorithms and software dna sequencing data from the roche illumina solexa and abi solid platforms typically present shorter read lengths higher coverage and different error profiles compared with sanger sequencing data since several assembly software packages have been created or revised specifically for de novo assembly of next generation sequencing data this review summarizes and compares the published descriptions of packages named ssake sharcgs vcake newbler celera assembler euler velvet abyss allpaths and soapdenovo more generally it compares the two standard methods known as the de bruijn graph approach and the overlap layout consensus approach assembly
we present an extensible software model for the genotype and phenotype community xgap readers can download a standard xgap http www xgap org or auto generate a custom version using molgenis with programming interfaces to r software and web services or user interfaces for biologists xgap has simple load formats for any type of genotype epigenotype transcript protein metabolite or other phenotype data current functionality includes tools ranging from eqtl analysis in mouse to genome wide association studies humans
abstract diploid genomes with divergent chromosomes present special problems for assembly software as two copies of especially polymorphic regions may be mistakenly constructed creating the appearance of a recent segmental duplication we developed a method for identifying such false duplications and applied it to four vertebrate genomes for each genome we corrected mis assemblies improved estimates of the amount of duplicated sequence and recovered polymorphisms between the chromosomes
gene expression is an important phenotype that informs about genetic and environmental effects on cellular state many studies have previously identified genetic variants for gene expression phenotypes using custom and commercially available second generation sequencing technologies are now providing unprecedented access to the fine structure of the we have sequenced the mrna fraction of the transcriptome in extended hapmap individuals of european descent and have combined these data with genetic variants from the we have quantified exon abundance based on read depth and have also developed methods to quantify whole transcript abundance we have found that approximately million reads of sequencing can provide access to the same dynamic range as arrays with better quantification of alternative and highly abundant transcripts correlation with snps small nucleotide polymorphisms leads to a larger discovery of eqtls expression quantitative trait loci than with arrays we also detect a substantial number of variants that influence the structure of mature transcripts indicating variants responsible for alternative splicing finally measures of allele specific expression allowed the identification of rare eqtls and allelic differences in transcript structure this analysis shows that high throughput sequencing technologies reveal new properties of genetic effects on the transcriptome and allow the exploration of genetic effects in processes
understanding the genetic mechanisms underlying natural variation in gene expression is a central goal of both medical and evolutionary genetics and studies of expression quantitative trait loci eqtls have become an important tool for achieving this goal although all eqtl studies so far have assayed messenger rna levels using expression microarrays recent advances in rna sequencing enable the analysis of transcript variation at unprecedented resolution we sequenced rna from lymphoblastoid cell lines derived from unrelated nigerian individuals that have been extensively genotyped by the international hapmap project by pooling data from all individuals we generated a map of the transcriptional landscape of these cells identifying extensive use of unannotated untranslated regions and more than new putative protein coding exons using the genotypes from the hapmap project we identified more than a thousand genes at which genetic variation influences overall expression levels or splicing we demonstrate that eqtls near genes generally act by a mechanism involving allele specific expression and that variation that influences the inclusion of an exon is enriched within and near the consensus splice sites our results illustrate the power of high throughput sequencing for the joint analysis of variation in transcription splicing and allele specific expression individuals
advances in imaging techniques and high throughput technologies are providing scientists with unprecedented possibilities to visualize internal structures of cells organs and organisms and to collect systematic image data characterizing genes and proteins on a large scale to make the best use of these increasingly complex and large image data resources the scientific community must be provided with methods to query analyze and crosslink these resources to give an intuitive visual representation of the data this review gives an overview of existing methods and tools for this purpose and highlights some of their limitations challenges
gr information about the binding preferences of many transcription factors is known and characterized by a sequence binding motif however determining regions of the genome in which a transcription factor binds based on its motif is a challenging problem particularly in species with large genomes since there are often many sequences containing matches to the motif but are not bound several rules based on sequence conservation or location relative to a transcription start site have been proposed to help differentiate true binding sites from random ones other evidence sources may also be informative for this task we developed a method for integrating multiple evidence sources using logistic regression classifiers our method works in two steps first we infer a score quantifying the general binding preferences of transcription factor binding at all locations based on a large set of evidence features without using any motif specific information then we combined this general binding preference score with motif information for specific transcription factors to improve prediction of regions bound by the factor using cross validation and new experimental data we show that surprisingly the general binding preference can be highly predictive of true locations of transcription factor binding even when no binding motif is used when combined with motif information our method outperforms previous methods for predicting locations of binding
the systems biology graphical notation sbgn is an emerging standard for graphical notation developed by an international systems biology community standardized graphical notation is crucial for efficient and accurate communication of biological knowledge between researchers with various backgrounds in the expanding field of systems biology here we highlight sbgn from a practical point of view and describe how the user can build and simulate sbgn models from a simple drag and drop graphical user interface pathwaylab
genes like organisms struggle for existence and the most successful genes persist and widely disseminate in nature the unbiased determination of the most successful genes requires access to sequence data from a wide range of phylogenetic taxa and ecosystems which has finally become achievable thanks to the deluge of genomic and metagenomic sequences here we analyzed million protein encoding genes and gene tags in sequenced bacterial archaeal eukaryotic and viral genomes and metagenomes and our analysis demonstrates that genes encoding transposases are the most prevalent genes in nature the finding that these genes classically considered as selfish genes outnumber essential or housekeeping genes suggests that they offer selective advantage to the genomes and ecosystems they inhabit a hypothesis in agreement with an emerging body of literature their mobile nature not only promotes dissemination of transposable elements within and between genomes but also leads to mutations and rearrangements that can accelerate biological diversification and consequently evolution by securing their own replication and dissemination transposases guarantee to thrive so long as nucleic acid based life forms nar
we analyzed the whole genome sequences of a family of four consisting of two siblings and their parents family based sequencing allowed us to delineate recombination sites precisely identify of the sequencing errors resulting in accuracy and identify very rare single nucleotide polymorphisms we also directly estimated a human intergeneration mutation rate of per position per haploid genome both offspring in this family have two recessive disorders miller syndrome for which the gene was concurrently identified and primary ciliary dyskinesia for which causative genes have been previously identified family based genome analysis enabled us to narrow the candidate genes for both of these mendelian disorders to only four our results demonstrate the value of complete genome sequencing families
background whole genome sequencing may revolutionize medical diagnostics through rapid identification of alleles that cause disease however even in cases with simple patterns of inheritance and unambiguous diagnoses the relationship between disease phenotypes and their corresponding genetic changes can be complicated comprehensive diagnostic assays must therefore identify all possible dna changes in each haplotype and determine which are responsible for the underlying disorder the high number of rare heterogeneous mutations present in all humans and the paucity of known functional variants in more than of annotated genes make this challenge particularly difficult thus the identification of the molecular basis of a genetic disease by means of whole genome sequencing has remained elusive we therefore aimed to assess the usefulness of human whole genome sequencing for genetic diagnosis in a patient with charcot marie tooth disease methods we identified a family with a recessive form of charcot marie tooth disease for which the genetic basis had not been identified we sequenced the whole genome of the proband identified all potential functional variants in genes likely to be related to the disease and genotyped these variants in the affected family members results we identified and validated compound heterozygous causative alleles in the domain and tetratricopeptide repeats gene involving two mutations in the proband and in family members affected by charcot marie tooth disease separate subclinical phenotypes segregated independently with each of the two mutations heterozygous mutations confer susceptibility to neuropathy including the carpal tunnel syndrome conclusions as shown in this study of a family with charcot marie tooth disease whole genome sequencing can identify clinically relevant variants and provide diagnostic information to inform the care patients
micrornas mirnas are integral elements in the post transcriptional control of gene expression after the identification of hundreds of mirnas the challenge is now to understand their specific biological function signalling pathways are ideal candidates for mirna mediated regulation owing to the sharp dose sensitive nature of their effects indeed emerging evidence suggests that mirnas affect the responsiveness of cells to signalling molecules such as transforming growth factor wnt notch and epidermal growth factor as such mirnas serve as nodes of signalling networks that ensure homeostasis and regulate cancer metastasis fibrosis and stem biology
in vivo variations in the concentrations of biomolecular species are inevitable these variations in turn propagate along networks of chemical reactions and modify the concentrations of still other species which influence biological activity because excessive variations in the amounts of certain active species might hamper cell function regulation systems have evolved that act to maintain concentrations within tight bounds we identify simple yet subtle structural attributes that impart concentration robustness to any mass action network possessing them we thereby describe a large class of robustness inducing networks that already embraces two quite different biochemical modules for which concentration robustness has been observed experimentally the escherichia coli osmoregulation system envz ompr and the glyoxylate bypass control system isocitrate dehydrogenase kinase phosphatase isocitrate dehydrogenase the structural attributes identified here might confer robustness far broadly
the matrix of evolutionary distances is a model based statistic derived from molecular sequences summarizing the pairwise phylogenetic relations between a collection of species phylogenetic tree reconstruction methods relying on this matrix are relatively fast and thus widely used in molecular systematics however because of their intrinsic reliance on summary statistics distance matrix methods are assumed to be less accurate than likelihood based approaches in this paper pairwise sequence comparisons are shown to be more powerful than previously hypothesized a statistical analysis of certain distance based techniques indicates that their data requirement for large evolutionary trees essentially matches the conjectured performance of maximum likelihood methods challenging the idea that summary statistics lead to suboptimal analyses on the basis of a connection between ancestral state reconstruction and distance averaging the critical role played by the covariances of the distance matrix identified
although genome wide association studies gwass have identified numerous loci associated with complex traits imprecise modeling of the genetic relatedness within study samples may cause substantial inflation of test statistics and possibly spurious associations variance component approaches such as efficient mixed model association emma can correct for a wide range of sample structures by explicitly accounting for pairwise relatedness between individuals using high density markers to model the phenotype distribution but such approaches are computationally impractical we report here a variance component approach implemented in publicly available software emma expedited emmax that reduces the computational time for analyzing large gwas data sets from years to hours we apply this method to two human gwas data sets performing association analysis for ten quantitative traits from the northern finland birth cohort and seven common diseases from the wellcome trust case control consortium we find that emmax outperforms both principal component analysis and genomic control in correcting for structure
background an old debate has undergone a resurgence in systems biology that of reductionism versus holism at least articles in the systems biology literature since have touched on this issue the histories of holism and reductionism in the philosophy of biology are reviewed and the current debate in systems biology is placed in context results inter theoretic reductionism in the strict sense envisaged by its creators from the to the is largely impractical in biology and was effectively abandoned by the early in favour of a more piecemeal approach using individual reductive explanations classical holism was a stillborn theory of the but the term survived in several fields as a loose umbrella designation for various kinds of anti reductionism which often differ markedly several of these different anti reductionisms are on display in the holistic rhetoric of the recent systems biology literature this debate also coincides with a time when interesting arguments are being proposed within the philosophy of biology for a new kind of reductionism conclusions engaging more deeply with these issues should sharpen our ideas concerning the philosophy of systems biology and its future best methodology as with previous decisive moments in the history of biology only those theories that immediately suggest relatively easy experiments will winners
deciphering transcription factor networks from microarray data remains difficult this study presents a simple method to infer the regulation of transcription factors from microarray data based on well characterized target genes we generated a catalog containing transcription factors associated with target genes and experimentally validated regulations when it was available a distinction between transcriptional activation and inhibition was included for each regulation next we built a tool www tfacts org that compares submitted gene lists with target genes in the catalog to detect regulated transcription factors tfacts was validated with published lists of regulated genes in various models and compared to tools based on in silico promoter analysis we next analyzed the cancer microarray data set and showed the regulation of mitf and jun in melanomas we then performed microarray experiments comparing gene expression response of human fibroblasts stimulated by different growth factors tfacts predicted the specific activation of signal transducer and activator of transcription factors by pdgf bb which was confirmed experimentally our results show that the expression levels of transcription factor target genes constitute a robust signature for transcription factor regulation and can be efficiently used for microarray data nar
traditionally studies of brain function have focused on task evoked responses by their very nature such experiments tacitly encourage a reflexive view of brain function although such an approach has been remarkably productive it ignores the alternative possibility that brain functions are mainly intrinsic involving information processing for interpreting responding to and predicting environmental demands here i argue that the latter view best captures the essence of brain function a position that accords well with the allocation of the brain s energy resources recognizing the importance of intrinsic activity will require integrating knowledge from cognitive and systems neuroscience with cellular and molecular neuroscience where ion channels receptors components of signal transduction and metabolic pathways are all in a constant state flux
background there is no evidence from randomized trials to support a strategy of lowering systolic blood pressure below to mm hg in persons with type diabetes mellitus we investigated whether therapy targeting normal systolic pressure i e mm hg reduces major cardiovascular events in participants with type diabetes at high risk for cardiovascular events methods a total of participants with type diabetes were randomly assigned to intensive therapy targeting a systolic pressure of less than mm hg or standard therapy targeting a systolic pressure of less than mm hg the primary composite outcome was nonfatal myocardial infarction nonfatal stroke or death from cardiovascular causes the mean follow up was years results after year the mean systolic blood pressure was mm hg in the intensive therapy group and mm hg in the standard therapy group the annual rate of the primary outcome was in the intensive therapy group and in the standard therapy group hazard ratio with intensive therapy confidence interval ci to p the annual rates of death from any cause were and in the two groups respectively hazard ratio ci to p the annual rates of stroke a prespecified secondary outcome were and in the two groups respectively hazard ratio ci to p serious adverse events attributed to antihypertensive treatment occurred in of the participants in the intensive therapy group and of the participants in the standard therapy group p conclusions in patients with type diabetes at high risk for cardiovascular events targeting a systolic blood pressure of less than mm hg as compared with less than mm hg did not reduce the rate of a composite outcome of fatal and nonfatal major cardiovascular events clinicaltrials number
motivation bayesian analysis through programs like beast drummond and rumbaut and mrbayes huelsenbeck et al provides a powerful method for reconstruction of evolutionary relationships one of the benefits of bayesian methods is that well founded estimates of uncertainty in models can be made available so for example not only the mean time of a most recent common ancestor tmrca is estimated but also the spread this distribution over model space is represented by a set of trees which can be rather large and difficult to interpret densitree is a tool that helps navigating these sets of trees results the main idea behind densitree is to draw all trees in the set transparently as a result areas where a lot of the trees agree in topology and branch lengths show up as highly colored areas while areas with little agreement show up as webs this makes it possible to quickly get an impression of properties of the tree set such as well supported clades distribution of tmrca and areas of topological uncertainty thus densitree provides a quick method for qualitative analysis of tree sets availability densitree is freely available from http compevol auckland ac nz software densitree the program is licensed under gpl and source code is available contact remco cs auckland ac bioinformatics
relations between users on social media sites often reflect a mixture of positive friendly and negative antagonistic interactions in contrast to the bulk of research on social networks that has focused almost exclusively on positive interpretations of links between people we study how the interplay between positive and negative relationships affects the structure of on line social networks we connect our analyses to theories of signed networks from social psychology we find that the classical theory of structural balance tends to capture certain common patterns of interaction but that it is also at odds with some of the fundamental phenomena we observe particularly related to the evolving directed nature of these on line networks we then develop an alternate theory of status that better explains the observed edge signs and provides insights into the underlying social mechanisms our work provides one of the first large scale evaluations of theories of signed networks using on line datasets as well as providing a perspective for reasoning about social sites
despite the yield of recent genome wide association gwa studies the identified variants explain only a small proportion of the heritability of most complex diseases this unexplained heritability could be partly due to gene environment gxe interactions or more complex pathways involving multiple genes and exposures this review provides a tutorial on the available epidemiological designs and statistical analysis approaches for studying specific gxe interactions and choosing the most appropriate methods i discuss the approaches that are being developed for studying entire pathways and available techniques for mining interactions in gwa data i also explore methods for marrying hypothesis driven pathway based approaches with agnostic studies
the emerging field of plasmonics has yielded methods for guiding and localizing light at the nanoscale well below the scale of the wavelength of light in free space now plasmonics researchers are turning their attention to photovoltaics where design approaches based on plasmonics can be used to improve absorption in photovoltaic devices permitting a considerable reduction in the physical thickness of solar photovoltaic absorber layers and yielding new options for solar cell design in this review we survey recent advances at the intersection of plasmonics and photovoltaics and offer an outlook on the future of solar cells based on principles
the system provides three dimensional visualization of gigabyte sized microscopy image stacks in real time on current laptops and desktops streamlines the online analysis measurement and proofreading of complicated image patterns by combining ergonomic functions for selecting a location in an image directly in space and for displaying biological measurements such as from fluorescent probes using the overlaid surface objects runs on all major computer platforms and can be enhanced by software plug ins to address specific biological problems to demonstrate this extensibility we built a based application neuron to reconstruct complex neuronal structures from high resolution brain images neuron can precisely digitize the morphology of a single neuron in a fruitfly brain in minutes with about a fold improvement in reliability and tenfold savings in time compared with other neuron reconstruction tools using neuron we demonstrate the feasibility of building a digital atlas of neurite tracts in the brain
background the most common application for the next generation sequencing technologies is resequencing where short reads from the genome of an individual are aligned to a reference genome sequence for the same species these mappings can then be used to identify genetic differences among individuals in a population and perhaps ultimately to explain phenotypic variation many algorithms capable of aligning short reads to the reference and determining differences between them have been reported much less has been reported on how to use these technologies to determine genetic differences among individuals of a species for which a reference sequence is not available which drastically limits the number of species that can easily benefit from these new technologies results we describe a computational pipeline called dial de novo identification of alleles for identifying single base substitutions between two closely related genomes without the help of a reference genome the method works even when the depth of coverage is insufficient for de novo assembly and it can be extended to determine small insertions deletions we evaluate the software s effectiveness using published roche sequence data from the genome of dr james watson to detect heterozygous positions and recent illumina data from orangutan in each case comparing our results to those from computational analysis that uses a reference genome assembly we also illustrate the use of dial to identify nucleotide differences among transcriptome sequences conclusions dial can be used for identification of nucleotide differences in species for which no reference sequence is available our main motivation is to use this tool to survey the genetic diversity of endangered species as the identified sequence differences can be used to design genotyping arrays to assist in the species management the dial source code is freely available at http www bx edu
software for visualizing sequence alignments and trees are essential tools for life scientists in this review we describe the major features and capabilities of a selection of stand alone and web based applications useful when investigating the function and evolution of a gene family these range from simple viewers to systems that provide sophisticated editing and analysis functions we conclude with a discussion of the challenges that these tools now face due to the flood of next generation sequence data and the increasingly complex network of bioinformatics sources
motivation intriguingly sequence analysis of genomes reveals that a large number of genes are unique to each organism the origin of these genes termed orfans is not known here we explore the origin of orfan genes by defining a simple measure called composition bias based on the deviation of the amino acid composition of a given sequence from the average composition of all proteins of a given genome results for a set of prokaryotic genomes we show that the amino acid composition bias of real proteins random proteins created by using the nucleotide frequencies of each genome and proteins translated from intergenic regions are distinct for orfans we observed a correlation between their composition bias and their relative evolutionary age recent orfan proteins have compositions more similar to those of random proteins while the compositions of more ancient orfan proteins are more similar to those of the set of all proteins of the organism this observation is consistent with an evolutionary scenario wherein orfan genes emerged and underwent a large number of random mutations and selection eventually adapting to the composition preference of their organism time
background a large effort to discover micrornas mirnas has been under way currently mirbase is their primary repository providing annotations of primary sequences precursors and probable genomic loci in many cases mirnas are identical or very similar between related or in some cases more distant species however mirbase focuses on those species for which mirnas have been directly confirmed secondly specific mirnas or their loci are sometimes not annotated even in well covered species we sought to address this problem by developing a computational system for automated mapping of mirnas within and across species given the sequence of a known mirna in one species it is relatively straightforward to determine likely loci of that mirna in other species our primary goal is not the discovery of novel mirnas but the mapping of validated mirnas in one species to their most likely orthologues in other species results we present mapmi a computational system for automated mirna mapping across and within species this method has a sensitivity of and a specificity of using the latest release of mirbase we obtained unannotated potential mirnas when mapmi was applied to all species in ensembl metazoa release and species from ensembl release conclusions the pipeline and an associated web server for mapping mirnas are freely available on http www ebi ac uk enright srv mapmi in addition precomputed mirna mappings of mirbase mirnas across a large number of species provided
bistability in signaling networks is frequently employed to promote stochastic switch like transitions between cellular differentiation states differentiation can also be triggered by antagonism of activators and repressors mediated by epigenetic processes that constitute regulatory circuits anchored to the chromosome their regulatory logic has remained unclear a reactiondiffusion model reveals that the same reaction mechanism can support both graded monostable and switch like bistable gene expression depending on whether recruited repressor proteins generate a single silencing gradient or two interacting gradients that flank a gene our experiments confirm that chromosomal recruitment of activator and repressor proteins permits a plastic form of control the stability of gene expression is determined by the spatial distribution of silencing nucleation sites along the chromosome the unveiled regulatory principles will help to understand the mechanisms of variegated gene expression to design synthetic genetic networks that combine transcriptional regulatory motifs with chromatin based epigenetic effects and to control differentiation
background micrornas have been discovered as important regulators of gene expression to identify the target genes of micrornas several databases and prediction algorithms have been developed only few experimentally confirmed microrna targets are available in databases many of the microrna targets stored in databases were derived from large scale experiments that are considered not very reliable we propose to use text mining of publication abstracts for extracting microrna gene associations including microrna target relations to complement current repositories results the microrna gene association database mirsel combines text mining results with existing databases and computational predictions text mining enables the reliable extraction of microrna gene and protein occurrences as well as their relationships from texts thereby we increased the number of human mouse and rat mirna gene associations by at least three fold as compared to e g tarbase a resource for mirna gene associations conclusions our database mirsel offers the currently largest collection of literature derived mirna gene associations comprehensive collections of mirna gene associations are important for the development of mirna target prediction tools and the analysis of regulatory networks mirsel is updated daily and can be queried using a web based interface via microrna identifiers gene and protein names pubmed queries as well as gene ontology go terms mirsel is freely available online at http services bio ifi lmu mirsel
in receptor ligand binding a question that generated considerable interest is whether the mechanism is induced fit or conformational selection this question is addressed here by a solvable model in which a receptor undergoes transitions between active and inactive forms the inactive form is favored while unbound but the active form is favored while a ligand is loosely bound as the active inactive transition rates increase the binding mechanism gradually shifts from conformational selection to induced fit the timescale of conformational transitions thus plays a crucial role in controlling mechanisms
background massively parallel sequencing of cdna is now an efficient route for generating enormous sequence collections that represent expressed genes this approach provides a valuable starting point for characterizing functional genetic variation in non model organisms especially where whole genome sequencing efforts are currently cost and time prohibitive the large and complex genomes of pines pinus spp have hindered the development of genomic resources despite the ecological and economical importance of the group while most genomic studies have focused on a single species p taeda genomic level resources for other pines are insufficiently developed to facilitate ecological genomic research lodgepole pine p contorta is an ecologically important foundation species of montane forest ecosystems and exhibits substantial adaptive variation across its range in western north america here we describe a sequencing study of expressed genes from p contorta including their assembly and annotation and their potential for molecular marker development to support population and association genetic studies results we obtained sequencing reads from a gs titanium pyrosequencer mean length base pairs a combination of reference based and de novo assemblies yielded contigs with reads remaining as singletons based on sequence similarity with known proteins these sequences represent approximately unique genes many of which are well covered by contig sequences this sequence collection also included a surprisingly large number of retrotransposon sequences suggesting that they are highly transcriptionally active in the tissues we sampled we located and characterized thousands of simple sequence repeats and single nucleotide polymorphisms as potential molecular markers in our assembled and annotated sequences high quality pcr primers were designed for a substantial number of the ssr loci and a large number of these were amplified successfully in initial screening conclusions this sequence collection represents a major genomic resource for p contorta and the large number of genetic markers characterized should contribute to future research in this and other pines our results illustrate the utility of next generation sequencing as a basis for marker development and population genomics in non species
we have developed a new strategy for de novo prediction of splice junctions in short read rna seq data suitable for detection of novel splicing events and chimeric transcripts when tested on mouse rna seq data splice events were predicted of which bridged between two regions separated by less than or equal to kb and connected two exons of the same refseq gene our method also reports genomic rearrangements such as insertions deletions
background evolutionarily divergent organisms often share developmental anatomies despite vast differences between their genome sequences the social amoebae dictyostelium discoideum and dictyostelium purpureum have similar developmental morphologies although their genomes are as divergent as those of man and jawed fish results here we show that the anatomical similarities are accompanied by extensive transcriptome conservation using rna sequencing we compared the abundance and developmental regulation of all the transcripts in the two species in both species most genes are developmentally regulated and the greatest expression changes occur during the transition from unicellularity to multicellularity the developmental regulation of transcription is highly conserved between orthologs in the two species in addition to timing of expression the level of mrna production is also conserved between orthologs and is consistent with the intuitive notion that transcript abundance correlates with the amount of protein required furthermore the conservation of transcriptomes extends to cell type specific expression conclusions these findings suggest that developmental programs are remarkably conserved at the transcriptome level considering the great evolutionary distance between the genomes moreover this transcriptional conservation may be responsible for the similar developmental anatomies of dictyostelium discoideum and purpureum
fusarium species are among the most important phytopathogenic and toxigenic fungi to understand the molecular underpinnings of pathogenicity in the genus fusarium we compared the genomes of three phenotypically diverse species fusarium graminearum fusarium verticillioides and fusarium oxysporum f sp lycopersici our analysis revealed lineage specific ls genomic regions in f oxysporum that include four entire chromosomes and account for more than one quarter of the genome ls regions are rich in transposons and genes with distinct evolutionary profiles but related to pathogenicity indicative of horizontal acquisition experimentally we demonstrate the transfer of two ls chromosomes between strains of f oxysporum converting a non pathogenic strain into a pathogen transfer of ls chromosomes between otherwise genetically isolated strains explains the polyphyletic origin of host specificity and the emergence of new pathogenic lineages in f oxysporum these findings put the evolution of fungal pathogenicity into a perspective
quantum mechanics provides a highly accurate description of a wide variety of physical systems however a demonstration that quantum mechanics applies equally to macroscopic mechanical systems has been a long standing challenge hindered by the difficulty of cooling a mechanical mode to its quantum ground state the temperatures required are typically far below those attainable with standard cryogenic methods so significant effort has been devoted to developing alternative cooling techniques once in the ground state quantum limited measurements must then be demonstrated here using conventional cryogenic refrigeration we show that we can cool a mechanical mode to its quantum ground state by using a microwave frequency mechanical oscillatora quantum drumcoupled to a quantum bit which is used to measure the quantum state of the resonator we further show that we can controllably create single quantum excitations phonons in the resonator thus taking the first steps to complete quantum control of a system
variation in transcriptional regulation is thought to be a major cause of phenotypic although widespread differences in gene expression among individuals of a species have been studies to examine the variability of transcription factor binding on a global scale have not been performed and thus the extent and underlying genetic basis of transcription factor binding diversity is unknown by mapping differences in transcription factor binding among individuals here we present the genetic basis of such variation on a genome wide scale whole genome binding profiles were determined using chromatin immunoprecipitation coupled with dna sequencing in pheromone treated cells of segregants of a cross between two highly diverged yeast strains and their parental lines we identified extensive binding variation among individuals and mapped underlying cis and trans acting loci responsible for such variation we showed that most transcription factor binding variation is cis linked and that many variations are associated with polymorphisms residing in the binding motifs of as well as those of several proposed cofactors we also identified two trans factors and that modulate binding to promoters of more than ten genes under factor treatment neither of these two genes was previously known to regulate and we suggest that they may be mediators of gene activity and phenotypic diversity binding strongly correlates with gene expression for more than genes indicating that binding variation is functional many of the variable bound genes are involved in cell wall organization and biogenesis overall these studies identified genetic regulators of molecular diversity among individuals and provide new insights into mechanisms of regulation
background the prediction of protein protein interactions is an important step toward the elucidation of protein functions and the understanding of the molecular mechanisms inside the cell while experimental methods for identifying these interactions remain costly and often noisy the increasing quantity of solved protein structures suggests that in silico methods to predict interactions between two protein structures will play an increasingly important role in screening candidate interacting pairs approaches using the knowledge of the structure are presumably more accurate than those based on sequence only approaches based on docking protein structures solve a variant of this problem but these methods remain very computationally intensive and will not scale in the near future to the detection of interactions at the level of an interactome involving millions of candidate pairs of proteins results here we describe a computational method to predict efficiently in silico whether two protein structures interact this yes no question is presumably easier to answer than the standard protein docking question how do these two protein structures interact our approach is to discriminate between interacting and non interacting protein pairs using a statistical pattern recognition method known as a support vector machine svm we demonstrate that our structure based method performs well on this task and scales well to the size of an interactome conclusions the use of structure information for the prediction of protein interaction yields significantly better performance than other sequence based methods among structure based classifiers the svm algorithm combined with the metric learning pairwise kernel and the mammoth kernel performs best in experiments
differences in gene expression may play a major role in speciation and phenotypic diversity we examined genome wide differences in transcription factor tf binding in several humans and a single chimpanzee by using chromatin immunoprecipitation followed by sequencing the binding sites of rna polymerase ii polii and a key regulator of immune responses nuclear factor b were mapped in lymphoblastoid cell lines and and of the respective binding regions were found to differ between individuals binding differences were frequently associated with single nucleotide polymorphisms and genomic structural variants and these differences were often correlated with differences in gene expression suggesting functional consequences of binding variation furthermore comparing polii binding between humans and chimpanzee suggests extensive divergence in tf binding our results indicate that many differences in individuals and species occur at the level of tf binding and they provide insight into the genetic events responsible for differences
the extent to which variation in chromatin structure and transcription factor binding may influence gene expression and thus underlie or contribute to variation in phenotype is unknown to address this question we have cataloged both individual to individual variation and differences between homologous chromosomes within the same individual allele specific variation in chromatin structure and transcription factor binding in lymphoblastoid cells derived from individuals of geographically diverse ancestry ten percent of active chromatin sites were individual specific and a similar proportion were allele specific both individual specific and allele specific sites were commonly transmitted from parent to child suggesting that they are heritable features of the human genome our study shows that heritable chromatin status and transcription factor binding differs on the basis of genetic variation and may underlie phenotypic variation humans
in recent years there has been an explosion in the availability of publicly accessible chemical information including chemical structures of small molecules structure derived properties and associated biological activities in a variety of assays these data sources present us with a significant opportunity to develop and apply computational tools to extract and understand the underlying structure activity relationships furthermore by integrating chemical data sources with biological information protein structure gene expression and so on we can attempt to build up a holistic view of the effects of small molecules in biological systems equally important is the ability for non experts to access and utilize state of the art cheminformatics method and models in this review we present recent developments in cheminformatics methodologies and infrastructure that provide a robust distributed approach to mining large and complex chemical datasets in the area of methodology development we highlight recent work on characterizing structure activity landscapes quantitative structure activity relationship qsar model domain applicability and the use of chemical similarity in text mining in the area of infrastructure we discuss a distributed web services framework that allows easy deployment and uniform access to computational statistics cheminformatics and computational chemistry methods data and models we also discuss the development of pubchem derived databases and highlight techniques that allow us to scale the infrastructure to extremely large compound collections by use of distributed processing on grids given that the above work is applicable to arbitrary types of cheminformatics problems we also present some case studies related to virtual screening for anti malarials and predictions of anti activity
finding the most promising genes among large lists of candidate genes has been defined as the gene prioritization problem it is a recurrent problem in genetics in which genetic conditions are reported to be associated with chromosomal regions in the last decade several different computational approaches have been developed to tackle this challenging task in this study we review computational solutions for human gene prioritization that are freely accessible as web tools and illustrate their differences we summarize the various biological problems to which they have been successfully applied ultimately we describe several research directions that could increase the quality and applicability of the tools in addition we developed a website http www esat kuleuven be gpp containing detailed information about these and other tools which is regularly updated this review and the associated website constitute together a guide to help users select a gene prioritization strategy that suits best needs
understanding the dynamics of eukaryotic transcriptome is essential for studying the complexity of transcriptional regulation and its impact on phenotype however comprehensive studies of transcriptomes at single base resolution are rare even for modern organisms and lacking for rice here we present the first transcriptome atlas for eight organs of cultivated rice using high throughput paired end rna seq we unambiguously detected transcripts expressing at an extremely low level as well as a substantial number of novel transcripts exons and untranslated regions an analysis of alternative splicing in the rice transcriptome revealed that alternative cis splicing occurred in of all rice genes this is far more than previously reported in addition we also identified putative chimeric transcripts that seem to be produced by trans splicing indicating that transcript fusion events are more common than expected in depth analysis revealed a multitude of fusion transcripts that might be by products of alternative splicing validation and chimeric transcript structural analysis provided evidence that some of these transcripts are likely to be functional in the cell taken together our data provide extensive evidence that transcriptional regulation in rice is vastly more complex than believed
analogies have had and continue to have an important role in the development of theoretical physics they may start from similarities of physical concepts followed by similarities in the mathematical formalization or it may be a purely mathematical aspect to suggest the development of analogous physical concepts more often a subtle non obvious interplay between these levels is involved in this paper i will discuss two cases sufficiently intricate to illustrate some ways of how analogies work the first topic is the introduction of spontaneous symmetry breaking in particle physics the second one is the use of the renormalization group in the theory of critical phenomena and its interpretation
biologists have long used model organisms to study human diseases particularly when the model bears a close resemblance to the disease we present a method that quantitatively and systematically identifies nonobvious equivalences between mutant phenotypes in different species based on overlapping sets of orthologous genes from human mouse yeast worm and plant gene phenotype associations these orthologous phenotypes or phenologs predict unique genes associated with diseases our method suggests a yeast model for angiogenesis defects a worm model for breast cancer mouse models of autism and a plant model for the neural crest defects associated with waardenburg syndrome among others using these models we show that regulates angiogenesis and that is a likely waardenburg gene phenologs reveal functionally coherent evolutionarily conserved gene networksmany predating the plant animal divergencecapable of identifying candidate genes
numerous methods have been developed for inferring gene regulatory networks from expression data however both their absolute and comparative performance remain poorly understood in this paper we introduce a framework for critical performance assessment of methods for gene network inference we present an in silico benchmark suite that we provided as a blinded community wide challenge within the context of the dream dialogue on reverse engineering assessment and methods project we assess the performance of gene network inference methods which have been applied independently by participating teams performance profiling reveals that current inference methods are affected to various degrees by different types of systematic prediction errors in particular all but the best performing method failed to accurately infer multiple regulatory inputs combinatorial regulation of genes the results of this community wide experiment show that reliable network inference from gene expression data remains an unsolved problem and they indicate potential ways of network improvements
changes in gene expression play an important role in evolution yet the molecular mechanisms underlying regulatory evolution are poorly understood here we compare genome wide binding of the six transcription factors that initiate segmentation along the anterior posterior axis in embryos of two closely related species drosophila melanogaster and drosophila yakuba where we observe binding by a factor in one species we almost always observe binding by that factor to the orthologous sequence in the other species levels of binding however vary considerably the magnitude and direction of the interspecies differences in binding levels of all six factors are strongly correlated suggesting a role for chromatin or other factor independent forces in mediating the divergence of transcription factor binding nonetheless factor specific quantitative variation in binding is common and we show that it is driven to a large extent by the gain and loss of cognate recognition sequences for the given factor we find only a weak correlation between binding variation and regulatory function these data provide the first genome wide picture of how modest levels of sequence divergence between highly morphologically similar species affect a system of coordinately acting transcription factors during animal development and highlight the dominant role of quantitative variation in transcription factor binding over short distances
with the exception of neanderthals from which dna sequences of numerous individuals have now been the number and genetic relationships of other hominin lineages are largely unknown here we report a complete mitochondrial mt dna sequence retrieved from a bone excavated in in denisova cave in the altai mountains in southern siberia it represents a hitherto unknown type of hominin mtdna that shares a common ancestor with anatomically modern human and neanderthal mtdnas about million years ago this indicates that it derives from a hominin migration out of africa distinct from that of the ancestors of neanderthals and of modern humans the stratigraphy of the cave where the bone was found suggests that the denisova hominin lived close in time and space with neanderthals as well as modern
although pioneered by human geneticists as a potential solution to the challenging problem of finding the genetic basis of common human diseases genome wide association gwa studies have owing to advances in genotyping and sequencing technology become an obvious general approach for studying the genetics of natural variation and traits of agricultural importance they are particularly useful when inbred lines are available because once these lines have been genotyped they can be phenotyped multiple times making it possible as well as extremely cost effective to study many different traits in many different environments while replicating the phenotypic measurements to reduce environmental noise here we demonstrate the power of this approach by carrying out a gwa study of phenotypes in arabidopsis thaliana a widely distributed predominantly self fertilizing model plant known to harbour considerable genetic variation for many adaptively important traits our results are dramatically different from those of human gwa studies in that we identify many common alleles of major effect but they are also in many cases harder to interpret because confounding by complex genetics and population structure make it difficult to distinguish true associations from false however a priori candidates are significantly over represented among these associations as well making many of them excellent candidates for follow up experiments our study demonstrates the feasibility of gwa studies in a thaliana and suggests that the approach will be appropriate for many organisms
background molecular studies of microbial diversity have provided many insights into the bacterial communities inhabiting the human body and the environment a common first step in such studies is a survey of conserved marker genes primarily rrna to characterize the taxonomic composition and diversity of these communities to date however there exists significant variability in analysis methods employed in these studies results here we provide a critical assessment of current analysis methodologies that cluster sequences into operational taxonomic units otus and demonstrate that small changes in algorithm parameters can lead to significantly varying results our analysis provides strong evidence that the species level diversity estimates produced using common otu methodologies are inflated due to overly stringent parameter choices we further describe an example of how semi supervised clustering can produce otus that are more robust to changes in algorithm parameters conclusions our results highlight the need for systematic and open evaluation of data analysis methodologies especially as targeted rrna diversity studies are increasingly relying on high throughput sequencing technologies all data and results from our study are available through the jgi fames website http fames jgi org
the nematode caenorhabditis elegans ages and dies in a few weeks but humans can live for years or more assuming that the ancestor we share with nematodes aged rapidly this means that over evolutionary time mutations have increased lifespan more than fold which genes can extend lifespan can we augment their activities and live even longer after centuries of wistful poetry and wild imagination we are now getting answers often unexpected ones to these questions
genome wide association studies gwas have achieved great success identifying common genetic variants associated with common human diseases however to date the massive amounts of data generated from gwas have not been maximally leveraged and integrated with other types of data to identify associations beyond those associations that meet the stringent genome wide significance threshold here we present a novel approach that leverages information from genetics of gene expression studies to identify biological pathways enriched for expression associated genetic loci associated with disease in publicly available gwas results specifically we first identify snps in population based human cohorts that associate with the expression of genes esnps in the metabolically active tissues liver subcutaneous adipose and omental adipose we then use this functionally annotated set of snps to investigate pathways enriched for esnps associated with disease in publicly available gwas data as an example we tested pathways from the kyoto encylopedia of genes and genomes kegg database and identified pathways enriched for genes corresponding to esnps that show evidence of association with type diabetes in the wellcome trust case control consortium wtccc gwas we then replicated these findings in the diabetes genetics replication and meta analysis diagram study many of the pathways identified have been proposed as important candidate pathways for including the calcium signaling pathway the ppar signaling pathway and tgf beta signaling importantly we identified other pathways not previously associated with including the tight junction complement and coagulation pathway and antigen processing and presentation pathway the integration of pathways and esnps provides putative functional bridges between gwas and candidate genes or pathways thus serving as a potential powerful approach to identifying biological mechanisms underlying findings
although they have become a widely used experimental technique for identifying differentially expressed de genes dna microarrays are notorious for generating noisy data a common strategy for mitigating the effects of noise is to perform many experimental replicates this approach is often costly and sometimes impossible given limited resources thus analytical methods are needed which increase accuracy at no additional cost one inexpensive source of microarray replicates comes from prior work to date data from hundreds of thousands of microarray experiments are in the public domain although these data assay a wide range of conditions they cannot be used directly to inform any particular experiment and are thus ignored by most de gene methods we present the svd augmented gene expression analysis tool sagat a mathematically principled data driven approach for identifying de genes sagat increases the power of a microarray experiment by using observed coexpression relationships from publicly available microarray datasets to reduce uncertainty in individual genes expression measurements we tested the method on three well replicated human microarray datasets and demonstrate that use of sagat increased effective sample sizes by as many as arrays we applied sagat to unpublished data from a microarray study investigating transcriptional responses to insulin resistance resulting in a increase in the number of significant genes detected we evaluated of these genes experimentally using qpcr confirming the directions of expression change for all and statistical significance for three use of sagat revealed coherent biological changes in three pathways inflammation differentiation and fatty acid synthesis furthering our molecular understanding of a type diabetes risk factor we envision sagat as a means to maximize the potential for biological discovery from subtle transcriptional responses and we provide it as a freely available software package that is immediately applicable to any human study
background recently supervised learning methods have been exploited to reconstruct gene regulatory networks from gene expression data the reconstruction of a network is modeled as a binary classification problem for each pair of genes a statistical classifier is trained to recognize the relationships between the activation profiles of gene pairs this approach has been proven to outperform previous unsupervised methods however the supervised approach raises open questions in particular although known regulatory connections can safely be assumed to be positive training examples obtaining negative examples is not straightforward because definite knowledge is typically not available that a given pair of genes do not interact results a recent advance in research on data mining is a method capable of learning a classifier from only positive and unlabeled examples that does not need labeled negative examples applied to the reconstruction of gene regulatory networks we show that this method significantly outperforms the current state of the art of machine learning methods we assess the new method using both simulated and experimental data and obtain major performance improvement conclusions compared to unsupervised methods for gene network inference supervised methods are potentially more accurate but for training they need a complete set of known regulatory connections a supervised method that can be trained using only positive and unlabeled data as presented in this paper is especially beneficial for the task of inferring gene regulatory networks because only an incomplete set of known regulatory connections is available in public databases such as regulondb trrd kegg transfac ipa
multiple protein sequence alignment methods are central to many applications in molecular biology these methods are typically assessed on benchmark datasets including balibase oxbench prefab and sabmark which are important to biologists in making informed choices between programs in this article annotations of domain homology and secondary structure are used to define new measures of alignment quality and are used to make the first systematic independent evaluation of these benchmarks these measures indicate sensitivity and specificity while avoiding the ambiguous residue correspondences and arbitrary distance cutoffs inherent to structural superpositions alignments by selected methods that indicate high confidence columns align m dialign t fsa and muscle are also assessed fold space coverage and effective benchmark database sizes are estimated by reference to domain annotations and significant redundancy is found in all benchmarks except sabmark questionable alignments are found in all benchmarks especially in balibase where of sequences have unknown structure of columns contain different folds according to superfamily and of core block columns have conflicting secondary structure according to dssp a careful analysis of current protein multiple alignment benchmarks calls into question their ability to determine reliable rankings
background small molecules are of increasing interest for bioinformatics in areas such as metabolomics and drug discovery the recent release of large open access chemistry databases generates a demand for flexible tools to process them and discover new knowledge to freely support open science based on these data resources it is desirable for the processing tools to be open source and available for everyone results here we describe a novel combination of the workflow engine taverna and the cheminformatics library chemistry development kit cdk resulting in a open source workflow solution for cheminformatics we have implemented more than different workers to handle specific cheminformatics tasks we describe the applications of cdk taverna in various usage scenarios conclusions the combination of the workflow engine taverna and the chemistry development kit provides the first open source cheminformatics workflow solution for the biosciences with the taverna community working towards a more powerful workflow engine and a more user friendly user interface cdk taverna has the potential to become a free alternative to existing proprietary tools
the capacity to collect fingerprints of individuals in online media has revolutionized the way researchers explore human society social systems can be seen as a non linear superposition of a multitude of complex social networks where nodes represent individuals and links capture a variety of different social relations much emphasis has been put on the network topology of social interactions however the multi dimensional nature of these interactions has largely been ignored in empirical studies mostly because of lack of data here for the first time we analyze a complete multi relational large social network of a society consisting of the odd players of a massive multiplayer online game we extract networks of six different types of one to one interactions between the players three of them carry a positive connotation friendship communication trade three a negative enmity armed aggression punishment we first analyze these types of networks as separate entities and find that negative interactions differ from positive interactions by their lower reciprocity weaker clustering and fatter tail degree distribution we then proceed to explore how the inter dependence of different network types determines the organization of the social system in particular we study correlations and overlap between different types of links and demonstrate the tendency of individuals to play different roles in different networks as a demonstration of the power of the approach we present the first empirical large scale verification of the long standing structural balance theory by focusing on the specific multiplex network of friendship and relations
summary despite the importance of using the semantic distance to improve the performance of conventional expression based clustering there are few freely available software that provides a clustering algorithm using the ontology based semantic distances as prior knowledge here we present the sicago semi supervised cluster analysis using semantic distance between gene pairs in gene ontology system that helps to discover the groups of genes more effectively using prior knowledge extracted from gene ontology availability http ai cau ac kr sicago html contact dwkim cau ac bioinformatics
with advances in high throughput techniques the volume of data generated has resulted in the creation of a plethora of resources for the cancer research community however a key factor in the utility sustainability and future use of a novel resource lies in its ability to allow for data sharing and to be interoperable with major international cancer research efforts this article will introduce some of these efforts the interoperable cancer data mining resources and repositories from a user perspective some of the considerations to be addressed when building interoperable sustainable cancer resources will be discussed with case studies hoping this will prove useful for researchers designing their own cancer bib
background this study reviewed twitter status updates mentioning antibiotic s to determine overarching categories and explore evidence of misunderstanding or misuse of antibiotics methods one thousand twitter status updates mentioning antibiotic s were randomly selected for content analysis and categorization to explore cases of potential misunderstanding or misuse these status updates were mined for co occurrence of the following terms cold antibiotic s extra antibiotic s flu antibiotic s leftover antibiotic s and share antibiotic s and reviewed to confirm evidence of misuse or misunderstanding results of the status updates were categorized into groups general use n advice information n side effects negative reactions n diagnosis n resistance n misunderstanding and or misuse n positive reactions n animals n other n wanting needing n and cost n cases of misunderstanding or abuse were identified for the following combinations flu antibiotic s n cold antibiotic s n leftover antibiotic s n share antibiotic s n and extra antibiotic s n conclusion social media sites offer means of health information sharing further study is warranted to explore how such networks may provide a venue to identify misuse or misunderstanding of antibiotics promote positive behavior change disseminate valid information and explore how such tools can be used to gather real time data
gr the regulation of gene expression is critical for organismal function and is an important source of phenotypic diversity between species understanding the genetic and molecular mechanisms responsible for regulatory divergence is therefore expected to provide insight into evolutionary change using deep sequencing we quantified total and allele specific mrna expression levels genome wide in two closely related species and and their f hybrids we show that of expressed genes have divergent expression between species and that and regulatory divergence affects and of expressed genes respectively with of genes showing evidence of both this is a relatively larger contribution of regulatory divergence than was expected based on prior studies and may result from the unique demographic history of genes with antagonistic and regulatory changes were more likely to be misexpressed in hybrids consistent with the idea that such regulatory changes contribute to hybrid incompatibilities in addition regulatory differences contributed more to divergent expression of genes that showed additive rather than nonadditive inheritance a correlation between sequence similarity and the conservation of regulatory activity was also observed that appears to be a general feature of regulatory evolution finally we examined regulatory divergence that may have contributed to the evolution of a specific traitdivergent feeding behavior in overall this study illustrates the power of mrna sequencing for investigating regulatory evolution provides novel insight into the evolution of gene expression in and reveals general trends that are likely to extend to species
structural variation sv is a rich source of genetic diversity in mammals but due to the challenges associated with mapping sv in complex genomes basic questions regarding their genomic distribution and mechanistic origins remain unanswered we have developed an algorithm hydra to localize sv breakpoints by paired end mapping and a general approach for the genome wide assembly and interpretation of breakpoint sequences we applied these methods to two inbred mouse strains and dba we demonstrate that hydra accurately maps diverse classes of sv including those involving repetitive elements such as transposons and segmental duplications however our analysis of the reference strain shows that incomplete reference genome assemblies are a major source of noise we report svs between the two strains more than two thirds of which are due to transposon insertions of the remainder are deletions relative to the reference are insertions of unlinked dna are tandem duplications and are inversions to investigate the origins of sv we characterized breakpoint sequences at single nucleotide resolution we find that approximately of non transposon svs have complex breakpoint patterns consistent with template switching during dna replication or repair and that this process appears to preferentially generate certain classes of complex variants moreover we find that svs are significantly enriched in regions of segmental duplication but that this effect is largely independent of dna sequence homology and thus cannot be explained by non allelic homologous recombination nahr alone this result suggests that the genetic instability of such regions is often the cause rather than the consequence of duplicated architecture
the zebra finch is an important model organism in several with unique relevance to human like other songbirds the zebra finch communicates through learned vocalizations an ability otherwise documented only in humans and a few other animals and lacking in the only bird with a sequenced genome until here we present a structural functional and comparative analysis of the genome sequence of the zebra finch taeniopygia guttata which is a songbird belonging to the large avian order we find that the overall structures of the genomes are similar in zebra finch and chicken but they differ in many intrachromosomal rearrangements lineage specific gene family expansions the number of long terminal repeat based retrotransposons and mechanisms of sex chromosome dosage compensation we show that song behaviour engages gene regulatory networks in the zebra finch brain altering the expression of long non coding rnas micrornas transcription factors and their targets we also show evidence for rapid molecular evolution in the songbird lineage of genes that are regulated during song experience these results indicate an active involvement of the genome in neural processes underlying vocal communication and identify potential genetic substrates for the evolution and regulation of behaviour
copy number variants cnvs account for a major proportion of human genetic polymorphism and have been predicted to have an important role in genetic susceptibility to common disease to address this we undertook a large direct genome wide study of association between cnvs and eight common human diseases using a purpose designed array we typed approximately individuals into distinct copy number classes at polymorphic cnvs including an estimated approximately of all common cnvs larger than base pairs we identified several biological artefacts that lead to false positive associations including systematic cnv differences between dnas derived from blood and cell lines association testing and follow up replication analyses confirmed three loci where cnvs were associated with disease irgm for crohn s disease hla for crohn s disease rheumatoid arthritis and type diabetes and for type diabetes although in each case the locus had previously been identified in single nucleotide polymorphism snp based studies reflecting our observation that most common cnvs that are well typed on our array are well tagged by snps and so have been indirectly explored through snp studies we conclude that common cnvs that can be typed on existing platforms are unlikely to contribute greatly to the genetic basis of common diseases
despite our rapidly growing knowledge about the human genome we do not know all of the genes required for some of the most basic functions of life to start to fill this gap we developed a high throughput phenotypic screening platform combining potent gene silencing by rna interference time lapse microscopy and computational image processing we carried out a genome wide phenotypic profiling of each of the approximately human protein coding genes by two day live imaging of fluorescently labelled chromosomes phenotypes were scored quantitatively by computational image processing which allowed us to identify hundreds of human genes involved in diverse biological functions including cell division migration and survival as part of the mitocheck consortium this study provides an in depth analysis of cell division phenotypes and makes the entire high content data set available as a resource to community
in recent years social media has become ubiquitous and important for social networking and content sharing and yet the content that is generated from these websites remains largely untapped in this paper we demonstrate how social media content can be used to predict real world outcomes in particular we use the chatter from twitter com to forecast box office revenues for movies we show that a simple model built from the rate at which tweets are created about particular topics can outperform market based predictors we further demonstrate how sentiments extracted from twitter can be further utilized to improve the forecasting power of media
thalamic inputs strongly drive neurons in the primary visual cortex even though these neurons constitute only of the synapses on layer spiny stellate simple cells we modeled the feedforward excitatory and inhibitory inputs to these cells based on in vivo recordings in cats and we found that the reliability of spike transmission increased steeply between and synchronous thalamic inputs in a time window of milliseconds when the reliability per spike was most energetically efficient the optimal range of synchronous inputs was influenced by the balance of background excitation and inhibition in the cortex which could gate the flow of information into the cortex ensuring reliable transmission by spike synchrony in small populations of neurons may be a general principle of cortical science
background an important objective of dna microarray based gene expression experimentation is determining inter relationships that exist between differentially expressed genes and biological processes molecular functions cellular components signaling pathways physiologic processes and diseases results here we describe genemesh a web based program that facilitates analysis of dna microarray gene expression data genemesh relates genes in a query set to categories available in the medical subject headings mesh hierarchical index the interface enables hypothesis driven relational analysis to a specific mesh subcategory e g cardiovascular system genetic processes immune system diseases etc or unbiased relational analysis to broader mesh categories e g anatomy biological sciences disease etc genes found associated with a given mesh category are dynamically linked to facilitate tabular and graphical depiction of entrez gene information gene ontology information kegg metabolic pathway diagrams and intermolecular interaction information expression intensity values of groups of genes that cluster in relation to a given mesh category gene ontology or pathway can be displayed as heat maps of z score normalized values genemesh operates on gene expression data derived from a number of commercial microarray platforms including affymetrix agilent and illumina conclusions genemesh is a versatile web based tool for testing and developing new hypotheses through relating genes in a query set e g differentially expressed genes from a dna microarray experiment to descriptors making up the hierarchical structure of the national library of medicine controlled vocabulary thesaurus mesh the system further enhances the discovery process by providing links between sets of genes associated with a given mesh category to a rich set of html linked tabular and graphic information including entrez gene summaries gene ontologies intermolecular interactions overlays of genes onto kegg pathway diagrams and heatmaps of expression intensity values genemesh is freely available online at http proteogenomics musc genemesh
rna transcripts are subject to posttranscriptional gene regulation involving hundreds of rna binding proteins rbps and microrna containing ribonucleoprotein complexes mirnps expressed ina cell type dependent fashion we developed a cell based crosslinking approach to determine at high resolution and transcriptome wide the binding sites of cellular rbps and mirnps the crosslinked sites are revealed by thymidine to cytidine transitions inthe cdnas prepared from immunopurified rnps of thiouridine treated cells we determined the binding sites and regulatory consequences for several intensely studied rbps and mirnps qki ago and c our study revealed that these factors bind thousands of sites containing defined sequence motifs and have distinct preferences for exonic versus intronic or coding versus untranslated transcript regions the precise mapping of binding sites across the transcriptome will be critical to the interpretation of the rapidly emerging data on genetic variation between individuals and how these variations contribute to complex genetic diseases par clip is a transcriptome wide crosslinking method for rna binding proteins rbp it is based on incorporation of photoactivatable nucleoside analogs into nascent rna characteristic sequence transitions in the prepared cdna reveal the precise binding site we deduced binding motifs and preferences for different families
background elucidating protein protein interactions ppis is essential to constructing protein interaction networks and facilitating our understanding of the general principles of biological systems previous studies have revealed that interacting protein pairs can be predicted by their primary structure most of these approaches have achieved satisfactory performance on datasets comprising equal number of interacting and non interacting protein pairs however this ratio is highly unbalanced in nature and these techniques have not been comprehensively evaluated with respect to the effect of the large number of non interacting pairs in realistic datasets moreover since highly unbalanced distributions usually lead to large datasets more efficient predictors are desired when handling such challenging tasks results this study presents a method for ppi prediction based only on sequence information which contributes in three aspects first we propose a probability based mechanism for transforming protein sequences into feature vectors second the proposed predictor is designed with an efficient classification algorithm where the efficiency is essential for handling highly unbalanced datasets third the proposed ppi predictor is assessed with several unbalanced datasets with different positive to negative ratios from to this analysis provides solid evidence that the degree of dataset imbalance is important to ppi predictors conclusions dealing with data imbalance is a key issue in ppi prediction since there are far fewer interacting protein pairs than non interacting ones this article provides a comprehensive study on this issue and develops a practical tool that achieves both good prediction performance and efficiency using only protein information
computational chemistry in particular virtual screening can provide valuable contributions in hit and lead compound discovery numerous software tools have been developed for this purpose however despite the applicability of virtual screening technology being well established it seems that there are relatively few examples of drug discovery projects in which virtual screening has been the key contributor has virtual screening reached its peak if not what aspects are limiting its potential at present and how can significant progress be made in future
accumulating evidence implicates heterogeneity within cancer cell populations in the response to stressful exposures including drug treatments while modeling the acute response to various anticancer agents in drug sensitive human tumor cell lines we consistently detected a small subpopulation of reversibly drug tolerant cells these cells demonstrate fold reduced drug sensitivity and maintain viability via engagement of igf receptor signaling and an altered chromatin state that requires the histone demethylase this drug tolerant phenotype is transiently acquired and relinquished at low frequency by individual cells within the population implicating the dynamic regulation of phenotypic heterogeneity in drug tolerance the drug tolerant subpopulation can be selectively ablated by treatment with igf receptor inhibitors or chromatin modifying agents potentially yielding a therapeutic opportunity together these findings suggest that cancer cell populations employa dynamic survival strategy in which individual cells transiently assume a reversibly drug tolerant state to protect the population from eradication by potentially lethal exposures human cancer cell lines harbor a subpopulation of drug tolerant cells the drug tolerant state can emerge de novo and is transiently maintained the emergence of drug tolerance requires igf signaling drug tolerance involves distinct chromatin states that can disrupted
summary large sets of data such as expression profiles from many samples require analytic tools to reduce their complexity the iterative signature algorithm isa is a biclustering algorithm it was designed to decompose a large set of data into so called modules in the context of gene expression data these modules consist of subsets of genes that exhibit a coherent expression profile only over a subset of microarray experiments genes and arrays may be attributed to multiple modules and the level of required coherence can be varied resulting in different resolutions of the modular mapping in this short note we introduce two bioconductor software packages written in gnu r the package includes an optimized implementation of the isa and the eisa package provides a convenient interface to run the isa visualize its output and put the biclusters into biological context potential users of these packages are all r and bioconductor users dealing with tabular e g gene expression data availability http www unil ch cbg isa contact sven bergmann unil bioinformatics
building on abstract reference models the open geospatial consor tium ogc has established standards for storing discovering and pro cessing geographical information these standards act as basis for the im plementation of speci c services and spatial data infrastructures sdi research on geo semantics plays an increasing role to support complex queries and retrieval across heterogeneous information sources as well as for service orchestration semantic translation and on the y integra tion so far this research targets individual solutions or focuses on the semantic web leaving the integration into sdi aside what is missing is a shared and transparent semantic enablement layer for spatial data infrastructures which also integrates reasoning services known from the semantic web instead of developing new semantically enabled services from scratch we propose to create pro les of existing services that im plement a transparent mapping between the ogc and the semantic web world finally we point out how to combine sdi with data
background the alignment of biological sequences is of chief importance to most evolutionary and comparative genomics studies yet the two main approaches used to assess alignment accuracy have flaws reference alignments are derived from the biased sample of proteins with known structure and simulated data lack realism results here we introduce tree based tests of alignment accuracy which not only use large and representative samples of real biological data but also enable the evaluation of the effect of gap placement on phylogenetic inference we show that i the current belief that consistency based alignments outperform scoring matrix based alignments is misguided ii gaps carry substantial phylogenetic signal but are poorly exploited by most alignment and tree building programs iii even so excluding gaps and variable regions is detrimental iv disagreement among alignment programs says little about the accuracy of resulting trees conclusions this study provides the broad community relying on sequence alignment with important practical recommendations sets superior standards for assessing alignment accuracy and paves the way for the development of phylogenetic inference methods of significantly resolution
background gene duplication is considered a major driving force for evolution of genetic novelty thereby facilitating functional divergence and organismal diversity including the process of speciation animals fungi and plants are major eukaryotic kingdoms and the divergences between them are some of the most significant evolutionary events although gene duplications in each lineage have been studied extensively in various contexts the extent of gene duplication prior to the split of plants and animals fungi is not clear results here we have studied gene duplications in early eukaryotes by phylogenetic relative dating we have reconstructed gene families with one or more orthogroups with members from both animals fungi and plants by using two different clustering strategies extensive phylogenetic analyses of the gene families show that among nearly orthogroups identified at least of them still retain duplication that occurred before the divergence of the three kingdoms we further found evidence that such duplications were also detected in some highly divergent protists suggesting that these duplication events occurred in the ancestors of most major extant eukaryotic groups conclusions our phylogenetic analyses show that numerous gene duplications happened at the early stage of eukaryotic evolution probably before the separation of known major eukaryotic lineages we discuss the implication of our results in the contexts of different models of eukaryotic phylogeny one possible explanation for the large number of gene duplication events is one or more large scale duplications possibly whole genome or segmental duplication s which provides a genomic basis for the successful radiation of eukaryotes
motivation a major challenge in utilizing microarray technologies to measure nucleic acid abundances is normalization the goal of which is to separate biologically meaningful signal from other confounding sources of signal often due to unavoidable technical factors it is intuitively clear that true biological signal and confounding factors need to be simultaneously considered when performing normalization however the most popular normalization approaches do not utilize what is known about the study both in terms of the biological variables of interest and the known technical factors in the study such as batch or array processing date results we show here that failing to include all study specific biological and technical variables when performing normalization leads to biased downstream analyses we propose a general normalization framework that fits a study specific model employing every known variable that is relevant to the expression study the proposed method is generally applicable to the full range of existing probe designs as well as to both single channel and dual channel arrays we show through real and simulated examples that the method has favorable operating characteristics in comparison to some of the most highly used normalization methods availability an r package called snm implementing the methodology will be made available from bioconductor http bioconductor org contact jstorey princeton edusupplementary information supplementary data are available at online
background in recent years the demand for computational power in computational biology has increased due to rapidly growing data sets from microarray and other high throughput technologies this demand is likely to increase standard algorithms for analyzing data such as cluster algorithms need to be parallelized for fast processing unfortunately most approaches for parallelizing algorithms largely rely on network communication protocols connecting and requiring multiple computers one answer to this problem is to utilize the intrinsic capabilities in current multi core hardware to distribute the tasks among the different cores of one computer results we introduce a multi core parallelization of the k means and k modes cluster algorithms based on the design principles of transactional memory for clustering gene expression microarray type data and categorial snp data our new shared memory parallel algorithms show to be highly efficient we demonstrate their computational power and show their utility in cluster stability and sensitivity analysis employing repeated runs with slightly changed parameters computation speed of our java based algorithm was increased by a factor of for large data sets while preserving computational accuracy compared to single core implementations and a recently published network based parallelization conclusions most desktop computers and even notebooks provide at least dual core processors our multi core algorithms show that using modern algorithmic concepts parallelization makes it possible to perform even such laborious tasks as cluster sensitivity and cluster number estimation on the computer
gr clustering of multiple transcription factor binding sites tfbss for the same transcription factor tf is a common feature of cis regulatory modules in invertebrate animals but the occurrence of such homotypic clusters of tfbss hcts in the human genome has remained largely unknown to explore whether hcts are also common in human and other vertebrates we used known binding motifs for vertebrate tfs and a hidden markov modelbased approach to detect hcts in the human mouse chicken and fugu genomes and examined their association with cis regulatory modules we found that evolutionarily conserved hcts occupy nearly of the human genome with experimental evidence for individual tfs supporting their binding to predicted hcts more than half of the promoters of human genes contain hcts with a distribution around the transcription start site in agreement with the experimental data from the encode project in addition almost half of the experimentally validated developmental enhancers contain them as wella number more than fold larger than expected by chance we also found evidence of negative selection acting on tfbss within hcts as the conservation of tfbss is stronger than the conservation of sequences separating them the important role of hcts as components of developmental enhancers is additionally supported by a strong correlation between hcts and the binding of the enhancer associated coactivator protein also known as experimental validation of hct containing elements in both zebrafish and mouse suggest that hcts could be used to predict both the presence of enhancers and their tissue specificity and are thus a feature that can be effectively used in deciphering the gene regulatory code in conclusion our results indicate that hcts are a pervasive feature of human cis regulatory modules and suggest that they play an important role in gene regulation in the human and other genomes
gut microbes supply the human body with energy from dietary polysaccharides through carbohydrate active enzymes or which are absent in the human genome these enzymes target polysaccharides from terrestrial plants that dominated diet throughout human the array of cazymes in gut microbes is highly diverse exemplified by the human gut symbiont bacteroides which contains glycoside hydrolases and polysaccharide lyases as well as homologues of susc and susd genes coding for two outer membrane proteins involved in starch a fundamental question that to our knowledge has yet to be addressed is how this diversity evolved by acquiring new genes from microbes living outside the gut here we characterize the first porphyranases from a member of the marine bacteroidetes zobellia galactanivorans active on the sulphated polysaccharide porphyran from marine red algae of the genus porphyra furthermore we show that genes coding for these porphyranases agarases and associated proteins have been transferred to the gut bacterium bacteroides plebeius isolated from japanese our comparative gut metagenome analyses show that porphyranases and agarases are frequent in the japanese and that they are absent in metagenome from north american individuals seaweeds make an important contribution to the daily diet in japan per person per day and porphyra spp nori is the most important nutritional seaweed traditionally used to prepare this indicates that seaweeds with associated marine bacteria may have been the route by which these novel cazymes were acquired in human gut bacteria and that contact with non sterile food may be a general factor in cazyme diversity in human microbes
the hypothesis of a hierarchy of the sciences with physical sciences at the top social sciences at the bottom and biological sciences in between is nearly years old this order is intuitive and reflected in many features of academic life but whether it reflects the hardness of scientific researchi e the extent to which research questions and results are determined by data and theories as opposed to non cognitive factorsis controversial this study analysed papers published in all disciplines and that declared to have tested a hypothesis it was determined how many papers reported a positive full or partial or negative support for the tested hypothesis if the hierarchy hypothesis is correct then researchers in softer sciences should have fewer constraints to their conscious and unconscious biases and therefore report more positive outcomes results confirmed the predictions at all levels considered discipline domain and methodology broadly defined controlling for observed differences between pure and applied disciplines and between papers testing one or several hypotheses the odds of reporting a positive result were around times higher among papers in the disciplines of psychology and psychiatry and economics and business compared to space science times higher in the domain of social sciences compared to the physical sciences and times higher in studies applying behavioural and social methodologies on people compared to physical and chemical studies on non biological material in all comparisons biological studies had intermediate values these results suggest that the nature of hypotheses tested and the logical and methodological rigour employed to test them vary systematically across disciplines and fields depending on the complexity of the subject matter and possibly other factors e g a field s level of historical and or intellectual development on the other hand these results support the scientific status of the social sciences against claims that they are completely subjective by showing that when they adopt a scientific approach to discovery they differ from the natural sciences only by a matter degree
transcription factors tfs direct gene expression by binding to dna regulatory regions to explore the evolution of gene regulation we used chromatin immunoprecipitation with high throughput sequencing chip seq to determine experimentally the genome wide occupancy of two tfs ccaat enhancer binding protein alpha and hepatocyte nuclear factor alpha in the livers of five vertebrates although each tf displays highly conserved dna binding preferences most binding is species specific and aligned binding events present in all five species are rare regions near genes with expression levels that are dependent on a tf are often bound by the tf in multiple species yet show no enhanced dna sequence constraint binding divergence between species can be largely explained by sequence changes to the bound motifs among the binding events lost in one lineage only half are recovered by another binding event within kilobases our results reveal large interspecies differences in transcriptional regulation and provide insight into evolution
motivation the performance of classifiers is often assessed using receiver operating characteristic roc or ac accumulation curve or enrichment curve curves and the corresponding areas under the curves aucs however in many fundamental problems ranging from information retrieval to drug discovery only the very top of the ranked list of predictions is of any interest and rocs and aucs are not very useful new metrics visualizations and optimization tools are needed to address this early retrieval problem results to address the early retrieval problem we develop the general concentrated roc croc framework in this framework any relevant portion of the roc or ac curve is magnified smoothly by an appropriate continuous transformation of the coordinates with a corresponding magnification factor appropriate families of magnification functions confined to the unit square are derived and their properties are analyzed together with the resulting croc curves the area under the croc curve auc croc can be used to assess early retrieval the general framework is demonstrated on a drug discovery problem and used to discriminate more accurately the early retrieval performance of five different predictors from this framework we propose a novel metric and visualization the croc exp an exponential transform of the roc curve as an alternative to other methods the croc exp provides a principled flexible and effective way for measuring and visualizing early retrieval performance with excellent statistical power corresponding methods for optimizing early retrieval are also described in the appendix availability datasets are publicly available python code and command line utilities implementing croc curves and metrics are available at http pypi python org pypi croc contact pfbaldi ics uci bioinformatics
social learning learning through observation or interaction with other individuals is widespread in nature and is central to the remarkable success of humanity yet it remains unclear why copying is profitable and how to copy most effectively to address these questions we organized a computer tournament in which entrants submitted strategies specifying how to use social learning and its asocial alternative for example trial and error learning to acquire adaptive behavior in a complex environment most current theory predicts the emergence of mixed strategies that rely on some combination of the two types of learning in the tournament however strategies that relied heavily on social learning were found to be remarkably successful even when asocial information was no more costly than social information social learning proved advantageous because individuals frequently demonstrated the highest payoff behavior in their repertoire inadvertently filtering information for copiers the winning strategy discountmachine relied nearly exclusively on social learning and weighted information according to the time acquisition
bioinformatics motivation high throughput sequencing technologies produce large sets of short reads that may contain errors these sequencing errors make de novo assembly challenging error correction aims to reduce the error rate prior assembly many de novo sequencing projects use reads from several sequencing technologies to get the benefits of all used technologies and to alleviate their shortcomings however combining such a mixed set of reads is problematic as many tools are specific to one sequencing platform the solid sequencing platform is especially problematic in this regard because of the two base color coding of the reads therefore new tools for working with mixed read sets are needed results we present an error correction tool for correcting substitutions insertions and deletions in a mixed set of reads produced by various sequencing platforms we first develop a method for correcting reads from any sequencing technology producing base space reads such as the solexa illumina and roche life sciences sequencing platforms we then further refine the algorithm to correct the color space reads from the applied biosystems solid sequencing platform together with normal base space reads our new tool is based on the shrec program that is aimed at correcting solexa illumina reads our experiments show that we can detect errors with sensitivity and specificity if the combined sequencing coverage of the sets is at least we also show that the error rate of the reads is greatly reduced availability the java source code is freely available at http www cs helsinki fi u lmsalmel hybrid shrec contact leena salmela cs fi
the genetic code the binding specificity of all transfer rnas defines how protein primary structure is determined by dna sequence dna also dictates when and where proteins are expressed and this information is encoded in a pattern of specific sequence motifs that are recognized by transcription factors however the dna binding specificity is only known for a small fraction of the approximately human transcription factors tfs we describe here a high throughput method for analyzing transcription factor binding specificity that is based on systematic evolution of ligands by exponential enrichment selex and massively parallel sequencing the method is optimized for analysis of large numbers of tfs in parallel through the use of affinity tagged proteins barcoded selection oligonucleotides and multiplexed sequencing data are analyzed by a new bioinformatic platform that uses the hundreds of thousands of sequencing reads obtained to control the quality of the experiments and to generate binding motifs for the tfs the described technology allows higher throughput and identification of much longer binding profiles than current microarray based methods in addition as our method is based on proteins expressed in mammalian cells it can also be used to characterize dna binding preferences of full length proteins or proteins requiring post translational modifications we validate the method by determining binding specificities of different classes of tfs and by confirming the specificities for and using chip seq our results reveal unexpected dimeric modes of binding for several factors that were thought to preferentially bind dna monomers
filamentous fungi are of great importance in ecology agriculture medicine and biotechnology thus it is not surprising that genomes for more than filamentous fungi have been sequenced most of them by sanger sequencing while next generation sequencing techniques have revolutionized genome resequencing e g for strain comparisons genetic mapping or transcriptome and chip analyses de novo assembly of eukaryotic genomes still presents significant hurdles because of their large size and stretches of repetitive sequences filamentous fungi contain few repetitive regions in their mb genomes and thus are suitable candidates to test de novo genome assembly from short sequence reads here we present a high quality draft sequence of the sordaria macrospora genome that was obtained by a combination of illumina solexa and roche sequencing paired end solexa sequencing of genomic dna to fold coverage and an additional fold coverage by single end sequencing resulted in gb of dna sequence reads were assembled to a mb draft version of kb with the velvet assembler comparative analysis with neurospora genomes increased the to kb the s macrospora genome contains even fewer repeat regions than its closest sequenced relative neurospora crassa comparison with genomes of other fungi showed that s macrospora a model organism for morphogenesis and meiosis harbors duplications of several genes involved in self nonself recognition furthermore s macrospora contains more polyketide biosynthesis genes than n crassa phylogenetic analyses suggest that some of these genes may have been acquired by horizontal gene transfer from a distantly related ascomycete group our study shows that for typical filamentous fungi de novo assembly of genomes from short sequence reads alone is feasible that a mixture of solexa and sequencing substantially improves the assembly and that the resulting data can be used for comparative studies to address basic questions of biology
the majority of expression quantitative trait locus eqtl studies have been carried out in single tissues or cell types using methods that ignore information shared across tissues although global analysis of rna expression in multiple tissues is now feasible few integrated statistical frameworks for joint analysis of gene expression across tissues combined with simultaneous analysis of multiple genetic variants have been developed to date here we propose sparse bayesian regression models for mapping eqtls within individual tissues and simultaneously across tissues testing these on a set of genes in four tissues we demonstrate that our methods are more powerful than traditional approaches in revealing the true complexity of the eqtl landscape at the systems level highlighting the power of our method we identified a two eqtl model cis trans for the hopx gene that was experimentally validated and was not detected by conventional approaches we showed common genetic regulation of gene expression across four tissues for approximately of transcripts providing fold increase in eqtls detection when compared with single tissue analyses at fdr level these findings provide a new opportunity to uncover complex genetic regulatory mechanisms controlling global gene expression while the generality of our modelling approach makes it adaptable to other model systems and humans with broad application to analysis of multiple intermediate and whole phenotypes
motivation integrating heterogeneous data across distributed sources is a major requirement for in silico bioinformatics supporting translational research for example genome scale data on patterns of gene expression in the fruit fly drosophila melanogaster are widely used in functional genomic studies in many organisms to inform candidate gene selection and validate experimental results however current data integration solutions tend to be heavy weight and require significant initial and ongoing investment of effort development of a common web based data integration infrastructure a k a data web using semantic web standards promises to alleviate these difficulties but little is known about the feasibility costs risks or practical means of migrating to such an infrastructure results we describe the development of openflydata a proof of concept system integrating gene expression data on d melanogaster combining semantic web standards with light weight approaches to web programming based on web design patterns to support researchers designing and validating functional genomic studies openflydata includes user facing search applications providing intuitive access to and comparison of gene expression data from flyatlas the bdgp in situ database and flyted using data from flybase to expand and disambiguate gene names openflydata s services are also openly accessible and are available for reuse by other bioinformaticians and application developers semi automated methods and tools were developed to support labour and knowledge intensive tasks involved in deploying sparql services these include methods for generating ontologies and relational to rdf mappings for relational databases which we illustrate using the flybase chado database schema and methods for mapping gene identifiers between databases the advantages of using semantic web standards for biomedical data integration are discussed as are open issues in particular although the performance of open source sparql implementations is sufficient to query gene expression data directly from user facing applications such as web based data fusions a k a mashups we found open sparql endpoints to be vulnerable to denial of service type problems which must be mitigated to ensure reliability of services based on this standard these results are relevant to data integration activities in translational bioinformatics availability the gene expression search applications and sparql endpoints developed for openflydata are deployed at http openflydata org flyui a library of javascript widgets providing re usable user interface components for drosophila gene expression data is available at http flyui googlecode com software and ontologies to support transformation of data from flybase flyatlas bdgp and flyted to rdf are available at http openflydata googlecode com sparqlite an implementation of the sparql protocol is available at http sparqlite googlecode com all software is provided under the gpl version open license
motivation in the past few years human genome structural variation discovery has enjoyed increased attention from the genomics research community many studies were published to characterize short insertions deletions duplications and inversions and associate copy number variants cnvs with disease detection of new sequence insertions requires sequence data however the detectable sequence length with read pair analysis is limited by the insert size thus longer sequence insertions that contribute to our genetic makeup are not extensively researched results we present novelseq a computational framework to discover the content and location of long novel sequence insertions using paired end sequencing data generated by the next generation sequencing platforms our framework can be built as part of a general sequence analysis pipeline to discover multiple types of genetic variation snps structural variation etc thus it requires significantly less computational resources than de novo sequence assembly we apply our methods to detect novel sequence insertions in the genome of an anonymous donor and validate our results by comparing with the insertions discovered in the same genome using various sources of sequence data availability the implementation of the novelseq pipeline is available at http compbio cs sfu ca strvar htmcontact eee gs washington edu cenk cs ca
motivation structural variation including deletions duplications and rearrangements of dna sequence are an important contributor to genome variation in many organisms in human many structural variants are found in complex and highly repetitive regions of the genome making their identification difficult a new sequencing technology called strobe sequencing generates strobe reads containing multiple subreads from a single contiguous fragment of dna strobe reads thus generalize the concept of paired reads or mate pairs that have been routinely used for structural variant detection strobe sequencing holds promise for unraveling complex variants that have been difficult to characterize with current sequencing technologies results we introduce an algorithm for identification of structural variants using strobe sequencing data we consider strobe reads from a test genome that have multiple possible alignments to a reference genome due to sequencing errors and or repetitive sequences in the reference we formulate the combinatorial optimization problem of finding the minimum number of structural variants in the test genome that are consistent with these alignments we solve this problem using an integer linear program using simulated strobe sequencing data we show that our algorithm has better sensitivity and specificity than paired read approaches for structural variation identification contact braphael brown bioinformatics
advances in sequencing technology allow genomes to be sequenced at vastly decreased costs however the assembled data frequently are highly fragmented with many gaps we present a practical approach that uses illumina sequences to improve draft genome assemblies by aligning sequences against contig ends and performing local assemblies to produce gap spanning contigs the continuity of a draft genome can thus be substantially improved often without the need to generate data
pnas meiotic recombination does not occur randomly along a chromosome but instead tends to be concentrated in small regions known as recombination hotspots recombination hotspots are thought to be short lived in evolutionary time due to their self destructive nature as gene conversion favors recombination suppressing alleles over recombination promoting alleles during double strand repair consistent with this expectation hotspots in humans are highly dynamic with little correspondence in location between humans and chimpanzees here we identify recombination hotspots in two lineages of the yeast saccharomyces paradoxus and compare their locations to those found previously in saccharomyces cerevisiae surprisingly we find considerable overlap between the two species despite the fact that they are at least times more divergent than humans and chimpanzees we attribute this unexpected result to the low frequency of sex and outcrossing in these yeasts acting to reduce the population genetic effect of biased gene conversion traces from two other signatures of recombination namely high mutagenicity and gc biased gene conversion are consistent with this interpretation thus recombination hotspots are not inevitably short lived but rather their persistence through evolutionary time will be determined by the frequency of outcrossing events in the cycle
most of the significant recent advances in cancer treatment have been based on the great strides that have been made in our understanding of the underlying biology of the disease nevertheless the exploitation of biological insight in the oncology clinic has been haphazard and we believe that this needs to be enhanced and optimized if patients are to receive maximum benefit here we discuss how research has driven cancer drug development in the past and describe how recent advances in biology technology our conceptual understanding of cell networks and removal of some roadblocks may facilitate therapeutic advances in the hopefully future
search engines make it easy to check facts online but finding some specific kinds of information sometimes proves to be difficult we studied the behavioral signals that suggest that a user is having trouble in a search task first we ran a lab study with users to gain a preliminary understanding on how users behavior changes when they struggle finding the information they re looking for the observations were then tested with participants who all completed an average of tasks from a pool of tasks the large scale study provided quantitative support for our qualitative observations from the lab study when having difficulty in finding information users start to formulate more diverse queries they use advanced operators more and they spend a longer time on the search result page as compared to the successful tasks the results complement the existing body of research focusing on successful strategies
we introduce a new approach to constructing networks with realistic features our method in spite of its conceptual simplicity it has only two parameters is capable of generating a wide variety of network types with prescribed statistical properties e g with degree or clustering coefficient distributions of various very different forms in turn these graphs can be used to test hypotheses or as models of actual data the method is based on a mapping between suitably chosen singular measures defined on the unit square and sparse infinite networks such a mapping has the great potential of allowing for graph theoretical results for a variety of network topologies the main idea of our approach is to go to the infinite limit of the singular measure and the size of the corresponding graph simultaneously a very unique feature of this construction is that the complexity of the generated network is increasing with the size we present analytic expressions derived from the parameters of the to be iterated initial generating measure for such major characteristics of graphs as their degree clustering coefficient and assortativity coefficient distributions the optimal parameters of the generating measure are determined from a simple simulated annealing process thus the present work provides a tool for researchers from a variety of fields such as biology computer science biology or complex systems enabling them to create a versatile model of their data
the international cancer genome consortium icgc was launched to coordinate large scale cancer genome studies in tumours from different cancer types and or subtypes that are of clinical and societal importance across the globe systematic studies of more than cancer genomes at the genomic epigenomic and transcriptomic levels will reveal the repertoire of oncogenic mutations uncover traces of the mutagenic influences define clinically relevant subtypes for prognosis and therapeutic management and enable the development of new therapies
most heritable traits including many human are caused by multiple loci studies in both humans and model organisms such as yeast have failed to detect a large fraction of the loci that underlie such complex a lack of statistical power to identify multiple loci with small effects is undoubtedly one of the primary reasons for this problem we have developed a method in yeast that allows the use of much larger sample sizes than previously possible and hence permits the detection of multiple loci with small effects the method involves generating very large numbers of progeny from a cross between two saccharomyces cerevisiae strains and then phenotyping and genotyping pools of these offspring we applied the method to chemical resistance traits and mitochondrial function and identified loci for each of these phenotypes we show that the level of genetic complexity underlying these quantitative traits is highly variable with some traits influenced by one major locus and others by at least loci our results provide an empirical demonstration of the genetic complexity of a number of traits and show that it is possible to identify many of the underlying factors using straightforward techniques our method should have broad applications in yeast and can be extended to organisms
massively parallel dna sequencing technologies provide an unprecedented ability to screen entire genomes for genetic changes associated with tumour progression here we describe the genomic analyses of four dna samples from an african american patient with basal like breast cancer peripheral blood the primary tumour a brain metastasis and a xenograft derived from the primary tumour the metastasis contained two de novo mutations and a large deletion not present in the primary tumour and was significantly enriched for shared mutations the xenograft retained all primary tumour mutations and displayed a mutation enrichment pattern that resembled the metastasis two overlapping large deletions encompassing were present in all three tumour samples the differential mutation frequencies and structural variation patterns in metastasis and xenograft compared with the primary tumour indicate that secondary tumours may arise from a minority of cells within the tumour
we describe a statistical and comparative genomic approach for quantifying error rates of genome sequence assemblies the method exploits not substitutions but the pattern of insertions and deletions indels in genome scale alignments for closely related species using two or three way alignments the approach estimates the amount of aligned sequence containing clusters of nucleotides that were wrongly inserted or deleted during sequencing or assembly thus the method is well suited to assessing fine scale sequence quality within single assemblies between different assemblies of a single set of reads and between genome assemblies for different species when applying this approach to four primate genome assemblies we found that average gap error rates per base varied considerably by up to sixfold as expected bacterial artificial chromosome bac sequences contained lower but still substantial predicted numbers of errors arguing for caution in regarding bacs as the epitome of genome fidelity we then mapped short reads at approximately fold statistical coverage from a bornean orangutan onto the sumatran orangutan genome assembly originally constructed from capillary reads this resulted in a reduced gap error rate and a separation of error prone from high fidelity sequence over predicted indel errors in protein coding sequence were corrected in a hybrid assembly our approach contributes a new fine scale quality metric for assemblies that should facilitate development of improved genome sequencing and strategies
atom chips provide a versatile quantum laboratory for experiments with ultracold atomic they have been used in diverse experiments involving low dimensional quantum cavity quantum atomsurface and chip based atomic and however a severe limitation of atom chips is that techniques to control atomic interactions and to generate entanglement have not been experimentally available so far such techniques enable chip based studies of entangled many body systems and are a key prerequisite for atom chip applications in quantum quantum information and quantum here we report the experimental generation of multi particle entanglement on an atom chip by controlling elastic collisional interactions with a state dependent we use this technique to generate spin squeezed states of a two component boseeinstein such states are a useful resource for quantum metrology the observed reduction in spin noise of combined with the spin coherence implies four partite entanglement between the condensate this could be used to improve an interferometric measurement by over the standard quantum our data show good agreement with a dynamical multi mode and allow us to reconstruct the wigner of the spin squeezed condensate the techniques reported here could be directly applied to chip based atomic clocks under
we used genome wide sequencing methods to study stimulus dependent enhancer function in mouse cortical neurons we identified neuronal activity regulated enhancers that are bound by the general transcriptional co activator cbp in an activity dependent manner a function of cbp at enhancers may be to recruit rna polymerase ii rnapii as we also observed activity regulated rnapii binding to thousands of enhancers notably rnapii at enhancers transcribes bi directionally a novel class of enhancer rnas ernas within enhancer domains defined by the presence of histone monomethylated at lysine the level of erna expression at neuronal enhancers positively correlates with the level of messenger rna synthesis at nearby genes suggesting that erna synthesis occurs specifically at enhancers that are actively engaged in promoting mrna synthesis these findings reveal that a widespread mechanism of enhancer activation involves rnapii binding and synthesis
p the transfer of scientific data has emerged as a significant challenge as datasets continue to grow in size and demand for open access sharing increases current methods for file transfer do not scale well for large files and can cause long transfer times in this study we present biotorrents a website that allows open access sharing of scientific data and uses the popular bittorrent peer to peer file sharing technology biotorrents allows files to be transferred rapidly due to the sharing of bandwidth across multiple institutions and provides more reliable file transfers due to the built in error checking of the file sharing technology biotorrents contains multiple features including keyword searching category browsing rss feeds torrent comments and a discussion forum biotorrents is available at ext link xmlns xlink http www org xlink ext link type uri xlink href http www biotorrents net xlink type simple http www biotorrents net ext p
strong evidence suggests that rare mutations of severe effect are responsible for a substantial portion of complex human disease evolutionary forces generate vast genetic heterogeneity in human illness by introducing many new variants in each generation current sequencing technologies offer the possibility of finding rare disease causing mutations and the genes that them
background with the rapid development of new genetic measurement methods several types of genetic alterations can be quantified in a high throughput manner while the initial focus has been on investigating each data set separately there is an increasing interest in studying the correlation structure between two or more data sets multivariate methods based on canonical correlation analysis cca have been proposed for integrating paired genetic data sets the high dimensionality of microarray data imposes computational difficulties which have been addressed for instance by studying the covariance structure of the data or by reducing the number of variables prior to applying the cca in this work we propose a new method for analyzing high dimensional paired genetic data sets which mainly emphasizes the correlation structure and still permits efficient application to very large data sets the method is implemented by translating a regularized cca to its dual form where the computational complexity depends mainly on the number of samples instead of the number of variables the optimal regularization parameters are chosen by cross validation we apply the regularized dual cca as well as a classical cca preceded by a dimension reducing principal components analysis pca to a paired data set of gene expression changes and copy number alterations in leukemia results using the correlation maximizing methods regularized dual cca and pca cca we show that without pre selection of known disease relevant genes and without using information about clinical class membership an exploratory analysis singles out two patient groups corresponding to well known leukemia subtypes furthermore the variables showing the highest relevance to the extracted features agree with previous biological knowledge concerning copy number alterations and gene expression changes in these subtypes finally the correlation maximizing methods are shown to yield results which are more biologically interpretable than those resulting from a covariance maximizing method and provide different insight compared to when each variable set is studied separately using pca conclusions we conclude that regularized dual cca as well as pca cca are useful methods for exploratory analysis of paired genetic data sets and can be efficiently implemented also when the number of variables is large
eukaryotic cytosine methylation represses transcription but also occurs in the bodies of active genes and the extent of methylation biology conservation is unclear we quantified dna methylation in eukaryotic genomes and found that gene body methylation is conserved between plants and animals whereas selective methylation of transposons is not we show that methylation of plant transposons in the chg context extends to green algae and that exclusion of histone z from methylated dna is conserved between plants and animals and we present evidence for rna directed dna methylation of fungal genes our data demonstrate that extant dna methylation systems are mosaics of conserved and derived features and indicate that gene body methylation is an ancient property of genomes
when the food intake of organisms such as yeast and rodents is reduced dietary restriction they live longer than organisms fed a normal diet a similar effect is seen when the activity of nutrient sensing pathways is reduced by mutations or chemical inhibitors in rodents both dietary restriction and decreased nutrient sensing pathway activity can lower the incidence of age related loss of function and disease including tumors and neurodegeneration dietary restriction also increases life span and protects against diabetes cancer and cardiovascular disease in rhesus monkeys and in humans it causes changes that protect against these age related pathologies tumors and diabetes are also uncommon in humans with mutations in the growth hormone receptor and natural genetic variants in nutrient sensing pathways are associated with increased human life span dietary restriction and reduced activity of nutrient sensing pathways may thus slow aging by similar mechanisms which have been conserved during evolution we discuss these findings and their potential application to prevention of age related disease and promotion of healthy aging in humans and the challenge of possible negative effects
summary picante is a software package that provides a comprehensive set of tools for analyzing the phylogenetic and trait diversity of ecological communities the package calculates phylogenetic diversity metrics performs trait comparative analyses manipulates phenotypic and phylogenetic data and performs tests for phylogenetic signal in trait distributions community structure and species interactions availability picante is a package for the r statistical language and environment written in r and c released under a gpl open source license and freely available on the web http picante r forge r project org and from cran http cran r project org contact skembel edu
many genome wide datasets are routinely generated to study different aspects of biological systems but integrating them to obtain a coherent view of the underlying biology remains a challenge we propose simultaneous clustering of multiple networks as a framework to integrate large scale datasets on the interactions among and activities of cellular components specifically we develop an algorithm jointcluster that finds sets of genes that cluster well in multiple networks of interest such as coexpression networks summarizing correlations among the expression profiles of genes and physical networks describing protein protein and protein dna interactions among genes or gene products our algorithm provides an efficient solution to a well defined problem of jointly clustering networks using techniques that permit certain theoretical guarantees on the quality of the detected clustering relative to the optimal clustering these guarantees coupled with an effective scaling heuristic and the flexibility to handle multiple heterogeneous networks make our method jointcluster an advance over earlier approaches simulation results showed jointcluster to be more robust than alternate methods in recovering clusters implanted in networks with high false positive rates in systematic evaluation of jointcluster and some earlier approaches for combined analysis of the yeast physical network and two gene expression datasets under glucose and ethanol growth conditions jointcluster discovers clusters that are more consistently enriched for various reference classes capturing different aspects of yeast biology or yield better coverage of the analysed genes these robust clusters which are supported across multiple genomic datasets and diverse reference classes agree with known biology of yeast under these growth conditions elucidate the genetic control of coordinated transcription and enable functional predictions for a number of genes
genome wide association gwa studies have identified a large number of snps associated with disease phenotypes as most gwa studies have been performed in populations of european descent this review examines the issues involved in extending the consideration of gwa studies to diverse worldwide populations although challenges exist with issues such as imputation admixture and replication investigation of a greater diversity of populations could make substantial contributions to the goal of mapping the genetic determinants of complex diseases for the human population as whole
generation of cdna using random hexamer priming induces biases in the nucleotide composition at the beginning of transcriptome sequencing reads from the illumina genome analyzer the bias is independent of organism and laboratory and impacts the uniformity of the reads along the transcriptome we provide a read count reweighting scheme based on the nucleotide frequencies of the reads that mitigates the impact of bias
gr noncoding rna ncrna constitutes a significant portion of the mammalian transcriptome emerging evidence suggests that it regulates gene expression in cis or trans by modulating the chromatin structure to uncover the functional role of ncrna in chromatin organization we deep sequenced chromatin associated rnas cars from human fibroblast hf cells this resulted in the identification of intronic regions and intergenic regions harboring cars the intronic and intergenic cars show significant conservation across species of placental mammals functional characterization of one of the intergenic cars revealed that it regulates gene expression of neighboring genes through modulating the chromatin structure in cis our data suggest that ncrna is an integral component of chromatin and that it may regulate various biological functions through fine tuning of the architecture
brain training or the goal of improved cognitive function through the regular use of computerized tests is a multimillion pound yet in our view scientific evidence to support its efficacy is lacking modest effects have been reported in some studies of older and preschool and video game players outperform non players on some tests of visual however the widely held belief that commercially available computerized brain training programs improve general cognitive function in the wider population in our opinion lacks empirical support the central question is not whether performance on cognitive tests can be improved by training but rather whether those benefits transfer to other untrained tasks or lead to any general improvement in the level of cognitive functioning here we report the results of a six week online study in which participants trained several times each week on cognitive tasks designed to improve reasoning memory planning visuospatial skills and attention although improvements were observed in every one of the cognitive tasks that were trained no evidence was found for transfer effects to untrained tasks even when those tasks were cognitively related
we describe an algorithm for gene identification in dna sequences derived from shotgun sequencing of microbial communities accurate ab initio gene prediction in a short nucleotide sequence of anonymous origin is hampered by uncertainty in model parameters while several machine learning approaches could be proposed to bypass this difficulty one effective method is to estimate parameters from dependencies formed in evolution between frequencies of oligonucleotides in protein coding regions and genome nucleotide composition original version of the method was proposed in and has been used since for i reconstructing codon frequency vector needed for gene finding in viral genomes and ii initializing parameters of self training gene finding algorithms with advent of new prokaryotic genomes en masse it became possible to enhance the original approach by using direct polynomial and logistic approximations of oligonucleotide frequencies as well as by separating models for bacteria and archaea these advances have increased the accuracy of model reconstruction and subsequently gene prediction we describe the refined method and assess its accuracy on known prokaryotic genomes split into short sequences also we show that as a result of application of the new method several thousands of new genes could be added to existing annotations of several human and mouse metagenomes
the growing competition and publish or perish culture in academia might conflict with the objectivity and integrity of research because it forces scientists to produce publishable results at all costs papers are less likely to be published and to be cited if they report negative results results that fail to support the tested hypothesis therefore if publication pressures increase scientific bias the frequency of positive results in the literature should be higher in the more competitive and productive academic environments this study verified this hypothesis by measuring the frequency of positive results in a large random sample of papers with a corresponding author based in the us across all disciplines papers were more likely to support a tested hypothesis if their corresponding authors were working in states that according to nsf data produced more academic papers per capita the size of this effect increased when controlling for state s per capita r d expenditure and for study characteristics that previous research showed to correlate with the frequency of positive results including discipline and methodology although the confounding effect of institutions prestige could not be excluded researchers in the more productive universities could be the most clever and successful in their experiments these results support the hypothesis that competitive academic environments increase not only scientists productivity but also their bias the same phenomenon might be observed in other countries where academic competition and pressures to publish high
microbial life dominates the earth but many species are difficult or even impossible to study under laboratory conditions sequencing dna directly from the environment a technique commonly referred to as metagenomics is an important tool for cataloging microbial life this culture independent approach involves collecting samples that include microbes in them extracting dna from the samples and sequencing the dna a sample may contain many different microorganisms macroorganisms and even free floating environmental dna a fundamental challenge in metagenomics has been estimating the abundance of organisms in a sample based on the frequency with which the organism s dna was observed in reads generated via sequencing
we generated a high resolution whole genome sequence and individually deleted genes in sigma a saccharomyces cerevisiae strain closely related to reference strain similar to the variation between human individuals sigma and average single nucleotide polymorphisms per kilobase a genome wide comparison of deletion mutant phenotypes identified a subset of genes that were conditionally essential by strain including essential genes unique to sigma and unique to genetic analysis indicates the conditional phenotype was most often governed by complex genetic interactions depending on multiple background specific modifiers our comprehensive analysis suggests that the presence of a complex set of modifiers will often underlie the phenotypic differences between science
the mechanisms for the origin and maintenance of biological diversity are not fully understood it is known that frequency dependent selection generating advantages for rare types can maintain genetic variation and lead to speciation but in models with simple phenotypes that is low dimensional phenotype spaces frequency dependence needs to be strong to generate diversity however we show that if the ecological properties of an organism are determined by multiple traits with complex interactions the conditions needed for frequency dependent selection to generate diversity are relaxed to the point where they are easily satisfied in high dimensional phenotype spaces mathematically this phenomenon is reflected in properties of eigenvalues of quadratic forms because all living organisms have at least hundreds of phenotypes this casts the potential importance of frequency dependence for the origin and maintenance of diversity in a new science
motivation the growth of sequence data has been accompanied by an increasing need to analyze data on distributed computer clusters the use of these systems for routine analysis requires scalable and robust software for data management of large datasets software is also needed to simplify data management and make large scale bioinformatics analysis accessible and reproducible to a wide class of target users results we have developed a workflow management system named ergatis that enables users to build execute and monitor pipelines for computational analysis of genomics data ergatis contains preconfigured components and template pipelines for a number of common bioinformatics tasks such as prokaryotic genome annotation and genome comparisons outputs from many of these components can be loaded into a chado relational database ergatis was designed to be accessible to a broad class of users and provides a user friendly web based interface ergatis supports high throughput batch processing on distributed compute clusters and has been used for data management in a number of genome annotation and comparative genomics projects availability ergatis is an open source project and is freely available at http ergatis net
the role of long non coding rnas lncrnas in controlling gene expression has garnered increased interest in recent years sequencing projects such as for mouse and h invdb for human have generated abundant data on transcribed components of mammalian cells the majority of which appear not to be protein coding however much of the non protein coding transcriptome could merely be a consequence of transcription noise it is therefore essential to use bioinformatic approaches to identify the likely functional candidates in a high manner
bioinformatics summary dendropy is a cross platform library for the python programming language that provides for object oriented reading writing simulation and manipulation of phylogenetic data with an emphasis on phylogenetic tree operations dendropy uses a splits hash mapping to perform rapid calculations of tree distances similarities and shape under various metrics it contains rich simulation routines to generate trees under a number of different phylogenetic and coalescent models dendropy s data simulation and manipulation facilities in conjunction with its support of a broad range of phylogenetic data formats nexus newick phylip fasta nexml etc allow it to serve a useful role in various phyloinformatics and phylogeographic pipelines availability the stable release of the library is available for download and automated installation through the python package index site http pypi python org pypi dendropy while the active development source code repository is available to the public from github http github com jeetsukumaran dendropy contact jeet edu
reversible cerebral vasoconstriction syndrome rcvs is more frequent than previously thought and is probably underdiagnosed the mean age of onset is years and it affects slightly more women than men rcvs is attributed to a transient reversible dysregulation of cerebral vascular tone which leads to multifocal arterial constriction and dilation more than half the cases are secondary to exposure to vasoactive substances e g cannabis antidepressants and nasal decongestants or occur in the postpartum period rcvs has a characteristic clinical and radiological course developing in a single phase after a sudden onset and there is generally no new event after month the main pattern of presentation begins with recurrent thunderclap headaches often triggered by sexual activity or various valsalva s maneuvers over a period of to weeks seizures and focal neurological deficits are less frequent and generally start after the headaches cortical subarachnoid hemorrhage intracerebral hemorrhage seizures and reversible posterior leukoencephalopathy are early complications occurring mainly within the first week ischemic events including tias and cerebral infarction occur significantly later than hemorrhagic strokes mainly during the second week diagnosis requires the demonstration of the characteristic string and beads on cerebral angiography and can be difficult for of patients have a normal initial magnetic resonance angiography mra and both a normal mra and a normal transcranial doppler in these cases the initial investigations must be repeated after a few days the final diagnosis is made when a follow up mra shows resolution or at least marked improvement of the arterial abnormalities within weeks rcvs is sometimes associated with other large artery lesions of the head and neck including dissections and unruptured aneurysms especially during the postpartum period nimodipine is the treatment most often recommended in our experience it is not especially effective in severe rcvs relapses are possible but rare and have not yet been reported in prospective series although the exact pathophysiology remains speculative strong recommendations against vasoactive substances prudent
motivation many pathway analysis or gene set enrichment analysis methods have been developed to identify enriched pathways under different biological states within a genomic study as more and more microarray datasets accumulate meta analysis methods have also been developed to integrate information among multiple studies currently most meta analysis methods for combining genomic studies focus on biomarker detection and meta analysis for pathway analysis has not been systematically pursued results we investigated two approaches of meta analysis for pathway enrichment mape by combining statistical significance across studies at the gene level or at the pathway level simulation results showed increased statistical power of meta analysis approaches compared to a single study analysis and showed complementary advantages of and under different scenarios we also developed an integrated method that incorporates advantages of both approaches comprehensive simulations and applications to real data on drug response of breast cancer cell lines and lung cancer tissues were evaluated to compare the performance of three mape variations has the advantage of not requiring gene matching across studies when and show complementary advantages the hybrid version of is generally recommended availability http www biostat pitt edu bioinfo contact ctseng pitt edu supplementary information supplementary data are available at online
monozygotic or identical twins have been widely studied to dissect the relative contributions of genetics and environment in human diseases in multiple sclerosis ms an autoimmune demyelinating disease and common cause of neurodegeneration and disability in young adults disease discordance in monozygotic twins has been interpreted to indicate environmental importance in its however genetic and epigenetic differences between monozygotic twins have been described challenging the accepted experimental model in disambiguating the effects of nature and here we report the genome sequences of one ms discordant monozygotic twin pair and messenger rna transcriptome and epigenome sequences of lymphocytes from three ms discordant monozygotic twin pairs no reproducible differences were detected between co twins among million single nucleotide polymorphisms snps or million insertion deletion polymorphisms nor were any reproducible differences observed between siblings of the three twin pairs in hla haplotypes confirmed ms susceptibility snps copy number variations mrna and genomic snp and insertion deletion genotypes or the expression of genes in t cells only to differences in the methylation of cpg dinucleotides were detected between siblings of the three twin pairs in contrast to methylation differences between t cells of unrelated individuals and several thousand differences between tissues or between normal and cancerous tissues in the first systematic effort to estimate sequence variation among monozygotic co twins we did not find evidence for genetic epigenetic or transcriptome differences that explained disease discordance these are the first to our knowledge female twin and autoimmune disease individual genome reported
rna rna abundance and dna copy number are routinely measured in high throughput using microarray and next generation sequencing ngs technologies and the attributes of different platforms have been extensively analyzed recently the application of both microarrays and ngs has expanded to include micrornas mirnas but the relative performance of these methods has not been rigorously characterized we analyzed three biological samples across six mirna microarray platforms and compared their hybridization performance we examined the utility of these platforms as well as ngs for the detection of differentially expressed mirnas we then validated the results for mirnas by real time rt pcr and challenged the use of this assay as a gold standard finally we implemented a novel method to evaluate false positive and false negative rates for all methods in the absence of a method
background genome scale metabolic reconstructions under the constraint based reconstruction and analysis cobra framework are valuable tools for analyzing the metabolic capabilities of organisms and interpreting experimental data as the number of such reconstructions and analysis methods increases there is a greater need for data uniformity and ease of distribution and use description we describe bigg a knowledgebase of biochemically genetically and genomically structured genome scale metabolic network reconstructions bigg integrates several published genome scale metabolic networks into one resource with standard nomenclature which allows components to be compared across different organisms bigg can be used to browse model content visualize metabolic pathway maps and export sbml files of the models for further analysis by external software packages users may follow links from bigg to several external databases to obtain additional information on genes proteins reactions metabolites and citations of interest conclusions bigg addresses a need in the systems biology community to have access to high quality curated metabolic models and reconstructions it is freely available for academic use at http bigg edu
motivation experimental and predicted data concerning gene transcriptional regulation are distributed among many heterogeneous sources however there are no resources to integrate these data automatically or to provide a one stop shop experience for users seeking information essential for deciphering and modeling gene regulatory networks results integromedb a semantic graph based deep web data integration system that automatically captures integrates and manages publicly available data concerning transcriptional regulation as well as other relevant biological information is proposed in this article the problems associated with data integration are addressed by ontology driven data mapping multiple data annotation and heterogeneous data querying also enabling integration of the user s data integromedb integrates over experimental and computational data sources relating to genomics transcriptomics genetics and functional and interaction data concerning gene transcriptional regulation in eukaryotes and prokaryotes availability integromedb is accessible through the integrated research environment biologicalnetworks at http www biologicalnetworks org contact baitaluk sdsc edu supplementary information supplementary data are available at online
science aims to develop an accurate understanding of reality through a variety of rigorously empirical and formal methods ontologies are used to formalize the meaning of terms within a domain of discourse the basic formal ontology bfo is an ontology of particular importance in the biomedical domains where it provides the top level for numerous ontologies including those admitted as part of the obo foundry collection the bfo requires that all classes in an ontology are actually instantiated in reality despite the fact that it is hard to show whether entities of some kind exist or do not exist in reality especially for unobservable entities like elementary particles this criterion fails to satisfy the need of scientists to communicate their findings and theories unambiguously we discuss the problems that arise due to the bfo s realism criterion and suggest alternatives
carotenoids are colored compounds produced by plants fungi and microorganisms and are required in the diet of most animals for oxidation control or light detection pea aphids display a red green color polymorphism which influences their susceptibility to natural enemies and the carotenoid torulene occurs only in red individuals unexpectedly we found that the aphid genome itself encodes multiple enzymes for carotenoid biosynthesis phylogenetic analyses show that these aphid genes are derived from fungal genes which have been integrated into the genome and duplicated red individuals have a kilobase region encoding a single carotenoid desaturase that is absent from green individuals a mutation causing an amino acid replacement in this desaturase results in loss of torulene and of red body color thus aphids are animals that make their carotenoids
background biological data have traditionally been stored and made publicly available through a variety of on line databases whereas biological knowledge has traditionally been found in the printed literature with journals now on line and providing an increasing amount of open access content often free of copyright restriction this distinction between database and literature is blurring to exploit this opportunity we present the integration of open access literature with the rcsb protein data bank pdb results biolit provides an enhanced view of articles with markup of semantic data and links to biological databases based on the content of the article for example words matching to existing biological ontologies are highlighted and database identifiers are linked to their database of origin among other functions it identifies pdb ids that are mentioned in the open access literature by parsing the full text for all research articles in pubmed central pmc and exposing the results as simple xml web services here we integrate biolit results with the rcsb pdb website by using these services to find pdb ids that are mentioned in research articles and subsequently retrieving abstract figures and text excerpts for those articles a new rcsb pdb literature view permits browsing through the figures and abstracts of the articles that mention a given structure the biolit web services that are providing the underlying data are publicly accessible a client library is provided that supports querying these services java conclusions the integration between literature and websites as demonstrated here with the rcsb pdb provides a broader view for how a given structure has been analyzed and used this approach detects the mention of a pdb structure even if it is not formally cited in the paper other structures related through the same literature references can also be identified possibly providing new scientific insight to our knowledge this is the first time that database and literature have been integrated in this way and it speaks to the opportunities afforded by open and free access to both database and content
understanding the mechanistic basis of transcriptional regulation has been a central focus of molecular biology since its inception new high throughput chromatin immunoprecipitation experiments have revealed that most regulatory proteins bind thousands of sites in mammalian genomes however the functional significance of these binding sites remains unclear we present a quantitative model of transcriptional regulation that suggests the contribution of each binding site to tissue specific gene expression depends strongly on its position relative to the transcription start site for three cell types we show that by considering binding position it is possible to predict relative expression levels between cell types with an accuracy approaching the level of agreement between different experimental platforms our model suggests that for the transcription factors profiled in these cell types a regulatory site s influence on expression falls off almost linearly with distance from the transcription start site in a kilobase range binding to both evolutionarily conserved and non conserved sequences contributes significantly to transcriptional regulation our approach also reveals the quantitative tissue specific role of individual proteins in activating or repressing transcription these results suggest that regulator binding position plays a previously unappreciated role in influencing expression and blurs the classical distinction between proximal promoter and distal events
background whole genome sequence alignment is an essential process for extracting valuable information about the functions evolution and peculiarities of genomes under investigation as available genomic sequence data accumulate rapidly there is great demand for tools that can compare whole genome sequences within practical amounts of time and space however most existing genomic alignment tools can treat sequences that are only a few mb long at once and no state of the art alignment program can align large sequences such as mammalian genomes directly on a conventional standalone computer results we previously proposed the cgat coarse grained alignment algorithm which performs an alignment job in two steps first at the block level and then at the nucleotide level the former is coarse grained alignment that can explore genomic rearrangements and reduce the sizes of the regions to be analyzed in the next step the latter is detailed alignment within limited regions in this paper we present an update of the algorithm and the open source program cgaln that implements the algorithm we compared the performance of cgaln with those of other programs on whole genomic sequences of several bacteria and of some mammalian chromosome pairs the results showed that cgaln is several times faster and more memory efficient than the best existing programs while its sensitivity and accuracy are comparable to those of the best programs cgaln takes less than hours to finish an alignment between the whole genomes of human and mouse in a single run on a conventional desktop computer with a single cpu and gb memory conclusions cgaln is not only fast and memory efficient but also effective in coping with genomic rearrangements our results show that cgaln is very effective for comparison of large genomes especially of intact chromosomal sequences we believe that cgaln provides novel viewpoint for reducing computational complexity and will contribute to various fields of science
cytosine dna methylation is a heritable epigenetic mark present in many eukaryotic organisms although dna methylation likely has a conserved role in gene silencing the levels and patterns of dna methylation appear to vary drastically among different organisms here we used shotgun genomic bisulfite sequencing bs seq to compare dna methylation in eight diverse plant and animal genomes we found that patterns of methylation are very similar in flowering plants with methylated cytosines detected in all sequence contexts whereas cg methylation predominates in animals vertebrates have methylation throughout the genome except for cpg islands gene body methylation is conserved with clear preference for exons in most organisms furthermore genes appear to be the major target of methylation in ciona and honey bee among the eight organisms the green alga chlamydomonas has the most unusual pattern of methylation having non cg methylation enriched in exons of genes rather than in repeats and transposons in addition the cofactor has a conserved function in maintaining cg methylation in both transposons and gene bodies in the mouse arabidopsis and genomes
we developed the genomic regions enrichment of annotations tool great to analyze the functional significance of cis regulatory regions identified by localized measurements of dna binding events across an entire genome whereas previous methods took into account only binding proximal to genes great is able to properly incorporate distal binding sites and control for false positives using a binomial test over the input genomic regions great incorporates annotations from ontologies and is available as a web application applying great to data sets from chromatin immunoprecipitation coupled with massively parallel sequencing chip seq of multiple transcription associated factors including srf nrsf gabp and in different developmental contexts we recover many functions of these factors that are missed by existing gene based tools and we generate testable hypotheses the utility of great is not limited to chip seq as it could also be applied to open chromatin localized epigenomic markers and similar functional data sets as well as comparative sets
massively parallel cdna sequencing rna seq provides an unbiased way to study a transcriptome including both coding and noncoding genes until now most rna seq studies have depended crucially on existing annotations and thus focused on expression levels and variation in known transcripts here we present scripture a method to reconstruct the transcriptome of a mammalian cell using only rna seq reads and the genome sequence we applied it to mouse embryonic stem cells neuronal precursor cells and lung fibroblasts to accurately reconstruct the full length gene structures for most known expressed genes we identified substantial variation in protein coding genes including thousands of novel start sites ends and internal coding exons we then determined the gene structures of more than a thousand large intergenic noncoding rna lincrna and antisense loci our results open the way to direct experimental manipulation of thousands of noncoding rnas and demonstrate the power of ab initio reconstruction to render a comprehensive picture of transcriptomes
doi dyslexia is primarily associated with a phonological processing deficit however the clinical manifestation also includes a reduced verbal working memory wm span it is unclear whether this wm impairment is caused by the phonological deficit or a distinct wm deficit the main aim of this study was to investigate neuronal activation related to phonological storage and rehearsal of serial order in wm in a sample of year old dyslexic children compared with age matched nondyslexic children a sequential verbal wm task with two tasks was used in the letter probe task the probe consisted of a single letter and the judgment was for the presence or absence of that letter in the prior sequence of six letters in the sequence probe sp task the probe consisted of all six letters and the judgment was for a match of their serial order with the temporal order in the prior sequence group analyses as well as single subject analysis were performed with the statistical parametric mapping software in the letter probe task the dyslexic readers showed reduced activation in the left precentral gyrus compared to control group in the sequence probe task the dyslexic readers showed reduced activation in the prefrontal cortex and the superior parietal cortex compared to the control subjects our findings suggest that a verbal wm impairment in dyslexia involves an extended neural network including the prefrontal cortex and the superior parietal cortex reduced activation in the left in both the letter probe and sequence probe tasks may be caused by a deficit in phonological processing however reduced bilateral activation in the in the sequence probe task only could indicate a distinct working memory deficit in dyslexia associated with temporal processing
layered on top of information conveyed by dna sequence and chromatin are higher order structures that encompass portions of chromosomes entire chromosomes and even whole interphase chromosomes are not positioned randomly within the nucleus but instead adopt preferred disparate dna elements co localize into functionally defined aggregates or factories for and dna in budding yeast drosophila and many other eukaryotes chromosomes adopt a rabl configuration with arms extending from centromeres adjacent to the spindle pole body to telomeres that abut the nuclear nonetheless the topologies and spatial relationships of chromosomes remain poorly understood here we developed a method to globally capture intra and inter chromosomal interactions and applied it to generate a map at kilobase resolution of the haploid genome of saccharomyces cerevisiae the map recapitulates known features of genome organization thereby validating the method and identifies new features extensive regional and higher order folding of individual chromosomes is observed chromosome xii exhibits a striking conformation that implicates the nucleolus as a formidable barrier to interaction between dna sequences at either end inter chromosomal contacts are anchored by centromeres and include interactions among transfer rna genes among origins of early dna replication and among sites where chromosomal breakpoints occur finally we constructed a three dimensional model of the yeast genome our findings provide a glimpse of the interface between the form and function of a genome
the genome has often been called the operating system os for a living organism a computer os is described by a regulatory control network termed the call graph which is analogous to the transcriptional regulatory network in a cell to apply our firsthand knowledge of the architecture of software systems to understand cellular design principles we present a comparison between the transcriptional regulatory network of a well studied bacterium escherichia coli and the call graph of a canonical os linux in terms of topology and evolution we show that both networks have a fundamentally hierarchical layout but there is a key difference the transcriptional regulatory network possesses a few global regulators at the top and many targets at the bottom conversely the call graph has many regulators controlling a small set of generic functions this top heavy organization leads to highly overlapping functional modules in the call graph in contrast to the relatively independent modules in the regulatory network we further develop a way to measure evolutionary rates comparably between the two networks and explain this difference in terms of network evolution the process of biological evolution via random mutation and subsequent selection tightly constrains the evolution of regulatory network hubs the call graph however exhibits rapid evolution of its highly connected generic components made possible by designers continual fine tuning these findings stem from the design principles of the two systems robustness for biological systems and cost effectiveness reuse for systems
high throughput mrna sequencing rna seq promises simultaneous transcript discovery and abundance estimation however this would require algorithms that are not restricted by prior gene annotations and that account for alternative transcription and splicing here we introduce such algorithms in an open source software program called cufflinks to test cufflinks we sequenced and analyzed million paired bp rna seq reads from a mouse myoblast cell line over a differentiation time series we detected known transcripts and previously unannotated ones of which are supported by independent expression data or by homologous genes in other species over the time series genes showed complete switches in the dominant transcription start site tss or splice isoform and we observed more subtle shifts in other genes these results suggest that cufflinks can illuminate the substantial regulatory flexibility and complexity in even this well studied model of muscle development and that it can improve transcriptome based annotation
the experiment in operation at the laboratori nazionali del gran sasso in italy is designed to search for dark matter wimps scattering off kg of liquid xenon in an ultra low background dual phase time projection chamber in this letter we present first dark matter results from the analysis of live days of non blind data acquired in october and november in the selected fiducial target of kg and within the pre defined signal region we observe no events and hence exclude spin independent wimp nucleon elastic scattering cross sections above x cm for gev c wimps at confidence level below gev c this result challenges the interpretation of the cogent and dama signals as being due to spin independent elastic light mass interactions
pnas in the absence of recent admixture between species bipartitions of individuals in gene trees that are shared across loci can potentially be used to infer the presence of two or more species this approach to species delimitation via molecular sequence data has been constrained by the fact that genealogies for individual loci are often poorly resolved and that ancestral lineage sorting hybridization and other population genetic processes can lead to discordant gene trees here we use a bayesian modeling approach to generate the posterior probabilities of species assignments taking account of uncertainties due to unknown gene trees and the ancestral coalescent process for tractability we rely on a user specified guide tree to avoid integrating over all possible species delimitations the statistical performance of the method is examined using simulations and the method is illustrated by analyzing sequence data from rotifers fence lizards and populations
pnas about of protein coding genes in the human genome are related through two whole genome duplication wgd events although wgd is often credited with great evolutionary importance the processes governing the retention of these genes and their biological significance remain unclear one increasingly popular hypothesis is that dosage balance constraints are a major determinant of duplicate gene retention we test this hypothesis and show that wgd duplicated genes ohnologs have rarely experienced subsequent small scale duplication ssd and are also refractory to copy number variation cnv in human populations and are thus likely to be sensitive to relative quantities i e they are dosage balanced by contrast genes that have experienced ssd in the vertebrate lineage are more likely to also display cnv this supports the hypothesis of biased retention of dosage balanced genes after wgd we also show that ohnologs have a strong association with human disease in particular down syndrome ds caused by trisomy is widely assumed to be caused by dosage effects and of previously reported candidate genes for this syndrome are ohnologs that experienced no other copy number changes we propose the remaining dosage balanced ohnologs on chromosome as candidate ds genes these observations clearly show a persistent resistance to dose changes in genes duplicated by wgd dosage balance constraints simultaneously explain duplicate gene retention and essentiality wgd
high throughput sequencing technologies have opened up a new avenue for studying extinct organisms here we identify and quantify biases introduced by particular characteristics of ancient dna samples these analyses demonstrate the importance of closely related genomic sequence for correctly identifying and classifying bona fide endogenous dna fragments we show that more accurate genome divergence estimates from ancient dna sequence can be attained using at least two outgroup genomes and filtering
next generation sequencing technologies are quickly becoming the preferred approach for characterizing and quantifying entire genomes even though data produced from these technologies are proving to be the most informative of any thus far very little attention has been paid to fundamental design aspects of data collection and analysis namely sampling randomization replication and blocking we discuss these concepts in an rna sequencing framework using simulations we demonstrate the benefits of collecting replicated rna sequencing data according to well known statistical designs that partition the sources of biological and technical variation examples of these designs and their corresponding models are presented with the goal of testing differential genetics
alternative splicing has a crucial role in the generation of biological complexity and its misregulation is often involved in human disease here we describe the assembly of a splicing code which uses combinations of hundreds of rna features to predict tissue dependent changes in alternative splicing for thousands of exons the code determines new classes of splicing patterns identifies distinct regulatory programs in different tissues and identifies mutation verified regulatory sequences widespread regulatory strategies are revealed including the use of unexpectedly large combinations of features the establishment of low exon inclusion levels that are overcome by features in specific tissues the appearance of features deeper into introns than previously appreciated and the modulation of splice variant levels by transcript structure characteristics the code detected a class of exons whose inclusion silences expression in adult tissues by activating nonsense mediated messenger rna decay but whose exclusion promotes expression during embryogenesis the code facilitates the discovery and detailed characterization of regulated alternative splicing events on a genome scale
gene expression is regulated both by cis elements which are dna segments closely linked to the genes they regulate and by trans factors which are usually proteins capable of diffusing to unlinked genes understanding the patterns and sources of regulatory variation is crucial for understanding phenotypic and genome evolution here we measure genome wide allele specific expression by deep sequencing to investigate the patterns of cis and trans expression variation between two strains of saccharomyces cerevisiae we propose a statistical modeling framework based on the binomial distribution that simultaneously addresses normalization of read counts derived from different parents and estimating the cis and trans expression variation parameters we find that expression polymorphism in yeast is common for both cis and trans though trans variation is more common constraint in expression evolution is correlated with other hallmarks of constraint including gene essentiality number of protein interaction partners and constraint in amino acid substitution indicating that both cis and trans polymorphism are clearly under purifying selection though trans variation appears to be more sensitive to selective constraint comparing interspecific expression divergence between s cerevisiae and s paradoxus to our intraspecific variation suggests a significant departure from a neutral model of molecular evolution a further examination of correlation between polymorphism and divergence within each category suggests that cis divergence is more frequently mediated by positive darwinian selection than is divergence
neandertals the closest evolutionary relatives of present day humans lived in large parts of europe and western asia before disappearing years ago we present a draft sequence of the neandertal genome composed of more than billion nucleotides from three individuals comparisons of the neandertal genome to the genomes of five present day humans from different parts of the world identify a number of genomic regions that may have been affected by positive selection in ancestral modern humans including genes involved in metabolism and in cognitive and skeletal development we show that neandertals shared more genetic variants with present day humans in eurasia than with present day humans in sub saharan africa suggesting that gene flow from neandertals into the ancestors of non africans occurred before the divergence of eurasian groups from other
it is now possible to perform whole genome shotgun sequencing as well as capture of specific genomic regions for extinct organisms however targeted resequencing of large parts of nuclear genomes has yet to be demonstrated for ancient dna here we show that hybridization capture on microarrays can successfully recover more than a megabase of target regions from neandertal dna even in the presence of microbial dna using this approach we have sequenced protein coding positions inferred to have changed on the human lineage since the last common ancestor shared with chimpanzees by generating the sequence of one neandertal and present day humans at these positions we have identified amino acid substitutions that have become fixed in humans since our divergence from the science
social tagging services allow users to annotate various online resources with freely chosen keywords tags they not only facilitate the users in finding and organizing online resources but also provide meaningful collaborative semantic data which can potentially be exploited by recommender systems traditional studies on recommender systems focused on user rating data while recently social tagging data is becoming more and more prevalent how to perform resource recommendation based on tagging data is an emerging research topic in this paper we consider the problem of document e g web pages research papers recommendation using purely tagging data that is we only have data containing users tags documents and the relationships among them we propose a novel graph based representation learning algorithm for this purpose the users tags and documents are represented in the same semantic space in which two related objects are close to each other for a given user we recommend those documents that are sufficiently close to him her experimental results on two data sets crawled from del icio us and citeulike show that our algorithm can generate promising recommendations and outperforms traditional algorithms
this pedagogical review of galaxy cluster simulations is based on three lectures given at the enrico fermi summer school entitled astrophysics of galaxy clusters it covers the standard cosmological framework growth of perturbations in the linear regime analytic models for nonlinear perturbation growth statistics of galaxy cluster populations virial scaling relations overview of numerical methods simulating gas in galaxy clusters basic results on adiabatic clusters santa barbara cluster comparison project effect of additional physics recent progress in galaxy clustering modeling galcons turbulence agn jets cluster wide b fields simulating statistical samples and lightcones and simulated surveys
new methods for analyzing rna seq data enable de novo reconstruction of the transcriptome sequencing of rna has long been recognized as an efficient method for gene and remains the gold standard for annotation of both coding and noncoding compared with earlier methods massively parallel sequencing of rna rna seq has vastly increased the throughput of rna sequencing and allowed global measurement of abundance
sequencing based approaches now allow high resolution genome scale investigation of cellular epigenetic landscapes for example mapping of open chromatin regions post translational histone modifications and dna methylation across a whole genome is now feasible and new non coding regulatory rnas can be sensitively identified via rna sequencing the resulting large scale data sets promise to contribute towards a more precise and complete understanding of gene regulation and to yield insights into the interplay between genomes and the environment in this article i review some of the conceptual issues and currently available software tools for the analysis of sequencing based whole genome data
summary epigenetics the study of heritable somatic phenotypic changes not related to dna sequence has emerged as a critical component of the landscape of gene regulation the epigenetic layers such as dna methylation histone modifications and nuclear architecture are now being extensively studied in many cell types and disease settings few software tools exist to summarize and interpret these datasets we have created a toolbox of procedures to interrogate and visualize epigenomic data both array and sequencing based and make available a software package for the cross platform r language availability the package is freely available under lgpl from the r forge web site http repitools r forge r project org contact mrobinson wehi au
background the maturing field of genomics is rapidly increasing the number of sequenced genomes and producing more information from those previously sequenced much of this additional information is variation data derived from sampling multiple individuals of a given species with the goal of discovering new variants and characterising the population frequencies of the variants that are already known these data have immense value for many studies including those designed to understand evolution and connect genotype to phenotype maximising the utility of the data requires that it be stored in an accessible manner that facilitates the integration of variation data with other genome resources such as gene annotation and comparative genomics description the ensembl project provides comprehensive and integrated variation resources for a wide variety of chordate genomes this paper provides a detailed description of the sources of data and the methods for creating the ensembl variation databases it also explores the utility of the information by explaining the range of query options available from using interactive web displays to online data mining tools and connecting directly to the data servers programmatically it gives a good overview of the variation resources and future plans for expanding the variation data within ensembl conclusions variation data is an important key to understanding the functional and phenotypic differences between individuals the development of new sequencing and genotyping technologies is greatly increasing the amount of variation data known for almost all genomes the ensembl variation resources are integrated into the ensembl genome browser and provide a comprehensive way to access this data in the context of a widely used genome bioinformatics system all ensembl data is freely available at http www ensembl org and from the public mysql database server at ensembldb org
background gene expression arrays are valuable and widely used tools for biomedical research today s commercial arrays attempt to measure the expression level of all of the genes in the genome effectively translating the results from the microarray into a biological interpretation requires an accurate mapping between the probesets on the array and the genes that they are targeting although major array manufacturers provide annotations of their gene expression arrays the methods used by various manufacturers are different and the annotations are difficult to keep up to date in the rapidly changing world of biological sequence databases results we have created a consistent microarray annotation protocol applicable to all of the major array manufacturers we constantly keep our annotations updated with the latest ensembl gene predictions and thus cross referenced with a large number of external biomedical sequence database identifiers we show that these annotations are accurate and address in detail reasons for the minority of probesets that cannot be annotated annotations are publicly accessible through the ensembl genome browser and programmatically through the ensembl application programming interface they are also seamlessly integrated into the biomart data mining tool and the biomart package of bioconductor conclusions consistent accurate and updated gene expression array annotations remain critical for biological research our annotations facilitate accurate biological interpretation of gene profiles
the number of databases in molecular biological fields has rapidly increased to provide a large scale resource though valuable information is available data can be difficult to access compare and integrate due to different formats and presentations of web interfaces this paper offers a practical guide to the integration of gene comparative genomic and functional genomics data using the ensembl website at http www ensembl org the ensembl genome browser and underlying databases focus on chordate organisms more species such as plants and microorganisms can be investigated using our sister browser at http www ensemblgenomes org in this study four examples are used that sample many pages and features of the ensembl browser we focus on comparative studies across over mostly chordate organisms variations linked to disease functional genomics and access of external information housed in databases outside the ensembl project researchers will learn how to go beyond simply exporting one gene sequence and explore how a genome browser can integrate data from various sources and databases to build a full and comprehensive picture
background the ensembl project produces updates to its comparative genomics resources with each of its several releases per year during each release cycle approximately two weeks are allocated to generate all the genomic alignments and the protein homology predictions the number of calculations required for this task grows approximately quadratically with the number of species we currently support species in ensembl and we expect the number to continue to grow in the future results we present ehive a new fault tolerant distributed processing system initially designed to support comparative genomic analysis based on blackboard systems network distributed autonomous agents dataflow graphs and block branch diagrams in the ehive system a mysql database serves as the central blackboard and the autonomous agent a perl script queries the system and runs jobs as required the system allows us to define dataflow and branching rules to suit all our production pipelines we describe the implementation of three pipelines pairwise whole genome alignments multiple whole genome alignments and gene trees with protein homology inference finally we show the efficiency of the system in real case scenarios conclusions ehive allows us to produce computationally demanding results in a reliable and efficient way with minimal supervision and high throughput further documentation is available at http www ensembl org info ehive
summary arcadia translates text based descriptions of biological networks sbml files into standardized diagrams sbgn pd maps users can view the same model from different perspectives and easily alter the layout to emulate traditional textbook representations availability and implementation arcadia is written in c the source code is available along with mac os and windows binaries under the gpl from http arcadiapathways net
analytical description of propagation phenomena on random networks has flourished in recent years yet more complex systems have mainly been studied through numerical means in this paper a mean field description is used to coherently couple the dynamics of the network elements nodes vertices individuals on the one hand and their recurrent topological patterns subgraphs groups on the other hand in a sis model of epidemic spread on social networks with community structure this approach yields a set of odes for the time evolution of the system as well as analytical solutions for the epidemic threshold and equilibria the results obtained are in good agreement with numerical simulations and reproduce random networks behavior in the appropriate limits which highlights the influence of topology on the processes we believe this is the first derivation of an analytical expression for the epidemic threshold of the sis model in continuous time on clustered and random networks finally it is demonstrated that our model predicts higher epidemic thresholds for clustered structures than for equivalent random topologies in the case of networks with zero correlation
adaptive networks have been recently introduced in the context of disease propagation on complex networks they account for the mutual interaction between the network topology and the states of the nodes until now existing models have been analyzed using low complexity analytical formalisms revealing nevertheless some novel dynamical features however current methods have failed to reproduce with accuracy the simultaneous time evolution of the disease and the underlying network topology in the framework of the adaptive susceptible infectious susceptible sis model of gross et al phys rev lett we introduce an improved compartmental formalism able to handle this coevolutionary task successfully with this approach we analyze the interplay and outcomes of both dynamical elements process and structure on adaptive networks featuring different degree distributions at the stage
background comparison of metabolic networks across species is a key to understanding how evolutionary pressures shape these networks by selecting taxa representative of different lineages or lifestyles and using a comprehensive set of descriptors of the structure and complexity of their metabolic networks one can highlight both qualitative and quantitative differences in the metabolic organization of species subject to distinct evolutionary paths or environmental constraints results we used a novel representation of metabolic networks termed network of interacting pathways or nip to focus on the modular high level organization of the metabolic capabilities of the cell using machine learning techniques we identified the most relevant aspects of cellular organization that change under evolutionary pressures we considered the transitions from prokarya to eukarya with a focus on the transitions among the archaea bacteria and eukarya from unicellular to multicellular eukarya from free living to host associated bacteria from anaerobic to aerobic as well as the acquisition of cell motility or growth in an environment of various levels of salinity or temperature intuitively we expect organisms with more complex lifestyles to have more complex and robust metabolic networks here we demonstrate for the first time that such organisms are not only characterized by larger denser networks of metabolic pathways but also have more efficiently organized cross communications as revealed by subtle changes in network topology these changes are unevenly distributed among metabolic pathways with specific categories of pathways being promoted to more central locations as an answer to environmental constraints conclusions combining methods from graph theory and machine learning we have shown here that evolutionary pressures not only affects gene and protein sequences but also specific details of the complex wiring of functional modules in the cell this approach allows the identification and quantification of those changes and provides an overview of the evolution of systems
microbes are the most abundant and diverse organisms on earth in contrast to macroscopic organisms their environmental preferences and ecological interdependencies remain difficult to assess requiring laborious molecular surveys at diverse sampling sites here we present a global meta analysis of previously sampled microbial lineages in the environment we grouped publicly available ribosomal rna sequences into operational taxonomic units at various levels of resolution and systematically searched these for co occurrence across environments naturally occurring microbes indeed exhibited numerous significant interlineage associations these ranged from relatively specific groupings encompassing only a few lineages to larger assemblages of microbes with shared habitat preferences many of the coexisting lineages were phylogenetically closely related but a significant number of distant associations were observed as well the increased availability of completely sequenced genomes allowed us for the first time to search for genomic correlates of such ecological associations genomes from coexisting microbes tended to be more similar than expected by chance both with respect to pathway content and genome size and outliers from these trends are discussed we hypothesize that groupings of lineages are often ancient and that they may have significantly impacted on evolution
rapidly evolving sequencing technologies produce data on an unparalleled scale a central challenge to the analysis of this data is sequence alignment whereby sequence reads must be compared to a reference a wide variety of alignment algorithms and software have been subsequently developed over the past two years in this article we will systematically review the current development of these algorithms and introduce their practical applications on different types of experimental data we come to the conclusion that short read alignment is no longer the bottleneck of data analyses we also consider future development of alignment algorithms with respect to emerging long sequence reads and the prospect of computing
after mapping rna seq data can be summarized by a sequence of read counts commonly modeled as poisson variables with constant rates along each transcript which actually fit data poorly we suggest using variable rates for different positions and propose two models to predict these rates based on local sequences these models explain more than of the variations and can lead to improved estimates of gene and isoform expressions for both illumina and applied data
background high throughput sequencing technologies offer new perspectives for biomedical agronomical and evolutionary research promising progresses now concern the application of these technologies to large scale studies of genetic variation such studies require the genotyping of high numbers of samples this is theoretically possible using pyrosequencing which generates billions of base pairs of sequence data however several challenges arise first in the attribution of each read produced to its original sample and second in bioinformatic analyses to distinguish true from artifactual sequence variation this pilot study proposes a new application for the gs flx platform allowing the individual genotyping of thousands of samples in one run a probabilistic model has been developed to demonstrate the reliability of this method results dna amplicons from rodent samples were individually barcoded using a combination of tags located in forward and reverse primers amplicons consisted in bp fragments corresponding to drb exon a highly polymorphic gene in mammals a total of reads were obtained of which were finally assigned to original samples rules based on a probabilistic model and a four step procedure were developed to validate sequences and provide a confidence level for each genotype the method gave promising results with the genotyping of drb exon sequences for samples from different rodent species and the sequencing of variants in one half of a run using replicates we estimated that the reproducibility of genotyping reached conclusions this new approach is a promising alternative to classical methods involving electrophoresis based techniques for variant separation and cloning sequencing for sequence determination the system is less costly and time consuming and may enhance the reliability of genotypes obtained when high numbers of samples are studied it opens up new perspectives for the study of evolutionary and functional genetics of highly polymorphic genes like major histocompatibility complex genes in vertebrates or loci regulating self compatibility in plants important applications in biomedical research will include the detection of individual variation in disease susceptibility similarly agronomy will benefit from this approach through the study of genes implicated in productivity or disease traits
insulators prevent promiscuous gene regulation by restricting the action of enhancers and silencers recent studies have revealed a number of similarities between insulators and promoters including binding of specific transcription factors chromatin modification signatures and localization to specific subnuclear positions we propose that enhancer blockers and silencing barrier insulators might have evolved as specialized derivatives of promoters and that the two types of element use related mechanisms to mediate their distinct functions these insights can help to reconcile different models of action
signalling networks regulate essentially all of the biology of cells and organisms in normal and disease states signalling is often studied using antibody based techniques such as western blots large scale precision proteomics based on mass spectrometry now enables the system wide characterization of signalling events at the levels of post translational modifications proteinprotein interactions and changes in protein expression this technology delivers accurate and unbiased information about the quantitative changes of thousands of proteins and their modifications in response to any perturbation current studies focus on phosphorylation but acetylation methylation glycosylation and ubiquitylation are also becoming amenable to investigation large scale proteomics based signalling research will fundamentally change our understanding of networks
universal common ancestry uca is a central pillar of modern evolutionary as first suggested by the theory of uca posits that all extant terrestrial organisms share a common genetic heritage each being the genealogical descendant of a single species from the distant the classic evidence for uca although massive is largely restricted to local common ancestryfor example of specific phyla rather than the entirety of lifeand has yet to fully integrate the recent advances from modern phylogenetics and probability theory although uca is widely assumed it has rarely been subjected to formal quantitative and this has led to critical commentary emphasizing the intrinsic technical difficulties in empirically evaluating a theory of such broad furthermore several researchers have proposed that early life was characterized by rampant horizontal gene transfer leading some to question the monophyly of here i provide the first to my knowledge formal fundamental test of uca without assuming that sequence similarity implies genetic kinship i test uca by applying model selection to molecular phylogenies focusing on a set of ubiquitously conserved proteins that are proposed to be orthologous among a wide range of biological models involving the independent ancestry of major taxonomic groups the model selection tests are found to overwhelmingly support uca irrespective of the presence of horizontal gene transfer and symbiotic fusion events these results provide powerful statistical evidence corroborating the monophyly of all life
abstract nbsp nbsp in science a relatively small pool of researchers garners a disproportionally large number of citations still very little is known about the social characteristics of highly cited scientists this is unfortunate as these researchers wield a disproportional impact on their fields and the study of highly cited scientists can enhance our understanding of the conditions which foster highly cited work the systematic social inequalities which exist in science and scientific careers more generally this study provides information on this understudied subject by examining the social characteristics and opinions of the most cited environmental scientists and ecologists overall the social characteristics of these researchers tend to reflect broader patterns of inequality in the global scientific community however while the social characteristics of these researchers mirror those of other scientific elites in important ways they differ in others revealing findings which are both novel and surprising perhaps indicating multiple pathways to becoming cited
web represents an emerging suite of applications that hold immense potential in enriching communication enabling collaboration and fostering innovation however little work has been done hitherto to research web applications in library websites this paper addresses the following three research questions a to what extent are web applications prevalent in libraries b in what ways have web applications been used in libraries and c does the presence of web applications enhance the quality of library websites divided equally between public and academic libraries websites from north america europe and asia were sampled and analyzed using a three step content analysis method the findings suggest that the order of popularity of web applications implemented in libraries is blogs rss instant messaging social networking services wikis and social tagging applications also libraries have recognized how different web applications can be used complementarily to increase the level of user engagement finally the presence of web applications was found to be associated with the overall quality and in particular service quality of library websites this paper concludes by highlighting implications for both librarians and scholars interested to delve deeper into the implementation of applications
summary we present a suite of unix shell programs for processing any number of phylogenetic trees of any size they perform frequently used tree operations without requiring user interaction they also allow tree drawing as scalable vector graphics svg suitable for high quality presentations and further editing and as ascii graphics for command line inspection as an example we include an implementation of bootscanning a procedure for finding recombination breakpoints in viral genomes availability c source code python bindings and executables for various platforms are available from http cegg unige ch the distribution includes a manual and example data the package is distributed under the bsd license contact thomas junier unige bioinformatics
deep sequencing will soon generate comprehensive sequence information in large disease samples although the power to detect association with an individual rare variant is limited pooling variants by gene or pathway into a composite test provides an alternative strategy for identifying susceptibility genes we describe a statistical method for detecting association of multiple rare variants in protein coding genes with a quantitative or dichotomous trait the approach is based on the regression of phenotypic values on individuals genotype scores subject to a variable allele frequency threshold incorporating computational predictions of the functional effects of missense variants statistical significance is assessed by permutation testing with variable thresholds we used a rigorous population genetics simulation framework to evaluate the power of the method and we applied the method to empirical sequencing data from three studies
motivation new generation sequencing technologies producing increasingly complex datasets demand new efficient and specialized sequence analysis algorithms often it is only the novel sequences in a complex dataset that are of interest and the superfluous sequences need to be removed results a novel algorithm fast and accurate classification of sequences facss is introduced that can accurately and rapidly classify sequences as belonging or not belonging to a reference sequence facs was first optimized and validated using a synthetic metagenome dataset an experimental metagenome dataset was then used to show that facs achieves comparable accuracy as blat and but is at least times faster in classifying sequences availability source code for facs bloom filters and metasim dataset used is available at http facs biotech kth se the bloom faster perl module can be downloaded from cpan at http search cpan org approximately palvaro bloom faster contacts henrik stranneheim biotech kth se joakiml biotech kth se supplementary information supplementary data are available at online
cuda and opencl offer two different interfaces for programming gpus opencl is an open standard that can be used to program cpus gpus and other devices from different vendors while cuda is specific to nvidia gpus although opencl promises a portable language for gpu programming its generality may entail a performance penalty in this paper we compare the performance of cuda and opencl using complex near identical kernels we show that when using nvidia compiler tools converting a cuda kernel to an opencl kernel involves minimal modifications making such a kernel compile with ati s build tools involves more modifications our performance tests measure and compare data transfer times to and from the gpu kernel execution times and end to end application execution times for both cuda opencl
despite a rapidly growing scientific and clinical brain imaging literature based on functional magnetic resonance imaging fmri using blood oxygenation level dependent bold signals it remains controversial whether bold signals in a particular region can be caused by activation of local excitatory this difficult question is central to the interpretation and utility of bold with major significance for fmri studies in basic research and clinical using a novel integrated technology unifying control of inputs with high field fmri signal readouts we show here that specific stimulation of local camkii expressing excitatory neurons either in the neocortex or thalamus elicits positive bold signals at the stimulus location with classical kinetics we also show that optogenetic fmri ofmri allows visualization of the causal effects of specific cell types defined not only by genetic identity and cell body location but also by axonal projection target finally we show that ofmri within the living and intact mammalian brain reveals bold signals in downstream targets distant from the stimulus indicating that this approach can be used to map the global effects of controlling a local cell population in this respect unlike both conventional fmri studies based on and fmri with electrical stimulation that will also directly drive afferent and nearby axons this ofmri approach provides causal information about the global circuits recruited by defined local neuronal activity patterns together these findings provide an empirical foundation for the widely used fmri bold signal and the features of ofmri define a potent tool that may be suitable for functional circuit analysis as well as global phenotyping of circuitry
although genome wide association gwa studies for common variants have thus far succeeded in explaining only a modest fraction of the genetic components of human common diseases recent advances in next generation sequencing technologies could rapidly facilitate substantial progress this outcome is expected if much of the missing genetic control is due to gene variants that are too rare to be picked up by gwa studies and have relatively large effects on risk here we evaluate the evidence for an important role of rare gene variants of major effect in common diseases and outline discovery strategies for identification
background micrornas mirnas are non coding rnas that regulate gene expression by binding to the messenger rna mrna of protein coding genes they control gene expression by either inhibiting translation or inducing mrna degradation a number of computational techniques have been developed to identify the targets of mirnas in this study we used predicted mirna gene interactions to analyse mrna gene expression microarray data to predict mirnas associated with particular diseases or conditions results here we combine correspondence analysis between group analysis and co inertia analysis cia to determine which mirnas are associated with differences in gene expression levels in microarray data sets using a database of mirna target predictions from targetscan targetscans and miranda and combining these data with gene expression levels from sets of microarrays this method produces a ranked list of mirnas associated with a specified split in samples we applied this to three different microarray datasets a papillary thyroid carcinoma dataset an in house dataset of lipopolysaccharide treated mouse macrophages and a multi tissue dataset in each case we were able to identified mirnas of biological importance conclusions we describe a technique to integrate gene expression data and mirna target predictions from sources
recent advances in dna sequencing have revolutionized the field of genomics making it possible for even single research groups to generate large amounts of sequence data very rapidly and at a substantially lower cost these high throughput sequencing technologies make deep transcriptome sequencing and transcript quantification whole genome sequencing and resequencing available to many more researchers and projects however while the cost and time have been greatly reduced the error profiles and limitations of the new platforms differ significantly from those of previous sequencing technologies the selection of an appropriate sequencing platform for particular types of experiments is an important consideration and requires a detailed understanding of the technologies available including sources of error error rate as well as the speed and cost of sequencing we review the relevant concepts and compare the issues raised by the current high throughput dna sequencing technologies we analyze how future developments may overcome these limitations and what remain
abstract background large comparative genomics studies and tools are becoming increasingly more compute expensive as the number of available genome sequences continues to rise the capacity and cost of local computing infrastructures are likely to become prohibitive with the increase especially as the breadth of questions continues to rise alternative computing architectures in particular cloud computing environments may help alleviate this increasing pressure and enable fast large scale and cost effective comparative genomics strategies going forward to test this we redesigned a typical comparative genomics algorithm the reciprocal smallest distance algorithm rsd to run within amazon s elastic computing cloud we then employed the rsd cloud for ortholog calculations across a wide selection of fully sequenced genomes results we ran more than rsd cloud processes within the these jobs were farmed simultaneously to high capacity compute nodes using the amazon web service elastic mapreduce and included a wide mix of large and small genomes the total computation time took just under hours and cost a total of usd conclusions the effort to transform existing comparative genomics algorithms from local compute infrastructures is not trivial however the speed and flexibility of cloud computing environments provides a substantial boost with manageable cost the procedure designed to transform the rsd algorithm into a cloud ready application is readily adaptable to similar comparative problems
although recent genome wide studies have provided valuable insights into the genetic basis of human disease they have explained relatively little of the heritability of most complex traits and the variants identified through these studies have small effect sizes this has led to the important and hotly debated issue of where the missing heritability of complex diseases might be found here seven leading geneticists offer their opinion about where this heritability is likely to lie what this could tell us about the underlying genetic architecture of common diseases and how this could inform research strategies for uncovering genetic factors
a series of reports over the last few years have indicated that a much larger portion of the mammalian genome is transcribed than can be accounted for by currently annotated genes but the quantity and nature of these additional transcripts remains unclear here we have used data from single and paired end rna seq and tiling arrays to assess the quantity and composition of transcripts in polya rna from human and mouse tissues relative to tiling arrays rna seq identifies many fewer transcribed regions seqfrags outside known exons and ncrnas most nonexonic seqfrags are in introns raising the possibility that they are fragments of pre mrnas the chromosomal locations of the majority of intergenic seqfrags in rna seq data are near known genes consistent with alternative cleavage and polyadenylation site usage promoter and terminator associated transcripts or new alternative exons indeed reads that bridge splice sites identified new exons affecting genes most of the remaining seqfrags correspond to either single reads that display characteristics of random sampling from a low level background or several thousand small transcripts median length bp present at higher levels which also tend to display sequence conservation and originate from regions with open chromatin we conclude that while there are bona fide new intergenic transcripts their number and abundance is generally low in comparison to known exons and the genome is not as pervasively transcribed as reported
background several genomes have now been sequenced with millions of genetic variants annotated while significant progress has been made in mapping single nucleotide polymorphisms snps and small bp insertion deletions indels the annotation of larger structural variants has been less comprehensive it is still unclear to what extent a typical genome differs from the reference assembly and the analysis of the genomes sequenced to date have shown varying results for copy number variation cnv and inversions results we have combined computational re analysis of existing whole genome sequence data with novel microarray based analysis and detect structural variants covering mb that were not reported in the initial sequencing of the first published personal genome we estimate a total non snp variation content of mb in a single genome our results indicate that this genome differs from the consensus reference sequence by approximately when considering indels cnvs by snps and approximately by inversions the structural variants impact genes and of structural variants would not be imputed by snp association conclusions our results indicate that a large number of structural variants have been unreported in the individual genomes published to date this significant extent and complexity of structural variants as well as the growing recognition of their medical relevance necessitate they be actively studied in health related analyses of personal genomes the new catalogue of structural variants generated for this genome provides a crucial resource for future studies
background one challenge facing biologists is to tease out useful information from massive data sets for further analysis a pathway based analysis may shed light by projecting candidate genes onto protein functional relationship networks we are building such a pathway based analysis system results we have constructed a protein functional interaction network by extending curated pathways with non curated sources of information including protein protein interactions gene coexpression protein domain interaction gene ontology go annotations and text mined protein interactions which cover close to of the human proteome by applying this network to two glioblastoma multiforme gbm data sets and projecting cancer candidate genes onto the network we found that the majority of gbm candidate genes form a cluster and are closer than expected by chance and the majority of gbm samples have sequence altered genes in two network modules one mainly comprising genes whose products are localized in the cytoplasm and plasma membrane and another comprising gene products in the nucleus both modules are highly enriched in known oncogenes tumor suppressors and genes involved in signal transduction similar network patterns were also found in breast colorectal and pancreatic cancers conclusions we have built a highly reliable functional interaction network upon expert curated pathways and applied this network to the analysis of two genome wide gbm and several other cancer data sets the network patterns revealed from our results suggest common mechanisms in the cancer biology our system should provide a foundation for a network or pathway based analysis platform for cancer and diseases
we present aardvark a social search engine with aardvark users ask a question either by instant message email web input text message or voice aardvark then routes the question to the person in the user s extended social network most likely to be able to answer that question as compared to a traditional web search engine where the challenge lies in finding the right document to satisfy a user s information need the challenge in a social search engine like aardvark lies in finding the right person to satisfy a user s information need further while trust in a traditional search engine is based on authority in a social search engine like aardvark trust is based on intimacy we describe how these considerations inform the architecture algorithms and user interface of aardvark and how they are reflected in the behavior of users
the use of web services to enable programmatic access to on line bioinformatics is becoming increasingly important in the life sciences however their number distribution and the variable quality of their documentation can make their discovery and subsequent use difficult a web services registry with information on available services will help to bring together service providers and their users the biocatalogue http www biocatalogue org provides a common interface for registering browsing and annotating web services to the life science community services in the biocatalogue can be described and searched in multiple ways based upon their technical types bioinformatics categories user tags service providers or data inputs and outputs they are also subject to constant monitoring allowing the identification of service problems and changes and the filtering out of unavailable or unreliable resources the system is accessible via a human readable web style interface and a programmatic web service interface the biocatalogue follows a community approach in which all services can be registered browsed and incrementally documented with annotations by any member of the community
with high dimensional data variable by variable statistical testing is often used to select variables whose behavior differs across conditions such an approach requires adjustment for multiple testing which can result in low statistical power a two stage approach that first filters variables by a criterion independent of the test statistic and then only tests variables which pass the filter can provide higher power we show that use of some filter test statistics pairs presented in the literature may however lead to loss of type i error control we describe other pairs which avoid this problem in an application to microarray data we found that gene by gene filtering by overall variance followed by a t test increased the number of discoveries by we also show that this particular statistic pair induces a lower bound on fold change among the set of discoveries independent filteringusing filter test pairs that are independent under the null hypothesis but correlated under the alternativeis a general approach that can substantially increase the efficiency experiments
we report the design synthesis and assembly of the mbp mycoplasma mycoides jcvi genome starting from digitized genome sequence information and its transplantation into a mycoplasma capricolum recipient cell to create new mycoplasma mycoides cells that are controlled only by the synthetic chromosome the only dna in the cells is the designed synthetic dna sequence including watermark sequences and other designed gene deletions and polymorphisms and mutations acquired during the building process the new cells have expected phenotypic properties and are capable of continuous replication
the human microbiome refers to the community of microorganisms including prokaryotes viruses and microbial eukaryotes that populate the human body the national institutes of health launched an initiative that focuses on describing the diversity of microbial species that are associated with health and disease the first phase of this initiative includes the sequencing of hundreds of microbial reference genomes coupled to metagenomic sequencing from multiple body sites here we present results from an initial reference genome sequencing of microbial genomes from predicted polypeptides that correspond to the gene complement of these strains previously unidentified novel polypeptides that had both unmasked sequence length greater than amino acids and no blastp match to any nonreference entry in the nonredundant subset were defined this analysis resulted in a set of polypeptides of which approximately were unique in addition this set of microbial genomes allows for approximately of random sequences from the microbiome of the gastrointestinal tract to be associated with organisms based on the match criteria used insights into pan genome analysis suggest that we are still far from saturating microbial species genetic data sets in addition the associated metrics and standards used by our group for quality assurance presented
the interactions of protein kinases and phosphatases with their regulatory subunits and substrates underpin cellular regulation we identified a kinase and phosphatase interaction kpi network of interactions in budding yeast by mass spectrometric analysis of protein complexes the kpi network contained many dense local regions of interactions that suggested new functions notably the cell cycle phosphatase associated with multiple kinases that revealed roles for in mitogen activated protein kinase signaling the dna damage response and metabolism whereas interactions of the target of rapamycin complex uncovered new effector kinases in nitrogen and carbon metabolism an extensive backbone of kinase kinase interactions cross connects the proteome and may serve to coordinate diverse cellular science
regions of the genome that have been the target of positive selection specifically along the human lineage are of special importance in human biology we used high throughput sequencing combined with methods to enrich human genomic samples for particular targets to obtain the sequence of chromosomal samples at high depth in kb neighborhoods of previously identified bp elements that show evidence for human accelerated evolution in addition to selection the pattern of nucleotide substitutions in several of these elements suggested an historical bias favoring the conversion of weak a or t alleles into strong g or c alleles here we found strong evidence in the derived allele frequency spectra of many of these kb regions for ongoing weak to strong fixation bias comparison of the nucleotide composition at polymorphic loci to the composition at sites of fixed substitutions additionally reveals the signature of historical weak to strong fixation bias in a subset of these regions most of the regions with evidence for historical bias do not also have signatures of ongoing bias suggesting that the evolutionary forces generating weak to strong bias are not constant over time to investigate the role of selection in shaping these regions we analyzed the spatial pattern of polymorphism in our samples we found no significant evidence for selective sweeps possibly because the signal of such sweeps has decayed beyond the power of our tests to detect them together these results do not rule out functional roles for the observed changes in these regionsindeed there is good evidence that the first two are functional elements in humansbut they suggest that a fixation process such as biased gene conversion that is biased at the nucleotide level but is otherwise selectively neutral could be an important evolutionary force at play in them both historically and present
while it is widely held that an organism s genomic information should remain constant several protein families are known to modify it members of the aid apobec protein family can deaminate dna similarly members of the adar family can deaminate rna characterizing the scope of these events is challenging here we use large genomic data sets such as the two billion sequences in the ncbi trace archive to look for clusters of mismatches of the same type which are a hallmark of editing events caused by and adar we align traces from the ncbi trace archive to their reference genomes in clusters of mismatches of increasing size at least one systematic sequencing error dominates the results g to a it is still present in mismatches with accuracy and only vanishes in mismatches at accuracy or higher the error appears to have entered into about of the hapmap possibly affecting other users that rely on this resource further investigation using stringent quality thresholds uncovers thousands of mismatch clusters with no apparent defects in their chromatograms these traces provide the first reported candidates of endogenous dna editing in human further elucidating rna editing in human and mouse and also revealing for the first time extensive rna editing in xenopus tropicali s we show that the ncbi trace archive provides a valuable resource for the investigation of the phenomena of dna and rna editing as well as setting the stage for a comprehensive mapping of editing events in large scale datasets
motivation high throughput nucleotide sequencing provides quantitative readouts in assays for rna expression rna seq protein dna binding chip seq or cell counting barcode sequencing statistical inference of differential signal in such data requires estimation of their variability throughout the dynamic range when the number of replicates is small error modelling is needed to achieve statistical power results we propose an error model that uses the negative binomial distribution with variance and mean linked by local regression to model the null distribution of the count data the method controls type i error and provides good detection power availability a free open source r software package deseq is available from the bioconductor project and from http www huber embl de users deseq
summary computational gene function prediction can serve to focus experimental resources on high priority experimental tasks funcbase is a web resource for viewing quantitative machine learning based gene function annotations quantitative annotations of genes including fungal and mammalian genes with gene ontology go terms are accompanied by a community feedback system evidence underlying function annotations is shown for example a custom cytoscape viewer shows functional linkage graphs relevant to the gene or function of interest funcbase provides links to external resources and may be accessed directly or via links from species specific databases availability funcbase as well as all underlying data and annotations are freely available via http func med harvard edu contact hms edu
we describe how our world dominated by science and scientists has been changed and will be revolutionized by technologies moving with internet time computers have always been well used tools but in the beginning only the science counted and little credit or significance was attached to any computing activities associated with scientific research some years ago this started to change and the area of computational science gathered support with the nsf supercomputer centers playing a critical role however this vision has stalled over the last years with information technology increasing in importance the holy grail of computational science scalable parallel computing is still important but is just one supporting component of the internet revolution we discuss the emergence of the field of internetics bridging computer science and all application areas whether simulation or information based internetics is an exciting field which seems complete and rich enough to be a lasting interdisciplinary area physics and other core science and engineering disciplines used to attract the very best minds but now their popularity is declining we describe curricula initiatives that can reinvigorate these fields this curricula turmoil must be addressed by our education infrastructure whose professorial staff find it hard to develop courses to satisfy student and employer interests in times of such rapid change distance education is very relevant as it can be used to disseminate expertise to students and teachers in these new areas all of this has implications for our educational institutions which could be profound
multiple sequence alignment is a difficult computational problem there have been compelling pleas for methods to assess whole genome multiple sequence alignments and compare the alignments produced by different tools we assess the four encode alignments each of which aligns vertebrates on mbp of total input sequence we measure the level of agreement among the alignments and compare their coverage and accuracy we find a disturbing lack of agreement among the alignments not only in species distant from human but even in mouse a well studied model organism overall the assessment shows that pecan produces the most accurate or nearly most accurate alignment in all species and genomic location categories while still providing coverage comparable to or better than that of the other alignments in the placental mammals our assessment reveals that constructing accurate whole genome multiple sequence alignments remains a significant challenge particularly for noncoding regions and distantly species
using high throughput sequencing we devised a technique to determine the insertion sites of virtually all members of the human specific retrotransposon family in any human genome using diagnostic nucleotides we were able to locate the approximately copies corresponding specifically to the pre ta ta and ta subfamilies with over of sequenced reads corresponding to human specific elements we find that any two individual genomes differ at an average of sites with respect to insertion presence or absence in total we assayed individuals of which are unrelated at sites including shared with the reference genome and nonreference insertions we show that profiles recapitulate genetic ancestry and determine the chromosomal distribution of these elements using these data we estimate that the rate of retrotransposition in humans is between and births and the number of dimorphic elements in the human population with gene frequencies greater than is and
the need to maintain the structural and functional integrity of an evolving protein severely restricts the repertoire of acceptable amino acid substitutions however it is not known whether these restrictions impose a global limit on how far homologous protein sequences can diverge from each other here we explore the limits of protein evolution using sequence divergence data we formulate a computational approach to study the rate of divergence of distant protein sequences and measure this rate for ancient proteins those that were present in the last universal common ancestor we show that ancient proteins are still diverging from each other indicating an ongoing expansion of the protein sequence universe the slow rate of this divergence is imposed by the sparseness of functional protein sequences in sequence space and the ruggedness of the protein fitness landscape approximately per cent of sites cannot accept an amino acid substitution at any given moment but a vast majority of all sites may eventually be permitted to evolve when other compensatory changes occur thus approximately x yr has not been enough to reach the limit of divergent evolution of proteins and for most proteins the limit of sequence similarity imposed by common function may not exceed that of sequences
context previous studies indicate that the interpretation of trial results can be distorted by authors of published reports objective to identify the nature and frequency of distorted presentation or spin ie specific reporting strategies whatever their motive to highlight that the experimental treatment is beneficial despite a statistically nonsignificant difference for the primary outcome or to distract the reader from statistically nonsignificant results in published reports of randomized controlled trials rcts with statistically nonsignificant results for primary outcomes data sources march search of medline via pubmed using the cochrane highly sensitive search strategy to identify reports of rcts published in december study selection articles were included if they were parallel group rcts with a clearly identified primary outcome showing statistically nonsignificant results ie p ge data extraction two readers appraised each selected article using a pretested standardized data abstraction form developed in a pilot test results from the published reports of rcts examined were eligible and appraised the title was reported with spin in articles confidence interval ci spin was identified in the results and conclusions sections of the abstracts of ci and ci reports respectively with the conclusions of ci focusing only on treatment effectiveness spin was identified in the main text results discussion and conclusions sections of ci ci and ci reports respectively more than of the reports had spin in at least of these sections in the main text conclusion in this representative sample of rcts published in with statistically nonsignificant primary outcomes the reporting and interpretation of findings was frequently inconsistent with results
gr promoters are important regulatory elements that contain the necessary sequence features for cells to initiate transcription to functionally characterize a large set of human promoters we measured the transcriptional activities of putative promoters across eight cell lines using transient transfection reporter assays in parallel we measured gene expression in the same cell lines and observed a significant correlation between promoter activity and endogenous gene expression r as transient transfection assays directly measure the promoting effect of a defined fragment of dna sequence decoupled from epigenetic chromatin or long range regulatory effects we sought to predict whether a promoter was active using sequence features alone cg dinucleotide content was highly predictive of ubiquitous promoter activity necessitating the separation of promoters into two groups high cg promoters mostly ubiquitously active and low cg promoters mostly cell linespecific computational models trained on the binding potential of transcriptional factor tf binding motifs could predict promoter activities in both high and low cg groups average area under the receiver operating characteristic curve auc of the models was and exceeded the auc of cg content by an average of known relationships for example between and hepatocytes were recapitulated in the corresponding cell lines in this case the liver derived cell line half of the associations between tissue specific tfs and cell linespecific promoters were new our study underscores the importance of collecting functional information from complementary assays and conditions to understand biology in a framework
lung cancer is the leading cause of cancer related mortality worldwide with non small cell lung carcinomas in smokers being the predominant form of the disease although previous studies have identified important common somatic mutations in lung cancers they have primarily focused on a limited set of genes and have thus provided a constrained view of the mutational spectrum recent cancer sequencing efforts have used next generation sequencing technologies to provide a genome wide view of mutations in leukaemia breast cancer and cancer cell lines here we present the complete sequences of a primary lung tumour coverage and adjacent normal tissue comparing the two genomes we identify a wide variety of somatic variations including high confidence single nucleotide variants we validated somatic single nucleotide variants in this tumour including one in the kras proto oncogene and others in coding regions as well as large scale structural variations these constitute a large set of new somatic mutations and yield an estimated per megabase genome wide somatic mutation rate notably we observe a distinct pattern of selection against mutations within expressed genes compared to non expressed genes and in promoter regions up to kilobases upstream of all protein coding genes furthermore we observe a higher rate of amino acid changing mutations in kinase genes we present a comprehensive view of somatic alterations in a single lung tumour and provide the first evidence to our knowledge of distinct selective pressures present within the environment
background the omics fields promise to revolutionize our understanding of biology and biomedicine however their potential is compromised by the challenge to analyze the huge datasets produced analysis of omics data is plagued by the curse of dimensionality resulting in imprecise estimates of model parameters and performance moreover the integration of omics data with other data sources is difficult to shoehorn into classical statistical models this has resulted in ad hoc approaches to address specific problems results we present a general approach to omics data analysis that alleviates these problems by combining escience and bayesian methods we retrieve scientific information and data from multiple sources and coherently incorporate them into large models these models improve the accuracy of predictions and offer new insights into the underlying mechanisms this escience bayes approach is demonstrated in two proof of principle applications one for breast cancer prognosis prediction from transcriptomic data and one for protein protein interaction studies based on proteomic data conclusions bayesian statistics provide the flexibility to tailor statistical models to the complex data structures in omics biology as well as permitting coherent integration of multiple data sources however bayesian methods are in general computationally demanding and require specification of possibly thousands of prior distributions escience can help us overcome these difficulties the escience bayes thus approach permits us to fully leverage on the advantages of bayesian methods resulting in models with improved predictive performance that gives more information about the underlying system
myexperiment http www myexperiment org is an online research environment that supports the social sharing of bioinformatics workflows these workflows are procedures consisting of a series of computational tasks using web services which may be performed on data from its retrieval integration and analysis to the visualization of the results as a public repository of workflows myexperiment allows anybody to discover those that are relevant to their research which can then be reused and repurposed to their specific requirements conversely developers can submit their workflows to myexperiment and enable them to be shared in a secure manner since its release in myexperiment currently has over registered users and contains more than workflows the social aspect to the sharing of these workflows is facilitated by registered users forming virtual communities bound together by a common interest or research project contributors of workflows can build their reputation within these communities by receiving feedback and credit from individuals who reuse their work further documentation about myexperiment including its rest web service is available from http wiki myexperiment org feedback and requests for support can be sent to bugs org
second generation sequencing technology can now be used to sequence an entire human genome in a matter of days and at low cost sequence read lengths initially very short have rapidly increased since the technology first appeared and we now are seeing a growing number of efforts to sequence large genomes de novo from these short reads in this perspective we describe the issues associated with short read assembly the different types of data produced by second gen sequencers and the latest assembly algorithms designed for these data we also review the genomes that have been assembled recently from short reads and make recommendations for sequencing strategies that will yield a high assembly
background there is considerable interest in the development of methods to efficiently identify all coding variants present in large sample sets of humans there are three approaches possible whole genome sequencing whole exome sequencing using exon capture methods and rna seq while whole genome sequencing is the most complete it remains sufficiently expensive that cost effective alternatives are important results here we provide a systematic exploration of how well rna seq can identify human coding variants by comparing variants identified through high coverage whole genome sequencing to those identified by high coverage rna seq in the same individual this comparison allowed us to directly evaluate the sensitivity and specificity of rna seq in identifying coding variants and to evaluate how key parameters such as the degree of coverage and the expression levels of genes interact to influence performance we find that although only of exonic variants identified by whole genome sequencing were captured using rna seq this number rose to when concentrating on genes known to be well expressed in the source tissue we also find that a high false positive rate can be problematic when working with rna seq data especially at higher levels of coverage conclusions we conclude that as long as a tissue relevant to the trait under study is available and suitable quality control screens are implemented rna seq is a fast and inexpensive alternative approach for finding coding variants in genes with sufficiently high levels
a powerful way to separate signal from noise in biology is to convert the molecular data from individual genes or proteins into an analysis of comparative biological network behaviors one of the limitations of previous network analyses is that they do not take into account the combinatorial nature of gene interactions within the network we report here a new technique differential rank conservation dirac which permits one to assess these combinatorial interactions to quantify various biological pathways or networks in a comparative sense and to determine how they change in different individuals experiencing the same disease process this approach is based on the relative expression values of participating genesi e the ordering of expression within network profiles dirac provides quantitative measures of how network rankings differ either among networks for a selected phenotype or among phenotypes for a selected network we examined disease phenotypes including cancer subtypes and neurological disorders and identified networks that are tightly regulated as defined by high conservation of transcript ordering interestingly we observed a strong trend to looser network regulation in more malignant phenotypes and later stages of disease at a sample level dirac can detect a change in ranking between phenotypes for any selected network variably expressed networks represent statistically robust differences between disease states and serve as signatures for accurate molecular classification validating the information about expression patterns captured by dirac importantly dirac can be applied not only to transcriptomic data but to any ordinal type
summary genome wide association studies gwas which produce huge volumes of data are now being carried out by many groups around the world creating a need for user friendly tools for data quality control qc and analysis one critical aspect of gwas qc is evaluating genotype cluster plots to verify sensible genotype calling in putatively associated single nucleotide polymorphisms snps evoker is a tool for visualizing genotype cluster plots and provides a solution to the computational and storage problems related to working with such large datasets availability http www sanger ac uk resources evoker
summary geworkbench genomics workbench is an open source java desktop application that provides access to an integrated suite of tools for the analysis and visualization of data from a wide range of genomics domains gene expression sequence protein structure and systems biology more than distinct plug in modules are currently available implementing both classical analyses several variants of clustering classification homology detection etc as well as state of the art algorithms for the reverse engineering of regulatory networks and for protein structure prediction among many others geworkbench leverages standards based middleware technologies to provide seamless access to remote data annotation and computational servers thus enabling researchers with limited local resources to benefit from available public infrastructure availability the project site http www geworkbench org includes links to self extracting installers for most operating system os platforms as well as instructions for building the application from scratch using the source code which is freely available from the project s svn subversion repository geworkbench support is available through the end user and developer forums of the cabig r molecular analysis tools knowledge center https cabig kc nci nih gov molecular forums contact geworkbench columbia edu supplementary information supplementary data are available at bioinformatics bioinformatics
nucleosomes compact and regulate access to dna in the nucleus and are composed of approximately bases of dna wrapped around a histone here we report a genome wide nucleosome positioning analysis of arabidopsis thaliana using massively parallel sequencing of mononucleosomes by combining this data with profiles of dna methylation at single base resolution we identified base periodicities in the dna methylation status of nucleosome bound dna and found that nucleosomal dna was more highly methylated than flanking dna these results indicate that nucleosome positioning influences dna methylation patterning throughout the genome and that dna methyltransferases preferentially target nucleosome bound dna we also observed similar trends in human nucleosomal dna indicating that the relationships between nucleosomes and dna methyltransferases are conserved finally as has been observed in animals nucleosomes were highly enriched on exons and preferentially positioned at intronexon and exonintron boundaries rna polymerase ii pol ii was also enriched on exons relative to introns consistent with the hypothesis that nucleosome positioning regulates pol ii processivity dna methylation is also enriched on exons consistent with the targeting of dna methylation to nucleosomes and suggesting a role for dna methylation in definition
gene expression measurements are influenced by a wide range of factors such as the state of the cell experimental conditions and variants in the sequence of regulatory regions to understand the effect of a variable of interest such as the genotype of a locus it is important to account for variation that is due to confounding causes here we present vbqtl a probabilistic approach for mapping expression quantitative trait loci eqtls that jointly models contributions from genotype as well as known and hidden confounding factors vbqtl is implemented within an efficient and flexible inference framework making it fast and tractable on large scale problems we compare the performance of vbqtl with alternative methods for dealing with confounding variability on eqtl mapping datasets from simulations yeast mouse and human employing bayesian complexity control and joint modelling is shown to result in more precise estimates of the contribution of different confounding factors resulting in additional associations to measured transcript levels compared to alternative approaches we present a threefold larger collection of cis eqtls than previously found in a whole genome eqtl scan of an outbred human population altogether of the tested probes show a significant genetic association in cis and we validate that the additional eqtls are likely to be real by replicating them in different sets of individuals our method is the next step in the analysis of high dimensional phenotype data and its application has revealed insights into genetic regulation of gene expression by demonstrating more abundant cis acting eqtls in human than previously shown our software is freely available online at http www sanger ac uk resources peer
we apply updated semi analytic galaxy formation models simultaneously to the stored halo subhalo merger trees of the millennium and millennium ii simulations these differ by a factor of in mass resolution allowing explicit testing of resolution effects on predicted galaxy properties we have revised the treatments of the transition between the rapid infall and cooling flow regimes of gas accretion of the sizes of bulges and of gaseous and stellar disks of supernova feedback of the transition between central and satellite status as galaxies fall into larger systems and of gas and star stripping once they become satellites plausible values of efficiency and scaling parameters yield an excellent fit not only to the observed abundance of low redshift galaxies over orders of magnitude in stellar mass and magnitudes in luminosity but also to the observed abundance of milky way satellites this suggests that reionisation effects may not be needed to solve the missing satellite problem except perhaps for the faintest objects the same model matches the observed large scale clustering of galaxies as a function of stellar mass and colour the fit remains excellent down to for massive galaxies for m lt x however the model overpredicts clustering at scales below mpc suggesting that the adopted in the simulations is too high galaxy distributions within rich clusters agree between the simulations and match those observed but only if galaxies without dark matter subhalos so called orphans are included our model predicts a larger passive fraction among low mass galaxies than is observed as well as an overabundance of galaxies beyond z reflecting deficiencies in the way star formation rates modelled
background transposable elements tes have played an important role in the diversification and enrichment of mammalian transcriptomes through various mechanisms such as exonization and intronization the birth of new exons introns from previously intronic exonic sequences respectively and insertion into first and last exons however no extensive analysis has compared the effects of tes on the transcriptomes of mammals non mammalian vertebrates and invertebrates results we analyzed the influence of tes on the transcriptomes of five species three invertebrates and two non mammalian vertebrates compared to previously analyzed mammals there were lower levels of te introduction into introns significantly lower numbers of exonizations originating from tes and a lower percentage of te insertion within the first and last exons although the transcriptomes of vertebrates exhibit significant levels of exonization of tes only anecdotal cases were found in invertebrates in vertebrates as in mammals the exonized tes are mostly alternatively spliced indicating that selective pressure maintains the original mrna product generated from such genes conclusions exonization of tes is widespread in mammals less so in non mammalian vertebrates and very low in invertebrates we assume that the exonization process depends on the length of introns vertebrates unlike invertebrates are characterized by long introns and short internal exons our results suggest that there is a direct link between the length of introns and exonization of tes and that this process became more prevalent following the appearance mammals
this paper presents the results of the second phase of a research information network study which sought to establish the impact of e journals on the scholarly behaviour of researchers in the uk the first phase of the project was a deep log analysis of the usage and information seeking behaviour of researchers in connection with the sciencedirect and oxford journals databases this paper reports on the second phase which sought to explain and provide context for the deep log data by taking the questions raised by the quantitative study to the research community via interview questionnaire and observation nine major research institutions took part six subjects were covered and the behaviour of about people was analyzed findings show that academic journals have become central to all disciplines and that the e form is the prime means of access most importantly the study demonstrates that computer usage logs provide an accurate picture of online behaviour high levels of gateway service use point to the re intermediating of the broken chain between publisher reader
motivation existing sequence assembly editors struggle with the volumes of data now readily available from the latest generation of dna sequencing instruments results we describe the software along with the data structures and algorithms used that allow it to be scalable we demonstrate this with an assembly of billion sequence fragments and compare the performance with several other programs we analyse the memory cpu i o usage and file sizes used by availability and implementation is part of the staden package and is available under an open source licence from http staden sourceforge net it is implemented in c and tcl tk currently it works on unix systems only contact jkb sanger ac uksupplementary information supplementary data are available at online
gr several studies support that antisense mediated regulation may affect a large proportion of genes using the illumina next generation sequencing platform we developed dsss direct strand specific sequencing a strand specific protocol for transcriptome sequencing we tested dsss with rna from two samples prokaryotic mycoplasma pneumoniae as well as eukaryotic mus musculus and obtained data containing strand specific information using single read and paired end sequencing we validated our results by comparison with a strand specific tiling array data set for strain of the simple prokaryote m pneumoniae and by quantitative pcr qpcr the results of dsss were very well supported by the results from tiling arrays and qpcr moreover dsss provided higher dynamic range and single base resolution thus enabling efficient antisense detection and the precise mapping of transcription start sites and untranslated regions dsss data for mouse confirmed strand specificity of the protocol and the general applicability of the approach to studying eukaryotic transcription we propose dsss as a simple and efficient strategy for strand specific transcriptome sequencing and as a tool for genome annotation exploiting the increased read lengths that next generation sequencing technology now is capable deliver
summary we present an open source platform independent tool called cisgenome browser which can work together with any other data analysis program to serve as a flexible component for genomic data visualization it can also work by itself as a standalone genome browser by working as a light weight web server cisgenome browser is a convenient tool for data sharing between labs it has features that are specifically designed for ultra high throughput sequencing data visualization availability http biogibbs stanford edu jiangh browser contact jiangh stanford bioinformatics
genes include cis regulatory regions that contain transcriptional enhancers recent reports have shown that developmental genes often possess multiple discrete enhancer modules that drive transcription in similar spatio temporal patterns primary enhancers located near the basal promoter and secondary or shadow enhancers located at more remote positions it has been proposed that the seemingly redundant activity of primary and secondary enhancers contributes to phenotypic robustness we tested this hypothesis by generating a deficiency that removes two newly discovered enhancers of shavenbaby svb a transcript of the ovo locus a gene encoding a transcription factor that directs development of drosophila larval trichomes at optimal temperatures for embryonic development this deficiency causes minor defects in trichome patterning in embryos that develop at both low and high extreme temperatures however absence of these secondary enhancers leads to extensive loss of trichomes these temperature dependent defects can be rescued by a transgene carrying a secondary enhancer driving transcription of the svb cdna finally removal of one copy of wingless a gene required for normal trichome patterning causes a similar loss of trichomes only in flies lacking the secondary enhancers these results support the hypothesis that secondary enhancers contribute to phenotypic robustness in the face of environmental and variability
for more than a century jews and non jews alike have tried to define the relatedness of contemporary jewish people previous genetic studies of blood group and serum markers suggested that jewish groups had middle eastern origin with greater genetic similarity between paired jewish populations however these and successor studies of monoallelic y chromosomal and mitochondrial genetic markers did not resolve the issues of within and between group jewish genetic identity here genome wide analysis of seven jewish groups iranian iraqi syrian italian turkish greek and ashkenazi and comparison with non jewish groups demonstrated distinctive jewish population clusters each with shared middle eastern ancestry proximity to contemporary middle eastern populations and variable degrees of european and north african admixture two major groups were identified by principal component phylogenetic and identity by descent ibd analysis middle eastern jews and european syrian jews the ibd segment sharing and the proximity of european jews to each other and to southern european populations suggested similar origins for european jewry and refuted large scale genetic contributions of central and eastern european and slavic populations to the formation of ashkenazi jewry rapid decay of ibd in ashkenazi jewish genomes was consistent with a severe bottleneck followed by large expansion such as occurred with the so called demographic miracle of population expansion from people at the beginning of the century to people at the beginning of the century thus this study demonstrates that european syrian and middle eastern jews represent a series of geographical isolates or clusters woven together by shared ibd threads
the extinction of conditioned fear memories requires plasticity in the infralimbic medial prefrontal cortex il mpfc but little is known about the molecular mechanisms involved brain derived neurotrophic factor bdnf is a key mediator of synaptic plasticity in multiple brain areas in rats subjected to auditory fear conditioning bdnf infused into the il mpfc reduced conditioned fear for up to hours even in the absence of extinction training which suggests that bdnf substituted for extinction similar to extinction bdnf induced reduction in fear required n methyl d aspartate receptors and did not erase the original fear memory rats failing to learn extinction showed reduced bdnf in hippocampal inputs to the il mpfc and augmenting bdnf in this pathway prevented extinction failure hence boosting bdnf activity in hippocampal infralimbic circuits may ameliorate disorders of learned science
the microbes that inhabit particular environments must be able to perform molecular functions that provide them with a competitive advantage to thrive in those environments as most molecular functions are performed by proteins and are conserved between related proteins we can expect that organisms successful in a given environmental niche would contain protein families that are specific for functions that are important in that environment for instance the human gut is rich in polysaccharides from the diet or secreted by the host and is dominated by bacteroides whose genomes contain highly expanded repertoire of protein families involved in carbohydrate metabolism to identify other protein families that are specific to this environment we investigated the distribution of protein families in the currently available human gut genomic and metagenomic data using an automated procedure we identified a group of protein families strongly overrepresented in the human gut these not only include many families described previously but also interestingly a large group of previously unrecognized protein families which suggests that we still have much to discover about this environment the identification and analysis of these families could provide us with new information about an environment critical to our health and being
massively parallel sequencing technologies continue to alter the study of human genetics as the cost of sequencing declines next generation sequencing ngs instruments and datasets will become increasingly accessible to the wider research community investigators are understandably eager to harness the power of these new technologies sequencing human genomes on these platforms however presents numerous production and bioinformatics challenges production issues like sample contamination library chimaeras and variable run quality have become increasingly problematic in the transition from technology development lab to production floor analysis of ngs data too remains challenging particularly given the short read lengths bp and sheer volume of data the development of streamlined highly automated pipelines for data analysis is critical for transition from technology adoption to accelerated research and publication this review aims to describe the state of current ngs technologies as well as the strategies that enable ngs users to characterize the full spectrum of dna sequence variation humans
micrornas mirnas and small interfering rnas sirnas bound to argonaute proteins risc destabilize mrnas through base pairing with the mrna however the gene expression changes after perturbations of these small rnas are only partially explained by predicted mirna sirna targeting targeting may be modulated by other mrna sequence elements such as binding sites for the hundreds of rna binding proteins rna bps expressed in any cell and this aspect has not been systematically explored across a panel of published experiments we systematically investigated to what extent sequence motifs in untranslated regions utrs correlate with expression changes following transfection of small rnas the most significantly overrepresented motifs in down regulated mrnas are two novel u rich motifs urms uuuuaaa and uuuguuu recently discovered as binding sites for the also known as hud rna bp surprisingly the most significantly overrepresented motif in up regulated mrnas is the heptanucleotide au rich element are uauuuau which is known to affect mrna stability via at least different rna bps we show that destabilization mediated by the transfected mirna is generally attenuated by are motifs and augmented by urm motifs these are and urm signatures were confirmed in different types of published experiments covering eight different cell lines finally we show that both are and urm motifs couple to presumed endogenous mirna binding sites in mrnas bound by argonaute proteins this is the first systematic investigation of utr motifs that globally couple to regulation by mirnas and may potentially antagonize or cooperate with mirna sirna regulation our results suggest that binding sites of mirnas and rna bps should be considered in combination when interpreting and predicting mirna regulation vivo
to get beyond the low hanging fruits so far identified by genome wide association gwa studies new methods must be developed in order to discover the numerous remaining genes that estimates of heritability indicate should be contributing to complex human phenotypes such as obesity here we describe a novel integrative method for complex disease gene identification utilizing both genome wide transcript profiling of adipose tissue samples and consequent analysis of genome wide association data generated in large snp scans we infer causality of genes with obesity by employing a unique set of monozygotic twin pairs discordant for bmi n pairs age years kg mean weight difference and contrast the transcript profiles with those from a larger sample of non related adult individuals n using this approach we were able to identify genes with possibly causal roles in determining the degree of human adiposity testing for association of snp variants in these genes in the population samples of the large engage consortium n revealed a significant deviation of p values from the expected p a total of genes contained snps nominally associated with bmi the top finding was blood coagulation factor identified as a novel obesity gene also replicated in a second gwa set of individuals this study presents a new approach to utilizing gene expression studies for informing choice of candidate genes for complex human phenotypes such obesity
motivation metabolic and signaling pathways are an increasingly important part of organizing knowledge in systems biology they serve to integrate collective interpretations of facts scattered throughout literature biologists construct a pathway by reading a large number of articles and interpreting them as a consistent network but most of the models constructed currently lack direct links to those articles biologists who want to check the original articles have to spend substantial amounts of time to collect relevant articles and identify the sections relevant to the pathway furthermore with the scientific literature expanding by several thousand papers per week keeping a model relevant requires a continuous curation effort in this article we present a system designed to integrate a pathway visualizer text mining systems and annotation tools into a seamless environment this will enable biologists to freely move between parts of a pathway and relevant sections of articles as well as identify relevant papers from large text bases the system pathtext is developed by systems biology institute okinawa institute of science and technology national centre for text mining university of manchester and the university of tokyo and is being used by groups of biologists from these locations contact brian monrovian bioinformatics
motivation sequence assembly is a difficult problem whose importance has grown again recently as the cost of sequencing has dramatically dropped most new sequence assembly software has started by building a de bruijn graph avoiding the overlap based methods used previously because of the computational cost and complexity of these with very large numbers of short reads here we show how to use suffix array based methods that have formed the basis of recent very fast sequence mapping algorithms to find overlaps and generate assembly string graphs asymptotically faster than previously described algorithms results standard overlap assembly methods have time complexity o where n is the sum of the lengths of the reads we use the ferraginamanzini index fm index derived from the burrowswheeler transform to find overlaps of length at least among a set of reads as well as an approach that finds all overlaps then implements transitive reduction to produce a string graph we show how to output directly only the irreducible overlaps significantly shrinking memory requirements and reducing compute time to o n independent of depth overlap based assembly methods naturally handle mixed length read sets including capillary reads or long reads promised by the third generation sequencing technologies the algorithms we present here pave the way for overlap based assembly approaches to be developed that scale to whole vertebrate genome de novo assembly contact sanger uk
detection of new genomic control elements is critical in understanding transcriptional regulatory networks in their entirety we studied the genome wide binding locations of three key regulatory proteins also known as nanog and ctcf in human and mouse embryonic stem cells in contrast to ctcf we found that the binding profiles of and nanog are markedly different with only of the regions being homologously occupied we show that transposable elements contributed up to of the bound sites in humans and mice and have wired new genes into the core regulatory network of embryonic stem cells these data indicate that species specific transposable elements have substantially altered the transcriptional circuitry of pluripotent cells
motivation there has recently been a notable shift in biomedical information extraction ie from relation models toward the more expressive event model facilitated by the maturation of basic tools for biomedical text analysis and the availability of manually annotated resources the event model allows detailed representation of complex natural language statements and can support a number of advanced text mining applications ranging from semantic search to pathway extraction a recent collaborative evaluation demonstrated the potential of event extraction systems yet there have so far been no studies of the generalization ability of the systems nor the feasibility of large scale extraction results this study considers event based ie at pubmed scale we introduce a system combining publicly available state of the art methods for domain parsing named entity recognition and event extraction and test the system on a representative sample of all pubmed citations we present the first evaluation of the generalization performance of event extraction systems to this scale and show that despite its computational complexity event extraction from the entire pubmed is feasible we further illustrate the value of the extraction approach through a number of analyses of the extracted information availability the event detection system and extracted data are open source licensed and available at http bionlp utu fi contact jari bjorne utu bioinformatics
systems biology recognizes in particular the importance of interactions between biological components and the consequences of these interactions such interactions and their downstream effects are known as events to computationally mine the literature for such events text mining methods that can detect extract and annotate them are required this review summarizes the methods that are currently available with a specific focus on proteinprotein interactions and pathway or network reconstruction the approaches described will be of considerable value in associating particular pathways and their components with higher order physiological properties including states
genetic screens for phenotypic similarity have made key contributions to associating genes with biological processes with rna interference rnai highly parallel phenotyping of loss of function effects in cells has become feasible one of the current challenges however is the computational categorization of visual phenotypes and the prediction of biological function and processes in this study we describe a combined computational and experimental approach to discover novel gene functions and explore functional relationships we performed a genome wide rnai screen in human cells and used quantitative descriptors derived from high throughput imaging to generate multiparametric phenotypic profiles we show that profiles predicted functions of genes by phenotypic similarity specifically we examined several candidates including the largely uncharacterized gene donson which shared phenotype similarity with known factors of dna damage response ddr and genomic integrity experimental evidence supports that donson is a novel centrosomal protein required for ddr signalling and genomic integrity multiparametric phenotyping by automated imaging and computational annotation is a powerful method for functional discovery and mapping the landscape of phenotypic responses to perturbations
detection comparison and analyses of binding pockets are pivotal to structure based drug design endeavors from hit identification screening of exosites and de orphanization of protein functions to the anticipation of specific and non specific binding to off and anti targets here we analyze protein ligand complexes and discuss methods that assist binding site identification prediction of druggability and binding site comparison the full potential of pockets is yet to be harnessed and we envision that better understanding of the pocket space will have far reaching implications in the field of drug discovery such as the design of pocket specific compound libraries and functions
integrating results from diverse experiments is an essential process in our effort to understand the logic of complex systems such as development homeostasis and responses to the environment with the advent of high throughput methods including genome wide association gwa studies chromatin immunoprecipitation followed by sequencing chipseq and rna sequencing rnaseq acquisition of genome scale data has never been easier epigenomics transcriptomics proteomics and genomics each provide an insightful and yet one dimensional view of genome function integrative analysis promises a unified global view however the large amount of information and diverse technology platforms pose multiple challenges for data access and processing this review discusses emerging issues and strategies related to data integration in the era of next genomics
bioinformatics summary the mirror application provides insights on microrna mirna regulation it is based on the notion of a combinatorial regulation by an ensemble of mirnas or genes mirror integrates predictions from a dozen of mirna resources that are based on complementary algorithms into a unified statistical framework for mirnas set as input the online tool provides a ranked list of targets based on set of resources selected by the user according to their significance of being coordinately regulated symmetrically a set of genes can be used as input to suggest a set of mirnas the user can restrict the analysis for the preferred tissue or cell line mirror is suitable for analyzing results from mirnas profiling proteomics and gene expression arrays availability http www proto cs huji ac il mirrorcontact michall cc huji il
high throughput quantitative genetic interaction gi measurements provide detailed information regarding the structure of the underlying biological pathways by reporting on functional dependencies between genes however the analytical tools for fully exploiting such information lag behind the ability to collect these data we present a novel bayesian learning method that uses quantitative phenotypes of double knockout organisms to automatically reconstruct detailed pathway structures we applied our method to a recent data set that measures gis for endoplasmic reticulum er genes using the unfolded protein response as a quantitative phenotype the results provided reconstructions of known functional pathways including n linked glycosylation and er associated protein degradation it also contained novel relationships such as the placement of in the tail anchored biogenesis pathway a finding that we experimentally validated our approach should be readily applicable to the next generation of quantitative gi data sets as assays become available for additional phenotypes and eventually higher organisms
contemporary jews comprise an aggregate of ethno religious communities whose worldwide members identify with each other through various shared religious historical and cultural traditions historical evidence suggests common origins in the middle east followed by migrations leading to the establishment of communities of jews in europe africa and asia in what is termed the jewish diaspora this complex demographic history imposes special challenges in attempting to address the genetic structure of the jewish people although many genetic studies have shed light on jewish origins and on diseases prevalent among jewish communities including studies focusing on uniparentally and biparentally inherited markers genome wide patterns of variation across the vast geographic span of jewish diaspora communities and their respective neighbours have yet to be addressed here we use high density bead arrays to genotype individuals from jewish diaspora communities and compare these patterns of genome wide diversity with those from old world non jewish populations of which have not previously been reported these samples were carefully chosen to provide comprehensive comparisons between jewish and non jewish populations in the diaspora as well as with non jewish populations from the middle east and north africa principal component and structure like analyses identify previously unrecognized genetic substructure within the middle east most jewish samples form a remarkably tight subcluster that overlies druze and cypriot samples but not samples from other levantine populations or paired diaspora host populations in contrast ethiopian jews beta israel and indian jews bene israel and cochini cluster with neighbouring autochthonous populations in ethiopia and western india respectively despite a clear paternal link between the bene israel and the levant these results cast light on the variegated genetic architecture of the middle east and trace the origins of most jewish diaspora communities to levant
the autism spectrum disorders asds are a group of conditions characterized by impairments in reciprocal social interaction and communication and the presence of restricted and repetitive individuals with an asd vary greatly in cognitive development which can range from above average to intellectual although asds are known to be highly heritable the underlying genetic determinants are still largely unknown here we analysed the genome wide characteristics of rare frequency copy number variation in asd using dense genotyping arrays when comparing asd individuals of european ancestry to matched controls cases were found to carry a higher global burden of rare genic copy number variants cnvs fold p especially so for loci previously implicated in either asd and or intellectual disability fold p among the cnvs there were numerous de novo and inherited events sometimes in combination in a given family implicating many novel asd genes such as and the x linked locus we also discovered an enrichment of cnvs disrupting functional gene sets involved in cellular proliferation projection and motility and gtpase ras signalling our results reveal many new genetic and functional targets in asd that may lead to final pathways
motivation in statistical bioinformatics research different optimization mechanisms potentially lead to over optimism in published papers so far however a systematic critical study concerning the various sources underlying this over optimism is lacking results we present an empirical study on over optimism using high dimensional classification as example specifically we consider a promising new classification algorithm namely linear discriminant analysis incorporating prior knowledge on gene functional groups through an appropriate shrinkage of the within group covariance matrix while this approach yields poor results in terms of error rate we quantitatively demonstrate that it can artificially seem superior to existing approaches if we fish for significance the investigated sources of over optimism include the optimization of datasets of settings of competing methods and most importantly of the method s characteristics we conclude that if the improvement of a quantitative criterion such as the error rate is the main contribution of a paper the superiority of new algorithms should always be demonstrated on independent validation data availability the r codes and relevant data can be downloaded from http www ibe med uni muenchen de organisation mitarbeiter boulesteix overoptimism such that the study is completely reproducible contact boulesteix ibe med uni muenchen bioinformatics
the species is a fundamental unit of biological organization but its relevance for bacteria and archaea is still hotly debated even more controversial is whether the deeper branches of the ribosomal rna derived phylogenetic tree such as the phyla have ecological importance here we discuss the ecological coherence of high bacterial taxa in the light of genome analyses and present examples of niche differentiation between deeply diverging groups in terrestrial and aquatic systems the ecological relevance of high bacterial taxa has implications for bacterial taxonomy evolution ecology
pnas with the advent of systems biology the prediction of whether two proteins form a complex has become a problem of increased importance a variety of experimental techniques have been applied to the problem but three dimensional structural information has not been widely exploited here we explore the range of applicability of such information by analyzing the extent to which the location of binding sites on protein surfaces is conserved among structural neighbors we find as expected that interface conservation is most significant among proteins that have a clear evolutionary relationship but that there is a significant level of conservation even among remote structural neighbors this finding is consistent with recent evidence that information available from structural neighbors independent of classification should be exploited in the search for functional insights the value of such structural information is highlighted through the development of a new protein interface prediction method predus that identifies what residues on protein surfaces are likely to participate in complexes with other proteins the performance of predus as measured through comparisons with other methods suggests that relationships across protein structure space can be successfully exploited in the prediction of protein interactions
background for more than two decades microbiologists have used a highly conserved microbial gene as a phylogenetic marker for bacteria and archaea the small subunit ribosomal rna gene also known as s rrna is encoded by ribosomal dna s rdna and has provided a powerful comparative tool to microbial ecologists over time the microbial ecology field has matured from small scale studies in a select number of environments to massive collections of sequence data that are paired with dozens of corresponding collection variables as the complexity of data and tool sets have grown the need for flexible automation and maintenance of the core processes of s rdna sequence analysis has increased correspondingly results we present waters an integrated approach for s rdna analysis that bundles a suite of publicly available s rdna analysis software tools into a single software package the toolkit includes sequence alignment chimera removal otu determination taxonomy assignment phylogentic tree construction as well as a host of ecological analysis and visualization tools waters employs a flexible collection oriented workflow approach using the open source kepler system as a platform conclusions by packaging available software tools into a single automated workflow waters simplifies s rdna analyses especially for those without specialized bioinformatics programming expertise in addition waters like some of the newer comprehensive rrna analysis tools allows researchers to minimize the time dedicated to carrying out tedious informatics steps and to focus their attention instead on the biological interpretation of the results one advantage of waters over other comprehensive tools is that the use of the kepler workflow system facilitates result interpretation and reproducibility via a data provenance sub system furthermore new actors can be added to the workflow as desired and we see waters as an initial seed for a sizeable and growing repository of interoperable easy to combine tools for asking increasingly complex microbial questions
the identification of an increasing number of cancer genes is opening up unexpected scenarios in cancer genetics when analyzed for their systemic properties these genes show a general fragility towards perturbation a recent paper published in bmc biology shows how the founder domains of known cancer genes emerged at two macroevolutionary transitions the advent of the first cell and the transition to metazoan multicellularity see research article http www com
background during the last decade the internet has become increasingly popular and is now an important part of our daily life when new web technologies are used in health care the terms health or medicine may be used objective the objective was to identify unique definitions of health medicine and recurrent topics within the definitions methods a systematic literature review of electronic databases pubmed scopus cinahl and gray literature on the internet using the search engines google bing and yahoo was performed to find unique definitions of health medicine we assessed all literature extracted unique definitions and selected recurrent topics by using the constant comparison method results we found a total of articles in scientific databases and in the gray literature we selected unique definitions for further analysis and identified main topics conclusions health medicine are still developing areas many articles concerning this subject were found primarily on the internet however there is still no general consensus regarding the definition of health medicine we hope that this study will contribute to building the concept of health medicine and facilitate discussion and research
the vast computational power of the brain has traditionally been viewed as arising from the complex connectivity of neural networks in which an individual neuron acts as a simple linear summation and thresholding device however recent studies show that individual neurons utilize a wealth of nonlinear mechanisms to transform synaptic input into output firing these mechanisms can arise from synaptic plasticity synaptic noise and somatic and dendritic conductances this tool kit of nonlinear mechanisms confers considerable computational power on both morphologically simple and more complex neurons enabling them to perform a range of arithmetic operations on signals encoded in a variety of ways
background the seed integrates many publicly available genome sequences into a single resource the database contains accurate and up to date annotations based on the subsystems concept that leverages clustering between genomes and other clues to accurately and efficiently annotate microbial genomes the backend is used as the foundation for many genome annotation tools such as the rapid annotation using subsystems technology rast server for whole genome annotation the metagenomics rast server for random community genome annotations and the annotation clearinghouse for exchanging annotations from different resources in addition to a web user interface the seed also provides web services based api for programmatic access to the data in the seed allowing the development of third party tools and mash ups results the currently exposed web services encompass over forty different methods for accessing data related to microbial genome annotations the web services provide comprehensive access to the database back end allowing any programmer access to the most consistent and accurate genome annotations available the web services are deployed using a platform independent service oriented approach that allows the user to choose the most suitable programming platform for their application example code demonstrate that web services can be used to access the seed using common bioinformatics programming languages such as perl python and java conclusions we present a novel approach to access the seed database using web services a robust api for access to genomics data is provided without requiring large volume downloads all at once the api ensures timely access to the most current datasets available including the new genomes as soon as they online
multiple constraints variously affect different parts of the genomes of diverse life forms the selective pressures that shape the evolution of viral archaeal bacterial and eukaryotic genomes differ markedly even among relatively closely related animal and bacterial lineages by contrast constraints affecting protein evolution seem to be more universal the constraints that shape the evolution of genomes and phenomes are complemented by the plasticity and robustness of genome architecture expression and regulation taken together these findings are starting to reveal complex networks of evolutionary processes that must be integrated to attain a new synthesis of biology
the groundwork for analysing the human microbiome sequencing the collective genome of all our resident microorganisms is now done this work is of significance for understanding both human health and disease did you know that humans have two genomes indeed although most people appreciate that humans inherit a genome many fail to realize that the collective genetic information encoded in all the microorganisms acquired from the environment which are collectively known as the microbiome and generally live harmoniously with us constitutes a genome
biological data and particularly annotation data are increasingly being represented in directed acyclic graphs dags however while relevant biological information is implicit in the links between multiple domains annotations from these different domains are usually represented in distinct unconnected dags making links between the domains represented difficult to determine we develop a novel family of general statistical tests for the discovery of strong associations between two directed acyclic graphs our method takes the topology of the input graphs and the specificity and relevance of associations between nodes into consideration we apply our method to the extraction of associations between biomedical ontologies in an extensive use case through a manual and an automatic evaluation we show that our tests discover biologically relevant relations the suite of statistical tests we develop for this purpose is implemented and freely available download
background massively parallel dna sequencing technologies have enabled the sequencing of several individual human genomes these technologies are also being used in novel ways for mrna expression profiling genome wide discovery of transcription factor binding sites small rna discovery etc the multitude of sequencing platforms each with their unique characteristics pose a number of design challenges regarding the technology to be used and the depth of sequencing required for a particular sequencing application here we describe a number of analytical and empirical results to address design questions for two applications detection of structural variations from paired end sequencing and estimating mrna transcript abundance results for structural variation our results provide explicit trade offs between the detection and resolution of rearrangement breakpoints and the optimal mix of paired read insert lengths specifically we prove that optimal detection and resolution of breakpoints is achieved using a mix of exactly two insert library lengths further more we derive explicit formulae to determine these insert length combinations enabling a improvement in breakpoint detection at the same experimental cost on empirical short read data these predictions show good concordance with illumina and insert length libraries for transcriptome sequencing we determine the sequencing depth needed to detect rare transcripts from a small pilot study with only million reads we derive corrections that enable almost perfect prediction of the underlying expression probability distribution and use this to predict the sequencing depth required to detect low expressed genes with greater than probability conclusions together our results form a generic framework for many design considerations related to high throughput sequencing we provide software tools http bix ucsd edu projects ngs designtools to derive platform independent guidelines for designing sequencing experiments amount of sequencing choice of insert length mix of libraries for novel applications of next sequencing
although more than genes have been shown to contain variants that cause mendelian disease there are still several thousand such diseases yet to be molecularly defined the ability of new whole genome sequencing technologies to rapidly indentify most of the genetic variants in any given genome opens an exciting opportunity to identify these disease genes here we sequenced the whole genome of a single patient with the dominant mendelian disease metachondromatosis omim and used partial linkage data from her small family to focus our search for the responsible variant in the proband we identified an bp deletion in exon four of which alters frame results in premature translation termination and co segregates with the phenotype in a second metachondromatosis family we confirmed our result by identifying a nonsense mutation in exon of that also co segregates with the phenotype sequencing exon in controls showed no such protein truncating variants supporting the pathogenicity of these two mutations this combination of a new technology and a classical genetic approach provides a powerful strategy to discover the genes responsible for unexplained disorders
as bioinformatics becomes increasingly central to research in the molecular life sciences the need to train non bioinformaticians to make the most of bioinformatics resources is growing here we review the key challenges and pitfalls to providing effective training for users of bioinformatics services and discuss successful training strategies shared by a diverse set of bioinformatics trainers we also identify steps that trainers in bioinformatics could take together to advance the state of the art in current training practices the ideas presented in this article derive from the first trainer networking session held under the auspices of the eu funded sling integrating activity which took place november
motivation the advent of high throughput sequencing hts technologies has made it affordable to sequence many individuals genomes simultaneously the computational analysis of the large volumes of data generated by the new sequencing machines remains a challenge while a plethora of tools are available to map the resulting reads to a reference genome and to conduct primary analysis of the mappings it is often necessary to visually examine the results and underlying data to confirm predictions and understand the functional effects especially in the context of other datasets results we introduce savant the sequence annotation visualization and analysis tool a desktop visualization and analysis browser for genomic data savant was developed for visualizing and analyzing hts data with special care taken to enable dynamic visualization in the presence of gigabases of genomic reads and references the size of the human genome savant supports the visualization of genome based sequence point interval and continuous datasets and multiple visualization modes that enable easy identification of genomic variants including single nucleotide polymorphisms structural and copy number variants and functional genomic information e g peaks in chip seq data in the context of genomic annotations availability savant is freely available at http compbio cs toronto edu savantcontact savant cs edu
summary here we describe a tool suite that functions on all of the commonly known fastq format variants and provides a pipeline for manipulating next generation sequencing data taken from a sequencing machine all the way through the quality filtering steps availability and implementation this open source toolset was implemented in python and has been integrated into the online data analysis platform galaxy public web access http usegalaxy org download http getgalaxy org two short movies that highlight the functionality of tools described in this manuscript as well as results from testing components of this tool suite against a set of previously published files are available at http usegalaxy org u dan fastq
motivation the accuracy of reference genomes is important for downstream analysis but a low error rate requires expensive manual interrogation of the sequence here we describe a novel algorithm iterative correction of reference nucleotides that iteratively aligns deep coverage of short sequencing reads to correct errors in reference genome sequences and evaluate their accuracy results using plasmodium falciparum a t content as an extreme example we show that the algorithm is highly accurate and corrects over errors in the reference sequence we give examples of its application to numerous other eukaryotic and prokaryotic genomes and suggest additional applications availability the software is available at http icorn sourceforge net contact tdo sanger ac uk cnewbold hammer imm ox ac uk supplementary information supplementary data are available at bioinformatics bioinformatics
eukaryotic cells spatially organize mrna processes such as translation and mrna decay much less is clear in bacterial cells where the spatial distribution of mature mrna remains ambiguous using a sensitive method based on quantitative fluorescence in situ hybridization we show here that in caulobacter crescentus and escherichia coli chromosomally expressed mrnas largely display limited dispersion from their site of transcription during their lifetime we estimate apparent diffusion coefficients at least two orders of magnitude lower than expected for freely diffusing mrna and provide evidence in c crescentus that this mrna localization restricts ribosomal mobility furthermore c crescentus rnase e appears associated with the dna independently of its mrna substrates collectively our findings show that bacteria can spatially organize translation and potentially mrna decay by using the chromosome layout as a template this chromosome centric organization has important implications for cellular physiology and for our understanding of gene expression bacteria
snps discovered by genome wide association studies gwass account for only a small fraction of the genetic variation of complex traits in human populations where is the remaining heritability we estimated the proportion of variance for human height explained by snps genotyped on unrelated individuals using a linear model analysis and validated the estimation method with simulations based on the observed genotype data we show that of variance can be explained by considering all snps simultaneously thus most of the heritability is not missing but has not previously been detected because the individual effects are too small to pass stringent significance tests we provide evidence that the remaining heritability is due to incomplete linkage disequilibrium between causal variants and genotyped snps exacerbated by causal variants having lower minor allele frequency than the snps explored date
pnas although preliminary estimates from published literature and expert surveys suggest striking agreement among climate scientists on the tenets of anthropogenic climate change acc the american public expresses substantial doubt about both the anthropogenic cause and the level of scientific agreement underpinning acc a broad analysis of the climate scientist community itself the distribution of credibility of dissenting researchers relative to agreeing researchers and the level of agreement among top climate experts has not been conducted and would inform future acc discussions here we use an extensive dataset of climate researchers and their publication and citation data to show that i of the climate researchers most actively publishing in the field surveyed here support the tenets of acc outlined by the intergovernmental panel on climate change and ii the relative climate expertise and scientific prominence of the researchers unconvinced of acc are substantially below that of the researchers
cito the citation typing ontology is an ontology for describing the nature of reference citations in scientific research articles and other scholarly works both to other such publications and also to web information resources and for publishing these descriptions on the semantic web citation are described in terms of the factual and rhetorical relationships between citing publication and cited publication the in text and global citation frequencies of each cited work and the nature of the cited work itself including its publication and peer review status this paper describes cito and illustrates its usefulness both for the annotation of bibliographic reference lists and for the visualization of citation networks the latest version of cito which this paper describes is cito version published on march cito is written in the web ontology language owl uses the namespace http purl org net cito and is available from http purl org net cito this site uses content negotiation to deliver to the user an owldoc web version of the ontology if accessed via a web browser or the owl ontology itself if accessed from an ontology management tool such as protege http protege stanford edu collaborative work is currently under way to harmonize cito with other ontologies describing bibliographies and the rhetorical structure of discourse
the canonical role of messenger rna mrna is to deliver protein coding information to sites of protein synthesis however given that micrornas bind to rnas we hypothesized that rnas could possess a regulatory role that relies on their ability to compete for microrna binding independently of their protein coding function as a model for the protein coding independent role of rnas we describe the functional relationship between the mrnas produced by the pten tumour suppressor gene and its pseudogene and the critical consequences of this interaction we find that is biologically active as it can regulate cellular levels of pten and exert a growth suppressive role we also show that the locus is selectively lost in human cancer we extended our analysis to other cancer related genes that possess pseudogenes such as oncogenic kras we also demonstrate that the transcripts of protein coding genes such as pten are biologically active these findings attribute a novel biological role to expressed pseudogenes as they can regulate coding gene expression and reveal a non coding function mrnas
background with advances in high throughput genomics and proteomics it is challenging for biologists to deal with large data files and to map their data to annotations in public databases results we developed tabsql a mysql based application tool for viewing filtering and querying data files with large numbers of rows tabsql provides functions for downloading and installing table files from public databases including the gene ontology database go the ensembl databases and genome databases from the ucsc genome bioinformatics site any other database that provides tab delimited flat files can also be imported the downloaded gene annotation tables can be queried together with users data in tabsql using either a graphic interface or command line conclusions tabsql allows queries across the user s data and public databases without programming it is a convenient tool for biologists to annotate and enrich data
summary a tool to predict the effect that newly discovered genomic variants have on known transcripts is indispensible in prioritizing and categorizing such variants in ensembl a web based tool the snp effect predictor and api interface can now functionally annotate variants in all ensembl and ensembl genomes supported species availability the ensembl snp effect predictor can be accessed via the ensembl website at http www ensembl org the ensembl api http www ensembl org info docs api html for installation instructions is open software
despite the recent rapid growth in genome wide data much of human variation remains entirely unexplained a significant challenge in the pursuit of the genetic basis for variation in common human traits is the efficient coordinated collection of genotype and phenotype data we have developed a novel research framework that facilitates the parallel study of a wide assortment of traits within a single cohort the approach takes advantage of the interactivity of the web both to gather data and to present genetic information to research participants while taking care to correct for the population structure inherent to this study design here we report initial results from a participant driven study of traits replications of associations in the genes tyr asip and for hair color eye color and freckling validate the web based self reporting paradigm the identification of novel associations for hair morphology near tchh near and near freckling in the ability to smell the methanethiol produced after eating asparagus near and photic sneeze reflex near and near illustrates the power of approach
the internet has recently made possible the free global availability of scientific journal articles open access oa can occur either via oa scientific journals or via authors posting manuscripts of articles published in subscription journals in open web repositories so far there have been few systematic studies showing how big the extent of oa is in particular studies covering all fields science
hamilton s rule states that cooperation will evolve if the fitness cost to actors is less than the benefit to recipients multiplied by their genetic relatedness this rule makes many simplifying assumptions however and does not accurately describe social evolution in organisms such as microbes where selection is both strong and nonadditive we derived a generalization of hamilton s rule and measured its parameters in myxococcus xanthus bacteria nonadditivity made cooperative sporulation remarkably resistant to exploitation by cheater strains selection was driven by higher order moments of population structure not relatedness these results provide an empirically testable cooperation principle applicable to both microbes and multicellular organisms and show how nonlinear interactions among cells insulate bacteria against science
summarytwo abundant classes of mobile elements namely alu and elements continue to generate new retrotransposon insertions in human genomes estimates suggest that these elements have generated millions of new germline insertions in individual human genomes worldwide unfortunately current technologies are not capable of detecting most of these young insertions and the true extent of germline mutagenesis by endogenous human retrotransposons has been difficult to examine here we describe technologies for detecting these young retrotransposon insertions and demonstrate that such insertions indeed are abundant in human populations we alsofound that new somatic insertions occur at high frequencies in human lung cancer genomes genome wide analysis suggests that altered dna methylation may be responsible for the high levels of mobilization observed in these tumors our data indicate that transposon mediated mutagenesis is extensive in human genomes and is likely to have a major impact on human biology and diseases graphical abstractfull size image high quality image highlights transposon seq methods were developed to find mobile element insertions in humans new germline retrotransposon insertions were identified in personal human genomes tumor specific somatic insertions were uncovered in human lung cancer genomes transposon mutagenesis is likely to have a major impact on human traits diseases
functional genomics is rapidly progressing towards the elucidation of elements that are crucial for the cis regulatory control of gene expression and population based studies of disease and gene expression traits are yielding widespread evidence of the influence of non coding variants on trait variance recently genome wide allele specific approaches that harness high throughput sequencing technology have started to allow direct evaluation of how these cis regulatory polymorphisms control gene expression and affect chromatin states the emerging data is providing exciting opportunities for comprehensive characterization of the allele specific events that govern human regulation
we review the current theory of how galaxies form within the cosmological framework provided by the cold dark matter paradigm for structure formation beginning with the pre galactic evolution of baryonic material we describe the analytical and numerical understanding of how baryons condense into galaxies what determines the structure of those galaxies and how internal and external processes including star formation merging active galactic nuclei etc determine their gross properties and evolution throughout we highlight successes and failures of current galaxy formation theory we include a review of computational implementations of galaxy formation theory and assess their ability to provide reliable modeling of this complex phenomenon we finish with a discussion of several hot topics in contemporary galaxy formation theory and assess future directions for field
background tiling arrays have been the tool of choice for probing an organism s transcriptome without prior assumptions about the transcribed regions but rna seq is becoming a viable alternative as the costs of sequencing continue to decrease understanding the relative merits of these technologies will help researchers select the appropriate technology for their needs results here we compare these two platforms using a matched sample of poly a enriched rna isolated from the second larval stage of c elegans we find that the raw signals from these two technologies are reasonably well correlated but that rna seq outperforms tiling arrays in several respects notably in exon boundary detection and dynamic range of expression by exploring the accuracy of sequencing as a function of depth of coverage we found that about million reads are required to match the sensitivity of two tiling array replicates the effects of cross hybridization were analyzed using a nearest neighbor classifier applied to array probes we describe a method for determining potential black list regions whose signals are unreliable finally we propose a strategy for using rna seq data as a gold standard set to calibrate tiling array data all tiling array and rna seq data sets have been submitted to the modencode data coordinating center conclusions tiling arrays effectively detect transcript expression levels at a low cost for many species while rna seq provides greater accuracy in several regards researchers will need to carefully select the technology appropriate to the biological investigations they are undertaking it will also be important to reconsider a comparison such as ours as sequencing technologies continue evolve
recent advances in computing have led to an explosion in the amount of data being generated processing the ever growing data in a timely manner has made throughput computing an important aspect for emerging applications our analysis of a set of important throughput computing kernels shows that there is an ample amount of parallelism in these kernels which makes them suitable for today s multi core cpus and gpus in the past few years there have been many studies claiming gpus deliver substantial speedups between and over multi core cpus on these kernels to understand where such large performance difference comes from we perform a rigorous performance analysis and find that after applying optimizations appropriate for both cpus and gpus the performance gap between an nvidia processor and the intel core processor narrows to only on average in this paper we discuss optimization techniques for both cpu and gpu analyze what architecture features contributed to performance differences between the two architectures and recommend a set of architectural features which provide significant improvement in architectural efficiency for kernels
background quantitative models of biochemical and cellular systems are used to answer a variety of questions in the biological sciences the number of published quantitative models is growing steadily thanks to increasing interest in the use of models as well as the development of improved software systems and the availability of better cheaper computer hardware to maximise the benefits of this growing body of models the field needs centralised model repositories that will encourage facilitate and promote model dissemination and reuse ideally the models stored in these repositories should be extensively tested and encoded in community supported and standardised formats in addition the models and their components should be cross referenced with other resources in order to allow their unambiguous identification description biomodels database http www ebi ac uk biomodels is aimed at addressing exactly these needs it is a freely accessible online resource for storing viewing retrieving and analysing published peer reviewed quantitative models of biochemical and cellular systems the structure and behaviour of each simulation model distributed by biomodels database are thoroughly checked in addition model elements are annotated with terms from controlled vocabularies as well as linked to relevant data resources models can be examined online or downloaded in various formats reaction network diagrams generated from the models are also available in several formats biomodels database also provides features such as online simulation and the extraction of components from large scale models into smaller submodels finally the system provides a range of web services that external software systems can use to access up to date data from the database conclusions biomodels database has become a recognised reference resource for systems biology it is being used by the community in a variety of ways for example it is used to benchmark different simulation systems and to study the clustering of models based upon their annotations model deposition to the database today is advised by several publishers of scientific journals the models in biomodels database are freely distributed and reusable the underlying software infrastructure is also available from sourceforge https sourceforge net projects biomodels under the gnu general license
the study of networks has grown into a substantial interdisciplinary endeavoracross the natural social and information sciences yet there have been veryfew attempts to investigate the interrelatedness of the different classes ofnetworks studied by different disciplines here we introduced a framework toestablish a taxonomy of networks from various origins the provision of thisfamily tree not only helps understand the kinship of networks but alsofacilitates the transfer of empirical analysis theoretical modeling andconceptual developments across disciplinary boundaries the framework is basedon probing the mesoscopic properties of networks an important source ofheterogeneity for their structure and function using our method we computed ataxonomy for individual networks and a separate taxonomy for networkclasses we also computed three within class taxonomies for political fungal and financial networks and found them to be insightful in case
summary in recent years the number of knowledge bases developed using wiki technology has exploded unfortunately next to their numerous advantages classical wikis present a critical limitation the invaluable knowledge they gather is represented as free text which hinders their computational exploitation this is in sharp contrast with the current practice for biological databases where the data is made available in a structured way here we present wikiopener an extension for the classical mediawiki engine that augments wiki pages by allowing on the fly querying and formatting resources external to the wiki those resources may provide data extracted from databases or das tracks or even results returned by local or remote bioinformatics analysis tools this also implies that structured data can be edited via dedicated forms hence this generic resource combines the structure of biological databases with the flexibility of collaborative wikis availability the source code and its documentation are freely available on the mediawiki website http www mediawiki org wiki extension wikiopener contact sbrohee esat be
background the study of protein small molecule interactions is vital for understanding protein function and for practical applications in drug discovery to benefit from the rapidly increasing structural data it is essential to improve the tools that enable large scale binding site prediction with greater emphasis on their biological validity results we have developed a new method for the annotation of protein small molecule binding sites using inference by homology which allows us to extend annotation onto protein sequences without experimental data available to ensure biological relevance of binding sites our method clusters similar binding sites found in homologous protein structures based on their sequence and structure conservation binding sites which appear evolutionarily conserved among non redundant sets of homologous proteins are given higher priority after binding sites are clustered position specific score matrices pssms are constructed from the corresponding binding site alignments together with other measures the pssms are subsequently used to rank binding sites to assess how well they match the query and to better gauge their biological relevance the method also facilitates a succinct and informative representation of observed and inferred binding sites from homologs with known three dimensional structures thereby providing the means to analyze conservation and diversity of binding modes furthermore the chemical properties of small molecules bound to the inferred binding sites can be used as a starting point in small molecule virtual screening the method was validated by comparison to other binding site prediction methods and to a collection of manually curated binding site annotations we show that our method achieves a sensitivity of at predicting biologically relevant binding sites and can accurately discriminate those sites that bind biological small molecules from non biological ones conclusions a new algorithm has been developed to predict binding sites with high accuracy in terms of their biological validity it also provides a common platform for function prediction knowledge based docking and for small molecule virtual screening the method can be applied even for a query sequence without structure the method is available at http www ncbi nlm nih gov structure ibis cgi
healthy aging is thought to reflect the combined influence of environmental factors lifestyle choices and genetic factors to explore the genetic contribution we undertook a genome wide association study of exceptional longevity el in centenarians and controls using these data we built a genetic model that includes single nucleotide polymorphisms snps and found that it could predict el with accuracy in an independent set of centenarians and controls further in silico analysis revealed that of centenarians can be grouped into clusters characterized by different combinations of snp genotypes or genetic signatures of varying predictive value the different signatures which attest to the genetic complexity of el correlated with differences in the prevalence and age of onset of age associated diseases e g dementia hypertension and cardiovascular disease and may help dissect this complex phenotype into subphenotypes of aging
residents of the tibetan plateau show heritable adaptations to extreme altitude we sequenced exomes of ethnic tibetans encompassing coding sequences of of human genes with an average coverage of per individual genes showing population specific allele frequency changes which represent strong candidates for altitude adaptation were identified the strongest signal of natural selection came from endothelial per arnt sim pas domain protein a transcription factor involved in response to hypoxia one single nucleotide polymorphism snp at shows a frequency difference between tibetan and han samples representing the fastest allele frequency change observed at any human gene to date this snp s association with erythrocyte abundance supports the role of in adaptation to hypoxia thus a population genomic survey has revealed a functionally important locus in genetic adaptation to altitude
summaryhighly active i e hot long interspersed element line or sequences comprise the bulk of retrotransposition activity in the human genome however the abundance of hot in the human population remains largely unexplored here we used a fosmid based paired end dna sequencing strategy to identify full length that are differentially present among individuals but are absent from the human genome reference sequence the majority of these were highly active in a cultured cell retrotransposition assay genotyping elements revealed that two are only found in africa and that two more are absent from the subset of the human genome diversity panel therefore these results suggest that hot are more abundant in the human population than previously appreciated and that ongoing retrotransposition continues to be a major source of interindividual genetic variation graphical abstractfull size image high quality image highlights the human population contains more active line than previously appreciated active line elements are underrepresented in existing sequence databases line retrotransposition is a major source of interindividual variation
background the wealth of prokaryotic genomic data available has revealed that the histories of many genes are inconsistent leading some to question the value of the tree of life hypothesis it has been argued that a tree like representation requires suppressing too much information and that a more pluralistic approach is necessary for understanding prokaryotic evolution we argue that trees may still be a useful representation for evolutionary histories in light of new data results genomic data alone can be highly misleading when trying to resolve the tree of life we present evidence from protein abundance data sets that genomic conservation greatly underestimates functional conservation function follows more of a tree like structure than genetic material even in the presence of horizontal transfer we argue that the tree of cells must be incorporated into any new synthesis in order to place horizontal transfers into their proper selective context we also discuss the role data sources other than primary sequence can play in resolving the tree of cells conclusions the tree of life is alive but not well construction of the tree of cells has been viewed as the end goal of the study of evolution where in reality we need to consider it more of a starting point we propose a duality where we must consider variation of genetic material in terms of networks and selection of cellular function in terms of trees otherwise one gets lost in the woods of neutral evolution reviewers this article was reviewed by dr eric bapteste dr arcady mushegian and dr brochier
people often act in order to realize desired outcomes or goals although behavioral science recognizes that people can skillfully pursue goals without consciously attending to their behavior once these goals are set conscious will is considered to be the starting point of goal pursuit indeed when we decide to work hard on a task it feels as if that conscious decision is the first and foremost cause of our behavior that is we are likely to say if asked that the decision to act produced the actions themselves recent discoveries however challenge this causal status of conscious will they demonstrate that under some conditions actions are initiated even though we are unconscious of the goals to be attained or their motivating effect on our behavior here we analyze how goal pursuit can possibly operate unconsciously copyright by the american association for the advancement of science all reserved
remarkable progress in optical microscopy has been made in the measurement of nanometre distances if diffraction blurs the image of a point object into an airy disk with a root mean squared r m s size of s for light with a wavelength of and an objective lens with a numerical aperture of na limiting the resolution of the far field microscope in use to d additional knowledge about the specimen can be used to great advantage for example if the source is known to be two spatially resolved fluorescent molecules the distance between them is given by the separation of the centres of the two fluorescence in high resolution microwave and optical spectroscopy there are numerous examples where the line centre is determined with a precision of less than of the linewidth in contrast in biological applications the brightest single fluorescent emitters can be detected with a signal to noise ratio of limiting the centroid localization precision to of the r m s size s of the microscope point spread function psf moreover the error in co localizing two or more single emitters is notably worse remaining greater than of the psf here we report a distance resolution of sreg and an absolute accuracy of sdistance in a measurement of the separation between differently coloured fluorescent molecules using conventional far field fluorescence imaging in physiological buffer conditions the statistical uncertainty in the mean for an ensemble of identical single molecule samples is limited only by the total number of collected photons to which is times the size of the optical psf our method may also be used to improve the resolution of many subwavelength far field imaging methods such as those based on co localization of molecules that are stochastically switched on in the improved resolution will allow the structure of large multisubunit biological complexes in biologically relevant environments to be deciphered at the single level
summary we present the first parallel implementation of the t coffee consistency based multiple aligner we benchmark it on the amazon elastic cloud and show that the parallelization procedure is reasonably effective we also conclude that for a web server with moderate usage hits month the cloud provides a cost effective alternative to in house deployment availability t coffee is a freeware open source package available from http www tcoffee org homepage htmlcontact cedric notredame es
high throughput sequencing platforms are generating massive amounts of genetic variation data for diverse genomes but it remains a challenge to pinpoint a small subset of functionally important variants to fill these unmet needs we developed the annovar tool to annotate single nucleotide variants snvs and insertions deletions such as examining their functional consequence on genes inferring cytogenetic bands reporting functional importance scores finding variants in conserved regions or identifying variants reported in the genomes project and dbsnp annovar can utilize annotation databases from the ucsc genome browser or any annotation data set conforming to generic feature format version we also illustrate a variants reduction protocol on snvs and indels from a human genome including two causal mutations for miller syndrome a rare recessive disease through a stepwise procedure we excluded variants that are unlikely to be causal and identified candidate genes including the causal gene using a desktop computer annovar requires to perform gene based annotation and to perform variants reduction on variants making it practical to handle hundreds of human genomes in a day annovar is freely available at http www openbioinformatics annovar
pyrosequencing of pcr amplified fragments that target variable regions within the rrna gene has quickly become a powerful method for analyzing the membership and structure of microbial communities this approach has revealed and introduced questions that were not fully appreciated by those carrying out traditional sanger sequencing based methods these include the effects of alignment quality the best method of calculating pairwise genetic distances for rrna genes whether it is appropriate to filter variable regions and how the choice of variable region relates to the genetic diversity observed in full length sequences i used a diverse collection of high quality full length sequences to assess each of these questions first alignment quality had a significant impact on distance values and downstream analyses specifically the greengenes alignment which does a poor job of aligning variable regions predicted higher genetic diversity richness and phylogenetic diversity than the silva and rdp based alignments second the effect of different gap treatments in determining pairwise genetic distances was strongly affected by the variation in sequence length for a region however the effect of different calculation methods was subtle when determining the sample s richness or phylogenetic diversity for a region third applying a sequence mask to remove variable positions had a profound impact on genetic distances by muting the observed richness and phylogenetic diversity finally the genetic distances calculated for each of the variable regions did a poor job of correlating with the full length gene thus while it is tempting to apply traditional cutoff levels derived for full length sequences to these shorter sequences it is not advisable analysis of diversity metrics showed that each of these factors can have a significant impact on the comparison of community membership and structure taken together these results urge caution in the design and interpretation of analyses using data
transcription is the first step connecting genetic information with an organism s phenotype while expression of annotated genes in the human brain has been characterized extensively our knowledge about the scope and the conservation of transcripts located outside of the known genes boundaries is limited here we use high throughput transcriptome sequencing rna seq to characterize the total non ribosomal transcriptome of human chimpanzee and rhesus macaque brain in all species only of non ribosomal transcripts correspond to annotated exons and to introns by contrast transcripts originating within intronic and intergenic repetitive sequences constitute of the total brain transcriptome notably some repeat families show elevated transcription in non repetitive intergenic regions we identify and characterize distinct regions highly expressed in the human brain these regions are conserved at the rna expression level across primates studied and at the dna sequence level across mammals a large proportion of these transcripts represents extensions of known genes and may play roles in alternative microrna directed regulation finally we show that while transcriptome divergence between species increases with evolutionary time intergenic transcripts show more expression differences among species and exons show less our results show that many yet uncharacterized evolutionary conserved transcripts exist in the human brain some of these transcripts may play roles in transcriptional regulation and contribute to evolution of human specific traits
long intergenic noncoding rnas lincrnas regulate chromatin states and epigenetic inheritance here we show that the lincrna hotair serves as a scaffold for at least two distinct histone modification complexes a domain of hotair binds polycomb repressive complex whereas a domain of hotair binds the corest rest complex the ability to tether two distinct complexes enables rna mediated assembly of and and coordinates targeting of and to chromatin for coupled histone lysine methylation and lysine demethylation our results suggest that lincrnas may serve as scaffolds by providing binding surfaces to assemble select histone modification enzymes thereby specifying the pattern of histone modifications on genes
next generation dna sequencing coupled with chromatin immunoprecipitation chip seq is revolutionizing our ability to interrogate whole genome protein dna interactions identification of protein binding sites from chip seq data has required novel computational tools distinct from those used for the analysis of chip chip experiments the growing popularity of chip seq spurred the development of many different analytical programs at last count we noted open source methods each with some purported advantage given that the literature is dense and empirical benchmarking challenging selecting an appropriate method for chip seq analysis has become a daunting task herein we compare the performance of eleven different peak calling programs on common empirical transcription factor datasets and measure their sensitivity accuracy and usability our analysis provides an unbiased critical assessment of available technologies and should assist researchers in choosing a suitable tool for handling chip data
background long considered to be the building block of life it is now apparent that protein is only one of many functional products generated by the eukaryotic genome indeed more of the human genome is transcribed into noncoding sequence than into protein coding sequence nevertheless whilst we have developed a deep understanding of the relationships between evolutionary constraint and function for protein coding sequence little is known about these relationships for non coding transcribed sequence this dearth of information is partially attributable to a lack of established non protein coding ncrna orthologs among birds and mammals within sequence and expression databases results here we performed a multi disciplinary study of four highly conserved and brain expressed transcripts selected from a list of mouse long intergenic noncoding lncrna loci that generally show pronounced evolutionary constraint within their putative promoter regions and across exon intron boundaries we identify some of the first lncrna orthologs present in birds chicken marsupial opossum and eutherian mammals mouse and investigate whether they exhibit conservation of brain expression in contrast to conventional protein coding genes the sequences transcriptional start sites exon structures and lengths for these non coding genes are all highly variable conclusions the biological relevance of lncrnas would be highly questionable if they were limited to closely related phyla instead their preservation across diverse amniotes their residual apparent conservation in exon structure and similarities in their pattern of brain expression during embryonic and early postnatal stages together indicate that these are functional rna molecules of which some have roles in vertebrate development
given the accumulation of dna sequence data sets at ever faster rates what are the key factors you should consider when using distributed and multicore computing systems for analysis in the race between dna sequencing throughput and computer speed sequencing is winning by a mile sequencing throughput has recently been improving at a rate of about fivefold per whereas computer performance generally follows moore s law doubling only or
the nad dependent deacetylase was initially identified as a mediator of replicative lifespan in budding yeast and was subsequently shown to modulate longevity in worms and flies its mammalian homologue seems to have evolved complex systemic roles in cardiac function dna repair and genomic stability recent studies suggest a functional relevance of in normal brain physiology and neurological disorders however it is unknown if has a role in higher order brain functions we report that modulates synaptic plasticity and memory formation via a microrna mediated mechanism activation of enhances whereas its loss of function impairs synaptic plasticity surprisingly these effects were mediated via post transcriptional regulation of camp response binding protein creb expression by a brain specific microrna mir normally functions to limit expression of mir via a repressor complex containing the transcription factor and unchecked mir expression following deficiency results in the downregulated expression of creb and brain derived neurotrophic factor bdnf thereby impairing synaptic plasticity these findings demonstrate a new role for in cognition and a previously unknown microrna based mechanism by which regulates these processes furthermore these results describe a separate branch of signalling in which has a direct role in regulating normal brain function in a manner that is disparate from its cell survival functions demonstrating its value as a potential therapeutic target for the treatment of central nervous system disorders copyright macmillan publishers limited all reserved
the rise and fall of a research field is the cumulative outcome of its intrinsic scientific value and social coordination among scientists the structure of the social component is quantifiable by the social network of researchers linked via co authorship relations which can be tracked through digital records here we use such co authorship data in theoretical physics and study their complete evolutionary trail since inception with a particular emphasis on the early transient stages we find that the co authorship networks evolve through three common major processes in time the nucleation of small isolated components the formation of a tree like giant component through cluster aggregation and the entanglement of the network by large scale loops the giant component is constantly changing yet robust upon link degradations forming the network s dynamic core the observed patterns are successfully reproducible through a new model
genomic imprinting results in preferential expression of the paternal or maternal allele of certain genes we have performed a genome wide characterization of imprinting in the mouse embryonic and adult brain this approach uncovered parent of origin allelic effects of more than loci we identified parental bias in the expression of individual genes and of specific transcript isoforms with differences between brain regions many imprinted genes are expressed in neural systems associated with feeding and motivated behaviors and parental biases preferentially target genetic pathways governing metabolism and cell adhesion we observed a preferential maternal contribution to gene expression in the developing brain and a major paternal contribution in the adult brain thus parental expression bias emerges as a major mode of epigenetic regulation in brain
most of the human genome consists of non protein coding dna recently progress has been made in annotating these non coding regions through the interpretation of functional genomics experiments and comparative sequence analysis one can conceptualize functional genomics analysis as involving a sequence of steps turning the output of an experiment into a signal at each base pair of the genome smoothing this signal and segmenting it into small blocks of initial annotation and then clustering these small blocks into larger derived annotations and networks finally one can relate functional genomics annotations to conserved units and measures of conservation derived from comparative analysis
the growing flood of scholarly literature is exposing the weaknesses of current citationbased methods of evaluating and filtering articles a novel and promising approach is to examine the use and citation of articles in a new forum web services like social bookmarking and microblogging metrics based on this data could build a scientometics supporting richer and more timely pictures of articles impact this paper develops the most comprehensive list of these services to date assessing the potential value and availability of data from each we also suggest the next steps toward building and validating metrics drawn from the web
background high throughput sequencing has become an increasingly important tool for biological research however the existing software systems for managing and processing these data have not provided the flexible infrastructure that research requires results existing software solutions provide static and well established algorithms in a restrictive package however as high throughput sequencing is a rapidly evolving field such static approaches lack the ability to readily adopt the latest advances and techniques which are often required by researchers we have used a loosely coupled service oriented infrastructure to develop seqadapt this system streamlines data management and allows for rapid integration of novel algorithms our approach also allows computational biologists to focus on developing and applying new methods instead of writing boilerplate infrastructure code conclusion the system is based around the addama service architecture and is available at our website as a demonstration web application an installable single download and as a collection of individual services
viral diversity and life cycles are poorly understood in the human gut and other body habitats phages and their encoded functions may provide informative signatures of a human microbiota and of microbial community responses to various disturbances and may indicate whether community health or dysfunction is manifest after apparent recovery from a disease or therapeutic intervention here we report sequencing of the viromes metagenomes of virus like particles isolated from faecal samples collected from healthy adult female monozygotic twins and their mothers at three time points over a one year period we compared these data sets with data sets of sequenced bacterial ribosomal rna genes and total faecal community dna co twins and their mothers share a significantly greater degree of similarity in their faecal bacterial communities than do unrelated individuals in contrast viromes are unique to individuals regardless of their degree of genetic relatedness despite remarkable interpersonal variations in viromes and their encoded functions intrapersonal diversity is very low with of virotypes retained over the period surveyed and with viromes dominated by a few temperate phages that exhibit remarkable genetic stability these results indicate that a predatory viralmicrobial dynamic manifest in a number of other characterized environmental ecosystems is notably absent in the very intestine
background the investigation of plant genome structure and evolution requires comprehensive characterization of repetitive sequences that make up the majority of higher plant nuclear dna since genome wide characterization of repetitive elements is complicated by their high abundance and diversity novel approaches based on massively parallel sequencing are being adapted to facilitate the analysis it has recently been demonstrated that the low pass genome sequencing provided by a single sequencing reaction is sufficient to capture information about all major repeat families thus providing the opportunity for efficient repeat investigation in a wide range of species however the development of appropriate data mining tools is required in order to fully utilize this sequencing data for repeat characterization results we adapted a graph based approach for similarity based partitioning of whole genome sequence reads in order to build clusters made of the reads derived from individual repeat families the information about cluster sizes was utilized for assessing the proportion and composition of repeats in the genomes of two model species pisum sativum and glycine max differing in genome size and sequencing coverage moreover statistical analysis and visual inspection of the topology of the cluster graphs using a newly developed program tool seqgrapher were shown to be helpful in distinguishing basic types of repeats and investigating sequence variability within repeat families conclusions repetitive regions of plant genomes can be efficiently characterized by the presented graph based analysis and the graph representation of repeats can be further used to assess the variability and evolutionary divergence of repeat families discover and characterize novel elements and aid in subsequent assembly of their sequences
in this paper we outline some of the main trends and changes we consider will affect science over the next years mainly driven by a new sociotechnological paradigm which results from the use of information and communication technologies we first analyze three main trends growth of scientific authorship growth in scientific publishing growth in data availability and processing which are already visible now but will grow exponentially in the coming decades and will thus affect the dynamics of science we then frame the above changes in the context of the transformation of the scientific production and publication conditions seen as production process of a cultural good which then feedback into the nature of science itself finally we will take together these interrelated growth trends of authors publications and data and pinpoint their profound and multiple impacts on the very nature of scientific work and its professional dynamics in terms of increased openness instability inequality
background protein domains are protein regions that are shared among different proteins and are frequently functionally and structurally independent from the rest of the protein novel domain combinations have a major role in evolutionary innovation however the relative contributions of the different molecular mechanisms that underlie domain gains in animals are still unknown by using animal gene phylogenies we were able to identify a set of high confidence domain gain events and by looking at their coding dna investigate the causative mechanisms results here we show that the major mechanism for gains of new domains in metazoan proteins is likely to be gene fusion through joining of exons from adjacent genes possibly mediated by non allelic homologous recombination retroposition and insertion of exons into ancestral introns through intronic recombination are in contrast to previous expectations only minor contributors to domain gains and have accounted for less than and of high confidence domain gain events respectively additionally exonization of previously non coding regions appears to be an important mechanism for addition of disordered segments to proteins we observe that gene duplication has preceded domain gain in at least of the gain events conclusions the interplay of gene duplication and domain gain demonstrates an important mechanism for fast neofunctionalization genes
the rise of social mediacontent created by internet users and hosted by popular sites such as facebook twitter youtube and wikipedia and blogshas brought several new hazards for medical professionalism first many physicians may find applying principles for medical professionalism to the online environment challenging in certain contexts second physicians may not consider the potential impact of their online content on their patients and the public third a momentary lapse in judgment by an individual physician to create unprofessional content online can reflect poorly on the entire profession to overcome these challenges we encourage individual physicians to realize that as they tread through the world wide web they leave behind a footprint that may have unintended negative consequences for them and for the profession at large we also recommend that institutions take a proactive approach to engage users of social media in setting consensus based standards for online professionalism finally given that professionalism encompasses more than the avoidance of negative behaviors we conclude with examples of more positive applications for this technology much like a mirror social media can reflect the best and worst aspects of the content placed before it for all see
most pairwise and multiple sequence alignment programs seek alignments with optimal scores central to defining such scores is selecting a set of substitution scores for aligned amino acids or nucleotides for local pairwise alignment substitution scores are implicitly of log odds form we now extend the log odds formalism to multiple alignments using bayesian methods to construct bild b ayesian i ntegral l og o d ds substitution scores from prior distributions describing columns of related letters this approach has been used previously only to define scores for aligning individual sequences to sequence profiles but it has much broader applicability we describe how to calculate bild scores efficiently and illustrate their uses in gibbs sampling optimization procedures gapped alignment and the construction of hidden markov model profiles bild scores enable automated selection of optimal motif and domain model widths and can inform the decision of whether to include a sequence in a multiple alignment and the selection of insertion and deletion locations other applications include the classification of related sequences into subfamilies and the definition of profile profile alignment scores although a fully realized multiple alignment program must rely upon more than substitution scores many existing multiple alignment programs can be modified to employ bild scores we illustrate how simple bild score based strategies can enhance the recognition of dna binding domains including the api domain in toxoplasma gondii and falciparum
the diels alder reaction is a cornerstone in organic synthesis forming two carbon carbon bonds and up to four new stereogenic centers in one step no naturally occurring enzymes have been shown to catalyze bimolecular diels alder reactions we describe the de novo computational design and experimental characterization of enzymes catalyzing a bimolecular diels alder reaction with high stereoselectivity and substrate specificity x ray crystallography confirms that the structure matches the design for the most active of the enzymes and binding site substitutions reprogram the substrate specificity designed stereoselective catalysts for carbon carbon bond forming reactions should be broadly useful in chemistry
summary bigwig and bigbed files are compressed binary indexed files containing data at several resolutions that allow the high performance display of next generation sequencing experiment results in the ucsc genome browser the visualization is implemented using a multi layered software approach that takes advantage of specific capabilities of web based protocols and linux and unix operating systems files r trees and various indexing and compression tricks as a result only the data needed to support the current browser view is transmitted rather than the entire file enabling fast remote access to large distributed data sets availability and implementation binaries for the bigwig and bigbed creation and parsing utilities may be downloaded at http hgdownload cse ucsc edu admin exe linux source code for the creation and visualization software is freely available for non commercial use at http hgdownload cse ucsc edu admin jksrc zip implemented in c and supported on linux the ucsc genome browser is available at http genome edu
background high density tiling arrays and new sequencing technologies are generating rapidly increasing volumes of transcriptome and protein dna interaction data visualization and exploration of this data is critical to understanding the regulatory logic encoded in the genome by which the cell dynamically affects its physiology and interacts with its environment results the gaggle genome browser is a cross platform desktop program for interactively visualizing high throughput data in the context of the genome important features include dynamic panning and zooming keyword search and open interoperability through the gaggle framework users may bookmark locations on the genome with descriptive annotations and share these bookmarks with other users the program handles large sets of user generated data using an in process database and leverages the facilities of sql and the r environment for importing and manipulating data a key aspect of the gaggle genome browser is interoperability by connecting to the gaggle framework the genome browser joins a suite of interconnected bioinformatics tools for analysis and visualization with connectivity to major public repositories of sequences interactions and pathways to this flexible environment for exploring and combining data the gaggle genome browser adds the ability to visualize diverse types of data in relation to its coordinates on the genome conclusions genomic coordinates function as a common key by which disparate biological data types can be related to one another in the gaggle genome browser heterogeneous data are joined by their location on the genome to create information rich visualizations yielding insight into genome organization transcription and its regulation and ultimately a better understanding of the mechanisms that enable the cell to dynamically respond to environment
next generation dna sequencing ngs projects such as the genomes project are already revolutionizing our understanding of genetic variation among individuals however the massive data sets generated by ngsthe genome pilot alone includes nearly five terabasesmake writing feature rich efficient and robust analysis tools difficult for even computationally sophisticated individuals indeed many professionals are limited in the scope and the ease with which they can answer scientific questions by the complexity of accessing and manipulating the data produced by these machines here we discuss our genome analysis toolkit gatk a structured programming framework designed to ease the development of efficient and robust analysis tools for next generation dna sequencers using the functional programming philosophy of mapreduce the gatk provides a small but rich set of data access patterns that encompass the majority of analysis tool needs separating specific analysis calculations from common data management infrastructure enables us to optimize the gatk framework for correctness stability and cpu and memory efficiency and to enable distributed and shared memory parallelization we highlight the capabilities of the gatk by describing the implementation and application of robust scale tolerant tools like coverage calculators and single nucleotide polymorphism snp calling we conclude that the gatk programming framework enables developers and analysts to quickly and easily write efficient and robust ngs tools many of which have already been incorporated into large scale sequencing projects like the genomes project and the cancer atlas
background biomedical research is set to greatly benefit from the use of semantic web technologies in the design of computational infrastructure however beyond well defined research initiatives substantial issues of data heterogeneity source distribution and privacy currently stand in the way towards the personalization of medicine results a computational framework for bioinformatic infrastructure was designed to deal with the heterogeneous data sources and the sensitive mixture of public and private data that characterizes the biomedical domain this framework consists of a logical model build with semantic web tools coupled with a markov process that propagates user operator states an accompanying open source prototype was developed to meet a series of applications that range from collaborative multi institution data acquisition efforts to data analysis applications that need to quickly traverse complex data structures this report describes the two abstractions underlying the based infrastructure logical and numerical and discusses its generality beyond the immediate confines of existing implementations conclusions the emergence of the web as a computer requires a formal model for the different functionalities involved in reading and writing to it the core model proposed was found to address the design criteria of biomedical computational infrastructure such as those supporting large scale multi investigator research clinical trials and epidemiology
somatic cell nuclear transfer and transcription factor based reprogramming revert adult cells to an embryonic state and yield pluripotent stem cells that can generate all tissues through different mechanisms and kinetics these two reprogramming methods reset genomic methylation an epigenetic modification of dna that influences gene expression leading us to hypothesize that the resulting pluripotent stem cells might have different properties here we observe that low passage induced pluripotent stem cells ipscs derived by factor based reprogramming of adult murine tissues harbour residual dna methylation signatures characteristic of their somatic tissue of origin which favours their differentiation along lineages related to the donor cell while restricting alternative cell fates such an epigenetic memory of the donor tissue could be reset by differentiation and serial reprogramming or by treatment of ipscs with chromatin modifying drugs in contrast the differentiation and methylation of nuclear transfer derived pluripotent stem cells were more similar to classical embryonic stem cells than were ipscs our data indicate that nuclear transfer is more effective at establishing the ground state of pluripotency than factor based reprogramming which can leave an epigenetic memory of the tissue of origin that may influence efforts at directed differentiation for applications in disease modelling treatment
ever since the pre molecular era the birth of new genes with novel functions has been considered to be a major contributor to adaptive evolutionary innovation here i review the origin and evolution of new genes and their functions in eukaryotes an area of research that has made rapid progress in the past decade thanks to the genomics revolution indeed recent work has provided initial whole genome views of the different types of new genes for a large number of different organisms the array of mechanisms underlying the origin of new genes is compelling extending way beyond the traditionally well studied source of gene duplication thus it was shown that novel genes also regularly arose from messenger rnas of ancestral genes protein coding genes metamorphosed into new rna genes genomic parasites were co opted as new genes and that both protein and rna genes were composed from scratch i e from previously nonfunctional sequences these mechanisms then also contributed to the formation of numerous novel chimeric gene structures detailed functional investigations uncovered different evolutionary pathways that led to the emergence of novel functions from these newly minted sequences and with respect to animals attributed a potentially important role to one specific tissuethe testisin the process of gene birth remarkably these studies also demonstrated that novel genes of the various types significantly impacted the evolution of cellular physiological morphological behavioral and reproductive phenotypic traits consequently it is now firmly established that new genes have indeed been major contributors to the origin of adaptive novelties
bioinformatics motivation micrornas mirnas are a class of endogenes derived from a precursor pre mirna and involved in post transcriptional regulation experimental identification of novel mirnas is difficult because they are often transcribed under specific conditions and cell types several computational methods were developed to detect new mirnas starting from known ones or from deep sequencing data and to validate their pre mirnas results we present a genome wide search algorithm called mirena that looks for mirna sequences by exploring a multidimensional space defined by only five physical and combinatorial parameters characterizing acceptable pre mirnas mirena validates pre mirnas with high sensitivity and specificity and detects new mirnas by homology from known mirnas or from deep sequencing data a performance comparison between mirena and four available predictive systems has been done mirena approach is strikingly simple but it turns out to be powerful at least as much as more sophisticated algorithmic methods mirena obtains better results than three known algorithms that validate pre mirnas it demonstrates that machine learning is not a necessary algorithmic approach for pre mirnas computational validation in particular machine learning algorithms can only confirm pre mirnas that look alike known ones this being a limitation while exploring species with no known pre mirnas the possibility to adapt the search to specific species possibly characterized by specific properties of their mirnas and pre mirnas is a major feature of mirena a parameter adjustment calibrates specificity and sensitivity in mirena a key feature for predictive systems which is not present in machine learning approaches comparison of mirena with mirdeep using deep sequencing data to predict mirnas highlights a highly specific predictive power of mirena availability at the address http www ihes fr carbone contact alessandra carbone frsupplementary information supplementary data are available at online
there is an intense debate over whether fuel economy standards or fuel taxation is the more efficient policy instrument to raise fuel economy and reduce co emissions of cars the aim of this paper is to analyze the impact of standards and fuel prices on new car fuel economy with the aid of cross section time series analysis of data from countries we employ a dynamic specification of new car fuel consumption as a function of fuel prices standards and per capita income it turns out that standards have induced considerable fuel savings throughout the world although their welfare impact is not examined here if standards are not further tightened then retail fuel prices would have to remain at high levels for more than a decade in order to attain similar fuel savings finally without higher fuel prices or tighter standards one should not expect any marked improvements in fuel economy under business as conditions
chromatin organization plays a major role in gene regulation and can affect the function and evolution of new transcriptional programs however it can be difficult to decipher the basis of changes in chromatin organization and their functional effect on gene expression here we present a large scale comparative genomic analysis of the relationship between chromatin organization and gene expression by measuring mrna abundance and nucleosome positions genome wide in hemiascomycota yeast species we found substantial conservation of global and functional chromatin organization in all species including prominent nucleosome free regions nfrs at gene promoters and distinct chromatin architecture in growth and stress genes chromatin organization has also substantially diverged in both global quantitative features such as spacing between adjacent nucleosomes and in functional groups of genes expression levels intrinsic anti nucleosomal sequences and trans acting chromatin modifiers all play important complementary and evolvable roles in determining nfrs we identify five mechanisms that couple chromatin organization to evolution of gene regulation and have contributed to the evolution of respiro fermentation and other key systems including compensatory evolution of alternative modifiers associated with conserved chromatin organization a gradual transition from constitutive to trans regulated nfrs a loss of intrinsic anti nucleosomal sequences accompanying changes in chromatin organization and gene expression re positioning of motifs from nfrs to nucleosome occluded regions and the expanded use of nfrs by paralogous activator repressor pairs our study sheds light on the molecular basis of chromatin organization and on the role of chromatin organization in the evolution of regulation
as access to computational resources continues to increase free energy calculations have emerged as a powerful tool that can play a predictive role in a wide range of research areas yet the reliability of these calculations can often be improved significantly if a number of precepts or good practices are followed although the theory upon which these good practices rely has largely been known for many years it is often overlooked or simply ignored in other cases the theoretical developments are too recent for their potential to be fully grasped and merged into popular platforms for the computation of free energy differences in this contribution the current best practices for carrying out free energy calculations using free energy perturbation and nonequilibrium work methods are discussed demonstrating that at little to no additional cost free energy estimates could be markedly improved and bounded by meaningful error estimates monitoring the probability distributions that underlie the transformation between the states of interest performing the calculation bidirectionally stratifying the reaction pathway and choosing the most appropriate paradigms and algorithms for transforming between states offer significant gains in both accuracy precision
bioinformatics summary cytoscape web is a web based network visualization toolmodeled after cytoscapewhich is open source interactive customizable and easily integrated into web sites multiple file exchange formats can be used to load data into cytoscape web including graphml xgmml and sif availability and implementation cytoscape web is implemented in flex actionscript with a javascript api and is freely available at http cytoscapeweb cytoscape org contact gary bader utoronto casupplementary information supplementary data are available at online
micrornas mirnas are a large family of post transcriptional regulators of gene expression that are nucleotides in length and control many developmental and cellular processes in eukaryotic organisms research during the past decade has identified major factors participating in mirna biogenesis and has established basic principles of mirna function more recently it has become apparent that mirna regulators themselves are subject to sophisticated control many reports over the past few years have reported the regulation of mirna metabolism and function by a range of mechanisms involving numerous proteinprotein and proteinrna interactions such regulation has an important role in the context specific functions mirnas
the systematic characterization of somatic mutations in cancer genomes is essential for understanding the disease and for developing targeted here we report the identification of somatic mutations across of dna representing coding genes from tumours comprising breast lung ovarian and prostate cancer types and subtypes we found that mutation rates and the sets of mutated genes varied substantially across tumour types and subtypes statistical analysis identified significantly mutated genes including protein kinases g protein coupled receptors such as also called aplnr and and other druggable targets integrated analysis of somatic mutations and copy number alterations identified another significantly altered genes including gnas indicating an expanded role for g subunits in multiple cancer types furthermore our experimental analyses demonstrate the functional roles of mutant a g subunit and mutant a member of the jnk signalling pathway in oncogenesis our study provides an overview of the mutational spectra across major human cancers and identifies several potential targets
protein and messenger rna mrna copy numbers vary from cell to cell in isogenic bacterial populations however these molecules often exist in low copy numbers and are difficult to detect in single cells we carried out quantitative system wide analyses of protein and mrna expression in individual cells with single molecule sensitivity using a newly constructed yellow fluorescent protein fusion library for escherichia coli we found that almost all protein number distributions can be described by the gamma distribution with two fitting parameters which at low expression levels have clear physical interpretations as the transcription rate and protein burst size at high expression levels the distributions are dominated by extrinsic noise we found that a single cell s protein and mrna copy numbers for any given gene uncorrelated
alternative splicing is a prevalent post transcriptional process which is not only important to normal cellular function but is also involved in human diseases the newly developed second generation sequencing technique provides high throughput data rna seq data to study alternative splicing events in different types of cells here we present a computational method splicemap to detect splice junctions from rna seq data this method does not depend on any existing annotation of gene structures and is capable of finding novel splice junctions with high sensitivity and specificity it can handle long reads nt and can exploit paired read information to improve mapping accuracy several parameters are included in the output to indicate the reliability of the predicted junction and help filter out false predictions we applied splicemap to analyze million paired nt reads from human brain tissue the results show at this depth of sequencing rna seq can support reliable detection of splice junctions except for those that are present at very low level compared to current methods splicemap can achieve higher sensitivity without sacrificing nar
vertebrate genomes contain numerous copies of retroviral sequences acquired over the course of evolution until recently they were thought to be the only type of rna viruses to be so represented because integration of a dna copy of their genome is required for their replication in this study an extensive sequence comparison was conducted in which viral genes from all known non retroviral families with single stranded rna genomes were matched against the germline genomes of vertebrate species to determine if such viruses could also contribute to the vertebrate genetic heritage in of the tested vertebrate species we discovered as many as high confidence examples of genomic dna sequences that appear to be derived as long ago as million years from ancestral members of currently circulating virus families with single strand rna genomes surprisingly almost all of the sequences are related to only two families in the order mononegavirales the bornaviruses and the filoviruses which cause lethal neurological disease and hemorrhagic fevers respectively based on signature landmarks some and perhaps all of the endogenous virus like dna sequences appear to be line element facilitated integrations derived from viral mrnas the integrations represent genes that encode viral nucleocapsid rna dependent rna polymerase matrix and possibly glycoproteins integrations are generally limited to one or very few copies of a related viral gene per species suggesting that once the initial germline integration was obtained or selected later integrations failed or provided little advantage to the host the conservation of relatively long open reading frames for several of the endogenous sequences the virus like protein regions represented and a potential correlation between their presence and a species resistance to the diseases caused by these pathogens are consistent with the notion that their products provide some important biological advantage to the species in addition the viruses could also benefit as some resistant species e g bats may serve as natural reservoirs for their persistence and transmission given the stringent limitations imposed in this informatics search the examples described here should be considered a low estimate of the number of such integration events that have persisted over evolutionary time scales clearly the sources of genetic information in vertebrate genomes are much more diverse than suspected
large scale genome sequencing gained general importance for life science because functional annotation of otherwise experimentally uncharacterized sequences is made possible by the theory of biomolecular sequence homology historically the paradigm of similarity of protein sequences implying common structure function and ancestry was generalized based on studies of globular domains having the same fold imposes strict conditions over the packing in the hydrophobic core requiring similarity of hydrophobic patterns the implications of sequence similarity among non globular protein segments have not been studied to the same extent nevertheless homology considerations are silently extended for them this appears especially detrimental in the case of transmembrane helices tms and signal peptides sps where sequence similarity is necessarily a consequence of physical requirements rather than common ancestry thus matching of sps tms creates the illusion of matching hydrophobic cores therefore inclusion of sps tms into domain models can give rise to wrong annotations more than domains among the models of pfam release and domains of smart version out of contain sp tm regions as expected fragment mode hmm searches generate promiscuous hits limited to solely the sp tm part among clearly unrelated proteins more worryingly we show explicit examples that the scores of clearly false positive hits even in global mode searches can be elevated into the significance range just by matching the hydrophobic runs in the pir iproclass database using conservative criteria we find that at least between and of its annotated pfam hits appear unjustified for a set of validated domain models thus false positive domain hits enforced by sp tm regions can lead to dramatic annotation errors where the hit has nothing in common with the problematic domain model except the sp tm region itself we suggest a workflow of flagging problematic hits arising from sp tm containing models for critical reconsideration by users
background meta analysis methods exist for combining multiple microarray datasets however there are a wide range of issues associated with microarray meta analysis and a limited ability to compare the performance of different meta analysis methods results we compare eight meta analysis methods five existing methods two naive methods and a novel approach mdeds comparisons are performed using simulated data and two biological case studies with varying degrees of meta analysis complexity the performance of meta analysis methods is assessed via roc curves and prediction accuracy where applicable conclusions existing meta analysis methods vary in their ability to perform successful meta analysis this success is very dependent on the complexity of the data and type of analysis our proposed method mdeds performs competitively as a meta analysis tool even as complexity increases because of the varying abilities of compared meta analysis methods care should be taken when considering the meta analysis method used for research
summary the first open source software suite for experimentalists and curators that i assists in the annotation and local management of experimental metadata from high throughput studies employing one or a combination of omics and other technologies ii empowers users to uptake community defined checklists and ontologies and iii facilitates submission to international public repositories availability and implementation software documentation case studies and implementations at http www isa tools orgcontact isatools com
motivation organic enzyme cofactors are involved in many enzyme reactions therefore the analysis of cofactors is crucial to gain a better understanding of enzyme catalysis to aid this we have created the cofactor database results cofactor provides a web interface to access hand curated data extracted from the literature on organic enzyme cofactors in biocatalysis as well as automatically collected information cofactor includes information on the conformational and solvent accessibility variation of the enzyme bound cofactors as well as mechanistic and structural information about the hosting enzymes availability the database is publicly available and can be accessed at http www ebi ac uk thornton srv databases cofactorcontact julia fischer ebi ac uksupplementary information supplementary data are available at online
sharing research resources of different kinds in new ways and on an increasing scale is a central element of the unfolding e research vision web is seen as providing the technical platform to enable these new forms of scholarly communications we report findings from a study of the use of web services by uk researchers and their use in novel forms of scholarly communication we document the contours of adoption the barriers and enablers and the dynamics of innovation in web services and scholarly practices we conclude by considering the steps that different stakeholders might take to encourage greater experimentation uptake
people exert large amounts of problem solving effort playing computer games simple image and text recognition tasks have been successfully crowd sourced through but it is not clear if more complex scientific problems can be solved with human directed computing protein structure prediction is one such problem locating the biologically relevant native conformation of a protein is a formidable computational challenge given the very large size of the search space here we describe foldit a multiplayer online game that engages non scientists in solving hard prediction problems foldit players interact with protein structures using direct manipulation tools and user friendly versions of algorithms from the rosetta structure prediction while they compete and collaborate to optimize the computed energy we show that top ranked foldit players excel at solving challenging structure refinement problems in which substantial backbone rearrangements are necessary to achieve the burial of hydrophobic residues players working collaboratively develop a rich assortment of new strategies and algorithms unlike computational approaches they explore not only the conformational space but also the space of possible search strategies the integration of human visual problem solving and strategy development capabilities with traditional computational algorithms through interactive multiplayer games is a powerful new approach to solving computationally limited problems
gene expression data from microarrays are being applied to predict preclinical and clinical endpoints but the reliability of these predictions has not been established in the maqc ii project independent teams analyzed six microarray data sets to generate predictive models for classifying a sample with respect to one of endpoints indicative of lung or liver toxicity in rodents or of breast cancer multiple myeloma or neuroblastoma in humans in total models were built using many combinations of analytical methods the teams generated predictive models without knowing the biological meaning of some of the endpoints and to mimic clinical reality tested the models on data that had not been used for training we found that model performance depended largely on the endpoint and team proficiency and that different approaches generated models of similar performance the conclusions and recommendations from maqc ii should be useful for regulatory agencies study committees and independent investigators that evaluate methods for global gene analysis
sponges are an ancient group of animals that diverged from other metazoans over million years ago here we present the draft genome sequence of amphimedon queenslandica a demosponge from the great barrier reef and show that it is remarkably similar to other animal genomes in content structure and organization comparative analysis enabled by the sequencing of the sponge genome reveals genomic events linked to the origin and early evolution of animals including the appearance expansion and diversification of pan metazoan transcription factor signalling pathway and structural genes this diverse toolkit of genes correlates with critical aspects of all metazoan body plans and comprises cell cycle control and growth development somatic and germ cell specification cell adhesion innate immunity and allorecognition notably many of the genes associated with the emergence of animals are also implicated in cancer which arises from defects in basic processes associated with multicellularity
plasma concentrations of total cholesterol low density lipoprotein cholesterol high density lipoprotein cholesterol and triglycerides are among the most important risk factors for coronary artery disease cad and are targets for therapeutic intervention we screened the genome for common variants associated with plasma lipids in individuals of european ancestry here we report significantly associated loci p x with showing genome wide significant association with lipid traits for the first time the newly reported associations include single nucleotide polymorphisms snps near known lipid regulators for example and as well as in scores of loci not previously implicated in lipoprotein metabolism the loci contribute not only to normal variation in lipid traits but also to extreme lipid phenotypes and have an impact on lipid traits in three non european populations east asians south asians and african americans our results identify several novel loci associated with plasma lipids that are also associated with cad finally we validated three of the novel genes and with experiments in mouse models taken together our findings provide the foundation to develop a broader biological understanding of lipoprotein metabolism and to identify new therapeutic opportunities for the prevention cad
abstract background functional genomic studies involving high throughput sequencing and tiling array applications such as chip seq and chip chip generate large numbers of experimentally derived signal peaks across the genome under study in analyzing these loci to determine their potential regulatory functions areas of signal enrichment must be considered relative to proximal genes and regulatory elements annotated throughout the target genome regions of chromatin association by transcriptional regulators should be distinguished as individual binding sites in order to enhance downstream analyses such as the identification of known and novel consensus motifs results peakanalyzer is a set of high performance utilities for the automated processing of experimentally derived peak regions and annotation of genomic loci the programs can accurately subdivide multimodal regions of signal enrichment into distinct subpeaks corresponding to binding sites or chromatin modifications retrieve genomic sequences encompassing the computed subpeak summits and identify positional features of interest such as intersection with exon intron gene components proximity to up or downstream transcriptional start sites and cis regulatory elements the software can be configured to run either as a pipeline component for high throughput analyses or as a cross platform desktop application with an intuitive user interface conclusions peakanalyzer comprises a number of utilities essential for chip seq and chip chip data analysis high performance implementations are provided for unix pipeline integration along with a gui version for interactive use source code in c and java is provided as are native binaries for linux mac os x and systems
pseudogenes are usually considered to be completely neutral sequences whose evolution is shaped by random mutations and chance events it is possible however for disrupted genes to generate products that are deleterious due either to the energetic costs of their transcription and translation or to the formation of toxic proteins we found that after their initial formation the youngest pseudogenes in salmonella genomes have a very high likelihood of being removed by deletional processes and are eliminated too rapidly to be governed by a strictly neutral model of stochastic loss those few highly degraded pseudogenes that have persisted in salmonella genomes correspond to genes with low expression levels and low connectivity in gene networks such that their inactivation and any initial deleterious effects associated with their inactivation are buffered although pseudogenes have long been considered the paradigm of neutral evolution the distribution of pseudogenes among salmonella strains indicates that removal of many of these apparently functionless regions is attributable to selection
the availability of genomes of many closely related bacteria with diverse metabolic capabilities offers the possibility of tracing metabolic evolution on a phylogeny relating the genomes to understand the evolutionary processes and constraints that affect the evolution of metabolic networks using simple independent loss gain of reactions or complex incorporating dependencies among reactions stochastic models of metabolic evolution it is possible to study how metabolic networks evolve over time here we describe a model that takes the reaction neighborhood into account when modeling metabolic evolution the model also allows estimation of the strength of the neighborhood effect during the course of evolution we present gibbs samplers for sampling networks at the internal node of a phylogeny and for estimating the parameters of evolution over a phylogeny without exploring the whole search space by iteratively sampling from the conditional distributions of the internal networks and parameters the samplers are used to estimate the parameters of evolution of metabolic networks of bacteria in the genus pseudomonas and to infer the metabolic networks of the ancestral pseudomonads the results suggest that pathway maps that are conserved across the pseudomonas phylogeny have a stronger neighborhood structure than those which have a variable distribution of reactions across the phylogeny and that some pseudomonas lineages are going through genome reduction resulting in the loss of a number of reactions from their networks
transcriptome analysis has important applications in many biological fields however assembling a transcriptome without a known reference remains a challenging task requiring algorithmic improvements we present two methods for substantially improving transcriptome de novo assembly the first method relies on the observation that the use of a single k mer length by current de novo assemblers is suboptimal to assemble transcriptomes where the sequence coverage of transcripts is highly heterogeneous we present the multiple k method in which various k mer lengths are used for de novo transcriptome assembly we demonstrate its good performance by assembling de novo a published next generation transcriptome sequence data set of aedes aegypti using the existing genome to check the accuracy of our method the second method relies on the use of a reference proteome to improve the de novo assembly we developed the scaffolding using translation mapping stm method that uses mapping against the closest available reference proteome for scaffolding contigs that map onto the same protein in a controlled experiment using simulated data we show that the stm method considerably improves the assembly with few errors we applied these two methods to assemble the transcriptome of the non model catfish loricaria gr cataphracta using the multiple k and stm methods the assembly increases in contiguity and in gene identification showing that our methods clearly improve quality and can be widely used the new methods were used to assemble successfully the transcripts of the core set of genes regulating tooth development in vertebrates while classic de novo failed
the rapidly accumulating genome sequence data allow researchers to address fundamental biological questions that were not even asked just a few years ago a major problem in genomics is the widening gap between the rapid progress in genome sequencing and the comparatively slow progress in the functional characterization of sequenced genomes here we discuss two key questions of genome biology whether we need more genomes and how deep is our understanding of biology based on genomic analysis we argue that overly specific annotations of gene functions are often less useful than the more generic but also more robust functional assignments based on protein family classification we also discuss problems in understanding the functions of the remaining conserved genes
motivation some recent comparative studies have revealed that regulatory regions can retain function over large evolutionary distances even though the dna sequences are divergent and difficult to align it is also known that such enhancers can drive very similar expression patterns this poses a challenge for the in silico detection of biologically related sequences as they can only be discovered using alignment free methods results here we present a new computational framework called regulatory region scoring rrs model for the detection of functional conservation of regulatory sequences using predicted occupancy levels of transcription factors of interest we demonstrate that our model can detect the functional and or evolutionary links between some non alignable enhancers with a strong statistical significance we also identify groups of enhancers that are likely to be similarly regulated our model is motivated by previous work on prediction of expression patterns and it can capture similarity by strong binding sites weak binding sites and even the statistically significant absence of sites our results support the hypothesis that weak binding sites contribute to the functional similarity of sequences our model fills a gap between two families of models detailed data intensive models for the prediction of precise spatio temporal expression patterns on the one side and crude generally applicable models on the other side our model borrows some of the strengths of each group and addresses their drawbacks availability the rrs source code is freely available upon publication of this manuscript http warwick ac uk fac sci systemsbiology staff rrs
abstract background over the past decade gene expression microarray studies have greatly expanded our knowledge of genetic mechanisms of human diseases meta analysis of substantial amounts of accumulated data by integrating valuable information from multiple studies is becoming more important in microarray research however collecting data of special interest from public microarray repositories often present major practical problems moreover including low quality data may significantly reduce meta analysis efficiency results is a human curated microarray database designed for easy querying based on clinical information and for interactive retrieval of either raw or uniformly pre processed data along with a set of quality control metrics the database contains more than previously published affymetrix genechip arrays performed using human clinical specimens allows online querying according to a flexible combination of five clinical annotations describing disease state and sampling location these annotations were manually curated by controlled vocabularies based on information obtained from geo arrayexpress and published papers for array based assessment control the online query provides sets of qc metrics generated using three available qc algorithms arrays with poor data quality can easily be excluded from the query interface the query provides values from two algorithms for gene based filtering and raw data and three kinds of pre processed data for downloading conclusions utilizes a user friendly interface for qc parameters sample clinical annotations and data formats to help users obtain clinical metadata this database provides a lower entry threshold and an integrated process of meta analysis we hope that this research will promote further evolution of microarray analysis
background high throughput sequencing has become an important technology for studying expression levels in many types of genomic and particularly transcriptomic data one key way of analysing such data is to look for elements of the data which display particular patterns of differential expression in order to take these forward for further analysis and validation results we propose a framework for defining patterns of differential expression and develop a novel algorithm bayseq which uses an empirical bayes approach to detect these patterns of differential expression within a set of sequencing samples the method assumes a negative binomial distribution for the data and derives an empirically determined prior distribution from the entire dataset we examine the performance of the method on real and simulated data conclusions our method performs at least as well and often better than existing methods for analyses of pairwise differential expression in both real and simulated data when we compare methods for the analysis of data from experimental designs involving multiple sample groups our method again shows substantial gains in performance we believe that this approach thus represents an important step forward for the analysis of count data from experiments
as sequencing throughput approaches dozens of gigabases per day there is a growing need for efficient software for analysis of transcriptome sequencing rna seq data myrna is a cloud computing pipeline for calculating differential gene expression in large rna seq datasets we apply myrna to the analysis of publicly available data sets and assess the goodness of fit of standard statistical models myrna is available from http bowtie bio sf net webcite
micrornas mirnas are endogenous nucleotide rnas that mediate important gene regulatory events by pairing to the mrnas of protein coding genes to direct their repression repression of these regulatory targets leads to decreased translational efficiency and or decreased mrna levels but the relative contributions of these two outcomes have been largely unknown particularly for endogenous targets expressed at low to moderate levels here we use ribosome profiling to measure the overall effects on protein production and compare these to simultaneously measured effects on mrna levels for both ectopic and endogenous mirna regulatory interactions lowered mrna levels account for most of the decreased protein production these results show that changes in mrna levels closely reflect the impact of mirnas on gene expression and indicate that destabilization of target mrnas is the predominant reason for reduced output
gut microbial composition depends on different dietary habits just as health depends on microbial metabolism but the association of microbiota with different diets in human populations has not yet been shown in this work we compared the fecal microbiota of european children eu and that of children from a rural african village of burkina faso bf where the diet high in fiber content is similar to that of early human settlements at the time of the birth of agriculture by using high throughput rdna sequencing and biochemical analyses we found significant differences in gut microbiota between the two groups bf children showed a significant enrichment in bacteroidetes and depletion in firmicutes p with a unique abundance of bacteria from the genus prevotella and xylanibacter known to contain a set of bacterial genes for cellulose and xylan hydrolysis completely lacking in the eu children in addition we found significantly more short chain fatty acids p in bf than in eu children also enterobacteriaceae shigella and escherichia were significantly underrepresented in bf than in eu children p we hypothesize that gut microbiota coevolved with the polysaccharide rich diet of bf individuals allowing them to maximize energy intake from fibers while also protecting them from inflammations and noninfectious colonic diseases this study investigates and compares human intestinal microbiota from children characterized by a modern western diet and a rural diet indicating the importance of preserving this treasure of microbial diversity from ancient rural worldwide
motivation biological sequence data is accumulating rapidly motivating the development of improved high throughput methods for sequence classification results ublast and usearch are new algorithms enabling sensitive local and global search of large sequence databases at exceptionally high speeds they are often orders of magnitude faster than blast in practical applications though sensitivity to distant protein relationships is lower uclust is a new clustering method that exploits usearch to assign sequences to clusters uclust offers several advantages over the widely used program cd hit including higher speed lower memory use improved sensitivity clustering at lower identities and classification of much larger datasets availability binaries are available at no charge for non commercial use at http www com usearchcontact robert comsupplementary information supplementary data are available at online
exome sequences which comprise all protein coding regions are promising data sets for studies of natural selection because they offer unbiased genome wide estimates of polymorphism while focusing on the portions of the genome that are most likely to be functionally important we examine genomic patterns of polymorphism within diploid autosomal exomes of european and african descent using coalescent simulations we show how polymorphism site frequency spectra and intercontinental divergence in these samples would be influenced by different modes of positive selection we examine putatively selected loci from four previous genome wide scans of snp genotypes and demonstrate that these regions indeed show unusual population genetic patterns in the exome data using a series of conservative criteria based on exome polymorphism we are able to fine scale map signatures of selection in many cases pinpointing a single candidate snp we also identify and evaluate novel candidate selection genes that show unusual patterns of polymorphism we sequence a portion of one novel candidate locus ivl in individuals from multiple continents and examine global genetic diversity thus we confirm narrow and supplement existing catalogs of putative targets of selection and show that exome data sets which are likely to soon become common will be powerful tools for identifying adaptive variation
strand specific massively parallel cdna sequencing rna seq is a powerful tool for transcript discovery genome annotation and expression profiling there are multiple published methods for strand specific rna seq but no consensus exists as to how to choose between them here we developed a comprehensive computational pipeline to compare library quality metrics from any rna seq method using the well annotated saccharomyces cerevisiae transcriptome as a benchmark we compared seven library construction protocols including both published and our own methods we found marked differences in strand specificity library complexity evenness and continuity of coverage agreement with known annotations and accuracy for expression profiling weighing each method s performance and ease we identified the dutp second strand marking and the illumina rna ligation methods as the leading protocols with the former benefitting from the current availability of paired end sequencing our analysis provides a comprehensive benchmark and our computational pipeline is applicable for assessment of future protocols in organisms
abstract background genome scale metabolic reconstructions have been recognised as a valuable tool for a variety of applications ranging from metabolic engineering to evolutionary studies however the reconstruction of such networks remains an arduous process requiring a high level of human intervention this process is further complicated by occurrences of missing or conflicting information and the absence of common annotation standards between different data sources results in this article we report a semi automated methodology aimed at streamlining the process of metabolic network reconstruction by enabling the integration of different genome wide databases of metabolic reactions we present results obtained by applying this methodology to the metabolic network of the plant arabidopsis thaliana a systematic comparison of compounds and reactions between two genome wide databases allowed us to obtain a high quality core consensus reconstruction which was validated for stoichiometric consistency a lower level of consensus led to a larger reconstruction which has a lower quality standard but provides a baseline for further manual curation conclusion this semi automated methodology may be applied to other organisms and help to streamline the process of genome scale network reconstruction in order to accelerate the transfer of such models applications
summary text mining from the biomedical literature is of increasing importance yet it is not easy for the bioinformatics community to create and run text mining workflows due to the lack of accessibility and interoperability of the text mining resources the u compare system provides a wide range of bio text mining resources in a highly interoperable workflow environment where workflows can very easily be created executed evaluated and visualized without coding we have linked u compare to taverna a generic workflow system to expose text mining functionality to the bioinformatics community availability http u compare org taverna html http u compare org contact kano is s u tokyo ac bioinformatics
it is well known that the microstructures of the transition metal including the high transition temperature high tc copper oxide are complex this is particularly so when there are oxygen interstitials or which influence the bulk properties for example the oxygen interstitials in the spacer layers separating the superconducting planes undergo ordering phenomena in ref y ref and y refs that induce enhancements in the transition temperatures with no changes in hole concentrations it is also known that complex systems often have a scale invariant structural but hitherto none had been found in high tc materials here we report that the ordering of oxygen interstitials in the y spacer layers of y high tc superconductors is characterized by a fractal distribution up to a maximum limiting size of m intriguingly these fractal distributions of dopants seem to enhance superconductivity at temperature
next generation dna sequencing platforms provide exciting new possibilities for in vitro genetic analysis of functional nucleic acids however the size of the resulting data sets presents computational and analytical challenges we present an open source software package that employs a locality sensitive hashing algorithm to enumerate all unique sequences in an entire illumina sequencing run sequences the algorithm results in quasilinear time processing of entire illumina lanes sequences on a desktop computer in minutes to facilitate visual analysis of sequencing data the software produces three dimensional scatter plots similar in concept to sewall wright and john maynard smiths adaptive or fitness landscape the software also contains functions that are particularly useful for doped selections such as mutation frequency analysis information content calculation multivariate statistical functions including principal component analysis sequence distance metrics sequence searches and sequence comparisons across multiple illumina data sets source code executable files and links to sample data sets are available at http www sourceforge net sewal
epigenetic modifications must underlie lineage specific differentiation as terminally differentiated cells express tissue specific genes but their dna sequence is unchanged haematopoiesis provides a well defined model to study epigenetic modifications during cell fate decisions as multipotent progenitors mpps differentiate into progressively restricted myeloid or lymphoid progenitors although dna methylation is critical for myeloid versus lymphoid differentiation as demonstrated by the myeloerythroid bias in hypomorphs a comprehensive dna methylation map of haematopoietic progenitors or of any multipotent oligopotent lineage does not exist here we examined million cpg sites throughout the genome for mpps common lymphoid progenitors clps common myeloid progenitors cmps granulocyte macrophage progenitors gmps and thymocyte progenitors marked epigenetic plasticity accompanied both lymphoid and myeloid restriction myeloid commitment involved less global dna methylation than lymphoid commitment supported functionally by myeloid skewing of progenitors following treatment with a dna methyltransferase inhibitor differential dna methylation correlated with gene expression more strongly at cpg island shores than cpg islands many examples of genes and pathways not previously known to be involved in choice between lymphoid myeloid differentiation have been identified such as and several transcription factors including were methylated and silenced during differentiation indicating a role in maintaining an undifferentiated state additionally epigenetic modification of modifiers of the epigenome seems to be important in haematopoietic differentiation our results directly demonstrate that modulation of dna methylation occurs during lineage specific differentiation and defines a comprehensive map of the methylation and transcriptional changes that accompany myeloid versus lymphoid decisions
today we can generate hundreds of gigabases of dna and rna sequencing data in a week for less than us the astonishing rate of data generation by these low cost high throughput technologies in genomics is being matched by that of other technologies such as real time imaging and mass spectrometry based flow cytometry success in the life sciences will depend on our ability to properly interpret the large scale high dimensional data sets that are generated by these technologies which in turn requires us to adopt advances in informatics here we discuss how we can master the different types of computational environments that exist such as cloud and heterogeneous computing to successfully tackle our big problems
a new generation of sequencing technologies is revolutionizing molecular biology illuminas solexa and applied biosystems solid generate gigabases of nucleotide sequence per week however a perceived limitation of these ultra high throughput technologies is their short read lengths de novo assembly of sequence reads generated by classical sanger capillary sequencing is a mature field of research unfortunately the existing sequence assembly programs were not effective for short sequence reads generated by illumina and solid platforms early studies suggested that in principle sequence reads as short as nucleotides could be used to generate useful assemblies of both prokaryotic and eukaryotic genome sequences albeit containing many gaps the early feasibility studies and proofs of principle inspired several bioinformatics research groups to implement new algorithms as freely available software tools specifically aimed at assembling reads of nucleotides in length this has led to the generation of several draft genome sequences based exclusively on short sequence illumina sequence reads recently culminating in the assembly of the gb genome of the giant panda from illumina sequence reads with an average length of just nucleotides as well as reviewing recent developments in the field we discuss some practical aspects such as data filtering and submission of assembly data to repositories
background in recent years there has been a huge increase in the amount of publicly available and proprietary information pertinent to drug discovery however there is a distinct lack of data mining tools available to harness this information and in particular for knowledge discovery across multiple information sources at indiana university we have an ongoing project with eli lilly to develop web service based tools for integrative mining of chemical and biological information in this paper we report on the first of these tools called wendi web engine for non obvious drug information that attempts to find non obvious relationships between a query compound and scholarly publications biological properties genes and diseases using multiple information sources results we have created an aggregate web service that takes a query compound as input calls multiple web services for computation and database search and returns an xml file that aggregates this information we have also developed a client application that provides an easy to use interface to this web service both the service and client are publicly available conclusions initial testing indicates this tool is useful in identifying potential biological applications of compounds that are not obvious and in identifying corroborating and conflicting information from multiple sources we encourage feedback on the tool to help us refine it further we are now developing further tools based on model
transcription factors control cell specific gene expression programs through interactions with diverse coactivators and the transcription apparatus gene activation may involve dna loop formation between enhancer bound transcription factors and the transcription apparatus at the core promoter but this process is not well understood here we report that mediator and cohesin physically and functionally connect the enhancers and core promoters of active genes in murine embryonic stem cells mediator a transcriptional coactivator forms a complex with cohesin which can form rings that connect two dna segments the cohesin loading factor nipbl is associated with mediatorcohesin complexes providing a means to load cohesin at promoters dna looping is observed between the enhancers and promoters occupied by mediator and cohesin mediator and cohesin co occupy different promoters in different cells thus generating cell type specific dna loops linked to the gene expression program of cell
initially thought to play a restricted role in calcium homeostasis the pleiotropic actions of vitamin d in biology and their clinical significance are only now becoming apparent however the mode of action of vitamin d through its cognate nuclear vitamin d receptor vdr and its contribution to diverse disorders remain poorly understood we determined vdr binding throughout the human genome using chromatin immunoprecipitation followed by massively parallel dna sequencing chip seq after calcitriol stimulation we identified genomic positions occupied by the vdr and genes with significant changes in expression in response to vitamin d vdr binding sites were significantly enriched near autoimmune and cancer associated genes identified from genome wide association gwa studies notable genes with vdr binding included associated with ms and associated with crohn s disease and furthermore a number of single nucleotide polymorphism associations from gwa were located directly within vdr binding intervals for example associated with sle and associated with we also observed significant enrichment of vdr intervals within regions of positive selection among individuals of asian and european descent chip seq determination of transcription factor binding in combination with gwa data provides a powerful approach to further understanding the molecular bases of diseases
all complex life is composed of eukaryotic nucleated cells the eukaryotic cell arose from prokaryotes just once in four billion years and otherwise prokaryotes show no tendency to evolve greater complexity why not prokaryotic genome size is constrained by bioenergetics the endosymbiosis that gave rise to mitochondria restructured the distribution of dna in relation to bioenergetic membranes permitting a remarkable fold expansion in the number of genes expressed this vast leap in genomic capacity was strictly dependent on mitochondrial power and prerequisite to eukaryote complexity the key innovation en route to life
transcription mrna decay translation and protein degradation are essential processes during eukaryotic gene expression but their relative global contributions to steady state protein concentrations in multi cellular eukaryotes are largely unknown using measurements of absolute protein and mrna abundances in cellular lysate from the human daoy medulloblastoma cell line we quantitatively evaluate the impact of mrna concentration and sequence features implicated in translation and protein degradation on protein expression sequence features related to translation and protein degradation have an impact similar to that of mrna abundance and their combined contribution explains two thirds of protein abundance variation mrna sequence lengths amino acid properties upstream open reading frames and secondary structures in the untranslated region utr were the strongest individual correlates of protein concentrations in a combined model characteristics of the coding region and the explained a larger proportion of protein abundance variation than characteristics of the the absolute protein and mrna concentration measurements for human genes described here represent one of the largest datasets currently available and reveal both general trends and specific examples of post regulation
increased reliance on computational approaches in the life sciences has revealed grave concerns about how accessible and reproducible computation reliant results truly are galaxy http usegalaxy org webcite an open web based platform for genomic research addresses these problems galaxy automatically tracks and manages data provenance and provides support for capturing the context and intent of computational methods galaxy pages are interactive web based documents that provide users with a medium to communicate a complete analysis
eusociality in which some individuals reduce their own lifetime reproductive potential to raise the offspring of others underlies the most advanced forms of social organization and the ecologically dominant role of social insects and humans for the past four decades kin selection theory based on the concept of inclusive fitness has been the major theoretical attempt to explain the evolution of eusociality here we show the limitations of this approach we argue that standard natural selection theory in the context of precise models of population structure represents a simpler and superior approach allows the evaluation of multiple competing hypotheses and provides an exact framework for interpreting observations
summary the bioruby software toolkit contains a comprehensive set of free development tools and libraries for bioinformatics and molecular biology written in the ruby programming language bioruby has components for sequence analysis pathway analysis protein modelling and phylogenetic analysis it supports many widely used data formats and provides easy access to databases external programs and public web services including blast kegg genbank medline and go bioruby comes with a tutorial documentation and an interactive environment which can be used in the shell and in the web browser availability bioruby is free and open source software made available under the ruby license bioruby runs on all platforms that support ruby including linux mac os x and windows and with jruby bioruby runs on the java virtual machine the source code is available from http www bioruby org contact katayama org
motivation the rapid development of next generation sequencing technologies able to produce huge amounts of sequence data is leading to a wide range of new applications this triggers the need for fast and accurate alignment software common techniques often restrict indels in the alignment to improve speed whereas more flexible aligners are too slow for large scale applications moreover many current aligners are becoming inefficient as generated reads grow ever larger our goal with our new aligner gassst global alignment short sequence search tool is thus foldachieving high performance with no restrictions on the number of indels with a design that is still effective on long reads results we propose a new efficient filtering step that discards most alignments coming from the seed phase before they are checked by the costly dynamic programming algorithm we use a carefully designed series of filters of increasing complexity and efficiency to quickly eliminate most candidate alignments in a wide range of configurations the main filter uses a precomputed table containing the alignment score of short four base words aligned against each other this table is reused several times by a new algorithm designed to approximate the score of the full dynamic programming algorithm we compare the performance of gassst against bwa bfast and pass we found that gassst achieves high sensitivity in a wide range of configurations and faster overall execution time than other state of the art aligners availability gassst is distributed under the cecill software license at http www irisa fr symbiose projects gassst contact guillaume rizk irisa fr dominique lavenier irisa frsupplementary information supplementary data are available at online
in everyday life many people believe that two heads are better than one our ability to solve problems together appears to be fundamental to the current dominance and future survival of the human species but are two heads really better than one we addressed this question in the context of a collective low level perceptual decision making task for two observers of nearly equal visual sensitivity two heads were definitely better than one provided they were given the opportunity to communicate freely even in the absence of any feedback about decision outcomes but for observers with very different visual sensitivities two heads were actually worse than the better one these seemingly discrepant patterns of group behavior can be explained by a model in which two heads are bayes optimal under the assumption that individuals accurately communicate their level of confidence on every science
abstract here we describe the genome variation format gvf and the dataset gvf an extension of generic feature format version is a simple tab delimited format for dna variant files which uses sequence ontology to describe genome variation data the dataset ten human genomes in gvf format is freely available for community analysis from the sequence ontology website and from an amazon elastic block storage ebs snapshot for use in amazon s cloud environment
summary jcvi metagenomics reports metarep is a web application designed to help scientists analyze and compare annotated metagenomics datasets it utilizes solr lucene a high performance scalable search engine to quickly query large data collections furthermore users can use its sql like query syntax to filter and refine datasets metarep provides graphical summaries for top taxonomic and functional classifications as well as a go ncbi taxonomy and kegg pathway browser users can compare absolute and relative counts of multiple datasets at various functional and taxonomic levels advanced comparative features comprise statistical tests as well as multidimensional scaling heatmap and hierarchical clustering plots summaries can be exported as tab delimited files publication quality plots in pdf format a data management layer allows collaborative data analysis and result sharing availability web site http www jcvi org metarep source code http github com jcvi metarepcontact syooseph jcvi orgsupplementary information supplementary data are available at online
mechanisms underlying the dramatic patterns of genome size variation across the tree of life remain mysterious effective population size n e has been proposed as a major driver of genome size selection is expected to efficiently weed out deleterious mutations increasing genome size in lineages with large but not small n e strong support for this model was claimed from a comparative analysis of n e u and genome size for phylogenetically diverse species ranging from bacteria to vertebrates but analyses at that scale have so far failed to account for phylogenetic nonindependence of species in our reanalysis accounting for phylogenetic history substantially altered the perceived strength of the relationship between n e u and genomic attributes there were no statistically significant associations between n e u and gene number intron size intron number the half life of gene duplicates transposon number transposons as a fraction of the genome or overall genome size we conclude that current datasets do not support the hypothesis of a mechanistic connection between n e and these genomic attributes and we suggest that further progress requires larger datasets phylogenetic comparative methods more robust estimators of genetic drift and a multivariate approach that accounts for correlations between putative variables
mirsvr is a new machine learning method for ranking microrna target sites by a down regulation score the algorithm trains a regression model on sequence and contextual features extracted from miranda predicted target sites in a large scale evaluation miranda mirsvr is competitive with other target prediction methods in identifying target genes and predicting the extent of their downregulation at the mrna or protein levels importantly the method identifies a significant number of experimentally determined non canonical and non sites
the intestinal microbiota consists of over species which play key roles in gut physiology and homeostasis imbalances in the composition of this bacterial community can lead to transient intestinal dysfunctions and chronic disease states understanding how to manipulate this ecosystem is thus essential for treating many disorders in this study we took advantage of recently developed tools for deep sequencing and phylogenetic clustering to examine the long term effects of exogenous microbiota transplantation combined with and without an antibiotic pretreatment in our rat model deep sequencing revealed an intestinal bacterial diversity exceeding that of the human gut by a factor of two to three the transplantation produced a marked increase in the microbial diversity of the recipients which stemmed from both capture of new phylotypes and increase in abundance of others however when transplantation was performed after antibiotic intake the resulting state simply combined the reshaping effects of the individual treatments including the reduced diversity from antibiotic treatment alone therefore lowering the recipient bacterial load by antibiotic intake prior to transplantation did not increase establishment of the donor phylotypes although some dominant lineages still transferred successfully remarkably all of these effects were observed after mo of treatment and persisted after mo overall our results indicate that the indigenous gut microbial composition is more plastic that previously anticipated however since antibiotic pretreatment counterintuitively interferes with the establishment of an exogenous community such plasticity is likely conditioned more by the altered microbiome gut homeostasis caused by antibiotics than by the primary loss
we go through the many considerations involved in fitting a model to data using as an example the fit of a straight line to a set of points in a two dimensional plane standard weighted least squares fitting is only appropriate when there is a dimension along which the data points have negligible uncertainties and another along which all the uncertainties can be described by gaussians of known variance these conditions are rarely met in practice we consider cases of general heterogeneous and arbitrarily covariant two dimensional uncertainties and situations in which there are bad data large outliers unknown uncertainties and unknown but expected intrinsic scatter in the linear relationship being fit above all we emphasize the importance of having a generative model for the data even an approximate one once there is a generative model the subsequent fitting is non arbitrary because the model permits direct computation of the likelihood of the parameters or the posterior probability distribution construction of a posterior probability distribution is indispensible if there are nuisance parameters to away
accurate control of tissue specific gene expression plays a pivotal role in heart development but few cardiac transcriptional enhancers have thus far been identified extreme noncoding sequence conservation has successfully predicted enhancers that are active in many tissues but has failed to identify substantial numbers of heart specific enhancers here we used chip seq with the enhancer associated protein from mouse embryonic day heart tissue to identify over candidate heart enhancers genome wide compared to enhancers active in othertissues we studied at this time point most candidate heart enhancers were less deeply conserved in vertebrate evolution nevertheless transgenic mouse assays of candidate regions revealed that most function reproducibly as enhancers active in the heart irrespective of their degree of evolutionary constraint these results provide evidence for a large population of poorly conserved heart enhancers and suggest that the evolutionary conservation of embryonic enhancers can vary depending on type
the development of high throughput sequencing hts technologies has opened the door to novel methods for detecting copy number variants cnvs in the human genome while in the past cnvs have been detected based on array cgh data recent studies have shown that depth of coverage information from hts technologies can also be used for the reliable identification of large copy variable regions such methods however are hindered by sequencing biases that lead certain regions of the genome to be over or undersampled lowering their resolution and ability to accurately identify the exact breakpoints of the variants in this work we develop a method for cnv detection that supplements the depth of coverage with paired end mapping information where mate pairs mapping discordantly to the reference serve to indicate the presence of variation our algorithm called cnver combines this information within a unified computational framework called the donor graph allowing us to better mitigate the sequencing biases that cause uneven local coverage and accurately predict cnvs we use cnver to detect cnvs in the recently described genome of a yoruban individual most of the calls coincide with previously known variants within the database of genomic variants while of deletion copy number variants previously known for this individual coincide with one of our loss calls furthermore we demonstrate that cnver can reconstruct the absolute copy counts of segments of the donor genome and evaluate the feasibility of using cnver with low datasets
motivation a number of methods have been reported that predict proteinprotein interactions ppis with high accuracy using only simple sequence based features such as amino acid content this is surprising given that many protein interactions have high specificity that depends on detailed atomic recognition between physiochemically complementary surfaces are the reported high accuracies realistic results we find that the reported accuracies of the predictions are significantly over estimated and strongly dependent on the structure of the training and testing datasets used the choice of which protein pairs are deemed as non interactions in the training data has a variable impact on the accuracy estimates and the accuracies can be artificially inflated by a bias towards dominant samples in the positive data which result from the presence of hub proteins in the protein interaction network to address this bias we propose a positive set specific method to create a balanced negative set maintaining the degree distribution for each protein leading to the conclusion that simple sequence based features contain insufficient information to be useful for predicting ppis but that protein domain based features have some predictive value availability our method named brs nonint is available at http www bioinformatics leeds ac uk brs nonint all the datasets used in this study are derived from publicly available data and are available at http www bioinformatics leeds ac uk brs nonint htmlcontact maozuguo hit edu cn d r westhead leeds uk
bacteria show remarkable adaptability in the face of antibiotic therapeutics resistance alleles in drug target specific sites and general stress responses have been identified in individual end point isolates less is known however about the population dynamics during the development of antibiotic resistant strains here we follow a continuous culture of escherichia coli facing increasing levels of antibiotic and show that the vast majority of isolates are less resistant than the population as a whole we find that the few highly resistant mutants improve the survival of the population s less resistant constituents in part by producing indole a signalling molecule generated by actively growing unstressed cells we show through transcriptional profiling that indole serves to turn on drug efflux pumps and oxidative stress protective mechanisms the indole production comes at a fitness cost to the highly resistant isolates and whole genome sequencing reveals that this bacterial altruism is made possible by drug resistance mutations unrelated to indole production this work establishes a population based resistance mechanism constituting a form of kin selection whereby a small number of resistant mutants can at some cost to themselves provide protection to other more vulnerable cells enhancing the survival capacity of the overall population in environments
despite great progress in identifying genetic variants that influence human disease most inherited risk remains unexplained a more complete understanding requires genome wide studies that fully examine less common alleles in populations with a wide range of ancestry to inform the design and interpretation of such studies we genotyped million common single nucleotide polymorphisms snps in reference individuals from global populations and sequenced ten kilobase regions in of these individuals this integrated data set of common and rare alleles called hapmap includes both snps and copy number polymorphisms cnps we characterized population specific differences among low frequency variants measured the improvement in imputation accuracy afforded by the larger reference panel especially in imputing snps with a minor allele frequency of or and demonstrated the feasibility of imputing newly discovered cnps and snps this expanded public resource of genome variants in global populations supports deeper interrogation of genomic variation and its role in human disease and serves as a step towards a high resolution map of the landscape of human variation
the structures of rna molecules are often important for their function and yet there are no experimental techniques for genome scale measurement of rna structure here we describe a novel strategy termed parallel analysis of rna structure pars which is based on deep sequencing fragments of rnas that were treated with structure specific enzymes thus providing simultaneous in vitro profiling of the secondary structure of thousands of rna species at single nucleotide resolution we apply pars to profile the secondary structure of the messenger rnas mrnas of the budding yeast saccharomyces cerevisiae and obtain structural profiles for over distinct transcripts analysis of these profiles reveals several rna structural properties of yeast transcripts including the existence of more secondary structure over coding regions compared with untranslated regions a three nucleotide periodicity of secondary structure across coding regions and an anti correlation between the efficiency with which an mrna is translated and the structure over its translation start site pars is readily applicable to other organisms and to profiling rna structure in diverse conditions thus enabling studies of the dynamics of secondary structure at a scale
how do social networks affect the spread of behavior a popular hypothesis states that networks with many clustered ties and a high degree of separation will be less effective for behavioral diffusion than networks in which locally redundant ties are rewired to provide shortcuts across the social space a competing hypothesis argues that when behaviors require social reinforcement a network with more clustering may be more advantageous even if the network as a whole has a larger diameter i investigated the effects of network structure on diffusion by studying the spread of health behavior through artificially structured online communities individual adoption was much more likely when participants received social reinforcement from multiple neighbors in the social network the behavior spread farther and faster across clustered lattice networks than across corresponding networks
one of the virtues of peer review is that it provides a self regulating selection mechanism for scientific work papers and projects peer review as a selection mechanism is hard to evaluate in terms of its efficiency serious efforts to understand its strengths and weaknesses have not yet lead to clear answers in theory peer review works if the involved parties editors and referees conform to a set of requirements such as love for high quality science objectiveness and absence of biases nepotism friend and clique networks selfishness etc if these requirements are violated what is the effect on the selection of high quality work we study this question with a simple agent based model in particular we are interested in the effects of rational referees who might not have any incentive to see high quality work other than their own published or promoted we find that a small fraction of incorrect selfish or rational referees can drastically reduce the quality of the published accepted scientific standard we quantify the fraction for which peer review will no longer select better than pure chance decline of quality of accepted scientific work is shown as a function of the fraction of rational and unqualified referees we show how a simple quality increasing policy of e g a journal can lead to a loss in overall scientific quality and how mutual support networks of authors and referees deteriorate system
high throughput post genomic studies are now routinely and promisingly investigated in biological and biomedical research the main statistical approach to select genes differentially expressed between two groups is to apply a t test which is subject of criticism in the literature numerous alternatives have been developed based on different and innovative variance modeling strategies however a critical issue is that selecting a different test usually leads to a different gene list in this context and given the current tendency to apply the t test identifying the most efficient approach in practice remains crucial to provide elements to answer we conduct a comparison of eight tests representative of variance modeling strategies in gene expression data welch s t test anova wilcoxon s test sam rvm limma varmixt and smvar our comparison process relies on four steps gene list analysis simulations spike in data and re sampling to formulate comprehensive and robust conclusions about test performance in terms of statistical power false positive rate execution time and ease of use our results raise concerns about the ability of some methods to control the expected number of false positives at a desirable level besides two tests limma and varmixt show significant improvement compared to the t test in particular to deal with small sample sizes in addition limma presents several practical advantages so we advocate its application to analyze gene data
background many areas of biology are open to mathematical and computational modelling the application of discrete logical formalisms defines the field of biomedical ontologies ontologies have been put to many uses in bioinformatics the most widespread is for description of entities about which data have been collected allowing integration and analysis across multiple resources there are now over ontologies in active use increasingly developed as large international collaborations there are however many opinions on how ontologies should be authored that is what is appropriate for representation recently a common opinion has been the realist approach that places restrictions upon the style of modelling considered to be appropriate methodology principal findings here we use a number of case studies for describing the results of biological experiments we investigate the ways in which these could be represented using both realist and non realist approaches we consider the limitations and advantages of each of these models conclusions significance from our analysis we conclude that while realist principles may enable straight forward modelling for some topics there are crucial aspects of science and the phenomena it studies that do not fit into this approach realism appears to be over simplistic which perversely results in overly complex ontological models we suggest that it is impossible to avoid compromise in modelling ontology a clearer understanding of these compromises will better enable appropriate modelling fulfilling the many needs for discrete mathematical models within biology
although examples of variation and diversity exist throughout the nervous system their importance remains a source of debate even neurons of the same molecular type have notable intrinsic differences largely unknown however is the degree to which these differences impair or assist neural coding we examined the outputs from a single type of neuron the mitral cells of the mouse olfactory bulb to identical stimuli and found that each cell s spiking response was dictated by its unique biophysical fingerprint using this intrinsic heterogeneity diverse populations were able to code for twofold more information than their homogeneous counterparts in addition biophysical variability alone reduced pair wise output spike correlations to low levels our results indicate that intrinsic neuronal diversity is important for neural coding and is not simply the result of imprecision
background it is necessary to analyze microarray experiments together with biological information to make better biological inferences we investigate the adequacy of current biological databases to address this need description our results show a low level of consistency comprehensiveness and compatibility among three popular pathway databases kegg ingenuity and wikipathways the level of consistency for genes in similar pathways across databases ranges from to the corresponding level of consistency for interacting genes pairs is these three original sources can be assumed to be reliable in the sense that the interacting gene pairs reported in them are correct because they are curated however the lack of concordance between these databases suggests each source has missed out many genes and interacting gene pairs conclusions researchers will hence find it challenging to obtain consistent pathway information out of these diverse data sources it is therefore critical to enable them to access these sources via a consistent comprehensive and unified pathway api we accumulated sufficient data to create such an aggregated resource with the convenience of an api to access its information this unified resource can be accessed at http www com
background recent studies generating complete human sequences from asian african and european subgroups have revealed population specific variation and disease susceptibility loci here choosing a dna sample from a population of interest due to its relative geographical isolation and genetic impact on further populations we extend the above studies through the generation of fold coverage of the first irish human genome sequence results using sequence data from a branch of the european ancestral tree as yet unsequenced we identify variants that may be specific to this population through comparisons with hapmap and previous genetic association studies we identified novel disease associated variants including a novel nonsense variant putatively associated with inflammatory bowel disease we describe a novel method for improving snp calling accuracy at low genome coverage using haplotype information this analysis has implications for future re sequencing studies and validates the imputation of irish haplotypes using data from the current human genome diversity cell line panel hgdp ceph finally we identify gene duplication events as constituting significant targets of recent positive selection in the human lineage conclusions our findings show that there remains utility in generating whole genome sequences to illustrate both general principles and reveal specific instances of human biology with increasing access to low cost sequencing we would predict that even armed with the resources of a small research group a number of similar initiatives geared towards answering specific biological questions emerge
motivation in recent years the gulf between the mass of accumulating research data and the massive literature describing and analyzing those data has widened the need for intelligent tools to bridge this gap to rescue the knowledge being systematically isolated in literature and data silos is now widely acknowledged results to this end we have developed utopia documents a novel pdf reader that semantically integrates visualization and data analysis tools with published research articles in a successful pilot with editors of the biochemical journal bj the system has been used to transform static document features into objects that can be linked annotated visualized and analyzed interactively http www biochemj org bj utopia documents is now used routinely by bj editors to mark up article content prior to publication recent additions include integration of various text mining and biodatabase plugins demonstrating the system s ability to seamlessly integrate on line content with pdf articles availability http com
to celebrate the first years of nature reviews genetics we asked eight leading researchers for their views on the key developments in genetics and genomics in the past decade and the prospects for the future their responses highlight the incredible changes that the field has seen from the explosion of genomic data and the many possibilities it has opened up to the ability to reprogramme adult cells to pluripotency the way ahead looks similarly exciting as we address questions such as how cells function as systems and how complex interactions among genetics epigenetics and the environment combine to phenotypes
motivation the world wide community of life scientists has access to a large number of public bioinformatics databases and tools which are developed and deployed using diverse technologies and designs more and more of the resources offer programmatic web service interface however efficient use of the resources is hampered by the lack of widely used standard data exchange formats for the basic everyday bioinformatics data types results bioxsd has been developed as a candidate for standard canonical exchange format for basic bioinformatics data bioxsd is represented by a dedicated xml schema and defines syntax for biological sequences sequence annotations alignments and references to resources we have adapted a set of web services to use bioxsd as the input and output format and implemented a test case workflow this demonstrates that the approach is feasible and provides smooth interoperability semantics for bioxsd is provided by annotation with the edam ontology we discuss in a separate section how bioxsd relates to other initiatives and approaches including existing standards and the semantic web availability the bioxsd xml schema is freely available at http www bioxsd org bioxsd xsd under the creative commons by nd license the http bioxsd org web page offers documentation examples of data in bioxsd format example workflows with source codes in common programming languages an updated list of compatible web services and tools and a repository of feature requests from the community contact matus kalas bccs uib no developers bioxsd org support org
negative feedback is common in biological processes and can increase a systems stability to internal and external perturbations but at the molecular level control loops always involve signalling steps with finite rates for random births and deaths of individual molecules here we show by developing mathematical tools that merge control and information theory with physical chemistry that seemingly mild constraints on these rates place severe limits on the ability to suppress molecular fluctuations specifically the minimum standard deviation in abundances decreases with the quartic root of the number of signalling events making it extremely expensive to increase accuracy our results are formulated in terms of experimental observables and existing data show that cells use brute force when noise suppression is essential for example regulatory genes are transcribed tens of thousands of times per cell cycle the theory challenges conventional beliefs about biochemical accuracy and presents an approach to the rigorous analysis of poorly characterized systems
biological pathway exchange biopax is a standard language to represent biological pathways at the molecular and cellular level and to facilitate the exchange of pathway data the rapid growth of the volume of pathway data has spurred the development of databases and computational tools to aid interpretation however use of these data is hampered by the current fragmentation of pathway information across many databases with incompatible formats biopax which was created through a community process solves this problem by making pathway data substantially easier to collect index interpret and share biopax can represent metabolic and signaling pathways molecular and genetic interactions and gene regulation networks using biopax millions of interactions organized into thousands of pathways from many organisms are available from a growing number of databases this large amount of pathway data in a computable form will support visualization analysis and discovery
background with the rapidly falling cost and availability of high throughput sequencing and microarray technologies the bottleneck for effectively using genomic analysis in the laboratory and clinic is shifting to one of effectively managing analyzing and sharing genomic data results here we present three open source platform independent software tools for generating analyzing distributing and visualizing genomic data these include a next generation sequencing microarray lims and analysis project center gnomex an application for annotating and programmatically distributing genomic data using the community vetted das data exchange protocol genopub and a standalone java swing application gwrap that makes cutting edge command line analysis tools available to those who prefer graphical user interfaces both gnomex and genopub use the rich client flex flash web browser interface to interact with java classes and a relational database on a remote server both employ a public private user group security model enabling controlled distribution of patient and unpublished data alongside public resources as such they function as genomic data repositories that can be accessed manually or programmatically through das enabled client applications such as the integrated genome browser conclusions these tools have gained wide use in our core facilities research laboratories and clinics and are freely available for non profit use see http sourceforge net projects gnomex http sourceforge net projects genoviz and http sourceforge net useq
summary pubdna finder is an online repository that we have created to link pubmed central manuscripts to the sequences of nucleic acids appearing in them it extends the search capabilities provided by pubmed central by enabling researchers to perform advanced searches involving sequences of nucleic acids this includes among other features i searching for papers mentioning one or more specific sequences of nucleic acids and ii retrieving the genetic sequences appearing in different articles these additional query capabilities are provided by a searchable index that we created by using the full text of the papers available at pubmed central at the time of writing and the sequences of nucleic acids appearing in them to automatically extract the genetic sequences occurring in each paper we used an original method we have developed the database is updated monthly by automatically connecting to the pubmed central ftp site to retrieve and index new manuscripts users can query the database via the web interface provided availability pubdna finder can be freely accessed at http servet dia fi upm es pubdnafindercontact mgarcia infomed dia fi es
accurately modeling the dna sequence preferences of transcription factors tfs and using these models to predict in vivo genomic binding sites for tfs are key pieces in deciphering the regulatory code these efforts have been frustrated by the limited availability and accuracy of tf binding site motifs usually represented as position specific scoring matrices pssms which may match large numbers of sites and produce an unreliable list of target genes recently protein binding microarray pbm experiments have emerged as a new source of high resolution data on in vitro tf binding specificities pbm data has been analyzed either by estimating pssms or via rank statistics on probe intensities so that individual sequence patterns are assigned enrichment scores e scores this representation is informative but unwieldy because every tf is assigned a list of thousands of scored sequence patterns meanwhile high resolution in vivo tf occupancy data from chip seq experiments is also increasingly available we have developed a flexible discriminative framework for learning tf binding preferences from high resolution in vitro and in vivo data we first trained support vector regression svr models on pbm data to learn the mapping from probe sequences to binding intensities we used a novel mer based string kernel called the di mismatch kernel to represent probe sequence similarities the svr models are more compact than e scores more expressive than pssms and can be readily used to scan genomics regions to predict in vivo occupancy using a large data set of yeast and mouse tfs we found that our svr models can better predict probe intensity than the e score method or pbm derived pssms moreover by using svrs to score yeast mouse and human genomic regions we were better able to predict genomic occupancy as measured by chip chip and chip seq experiments finally we found that by training kernel based models directly on chip seq data we greatly improved in vivo occupancy prediction and by comparing a tf s in vitro and in vivo models we could identify cofactors and disambiguate direct and binding
we present the analysis of twenty human genomes to evaluate the prospects for identifying rare functional variants that contribute to a phenotype of interest we sequenced at high coverage ten case genomes from individuals with severe hemophilia a and ten control genomes we summarize the number of genetic variants emerging from a study of this magnitude and provide a proof of concept for the identification of rare and highly penetrant functional variants by confirming that the cause of hemophilia a is easily recognizable in this data set we also show that the number of novel single nucleotide variants snvs discovered per genome seems to stabilize at about new variants per genome after the first individuals have been sequenced finally we find that on average each genome carries homozygous protein truncating or stop loss variants in genes representing a diverse set pathways
in alternative expression analysis by sequencing alexa seq we developed a method to analyze massively parallel rna sequence data to catalog transcripts and assess differential and alternative expression of known and predicted mrna isoforms in cells and tissues as proof of principle we used the approach to compare fluorouracil resistant and nonresistant human colorectal cancer cell lines we assessed the sensitivity and specificity of the approach by comparison to exon tiling and splicing microarrays and validated the results with reverse transcriptionpcr quantitative pcr and sanger sequencing we observed global disruption of splicing in fluorouracil resistant cells characterized by expression of new mrna isoforms resulting from exon skipping alternative splice site usage and intron retention alternative expression annotation databases source code a data viewer and other resources to facilitate analysis are available at http www org
many tree species that depend on scatter hoarding animals for seed dispersal produce massive crops of large seeds at irregular intervals mast seeding and large seed size in these species have been explained as adaptations to increase animal dispersal and reduce predation we studied how seed size and seed abundance simultaneously influenced seed dispersal and predation by scatter hoarding rodents in the large seeded rain forest tree carapa procera meliaceae in french guiana we individually tracked the fates of seeds using remote video monitoring and thread marking seed size was manipulated by broadly varying intraspecific seed mass whereas effects of seed abundance were examined by tracking seeds in three seed rich years and two seed poor years the hypotheses that seed mass and seed abundance both enhance dispersal success and that seed abundance reinforces the effect of seed mass were supported by the results most seeds were removed by the scatter hoarding rodent red acouchy myoprocta acouchy and subsequently were buried in scattered single seeded caches up to distances m seeds that were not removed failed to establish seedlings seed removal was slower pre removal seed predation was greater and seed dispersal was less far in seed rich years than in seed poor years suggesting poorer dispersal under seed abundance however this was more than counter balanced by a disproportionally greater survival of cached seeds in seed rich years the per capita probability of seed survival and seedling establishment was at least times greater under seed abundance large seeds were removed faster were more likely to be scatter hoarded and were dispersed farther away than smaller ones resulting in a higher probability of seedling establishment for larger seeds size discrimination was greater under seed abundance albeit only during seed removal overall large seeds shed in rich years had the highest probability of seedling establishment hence both larger seed size and greater seed abundance stimulate rodents to act more as dispersers and less as predators of seeds we conclude that scatter hoarding rodents can select for both large seed crops and large seeds which may reinforce seeding
high throughput technologies are widely used for example to assay genetic variants gene and protein expression and epigenetic modifications one often overlooked complication with such studies is batch effects which occur because measurements are affected by laboratory conditions reagent lots and personnel differences this becomes a major problem when batch effects are correlated with an outcome of interest and lead to incorrect conclusions using both published studies and our own analyses we argue that batch effects as well as other technical and biological artefacts are widespread and critical to address we review experimental and computational approaches for so
experimental evolution systems allow the genomic study of adaptation and so far this has been done primarily in asexual systems with small genomes such as bacteria and here we present whole genome resequencing data from drosophila melanogaster populations that have experienced over generations of laboratory selection for accelerated development flies in these selected populations develop from egg to adult faster than flies of ancestral control populations and have evolved a number of other correlated phenotypes on the basis of intermediate frequency high quality single nucleotide polymorphisms we identify several dozen genomic regions that show strong allele frequency differentiation between a pooled sample of five replicate populations selected for accelerated development and pooled controls on the basis of resequencing data from a single replicate population with accelerated development as well as single nucleotide polymorphism data from individual flies from each replicate population we infer little allele frequency differentiation between replicate populations within a selection treatment signatures of selection are qualitatively different than what has been observed in asexual species in our sexual populations adaptation is not associated with classic sweeps whereby newly arising unconditionally advantageous mutations become fixed more parsimonious explanations include incomplete sweep models in which mutations have not had enough time to fix and soft sweep models in which selection acts on pre existing common genetic variants we conclude that at least for life history characters such as development time unconditionally advantageous alleles rarely arise are associated with small net fitness gains or cannot fix because selection coefficients change time
estimating errors is a crucial part of any scientific analysis whenever a parameter is estimated model based or not an error estimate is necessary any parameter estimate that is given without an error estimate is meaningless nevertheless many undergraduate or graduate students have to teach such methods for error estimation to themselves when working scientifically for the first time this manuscript presents an easy to understand overview of different methods for error estimation that are applicable to both model based and model independent parameter estimates these methods are not discussed in detail but their basics are briefly outlined and their assumptions carefully noted in particular the methods for error estimation discussed are grid search varying chi the fisher matrix monte carlo methods error propagation data resampling and bootstrapping finally a method is outlined how to propagate measurement errors through complex data pipelines
summary computational pipelines are common place in scientific research however most of the resources for constructing pipelines are heavyweight systems with graphical user interfaces ruffus is a library for the creation of computational pipelines its lightweight and unobtrusive design recommends it for use even for the most trivial of analyses at the same time it is powerful enough to have been used for complex workflows involving more than interdependent stages availability and implementation ruffus is written in python source code a short tutorial examples and a comprehensive user manual are freely available at http www ruffus org uk the example program is available at http www ruffus org uk examples bioinformaticscontact ruffus llew uk
cancers are caused by the accumulation of genomic alterations therefore analyses of cancer genome sequences and structures provide insights for understanding cancer biology diagnosis and therapy the application of second generation dna sequencing technologies also known as next generation sequencing through whole genome whole exome and whole transcriptome approaches is allowing substantial advances in cancer genomics these methods are facilitating an increase in the efficiency and resolution of detection of each of the principal types of somatic cancer genome alterations including nucleotide substitutions small insertions and deletions copy number alterations chromosomal rearrangements and microbial infections this review focuses on the methodological considerations for characterizing somatic genome alterations in cancer and the future prospects for approaches
background linking high throughput experimental data with biological networks is a key step for understanding complex biological systems currently visualization tools for large metabolic networks often result in a dense web of connections that is difficult to interpret biologically the metnetge application organizes and visualizes biological networks in a meaningful way to improve performance and biological interpretability results metnetge is an interactive visualization tool based on the google earth platform metnetge features novel visualization techniques for pathway and ontology information display instead of simply showing hundreds of pathways in a complex graph metnetge gives an overview of the network using the hierarchical pathway ontology using a novel layout called the enhanced radial space filling ersf approach that allows the network to be summarized compactly the non tree edges in the pathway or gene ontology which represent pathways or genes that belong to multiple categories are linked using orbital connections in a third dimension biologists can easily identify highly activated pathways or gene ontology categories by mapping of summary experiment statistics such as coefficient of variation and overrepresentation values onto the visualization after identifying such pathways biologists can focus on the corresponding region to explore detailed pathway structure and experimental data in an aligned tiered layout in this paper the use of metnetge is illustrated with pathway diagrams and data from e coli and arabidopsis conclusions metnetge is a visualization tool that organizes biological networks according to a hierarchical ontology structure the ersf technique assigns attributes in space such as color height and transparency to any ontological structure for hierarchical data the novel ersf layout enables the user to identify pathways or categories that are differentially regulated in particular experiments metnetge also displays complex biological pathway in an aligned tiered layout exploration
motivation micrornas mirnas are short abundant non coding rnas critical for many cellular processes deep sequencing next generation sequencing technologies are being readily used to receive a more accurate depiction of mirna expression profiles in living cells this type of analysis is a key step towards improving our understanding of the complexity and mode of mirna regulation results mirnakey is a software package designed to be used as a base station for the analysis of mirna deep sequencing data the package implements common steps taken in the analysis of such data as well as adds unique features such as data statistics and multiple read determination generating a novel platform for the analysis of mirna expression a user friendly graphical interface is applied to determine the analysis steps the tabular and graphical output contains general and detailed reports on the sequence reads and provides an accurate picture of the differentially expressed mirnas in paired samples availability and implementation see http ibis tau ac il mirnakeycontact nshomron post tau ac ilsupplementary information supplementary data are available at online
expression levels of human genes vary extensively among individuals this variation facilitates analyses of expression levels as quantitative phenotypes in genetic studies where the entire genome can be scanned for regulators without prior knowledge of the regulatory mechanisms thus enabling the identification of unknown regulatory relationships here we carried out such genetic analyses with a large sample size and identified cis and trans acting polymorphic regulators for about human genes we validated the cis acting regulators by demonstrating differential allelic expression with sequencing of transcriptomes rna seq and the trans regulators by gene knockdown metabolic assays and chromosome conformation capture analysis the majority of the regulators act in trans to the target regulated genes most of these trans regulators were not known to play a role in gene expression regulation the identification of these regulators enabled the characterization of polymorphic regulation of human gene expression at a resolution that was unattainable in past
genomes encode multiple signals raising the question of how these different codes are organized along the linear genome sequence within protein coding regions the redundancy of the genetic code can in principle allow for the overlapping encoding of signals in addition to the amino acid sequence but it is not known to what extent genomes exploit this potential and if so for what purpose here we systematically explore whether protein coding regions accommodate overlapping codes by comparing the number of occurrences of each possible short sequence within the protein coding regions of over species from viruses to plants to the same number in randomizations that preserve amino acid sequence and codon bias we find that coding regions across all phyla encode additional information with bacteria carrying more information than eukaryotes the detailed signals consist of both known and potentially novel codes including position dependent secondary rna structure bacteria specific depletion of transcription and translation initiation signals and eukaryote specific enrichment of microrna target sites our results suggest that genomes may have evolved to encode extensive overlapping information within protein regions
the amount of information regarding proteinprotein interactions ppi at a proteomic scale is constantly increasing this is paralleled with an increase of databases making information available consequently there are diverse ways of delivering information about not only ppis but also regarding the databases themselves this creates a time consuming obstacle for many researchers working in the field our survey provides a valuable tool for researchers to reduce the time necessary to gain a broad overview of ppi databases and is supported by a graphical representation of data exchange the graphical representation is made available in cooperation with the team maintaining www pathguide org and can be accessed at http www pathguide org interactions php in a new cytoscape web implementation the local copy of cytoscape cys file can be downloaded from http bio icm edu pl darman ppi page
plasmodium falciparum is the most prevalent and lethal of the malaria parasites infecting humans yet the origin and evolutionary history of this important pathogen remain controversial here we develop a single genome amplification strategy to identify and characterize plasmodium spp dna sequences in faecal samples from wild living apes among nearly specimens collected from field sites throughout central africa we found plasmodium infection in chimpanzees pan troglodytes and western gorillas gorilla gorilla but not in eastern gorillas gorilla beringei or bonobos pan paniscus ape plasmodial infections were highly prevalent widely distributed and almost always made up of mixed parasite species analysis of more than mitochondrial apicoplast and nuclear gene sequences from chimpanzees and gorillas revealed that grouped within one of six host specific lineages representing distinct plasmodium species within the subgenus laverania one of these from western gorillas comprised parasites that were nearly identical to p falciparum in phylogenetic analyses of full length mitochondrial sequences human p falciparum formed a monophyletic lineage within the gorilla parasite radiation these findings indicate that p falciparum is of gorilla origin and not of chimpanzee bonobo or ancient origin
this article reports a empirical study using a study as a base to compare google scholar s coverage of scholarly journals with commercial services through random samples of eight databases the author finds that as of google scholar covers to percent of scholarly journals from both publicly accessible web contents and from subscription based databases that google scholar partners with in the coverage of the same databases ranged from to percent the author explores de duplication of search results by google scholar and discusses its impacts on searches and library resources with the dramatic improvement of google scholar the uniqueness and effectiveness of subscription based abstracts and indexes have changed
coping with variations in network dosage is crucial for maintaining optimal function in gene networks we explored how network structure facilitates network level dosage compensation by using the yeast galactose network as a model we combinatorially deleted one of the two copies of its four regulatory genes and found that network activity was robust to the change in network dosage a mathematical analysis revealed that a two component genetic circuit with elements of opposite regulatory activity activator and inhibitor constitutes a minimal requirement for network dosage invariance specific interaction topologies and a one to one interaction stoichiometry between the activating and inhibiting agents were additional essential elements facilitating dosage invariance this mechanism of network dosage invariance could represent a general design for gene network structure cells
observers in relative motion or at different gravitational potentials measure disparate clock rates these predictions of relativity have previously been observed with atomic clocks at high velocities and with large changes in elevation we observed time dilation from relative speeds of less than meters per second by comparing two optical atomic clocks connected by a meter length of optical fiber we can now also detect time dilation due to a change in height near earth s surface of less than meter this technique may be extended to the field of geodesy with applications in geophysics and hydrology as well as in space based tests of physics
the turing or reaction diffusion rd model is one of the best known theoretical models used to explain self regulated pattern formation in the developing animal embryo although its real world relevance was long debated a number of compelling examples have gradually alleviated much of the skepticism surrounding the model the rd model can generate a wide variety of spatial patterns and mathematical studies have revealed the kinds of interactions required for each giving this model the potential for application as an experimental working hypothesis in a wide variety of morphological phenomena in this review we describe the essence of this theory for experimental biologists unfamiliar with the model using examples from experimental studies in which the rd model is incorporated
the scientific literature represents a rich source for retrieval of knowledge on associations between biomedical concepts such as genes diseases and cellular processes a commonly used method to establish relationships between biomedical concepts from literature is co occurrence apart from its use in knowledge retrieval the co occurrence method is also well suited to discover new hidden relationships between biomedical concepts following a simple abc principle in which a and c have no direct relationship but are connected via shared b intermediates in this paper we describe copub discovery a tool that mines the literature for new relationships between biomedical concepts statistical analysis using roc curves showed that copub discovery performed well over a wide range of settings and keyword thesauri we subsequently used copub discovery to search for new relationships between genes drugs pathways and diseases several of the newly found relationships were validated using independent literature sources in addition new predicted relationships between compounds and cell proliferation were validated and confirmed experimentally in an in vitro cell proliferation assay the results show that copub discovery is able to identify novel associations between genes drugs pathways and diseases that have a high probability of being biologically valid this makes copub discovery a useful tool to unravel the mechanisms behind disease to find novel drug targets or to find novel applications for drugs
dna methylation plays a key role in regulating eukaryotic gene expression although mitotically heritable and stable over time patterns of dna methylation frequently change in response to cell differentiation disease and environmental influences several methods have been developed to map dna methylation on a genomic scale here we benchmark four of these approaches by analyzing two human embryonic stem cell lines derived from genetically unrelated embryos and a matched pair of colon tumor and adjacent normal colon tissue obtained from the same donor our analysis reveals that methylated dna immunoprecipitation sequencing medip seq methylated dna capture by affinity purification methylcap seq reduced representation bisulfite sequencing rrbs and the infinium assay all produce accurate dna methylation data however these methods differ in their ability to detect differentially methylated regions between pairs of samples we highlight strengths and weaknesses of the four methods and give practical recommendations for the design of epigenomic case studies
resonances in the superconducting properties in a regime of crossover from bcs to mixed bose fermi superconductivity are investigated in a two band superconductor where the chemical potential is tuned near the band edge of the second miniband generated by quantum confinement effects the shape resonances at t in the superconducting gaps belonging to the class of feshbach like resonances is manifested by interference effects in the superconducting gap at the first large fermi surface when the chemical potential is in the proximity of the band edge of the second miniband the case of a superlattice of quantum wells is considered and the amplification of the superconducting gaps at the lifshitz transition of the type neck collapsing of fermi surface topology is clearly shown the results are found to be in good agreement with available experimental data on a superlattice of honeycomb boron layers intercalated by al and mg layers
background illumina s second generation sequencing platform is playing an increasingly prominent role in modern dna and rna sequencing efforts however rapid simple standardized and independent measures of run quality are currently lacking as are tools to process sequences for use in downstream applications based on read level quality data results we present solexaqa a user friendly software package designed to generate detailed statistics and at a glance graphics of sequence data quality both quickly and in an automated fashion this package contains associated software to trim sequences dynamically using the quality scores of bases within individual reads conclusion the solexaqa package produces standardized outputs within minutes thus facilitating ready comparison between flow cell lanes and machine runs as well as providing immediate diagnostic information to guide the manipulation of sequence data for analyses
abstract background an increase in work on the full text of journal articles and the growth of pubmedcentral have the opportunity to create a major paradigm shift in how biomedical text mining is done however until now there has been no comprehensive characterization of how the bodies of full text journal articles differ from the abstracts that until now have been the subject of most biomedical text mining research results we examined the structural and linguistic aspects of abstracts and bodies of full text articles the performance of text mining tools on both and the distribution of a variety of semantic classes of named entities between them we found marked structural differences with longer sentences in the article bodies and much heavier use of parenthesized material in the bodies than in the abstracts we found content differences with respect to linguistic features three out of four of the linguistic features that we examined were statistically significantly differently distributed between the two genres we also found content differences with respect to the distribution of semantic features there were significantly different densities per thousand words for three out of four semantic classes and clear differences in the extent to which they appeared in the two genres with respect to the performance of text mining tools we found that a mutation finder performed equally well in both genres but that a wide variety of gene mention systems performed much worse on article bodies than they did on abstracts pos tagging was also more accurate in abstracts than in article bodies conclusions aspects of structure and content differ markedly between article abstracts and article bodies a number of these differences may pose problems as the text mining field moves more into the area of processing full text articles however these differences also present a number of opportunities for the extraction of data types particularly that found in parenthesized text that is present in article bodies but not in abstracts
oceanic bacteria perform many environmental functions including biogeochemical cycling of many elements metabolizing of greenhouse gases functioning in oceanic food webs microbial loop and producing valuable natural products and viruses we demonstrate that the widespread capability of marine bacteria to participate in horizontal gene transfer hgt in coastal and oceanic environments may be the result of gene transfer agents gtas viral like particles produced by proteobacteria we documented gta mediated gene transfer frequencies a thousand to a hundred million times higher than prior estimates of hgt in the oceans with as high as of the culturable natural microbial community confirmed as gene recipients these findings suggest a plausible mechanism by which marine bacteria acquire novel traits thus ensuring resilience in the face of change
psychologists have repeatedly shown that a single statistical factoroften called general intelligenceemerges from the correlations among peoples performance on a wide variety of cognitive tasks but no one has systematically examined whether a similar kind of collective intelligence exists for groups of people in two studies with people working in groups of two to five we find converging evidence of a general collective intelligence factor that explains a groups performance on a wide variety of tasks this c factor is not strongly correlated with the average or maximum individual intelligence of group members but is correlated with the average social sensitivity of group members the equality in distribution of conversational turn taking and the proportion of females in group
the feynman path integral of ordinary quantum mechanics is complexified and it is shown that possible integration cycles for this complexified integral are associated with branes in a two dimensional a model this provides a fairly direct explanation of the relationship of the a model to quantum mechanics such a relationship has been explored from several points of view in the last few years these phenomena have an analog for chern simons gauge theory in three dimensions integration cycles in the path integral of this theory can be derived from n super yang mills theory in four dimensions hence under certain conditions a chern simons path integral in three dimensions is equivalent to an n path integral in dimensions
summarywhile the long noncoding rnas ncrnas constitute a large portion of the mammalian transcriptome their biological functions has remained elusive a few long ncrnas that have been studied in any detail silence gene expression in processes such as x inactivation and imprinting we used a gencode annotation of the human genome to characterize over a thousand long ncrnas that are expressed in multiple cell lines unexpectedly we found an enhancer like function for a set of these long ncrnas in human cell lines depletion of a number of ncrnas led to decreased expression of their neighboring protein coding genes including the master regulator of hematopoiesis scl also called and using heterologous transcription assays we demonstrated a requirement for the ncrnas in activation of gene expression these results reveal an unanticipated role for a class of long ncrnas in activation of critical regulators of development and differentiation paperclip mmcvcomponents mmcvaudio mmcvcomponents mmcvaudio length new mmcvcomponent science page static science gif science page static science gif mmcvcomponents mmcvaudio mmcvcomponents mmcvaudio length mmcvflashvars audiolink http www sciencedirect com science miamimultimediaurl c c abst f mmcvplayertype inline to listen to this audio enable javascript on your browser however you can download and play the audio by clicking on the icon below if flashplayerversion supported document write span class mmcvinstallfp document write span dummyimageurl science page images inlineplaydisabledurl if flashplayerversion supported document write img src document write dummyimageurl document write reason document write flashplayerversion document write style display none document write alt mmcvcaptionreplaceimg scidirimg captionimgtblicon gif download this audio k graphical abstractfull size image high quality image highlights long noncoding rnas activate neighboring protein coding genes activating long ncrnas behave similarly to classically defined enhancer elements depletion of or its adjacent ncrna show similar cellular defects
summarychromatin is important for the regulation of transcription and other functions yet the diversity of chromatin composition and the distribution along chromosomes are still poorly characterized by integrative analysis of genome wide binding maps of broadly selected chromatin components in drosophila cells we show that the genome is segmented into five principal chromatin types that are defined by unique yet overlapping combinations of proteins and form domains that can extend over kb we identify a repressive chromatin type that covers about half of the genome and lacks classic heterochromatin markers furthermore transcriptionally active euchromatin consists of two types that differ in molecular organization and methylation and regulate distinct classes of genes finally we provide evidence that the different chromatin types help to target dna binding factors to specific genomic regions these results provide a global view of chromatin diversity and domain organization in a metazoan cell graphical abstractfull size image high quality image highlights unique combinations of proteins define five principal chromatin types in drosophila two chromatin types are known types of heterochromatin a previously unknown type of repressive chromatin covers of the genome two distinct types of euchromatin regulate different classes genes
clinical application of induced pluripotent stem cells ipscs is limited by the low efficiency of ipsc derivation and the fact that most protocols modify the genome to effect cellular reprogramming moreover safe and effective means of directing the fate of patient specific ipscs toward clinically useful cell types are lacking here we describe a simple nonintegrating strategy for reprogramming cell fate based on administration of synthetic mrna modified to overcome innate antiviral responses we show that this approach can reprogram multiple human cell types to pluripotency with efficiencies that greatly surpass established protocols we further show that the same technology can be used to efficiently direct the differentiation of rna induced pluripotent stem cells ripscs into terminally differentiated myogenic cells this technology represents a safe efficient strategy for somatic cell reprogramming and directing cell fate that has broad applicability for basic research disease modeling and regenerative medicine modified mrnas can express reprogramming proteins and evade antiviral response highly efficient derivation of human ipscs without genomic integration rna derived ipscs faithfully recapitulate the properties of human escs efficient directed differentiation of ipscs to myotubes
targeted capture combined with massively parallel exome sequencing is a promising approach to identify genetic variants implicated in human traits we report exome sequencing of individuals from denmark with targeted capture of coding genes and sequence coverage of each individual exome at an average depth of fold on average about of the target regions were covered by at least one read we identified snps in the sample population including coding snps csnps using a statistical method for snp calling and an estimation of allelic frequencies based on our population data we derived the allele frequency spectrum of csnps with a minor allele frequency greater than we identified a fold excess of deleterious non syonomyous csnps over synonymous csnps in the low frequency range minor allele frequencies between and this excess was more pronounced for x linked snps suggesting that deleterious substitutions are recessive
proteins such as many transcription factors that bind to specific dna sequences are essential for the proper regulation of gene expression identifying the specific sequences that each factor binds can help to elucidate regulatory networks within cells and how genetic variation can cause disruption of normal gene expression which is often associated with disease traditional methods for determining the specificity of dna binding proteins are slow and laborious but several new high throughput methods can provide comprehensive binding information much more rapidly combined with in vivo determinations of transcription factor binding locations this information provides more detailed views of the regulatory circuitry of cells and the effects of variation on expression
intercellular between cell communication networks maintain homeostasis and coordinate regenerative and developmental cues in multicellular organisms despite the importance of intercellular networks in stem cell biology their rules structure and molecular components are poorly understood herein we describe the structure and dynamics of intercellular and intracellular networks in a stem cell derived hierarchically organized tissue using experimental and theoretical analyses of cultured human umbilical cord blood progenitors by integrating high throughput molecular profiling database and literature mining mechanistic modeling and cell culture experiments we show that secreted factor mediated intercellular communication networks regulate blood stem cell fate decisions in particular self renewal is modulated by a coupled positive negative intercellular feedback circuit composed of megakaryocyte derived stimulatory growth factors vegf pdgf egf and serotonin versus monocyte derived inhibitory factors and we reconstruct a stem cell intracellular network and identify raf akt and plc as functionally distinct signal integration nodes linking extracellular and intracellular signaling this represents the first systematic characterization of how stem cell fate decisions are regulated non autonomously through lineage specific interactions with progeny
background the use of ontologies to control vocabulary and structure annotation has added value to genome scale data and contributed to the capture and re use of knowledge across research domains gene ontology go is widely used to capture detailed expert knowledge in genomic scale datasets and as a consequence has grown to contain many terms making it unwieldy for many applications to increase its ease of manipulation and efficiency of use subsets called go slims are often created by collapsing terms upward into more general high level terms relevant to a particular context creation of a go slim currently requires manipulation and editing of go by an expert or community familiar with both the ontology and the biological context decisions about which terms to include are necessarily subjective and the creation process itself and subsequent curation are time consuming and largely manual results here we present an objective framework for generating customised ontology slims for specific annotated datasets exploiting information latent in the structure of the ontology graph and in the annotation data this framework combines ontology engineering approaches and a data driven algorithm that draws on graph and information theory we illustrate this method by application to go generating go slims at different information thresholds characterising their depth of semantics and demonstrating the resulting gains in statistical power conclusions our go slim creation pipeline is available for use in conjunction with any go annotated dataset and creates dataset specific objectively defined slims this method is fast and scalable for application to other ontologies
a primary component of next generation sequencing analysis is to align short reads to a reference genome with each read aligned independently however reads that observe the same non reference dna sequence are highly correlated and can be used to better model the true variation in the target genome a novel short read micro re aligner srma that leverages this correlation to better resolve a consensus of the underlying dna sequence of the targeted genome is here
an increasing number of evidences show that genes are not distributed randomly across eukaryotic chromosomes but rather in functional neighborhoods nevertheless the driving force that originated and maintains such neighborhoods is still a matter of controversy we present the first detailed multispecies cartography of genome regions enriched in genes with related functions and study the evolutionary implications of such clustering our results indicate that the chromosomes of higher eukaryotic genomes contain up to of genes arranged in functional neighborhoods with a high level of gene co expression which are consistently distributed in phylogenies unexpectedly neighborhoods with homologous functions are formed by different non orthologous genes in different species actually instead of being conserved functional neighborhoods present a higher degree of synteny breaks than the genome average this scenario is compatible with the existence of selective pressures optimizing the coordinated transcription of blocks of functionally related genes if these neighborhoods were broken by chromosomal rearrangements selection would favor further rearrangements reconstructing other neighborhoods of similar function the picture arising from this study is a dynamic genomic landscape with a high level of organization
optimal foraging theory is a set of related models from evolutionary ecology that predict the range and proportions of food items a predator should consume diet breadth where and how long it should hunt patch choice and how it should move path choice this paper assesses the utility of such models in anthropology by applying an optimal diet breadth approach to the analysis of hunting yields in three amazonian societies specifically we analyze diet breadth as a function of settlement age distance and technology data from the siona secoya ye kwana and yanomam indicate that these factors have a significant influence on diet breadth and support the basic predictions of the optimization model ecological anthropology foraging strategies amazonia south american indians theory
la mirmecofilia el desarrollo de relaciones mutualistas entre plantas y hormigas es un campo de estudio fascinante que ha atrado la atencin de naturalistas y eclogos desde hace mucho tiempo si bien la mirmecofilia incluye interacciones diversas como el cultivo de jardines epfitos por parte de hormigas y la dispersin de semillas de varias especies de plantas tradicionalmente se refiere a la interaccin defensiva anti herbvoro por parte de hormigas las cuales son recompensadas por las plantas a travs de la produccin de alimento y o sitios de albergue en este trabajo se presenta un breve anlisis histrico del desarrollo de este campo de estudio y se discuten las adaptaciones desarrolladas por las plantas para mantener la asociacin mirmecfila tambin se hace un anlisis de la literatura disponible a partir del cual se discute la distribucin geogrfica taxonmica y ecolgica de plantas que han desarrollado el hbito mirmecfito tal anlisis revela que la mirmecofilia es predominante en los trpicos pero particularmente en el neotrpico que se encuentra representada en una gama amplia de linajes vegetales y que entre las mirmecfitas sobresalen aquellas de rpido crecimiento asociadas a hbitats de alta disponibilidad lumnica se sealan algunos de los campos promisorios en el estudio de tal relacin bitica y se discute cmo este campo de estudio ofrece oportunidades para entender la evolucin de atributos de las plantas influenciados por animales viceversa
in neotropical forests large fruit eating primates play important ecological roles as dispersal agents of large seeds bushmeat hunting threatens to disrupt populations of primates and large seeded trees we test the hypothesis that otherwise intact neotropical forests with depressed populations of large primates experience decline in recruitment of large seeded trees we quantify the proportion of small juveniles m tall cm diameter at breast height dbh of large primate dispersed tree species found underneath heterospecifc trees that are also dispersed by large primates at two protected sites in manu national park and one hunted site outside manu n p in southeastern peru the forests are comparable in edaphic and climatic qualities successional stage and adult tree species composition we found that hunting locally exterminates populations of large primates and reduced primates of intermediate body size hereafter medium primates by percent moreover tree species richness was percent lower and density of species dispersed by large and medium bodied primates percent lower in hunted than in protected sites in addition richness and density of abiotically dispersed species and plants dispersed by non game animals are greater in hunted sites overhunting threatens to disrupt the ecological interactions between primates and the plants that rely on them for seed dispersal and recruitment sustainable wildlife management plans are urgently needed because protected areas are at risk of becoming island parks if buffer zones become empty of animals and have impoverished flora abstract in spanish is available at http www blackwell syncrgy com btp
summary qualitative mensural genetic and phylogenetic analyses were conducted to clarify species limits distributions and relationships in the poorly known amazonian rodent genus scolomys scolomys melanops is characterized by consistent differences in comparison with all other samples of the genus however our data do not support the recognition of s juruaense as a valid species but rather as a synonym of s ucayalensis only minimal differences in cyt b sequences were present between s juruaense and s ucayalensis and phylogenetic analyses consistently retrieved a monophyletic genus scolomys with the relationship s juruaense s ucayalensis s melanops the apparent morphological distinctiveness of s juruaense in the original description was due to small sample sizes and comparisons of adult specimens from brazil with the juvenal type material of s ucayalensis from peru the two species that we recognize s melanops and s ucayalensis can be separated by the short relatively wide rostrum and wide zygomatic arches of s melanops versus the longer relatively thinner rostrum and absolutely narrower zygomatic arches of s ucayalensis scolomys melanops ranges from the eastern slopes of the andes in ecuador to near iquitos in amazonian peru and we document the presence of s ucayalensis in western brazil northern peru and colombia
frequencies of plants with extrafloral nectaries were determined for two elevations in jamaica extrafloral nectaries were found on of the plants at sea level happy grove portland and of the plants at m whitfield hall st thomas ant abundance as indicated by discovery of and recruitment to baits was greater at the lower elevation site however despite the apparent absence of plants with extrafloral nectaries there were abundant ants m
seed dispersal fundamentally influences plant population and community dynamics but is difficult to quantify directly consequently models are frequently used to describe the seed shadow the seed deposition pattern of a plant population for vertebrate dispersed plants animal behavior is known to influence seed shadows but is poorly integrated in seed dispersal models here we illustrate a modeling approach that incorporates animal behavior and develop a stochastic spatially explicit simulation model that predicts the seed shadow for a primate dispersed tree species virola calophylla myristicaceae at the forest stand scale the model was parameterized from field collected data on fruit production and seed dispersal behaviors and movement patterns of the key disperser the spider monkey ateles paniscus densities of dispersed and non dispersed seeds and direct estimates of seed dispersal distances our model demonstrated that the spatial scale of dispersal for this v calophylla population was large as spider monkeys routinely dispersed seeds m a commonly used threshold for long distance dispersal the simulated seed shadow was heterogeneous with high spatial variance in seed density resulting largely from behaviors and movement patterns of spider monkeys that aggregated seeds dispersal at their sleeping sites and that scattered seeds dispersal during diurnal foraging and resting the single distribution dispersal kernels frequently used to model dispersal substantially underestimated this variance and poorly fit the simulated seed dispersal curve primarily because of its multimodality and a mixture distribution always fit the simulated dispersal curve better both seed shadow heterogeneity and dispersal curve multimodality arose directly from these different dispersal processes generated by spider monkeys compared to models that did not account for disperser behavior our modeling approach improved prediction of the seed shadow of this v calophylla population an important function of seed dispersal models is to use the seed shadows they predict to estimate components of plant demography particularly seedling population dynamics and distributions our model demonstrated that improved seed shadow prediction for animal dispersed plants can be accomplished by incorporating spatially explicit information on disperser behavior and movements using scales large enough to capture routine long distance dispersal and using dispersal kernels such as mixture distributions that account for spatially dispersal
the white striped free tailed bat tadarida australis is a common species of insectivorous bat found in mainland australia despite its abundance very little is known about its roosting ecology i used radiotelemetry collected during seasons to examine roost fidelity and roosting associations of a summer roosting group bats were trapped at a large communal roost in subtropical urban brisbane australia a total of radiotracking days and nights provided bat days of data with each bat being tracked for days pm sd seventeen new roost trees were found scattered throughout an area of km roost cavities were located inside eucalypt trees cm diameter at breast height tagged t australis switched roosts every pm days on average and spent most of their time in day roosts with smaller roosting groups despite being caught at the communal roost i quantified associations between pairs of tagged bats using a pairwise sharing index the consistent negative values of the pairwise sharing index i calculated indicated that members associated with roost mates less often than predicted by chance however bats typically associated at night at the communal roost even when they did not occupy it during the day for every day visit recorded per bat at the communal roost each bat visited the same roost twice at night on average this suggests that nocturnal movements of individuals should be included in assessments of associations between individuals i postulate that this species employs a fission fusion pattern based on individual movements to and from communal site i also argue that the roost network of communal roost and many satellite roosts may be regarded as a single unit
previous studies of forest dwelling bats have identified physical features of trees and forests that correlate with the presence of bats by comparing roost sites to paired randomly selected sites this method may be limited if the absence of bats from random sites cannot be confirmed our purpose was to address roosting ecology of female big brown bats eptesicus fuscus using a different approach we quantified relative use of trees with different types of cavity openings long crevices multiple holes or single holes and compared the relative use of these potential roosts to the availability of each roost type in the study area bats used trees with multiple holes and crevices significantly more often than expected based on their availability and trees with single holes less often than expected crevice roosts had significantly larger cavities than did single holes and roosting group size was positively correlated with cavity volume no relationship was found between cavity volume and tree height or stem diameter of roost trees variables that have been reported to correlate with roost selection in other studies of forest bats examination of our data suggests that the volume of roost cavities may be an important selection criterion for colonial forest living bats and that standard interpretations of the roost versus random tree approach may not accurately identify patterns of roost selection in systems
the reaction of seven grass species to variations in nitrate nitrogen concentration was studied in sand culture the species showed very marked differences in yield response lolium perenne and agrostis stolonifera showed the greatest response and yielded most at the highest level used ppm n agrostis tenuis showed a response similar to that of lolium perenne at low nitrogen levels but was significantly less responsive at high levels there was however a significant difference between the response of lead resistant and normal populations of agrostis tenuis the response of cynosurus cristatus and festuca ovina was significantly less than the above species at low nitrogen levels and both species were adversely affected by the highest level ppm n agrostis canina was very responsive to nitrogen at the lower levels but failed to respond to nitrogen above ppm n nardus stricta responded only weakly up to ppm n and showed a very strong depression of yield above this level the response of the various species to nitrogen in sand culture shows a considerable measure of agreement with determinations of nitrogen response made by other investigators in fertilizer trials and in studies of the correlation between species distribution and soil nitrogen levels in permanent pastures it is concluded that variation in soil nitrogen levels is probably an important factor determining the distribution of plant species under natural conditions the possible ecological significance of differences in overall yield between species is discussed in relation to levels of fertility
we describe trans abyss a de novo short read transcriptome assembly and analysis pipeline that addresses variation in local read densities by assembling read substrings with varying stringencies and then merging the resulting contigs before analysis analyzing gigabases of base pair paired end illumina reads from an adult mouse liver poly a rna library we identified known new and alternative structures in expressed transcripts and achieved high sensitivity and specificity relative to reference based methods
social influence drives both offline and online human behavior it pervades cultural markets and manifests itself in the adoption of scientific and technical innovations as well as the spread of social practices prior empirical work on the diffusion of innovations in spatial regions or social networks has largely focused on the spread of one particular technology among a subset of all potential adopters here we choose an online context that allows us to study social influence processes by tracking the popularity of a complete set of applications installed by the user population of a social networking site thus capturing the behavior of all individuals who can influence each other in this context by extending standard fluctuation scaling methods we analyze the collective behavior induced by million application installations and show that two distinct regimes of behavior emerge in the system once applications cross a particular threshold of popularity social influence processes induce highly correlated adoption behavior among the users which propels some of the applications to extraordinary levels of popularity below this threshold the collective effect of social influence appears to vanish almost entirely in a manner that has not been observed in the offline world our results demonstrate that even when external signals are absent social influence can spontaneously assume an onoff nature in a digital environment it remains to be seen whether a similar outcome could be observed in the offline world if equivalent experimental conditions could replicated
epigenetics is one of the most rapidly expanding fields in biology the recent characterization of a human dna methylome at single nucleotide resolution the discovery of the cpg island shores the finding of new histone variants and modifications and the unveiling of genome wide nucleosome positioning maps highlight the accelerating speed of discovery over the past two years increasing interest in epigenetics has been accompanied by technological breakthroughs that now make it possible to undertake large scale epigenomic studies these allow the mapping of epigenetic marks such as dna methylation histone modifications and nucleosome positioning which are critical for regulating gene and noncoding rna expression in turn we are learning how aberrant placement of these epigenetic marks and mutations in the epigenetic machinery is involved in disease thus a comprehensive understanding of epigenetic mechanisms their interactions and alterations in health and disease has become a priority in research
freely provided working code whatever its quality improves programming and enables others to engage with your research says nick barnes i am a professional software engineer and i want to share a trade secret with scientists most professional computer software isn t very good the code inside your laptop television phone or car is often badly documented inconsistent and tested
n hackers leaked thousands of e mails from the climatic research unit cru at the university of east anglia in norwich uk last year global warming sceptics pored over the documents for signs that researchers had manipulated data no such evidence emerged but the e mails did reveal another problem one described by a cru employee named harry who often wrote of his wrestling matches with wonky computer software yup my awful programming strikes again harry lamented in one of his notes as he attempted to correct a code analysing weather station data from mexico bringing industrial software development practices into the lab cannot come too soon says wilson the cru e mail affair was a warning to scientists to get their houses in order he says to all scientists out there ask yourselves what you would do if tomorrow some republican senator trains the spotlight on you and decides to turn you into a political football could your code stand up attack
background datasets generated on deep sequencing platforms have been deposited in various public repositories such as the gene expression omnibus geo sequence read archive sra hosted by the ncbi or the dna data bank of japan ddbj despite being rich data sources they have not been used much due to the difficulty in locating and analyzing datasets of interest results geoseq http geoseq mssm edu provides a new method of analyzing short reads from deep sequencing experiments instead of mapping the reads to reference genomes or sequences geoseq maps a reference sequence against the sequencing data it is web based and holds pre computed data from public libraries the analysis reduces the input sequence to tiles and measures the coverage of each tile in a sequence library through the use of suffix arrays the user can upload custom target sequences or use gene mirna names for the search and get back results as plots and spreadsheet files geoseq organizes the public sequencing data using a controlled vocabulary allowing identification of relevant libraries by organism tissue and type of experiment conclusions analysis of small sets of sequences against deep sequencing datasets as well as identification of public datasets of interest is simplified by geoseq we applied geoseq to a identify differential isoform expression in mrna seq datasets b identify mirnas micrornas in libraries and identify mature and star sequences in mirnas and c to identify potentially mis annotated mirnas the ease of using geoseq for these analyses suggests its utility and uniqueness as an tool
background high throughput sequencing hts technologies play important roles in the life sciences by allowing the rapid parallel sequencing of very large numbers of relatively short nucleotide sequences in applications ranging from genome sequencing and resequencing to digital microarrays and chip seq experiments as experiments scale up hts technologies create new bioinformatics challenges for the storage and sharing of hts data results we develop data structures and compression algorithms for hts data a processing stage maps short sequences to a reference genome or a large table of sequences then the integers representing the short sequence absolute or relative addresses their length and the substitutions they may contain are compressed and stored using various entropy coding algorithms including both old and new fixed codes e g golomb elias gamma mov and variable codes e g huffman the general methodology is illustrated and applied to several hts data sets results show that the information contained in hts files can be compressed by a factor of or more depending on the statistical properties of the data sets and various other choices and constraints our algorithms fair well against general purpose compression programs such as gzip and timing results show that our algorithms are consistently faster than the best general purpose compression programs conclusions it is not likely that exactly one encoding strategy will be optimal for all types of hts data different experimental conditions are going to generate various data distributions whereby one encoding strategy can be more effective than another we have implemented some of our encoding algorithms into the software package gencompress which is available upon request from the authors with the advent of hts technology and increasingly new experimental protocols for using the technology sequence databases are expected to continue rising in size the methodology we have proposed is general and these advanced compression techniques should allow researchers to manage and share their hts data in a more fashion
molecular dynamics md simulations are widely used to study protein motions at an atomic level of detail but they have been limited to time scales shorter than those of many biologically critical conformational changes we examined two fundamental processes in protein dynamicsprotein folding and conformational change within the folded stateby means of extremely long all atom md simulations conducted on a special purpose machine equilibrium simulations of a ww protein domain captured multiple folding and unfolding events that consistently follow a well defined folding pathway separate simulations of the proteins constituent substructures shed light on possible determinants of this pathway a millisecond simulation of the folded protein bpti reveals a small number of structurally distinct conformational states whose reversible interconversion is slower than local relaxations within those states by a factor of than
behavioral economics tells us that emotions can profoundly affect individual behavior and decision making does this also apply to societies at large i e can societies experience mood states that affect their collective decision making by extension is the public mood correlated or even predictive of economic indicators here we investigate whether measurements of collective mood states derived from large scale twitter feeds are correlated to the value of the dow jones industrial average djia over time we analyze the text content of daily twitter feeds by two mood tracking tools namely opinionfinder that measures positive vs negative mood and google profile of mood states gpoms that measures mood in terms of dimensions calm alert sure vital kind and happy we cross validate the resulting mood time series by comparing their ability to detect the public s response to the presidential election and thanksgiving day in a granger causality analysis and a self organizing fuzzy neural network are then used to investigate the hypothesis that public mood states as measured by the opinionfinder and gpoms mood time series are predictive of changes in djia closing values our results indicate that the accuracy of djia predictions can be significantly improved by the inclusion of specific public mood dimensions but not others we find an accuracy of in predicting the daily up and down changes in the closing values of the djia and a reduction of the mean average percentage error by than
background roche pyrosequencing has become a method of choice for generating transcriptome data from non model organisms once the tens to hundreds of thousands of short base reads have been produced it is important to correctly assemble these to estimate the sequence of all the transcripts most transcriptome assembly projects use only one program for assembling pyrosequencing reads but there is no evidence that the programs used to date are optimal we have carried out a systematic comparison of five assemblers mira newbler seqman and clc to establish best practices for transcriptome assemblies using a new dataset from the parasitic nematode litomosoides sigmodontis results although no single assembler performed best on all our criteria newbler gave longer contigs better alignments to some reference sequences and was fast and easy to use seqman assemblies performed best on the criterion of recapitulating known transcripts and had more novel sequence than the other assemblers but generated an excess of small redundant contigs the remaining assemblers all performed almost as well with the exception of newbler the version currently used by most assembly projects which generated assemblies that had significantly lower total length as different assemblers use different underlying algorithms to generate contigs we also explored merging of assemblies and found that the merged datasets not only aligned better to reference sequences than individual assemblies but were also more consistent in the number and size of contigs conclusions transcriptome assemblies are smaller than genome assemblies and thus should be more computationally tractable but are often harder because individual contigs can have highly variable read coverage comparing single assemblers newbler performed best on our trial data set but other assemblers were closely comparable combining differently optimal assemblies from different programs however gave a more credible final product and this strategy recommended
the limitations of genome wide association gwa studies that focus on the phenotypic influence of common genetic variants have motivated human geneticists to consider the contribution of rare variants to phenotypic expression the increasing availability of high throughput sequencing technologies has enabled studies of rare variants but these methods will not be sufficient for their success as appropriate analytical methods are also needed we consider data analysis approaches to testing associations between a phenotype and collections of rare variants in a defined genomic region or set of regions ultimately although a wide variety of analytical approaches exist more work is needed to refine them and determine their properties and power in contexts
those who seek answers to big broad questions about biology especially questions emphasizing the organism taxonomy evolution and ecology will soon benefit from an emerging names based infrastructure it will draw on the almost universal association of organism names with biological information to index and interconnect information distributed across the internet the result will be a virtual data commons expanding as further data are shared allowing biology to become more of a big science informatics devices will exploit this big new biology revitalizing comparative biology with a broad perspective to reveal previously inaccessible trends and discontinuities so helping us to reveal unfamiliar biological truths here we review the first components of this freely available participatory and semantic global architecture
a large bulk flow which is in tension with the lambda cold dark matter cosmological model has been observed cite in this letter we provide a physical explanation for this very large bulk flow based on the assumption that the cosmic microwave background cmb rest frame does not coincide with the matter rest frame resulting in a tilted universe we propose a model that takes into account the relative velocity of cmb frame with respect to to the matter rest frame hereafter tilted velocity and use type ia supernovae sn enear sfi smac and composite galaxy catalogues to constrain this tilted velocity we find that the magnitude of the tilted velocity u is around km s and its direction is close to what is found by cite for sn smac and composite catalogues u is excluded at the two to three sigma level the constraints on the magnitude of the tilted velocity can result in the constraints on the duration of inflation due to the fact that inflation can neither be too long no dipole effect nor too short very large dipole effect under certain assumptions the constraints on the tilted velocity requires that inflation lasts at least e folds longer than that required to solve the horizon problem this opens a new window for testing inflation and the models of the early universe from observations of large structure
many dna variants have been identified on more than diseases and traits using genome wide association studies gwass some have been validated using deep sequencing but many fewer have been validated functionally primarily focused on non synonymous coding snps nssnps it is an open question whether synonymous coding snps ssnps and other non coding snps can lead to as high odds ratios as nssnps we conducted a broad survey across disease snp associations curated from publications studying human genetic association and found that nssnps and ssnps shared similar likelihood and effect size for disease association the enrichment of disease associated snps around the th base in the first introns might provide an effective way to prioritize intronic snps for functional studies we further found that the likelihood of disease association was positively associated with the effect size across different types of snps and snps in the untranslated regions such as the microrna binding sites might be under investigated our results suggest that ssnps are just as likely to be involved in disease mechanisms so we recommend that ssnps discovered from gwas should also be examined with studies
transparency in reporting of conflict of interest is an increasingly important aspect of publication in medical journals publication of large industry supported trials may generate many citations and journal income through reprint sales and thereby be a source of conflicts of interest for journals we investigated industry supported trials influence on journal impact factors revenue
the genomes project aims to provide a deep characterization of human genome sequence variation as a foundation for investigating the relationship between genotype and phenotype here we present results of the pilot phase of the project designed to develop and compare different strategies for genome wide sequencing with high throughput platforms we undertook three projects low coverage whole genome sequencing of individuals from four populations high coverage sequencing of two mother father child trios and exon targeted sequencing of individuals from seven populations we describe the location allele frequency and local haplotype structure of approximately million single nucleotide polymorphisms million short insertions and deletions and structural variants most of which were previously undescribed we show that because we have catalogued the vast majority of common variation over of the currently accessible variants found in any individual are present in this data set on average each person is found to carry approximately to loss of function variants in annotated genes and to variants previously implicated in inherited disorders we demonstrate how these results can be used to inform association and functional studies from the two trios we directly estimate the rate of de novo germline base substitution mutations to be approximately per base pair per generation we explore the data with regard to signatures of natural selection and identify a marked reduction of genetic variation in the neighbourhood of genes due to selection at linked sites these methods and public data will support the next phase of human research
pancreatic cancer is an aggressive malignancy with a five year mortality of usually due to widespread metastatic disease previous studies indicate that this disease has a complex genomic landscape with frequent copy number changes and point but genomic rearrangements have not been characterized in detail despite the clinical importance of metastasis there remain fundamental questions about the clonal structures of metastatic including phylogenetic relationships among metastases the scale of ongoing parallel evolution in metastatic and primary and how the tumour disseminates here we harness advances in dna to annotate genomic rearrangements in patients with pancreatic cancer and explore clonal relationships among metastases we find that pancreatic cancer acquires rearrangements indicative of telomere dysfunction and abnormal cell cycle control namely dysregulated to s phase transition with intact checkpoint these initiate amplification of cancer genes and occur predominantly in early cancer development rather than the later stages of the disease genomic instability frequently persists after cancer dissemination resulting in ongoing parallel and even convergent evolution among different metastases we find evidence that there is genetic heterogeneity among metastasis initiating cells that seeding metastasis may require driver mutations beyond those required for primary tumours and that phylogenetic trees across metastases show organ specific branches these data attest to the richness of genetic variation in cancer brought about by the tandem forces of genomic instability and selection
metastasis the dissemination and growth of neoplastic cells in an organ distinct from that in which they is the most common cause of death in cancer patients this is particularly true for pancreatic cancers where most patients are diagnosed with metastatic disease and few show a sustained response to chemotherapy or radiation whether the dismal prognosis of patients with pancreatic cancer compared to patients with other types of cancer is a result of late diagnosis or early dissemination of disease to distant organs is not known here we rely on data generated by sequencing the genomes of seven pancreatic cancer metastases to evaluate the clonal relationships among primary and metastatic cancers we find that clonal populations that give rise to distant metastases are represented within the primary carcinoma but these clones are genetically evolved from the original parental non metastatic clone thus genetic heterogeneity of metastases reflects that within the primary carcinoma a quantitative analysis of the timing of the genetic evolution of pancreatic cancer was performed indicating at least a decade between the occurrence of the initiating mutation and the birth of the parental non metastatic founder cell at least five more years are required for the acquisition of metastatic ability and patients die an average of two years thereafter these data provide novel insights into the genetic features underlying pancreatic cancer progression and define a broad time window of opportunity for early detection to prevent deaths from disease
we report the analysis of a japanese male using high throughput sequencing to coverage more than of the sequence reads were mapped to the reference human genome using a bayesian decision method we identified single nucleotide variations snvs comparison with six previously reported genomes revealed an excess of singleton nonsense and nonsynonymous snvs as well as singleton snvs in conserved non coding regions we also identified deletions smaller than kb with high accuracy in addition to copy number variations and rearrangements de novo assembly of the unmapped sequence reads generated around mb of novel sequence which showed high similarity to non reference human genomes and the human herpesvirus genome our analysis suggests that considerable variation remains undiscovered in the human genome and that whole genome sequencing is an invaluable tool for obtaining a complete understanding of human variation
copy number variants affect both disease and normal phenotypic variation but those lying within heavily duplicated highly identical sequence have been difficult to assay by analyzing short read mapping depth for human genomes we demonstrated accurate estimation of absolute copy number for duplications as small as kilobase pairs ranging from to copies we identified million singly unique nucleotide positions informative in distinguishing specific copies and used them to genotype the copy and content of specific paralogs within highly duplicated gene families these data identify human specific expansions in genes associated with brain development reveal extensive population genetic diversity and detect signatures consistent with gene conversion in the human species our approach makes genes accessible to genetic studies of association
small insertions and deletions indels are a common and functionally important type of sequence polymorphism most of the focus of studies of sequence variation is on single nucleotide variants snvs and large structural variants in principle high throughput sequencing studies should allow identification of indels just as snvs however inference of indels from next generation sequence data is challenging and so far methods for identifying indels lag behind methods for calling snvs in terms of sensitivity and specificity we propose a bayesian method to call indels from short read sequence data in individuals and populations by realigning reads to candidate haplotypes that represent alternative sequence to the reference the candidate haplotypes are formed by combining candidate indels and snvs identified by the read mapper while allowing for known sequence variants or candidates from other methods to be included in our probabilistic realignment model we account for base calling errors mapping errors and also importantly for increased sequencing error indel rates in long homopolymer runs we show that our method is sensitive and achieves low false discovery rates on simulated and real data sets although challenges remain the algorithm is implemented in the program dindel which has been used in the genomes project sets
observation of even a single massive cluster especially at high redshift can falsify the standard cosmological framework consisting of a cosmological constant and cold dark matter lcdm with gaussian initial conditions by exposing an inconsistency between the well measured expansion history and the growth of structure it predicts through a likelihood analysis of current cosmological data that constrain the expansion history we show that the lcdm upper limits on the expected number of massive distant clusters are nearly identical to limits predicted by all quintessence models where dark energy is a minimally coupled scalar field with a canonical kinetic term we provide convenient fitting formulas for the confidence level at which the observation of a cluster of mass m at redshift z can falsify lcdm and quintessence given cosmological parameter uncertainties and sample variance as well as for the expected number of such clusters in the light cone and the eddington bias factor that must be applied to observed masses by our conservative confidence criteria which equivalently require masses times larger than typically expected in surveys of a few hundred square degrees none of the presently known clusters falsify these models various systematic errors including uncertainties in the form of the mass function and differences between supernova light curve fitters typically shift the exclusion curves by less than in mass making current statistical and systematic uncertainties in cluster mass determination the most critical factor in assessing falsification of lcdm quintessence
background in biological and medical domain the use of web services made the data and computation functionality accessible in a unified manner which helped automate the data pipeline that was previously performed manually workflow technology is widely used in the orchestration of multiple services to facilitate in silico research cancer biomedical informatics grid cabig is an information network enabling the sharing of cancer research related resources and cagrid is its underlying service based computation infrastructure cabig requires that services are composed and orchestrated in a given sequence to realize data pipelines which are often called scientific workflows results cagrid selected taverna as its workflow execution system of choice due to its integration with web service technology and support for a wide range of web services plug in architecture to cater for easy integration of third party extensions etc the cagrid workflow toolkit or the toolkit for short an extension to the taverna workflow system is designed and implemented to ease building and running cagrid workflows it provides users with support for various phases in using workflows service discovery composition and orchestration data access and secure service invocation which have been identified by the cagrid community as challenging in a multi institutional and cross discipline domain conclusions by extending the taverna workbench cagrid workflow toolkit provided a comprehensive solution to compose and coordinate services in cagrid which would otherwise remain isolated and disconnected from each other using it users can access more than services and are offered with a rich set of features including discovery of data and analytical services query and transfer of data security protections for service invocations state management in service interactions and sharing of workflows experiences and best practices the proposed solution is general enough to be applicable and reusable within other service computing infrastructures that leverage similar stack
it is currently believed that the atlas of existing protein structures is faithfully represented in the protein data bank however whether this atlas covers the full universe of all possible protein structures is still a highly debated issue by using a sophisticated numerical approach we performed an exhaustive exploration of the conformational space of a amino acid polypeptide chain described with an accurate all atom interaction potential we generated a database of around compact folds with at least of secondary structure corresponding to local minima of the potential energy this ensemble plausibly represents the universe of protein folds of similar length indeed all the known folds are represented in the set with good accuracy however we discover that the known folds form a rather small subset which cannot be reproduced by choosing random structures in the database rather natural and possible folds differ by the contact order on average significantly smaller in the former this suggests the presence of an evolutionary bias possibly related to kinetic accessibility towards structures with shorter loops between contacting residues beside their conceptual relevance the new structures open a range of practical applications such as the development of accurate structure prediction strategies the optimization of force fields and the identification and design of folds
classical approaches to determine structures of noncoding rna ncrna probed only one rna at a time with enzymes and chemicals using gel electrophoresis to identify reactive positions to accelerate rna structure inference we developed fragmentation sequencing fragseq a high throughput rna structure probing method that uses high throughput rna sequencing of fragments generated by digestion with nuclease which specifically cleaves single stranded nucleic acids in experiments probing the entire mouse nuclear transcriptome we accurately and simultaneously mapped single stranded rna regions in multiple ncrnas with known structure we probed in two cell types to verify reproducibility we also identified and experimentally validated structured regions in ncrnas with to our knowledge no previously reported data
in honey bees apis mellifera the behaviorally and reproductively distinct queen and worker female castes derive from the same genome as a result of differential intake of royal jelly and are implemented in concert with dna methylation to determine if these very different diet controlled phenotypes correlate with unique brain methylomes we conducted a study to determine the methyl cytosine mc distribution in the brains of queens and workers at single base pair resolution using shotgun bisulfite sequencing technology the whole genome sequencing was validated by deep sequencing of selected amplicons representing eight methylated genes we found that nearly all mcs are located in cpg dinucleotides in the exons of genes showing greater sequence conservation than non methylated genes over genes show significant methylation differences between queens and workers revealing the intricate dynamics of methylation patterns the distinctiveness of the differentially methylated genes is underscored by their intermediate cpg densities relative to drastically cpg depleted methylated genes and to cpg richer non methylated genes we find a strong correlation between methylation patterns and splicing sites including those that have the potential to generate alternative exons we validate our genome wide analyses by a detailed examination of two transcript variants encoded by one of the differentially methylated genes the link between methylation and splicing is further supported by the differential methylation of genes belonging to the histone gene family we propose that modulation of alternative splicing is one mechanism by which dna methylation could be linked to gene regulation in the honey bee our study describes a level of molecular diversity previously unknown in honey bees that might be important for generating phenotypic flexibility not only during development but also in the adult post brain
comparison of a full collection of the transposable element te sequences of vertebrates with genome sequences shows that the human genome makes perfect full length matches the cause is that the human genome contains many active tes that have caused te inserts in relatively recent times these te inserts in the human genome are several types of young alus etc work in many laboratories has shown that such inserts have many effects including changes in gene expression increases in recombination and unequal crossover the time of these very effective changes in the human lineage genome extends back about million years according to these data and very likely much earlier rapid human lineage specific evolution including brain size is known to have also occurred in the last few million years alu insertions likely underlie rapid human lineage evolution they are known to have many effects examples are listed in which te sequences have influenced human specific genes the proposed model is that the many te insertions created many potentially effective changes and those selected were responsible for a part of the striking human lineage evolution the combination of the results of these events that were selected during human lineage evolution was apparently effective in producing a successful and rapidly species
in le szilrd invented a feedback in which a hypothetical intelligencedubbed maxwells demonpumps heat from an isothermal environment and transforms it into work after a long lasting and intense controversy it was finally clarified that the demons role does not contradict the second law of thermodynamics implying that we can in principle convert information to free an experimental demonstration of this information to energy conversion however has been elusive here we demonstrate that a non equilibrium feedback manipulation of a brownian particle on the basis of information about its location achieves a szilrd type information to energy conversion using real time feedback control the particle is made to climb up a spiral staircase like potential exerted by an electric field and gains free energy larger than the amount of work done on it this enables us to verify the generalized jarzynski and suggests a new fundamental principle of an information to heat engine that converts information into energy by control
the characterization of functional redundancy and divergence between duplicate genes is an important step in understanding the evolution of genetic systems large scale genetic network analysis in saccharomyces cerevisiae provides a powerful perspective for addressing these questions through quantitative measurements of genetic interactions between pairs of duplicated genes and more generally through the study of genome wide genetic interaction profiles associated with duplicated genes we show that duplicate genes exhibit fewer genetic interactions than other genes because they tend to buffer one another functionally whereas observed interactions are non overlapping and reflect their divergent roles we also show that duplicate gene pairs are highly imbalanced in their number of genetic interactions with other genes a pattern that appears to result from asymmetric evolution such that one duplicate evolves or degrades faster than the other and often becomes functionally or conditionally specialized the differences in genetic interactions are predictive of differences in several other evolutionary and physiological properties of pairs
conformal cyclic cosmology ccc posits the existence of an aeon preceding our big bang b whose conformal infinity i is identified conformally with b now regarded as a spacelike surface black hole encounters within bound galactic clusters in that previous aeon would have the observable effect in our cmb sky of families of concentric circles over which the temperature variance is anomalously low the centre of each such family representing the point of i at which the cluster converges these centres appear as fairly randomly distributed fixed points in our cmb sky the analysis of wilkinson microwave background probe s wmap cosmic microwave background year maps does indeed reveal such concentric circles of up to sigma significance this is confirmed when the same analysis is applied to data eliminating the possibility of an instrumental cause for the effects these observational predictions of ccc would not be easily explained within standard cosmology
genome wide association gwa studies have typically focused on the analysis of single markers which often lacks the power to uncover the relatively small effect sizes conferred by most genetic variants recently pathway based approaches have been developed which use prior biological knowledge on gene function to facilitate more powerful analysis of gwa study data sets these approaches typically examine whether a group of related genes in the same functional pathway are jointly associated with a trait of interest here we review the development of pathway based approaches for gwa studies discuss their practical use and caveats and suggest that pathway based approaches may also be useful for future gwa studies with data
evolution is the fundamental physical process that gives rise to biological phenomena yet it is widely treated as a subset of population genetics and thus its scope is artificially limited as a result the key issues of how rapidly evolution occurs and its coupling to ecology have not been satisfactorily addressed and formulated the lack of widespread appreciation for and understanding of the evolutionary process has arguably retarded the development of biology as a science with disastrous consequences for its applications to medicine ecology and the global environment this review focuses on evolution as a problem in non equilibrium statistical mechanics where the key dynamical modes are collective as evidenced by the plethora of mobile genetic elements whose role in shaping evolution has been revealed by modern genomic surveys we discuss how condensed matter physics concepts might provide a useful perspective in evolutionary biology the conceptual failings of the modern evolutionary synthesis the open ended growth of complexity and the quintessentially self referential nature of dynamics
high throughput sequencing technologies promise to transform the fields of genetics and comparative biology by delivering tens of thousands of genomes in the near future although it is feasible to construct de novo genome assemblies in a few months there has been relatively little attention to what is lost by sole application of short sequence reads we compared the recent de novo assemblies using the short oligonucleotide analysis package soap generated from the genomes of a han chinese individual and a yoruban individual to experimentally validated genomic features we found that de novo assemblies were shorter than the reference genome and that megabase pairs of common repeats and of validated duplicated sequences were missing from the genome consequently over coding exons were completely missing we conclude that high quality sequencing approaches must be considered in conjunction with high throughput sequencing for comparative genomics analyses and studies of evolution
accurate functional annotation of regulatory elements is essential for understanding global gene regulation here we report a genome wide map of transcription factor binding sites in human lymphoblastoid cell lines which is comprised of sites corresponding to position weight matrices of known transcription factor binding motifs and novel sequence motifs to generate this map we developed a probabilistic framework that integrates cell or tissue specific experimental data such as histone modifications and dnasei cleavage patterns with genomic information such as gene annotation and evolutionary conservation comparison to empirical chip seq data suggests that our method is highly accurate yet has the advantage of targeting many factors in a single assay we anticipate that this approach will be a valuable tool for genome wide studies of gene regulation in a wide variety of cell types or tissues under conditions
abstract a recent article in bmc bioinformatics describes new advances in workflow systems for computational modeling in systems biology such systems can accelerate and improve the consistency of modeling through automation not only at the simulation and results production stages but also at the model generation stage their work is a harbinger of the next generation of more powerful software for systems biologists see research article http www biomedcentral com abstract ever since the rise of systems biology at the end of the last century mathematical representations of biological systems and their activities have flourished they are being used to describe everything from biomolecular networks such as gene regulation metabolic processes and signaling pathways at the lowest biological scales to tissue growth and differentiation drug effects environmental interactions and more a very active area in the field has been the development of techniques that facilitate the construction analysis and dissemination of computational models the heterogeneous distributed nature of most data resources today has increased not only the opportunities for but also the difficulties of developing software systems to support these tasks the work by li et al published in bmc bioinformatics represents a promising evolutionary step forward in this area they describe a workflow system a visual software environment enabling a user to create a connected set of operations to be performed sequentially using seperate tools and resources their system uses third party data resources accessible over the internet to elaborate and parametrize that is assign parameter values to computational models in a semi automated manner in li et al s work the authors point towards a promising future for computational modeling and simultaneously highlight some of the difficulties that need to be overcome before we there
life is mostly composed of the elements carbon hydrogen nitrogen oxygen sulfur and phosphorus although these six elements make up nucleic acids proteins and lipids and thus the bulk of living matter it is theoretically possible that some other elements in the periodic table could serve the same functions here we describe a bacterium strain gfaj of the halomonadaceae isolated from mono lake california which substitutes arsenic for phosphorus to sustain its growth our data show evidence for arsenate in macromolecules that normally contain phosphate most notably nucleic acids and proteins exchange of one of the major bioelements may have profound evolutionary and significance
